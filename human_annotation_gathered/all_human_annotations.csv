review_point,paper_id,venue,focused_review,actionability,actionability_label,actionability_label_type,batch,grounding_specificity,grounding_specificity_label,grounding_specificity_label_type,verifiability,verifiability_label,verifiability_label_type,helpfulness,helpfulness_label,helpfulness_label_type,professional_tone,professional_tone_label,professional_tone_label_type,valid_point,valid_point_label,valid_point_label_type,addressed_to_author,addressed_to_author_label,addressed_to_author_label_type
"- There were too many missing details (for example, what is the distribution of people with ‘free off speech’ attitudes? What is the correlation of the chosen scale item in the breadth-of-posts study?). On a minor note, many important points are relegated to the appendix.",ARR_2022_60_review,ARR_2022,"- Underdefined and conflation of concepts - Several important details missing - Lack of clarity in how datasets were curated prevents one from assessing their validity - Too many results which are not fully justified or explained
This is a very important, interesting, and valuable paper with many positives. First and foremost, annotators’ backgrounds are an important factor and should be taken into consideration when designing datasets for hate speech, toxicity, or related phenomena. The paper not only accounts for demographic variables as done in previous work but other attitudinal covariates like attitude towards free speech that are well-chosen. The paper presents two well-thought out experiments and presents results in a clear manner which contain several important findings.
It is precisely because of the great potential and impact of this paper, I think the current manuscript requires more consideration and fine-tuning before it can reach its final stage. At this point, there seems to be a lack of important details that prevent me from fully gauging the paper’s findings and claims. Generally: - There were too many missing details (for example, what is the distribution of people with ‘free off speech’ attitudes? What is the correlation of the chosen scale item in the breadth-of-posts study?). On a minor note, many important points are relegated to the appendix.
- Certain researcher choices and experiment design choices were not justified (for example, why were these particular scales used?)
- The explanation of the creation of the breadth-of-posts was confusing. How accurate was the classification of AAE dialect and vulgarity? - The toxicity experiment was intriguing but there was too little space to be meaningful.
More concretely, - With regard to terminology and concepts, toxicity and hate speech may be related but are not the same thing. The instructions to the annotators seem to conflate both. The paper also doesn’t present a concrete definition of either. While it might seem redundant or trivial, the wording to annotators plays an important role and can confound the results presented here.
- Why were the particular scales chosen for obtaining attitudes? Particularly, for empathy there are several scale items [1], so why choose the Interpersonal Reactivity Index?
- What was the distribution of the annotator’s background with respect to the attitudes? For example, if there are too few ‘free of speech’ annotators, then the results shown in Table 3, 4, etc are underpowered. - What were the correlations of the chosen attitudinal scale item for the breadth-of-posts study with the toxicity in the breadth-of-workers study?
- How accurate are the automated classification in the breadth-of-posts experiment, i.e., how well does the states technique differentiate identity vs non-identity vulgarity or AAE language for that particular dataset. Particularly, how can it be ascertained whether the n-word was used as a reclaimed slur or not? - In that line, Section 6 discusses perceptions of vulgarity, but there are too many confounds here. Using b*tch in a sentence can be an indication of vulgarity and toxicity (due to sexism).
- In my opinion, the perspective API experiment was interesting but rather shallow. My suggestion would be to follow up on it in more detail in a new paper rather than include it in this one. The newly created space could be used to enter the missing details mentioned in the review. - Finally, given that the paper notes that MTurk tends to be predominantly liberal and the authors (commendably) took several steps to ensure greater participation from conservatives, I was wondering if ‘typical’ hate speech datasets are annotated by more homogenous annotators compared to the sample in this paper. What could be the implications of this? Do this paper's findings then hold for existing hate speech datasets?
Besides these, I also note some ethical issues in the ‘Ethical Concerns’ section. To conclude, while my rating might seem quite harsh, I believe this work has great potential and I hope to see it enriched with the required experimental details.
References: [1] Gerdes, Karen E., Cynthia A. Lietz, and Elizabeth A. Segal. "" Measuring empathy in the 21st century: Development of an empathy index rooted in social cognitive neuroscience and social justice."" Social Work Research 35, no. 2 (2011): 83-93.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '5', '4']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '1', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '1', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.",ACL_2017_333_review,ACL_2017,"There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained. - General Discussion: - Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.
- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both ""represent the meaning"". Are both indeed necessary? Did you trying using only one of them.
- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.
- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?
- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.
- page 1, lines 93-96: please provide a reference for this passage: ""This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.""
- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.
- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.
- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: ""In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence""; ""Some previous works apply this framework to summarization generation tasks.""
- Figure 2: What is ""MLP""? It seems not to be described in the paper.
- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.
- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).
- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).
- Table 2: what does ""#(ref)"" mean?
- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.
- Page 5, line 450: remove ""the"" word in this line? "" SGD as our optimizing algorithms"" instead of ""SGD as our the optimizing algorithms.""
- Page 5, beam search: please include a reference for beam search.
- Figure 4: Is there a typo in the true sentence? "" council of europe again slams french prison conditions"" (again or against?)
- typo ""supper script"" -> ""superscript"" (4 times)","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.",ARR_2022_269_review,ARR_2022,"- It is not clear for me about the novelty of the proposed methods. - The proposed method relies on the quality of translation systems. - I'm not sure whether the differences of some results are significant (see Table 1). - The differences in results in Table 2 are very small that make the interpretation of results rather difficult. Furthermore, it is then unclear which proposed methods are really effective.
- Did the authors run their experiments several times with different random initializations?","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '1']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '1', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', 'X', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '1', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['DVRTnFRi', 'boda'], 'labels': ['1', '0']}",,,"{'annotators': [], 'labels': []}",,
"- While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.",ARR_2022_331_review,ARR_2022,"- While the language has been improved, there are still some awkward phrases. I suggest the authors have the paper reviewed by a native English speaker.
1) Line 29: ""To support the GEC study..."". Your study or GEC in general? Maybe you mean ""To support GEC research/development/solutions""?
2) Line 53: ""Because, obviously, there are usually multiple acceptable references with close meanings for an incorrect sentence, as illustrated by the example in Table 1."" This is not a well-formed sentence. Rewrite or attach to the previous one.
3) Line 59: Choose either ""the model will be unfairly *penalised*"" or ""*performance* will be unfairly underestimated"".
4) Line 83: ""... for detailed illustration"". ???
5) Line 189: ""To facilitate illustration, our guidelines adopt a two-tier hierarchical error taxonomy..."" You said earlier that you adopted the ""direct re-rewriting"" approach so why does your annotation guidelines provide a taxonomy or errors? Is it just to make sure that all ocurrences of the same types of errors are handled equally by all annotators? Weren't they free to correct the sentences in any way they wanted as you stated in lines 178-180?
6) Line 264: ""We attribute this to our strict control of the over-correction phenomenon."" What do you mean exactly? The original annotation considered some sentences to be erroneous while your guidelines did not?
7) Line 310: ""Since it is usually ..."" This is not a well-formed sentence. Rewrite or attach to the previous one.
8) Line 399: ""... and only use the erroneous part for training"" Do you mean you discard correct sentences? As it stands, it sounds as if you only kept the incorrect sentences without their corrections. You might want to make this clearer.
9) ""... which does not need error-coded annotation"". This is not exactly so. ERRANT computes P, R and F from M2 files containing span-level annotations. For English, it is able to automatically generate these annotations from parallel text using an alignment and edit extraction algorithm. In the case of Chinese, you did this yourself. So while it is not necessary to manually annotate the error spans, you do need to extract them somehow before ERRANT can compute the measures.
10) Table 5: ""For calculating the human performance, each submitted result is considered as a sample if an annotator submits multiple results."" I am afraid this does not clearly explain how human performance was computed. Each annotator against the rest? Averaged across all of them? How are multiple corrections from a single annotator handled? If you compared each annotation to the rest but the systems were compared to all the annotations, then I believe human evaluation is an underestimation. This is still not clear.
11) Line 514: ""The word-order errors can be identified by heuristic rules following Hinson et al. (2020)."" Did you classify the errors in the M2 files before feeding them into ERRANT?
12) Line 544: ""... we remove all extra references if a sentence has more than 2 gold-standard references"". Do you remove them randomly or sequentially?
13) TYPOS/ERRORS: ""them"" -> ""this approach""? ( line 72), ""both formal/informal"" -> ""both formal and informal"" (line 81), ""supplement"" -> ""supply""? ( lines 89, 867), ""from total"" -> ""from *the* total"" (line 127), ""Finally, we have obtained 7,137 sentences"" -> ""In the end, we obtained 7,137 sentences"" (line 138), ""suffers from"" -> ""poses"" (line 155), ""illustration"" -> ""annotation""? ( line 189), ""Golden"" -> ""Gold"" (lines 212, 220, 221, 317), ""sentence numbers"" -> ""number of sentences"" (Table 3 caption), ""numbers (proportion)"" -> ""number (proportion)"" (Table 3 caption), ""averaged character numbers"" -> ""average number of characters"" (Table 3 caption), ""averaged edit numbers"" -> ""average number of edits"" (Table 3 caption), ""averaged reference numbers"" -> ""average number of references"" (Table 3 caption), ""in the parenthesis of the..."" -> ""in parentheses in the..."" (Table 3 caption), ""previous"" -> ""original""? ( line 262), ""use"" (delete, line 270), ""in *the* re-annotated"" (line 271), ""twice of that"" -> ""twice that"" (line 273), ""edit number"" -> ""number of edits"" (line 281), ""the sentence length"" -> ""sentence length"" (line 282), ""numbers"" -> ""number"" (lines 283, 297), ""numbers"" -> ""the number"" (line 295), ""Same"" -> ""Identical"" (line 298), ""calculated"" -> ""counted"" (line 299), ""the different"" -> ""different"" (Figure 1 caption), ""reference number"" -> ""number of references"" (line 305), ""for"" -> ""to"" (line 307), ""the descending"" -> ""descending"" (line 326), ""sentence numbers"" -> ""number of sentences"" (line 327), ""It"" -> ""This"" (line 331), ""annotate"" -> ""annotated"" (Figure 2 caption), ""limitation"" -> ""limitations"" (line 343), ""SOTA"" -> ""state-of-the-art (SOTA)"" (line 353), ""these"" -> ""this"" (line 369), ""where"" -> ""on which"" (line 393), ""hugging face"" -> ""Hugging Face"" (431), ""these"" -> ""this"" (line 464), ""The"" -> ""A"" (line 466), ""reference number"" -> ""the number of references"" (Figure 3 caption), ""start"" -> ""have started"" (line 571), ""will be"" -> ""are"" (line 863), ""false"" -> ""incorrect""? ( line 865).","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '3']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1) The character tri-gram LSTM seems a little unmotivated. Did the authors try other character n-grams as well? As a reviewer, I can guess that character tri-grams roughly correspond to morphemes, especially in Semitic languages, but what made the authors report results for 3-grams as opposed to 2- or 4-? In addition, there are roughly 26^3=17576 possible distinct trigrams in the Latin lower-case alphabet, which is enough to almost constitute a word embedding table. Did the authors only consider observed trigrams? How many distinct observed trigrams were there?",ACL_2017_477_review,ACL_2017,"1) The character tri-gram LSTM seems a little unmotivated. Did the authors try other character n-grams as well? As a reviewer, I can guess that character tri-grams roughly correspond to morphemes, especially in Semitic languages, but what made the authors report results for 3-grams as opposed to 2- or 4-? In addition, there are roughly 26^3=17576 possible distinct trigrams in the Latin lower-case alphabet, which is enough to almost constitute a word embedding table. Did the authors only consider observed trigrams? How many distinct observed trigrams were there?
2) I don't think you can meaningfully claim to be examining the effectiveness of character-level models on root-and-pattern morphology if your dataset is unvocalised and thus doesn't have the 'pattern' bit of 'root-and-pattern'. I appreciate that finding transcribed Arabic and Hebrew with vowels may be challenging, but it's half of the typology.
3) Reduplication seems to be a different kind of phenomenon to the other three, which are more strictly morphological typologies. Indonesian and Malay also exhibit various word affixes, which can be used on top of reduplication, which is a more lexical process. I'm not sure splitting it out from the other linguistic typologies is justified.
- General Discussion: 1) The paper was structured very clearly and was very easy to read.
2) I'm a bit puzzled about why the authors chose to use 200 dimensional character embeddings. Once the dimensionality of the embedding is greater than the size of the vocabulary (here the number of characters in the alphabet), surely you're not getting anything extra?
------------------------------- Having read the author response, my opinions have altered little. I still think the same strengths and weakness that I have already discussed hold.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods.,ACL_2017_104_review,ACL_2017,"- Comparison with ALIGN could be better. ALIGN used content window size 10 vs this paper's 5, vector dimension of 500 vs this paper's 200. Also its not clear to me whether N(e_j) includes only entities that link to e_j. The graph is directed and consists of wikipedia outlinks, but is adjacency defined as it would be for an undirected graph? For ALIGN, the context of an entity is the set of entities that link to that entity. If N(e_j) is different, we cannot tell how much impact this change has on the learned vectors, and this could contribute to the difference in scores on the entity similarity task. - It is sometimes difficult to follow whether ""mention"" means a string type, or a particular mention in a particular document. The phrase ""mention embedding"" is used, but it appears that embeddings are only learned for mention senses.
- It is difficult to determine the impact of sense disambiguation order without comparison to other unsupervised entity linking methods. - General Discussion:","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '2', '1']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '2', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well.",ARR_2022_89_review,ARR_2022,"1. The experiments are held on a private datasets and the exact setup is impossible to reproduce.
2. A minor point would be that few-shot would be a more realistic setup for that task, as domain-specific TODOs are easy to acquire, however I agree that the current setup is adequate as well.
3. More error analysis could be useful, especially on the public dataset, as their data could be included without any restrictions, e.g., error types/examples? patterns? Examples when non-contextualized embeddings outperform contextualized ones, or even LITE?
I urge the authors to release at least some part of the dataset to the wider public, or under some end user-agreement.
Comments: 1. I suggest the authors to focus their comparison on word2vec baselines (currently in appendix), instead of Sentence-BERT, as the latter does not show good performance on the short texts. It seems that non-contextualized embeddings are more suitable for the task.
2. Maybe it makes more sense to try out models pre-trained on conversations, e.g., text from Twitter or natural language conversations.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '4']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).,ARR_2022_65_review,ARR_2022,"1. The paper covers little qualitative aspects of the domains, so it is hard to understand how they differ in linguistic properties. For example, I think it is vague to say that the fantasy novel is more “canonical” (line 355). Text from a novel may be similar to that from news articles in that sentences tend to be complete and contain fewer omissions, in contrast to product comments which are casually written and may have looser syntactic structures. However, novel text is also very different from news text in that it contains unusual predicates and even imaginary entities as arguments. It seems that the authors are arguing that syntactic factors are more significant in SRL performance, and the experimental results are also consistent with this. Then it would be helpful to show a few examples from each domain to illustrate how they differ structurally.
2. The proposed dataset uses a new annotation scheme that is different from that of previous datasets, which introduces difficulties of comparison with previous results. While I think the frame-free scheme is justified in this paper, the compatibility with other benchmarks is an important issue that needs to be discussed. It may be possible to, for example, convert frame-based annotations to frame-free ones. I believe this is doable because FrameNet also has the core/non-core sets of argument for each frame. It would also be better if the authors can elaborate more on the relationship between this new scheme and previous ones. Besides eliminating the frame annotation, what are the major changes to the semantic role labels?
- In Sec. 3, it is a bit confusing why there is a division of source domain and target domain. Thus, it might be useful to mention explicitly that the dataset is designed for domain transfer experiments.
- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)’s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.
- More information about the annotators would be needed. Are they all native Chinese speakers? Do they have linguistics background?
- Were pred-wise/arg-wise consistencies used in the construction of existing datasets? I think they are not newly invented. It is useful to know where they come from.
- In the SRL formulation (Sec. 5), I am not quite sure what is “the concerned word”. Is it the predicate? Does this formulation cover the task of identifying the predicate(s), or are the predicates given by syntactic parsing results?
- From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).
- How was the train/dev/test split determined? This should be noted (even if it is simply done randomly).","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '4']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '5', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"5. The authors may choose to insert into Figure 1 the explicit ""first layer"", ""second layer"" and ""third layer"" labels they use in the accompanying text.",ACL_2017_37_review,ACL_2017,"Weak results/summary of ""side-by-side human"" comparison in Section 5. Some disfluency/agrammaticality.
- General Discussion: The article proposes a principled means of modeling utterance context, consisting of a sequence of previous utterances. Some minor issues: 1. Past turns in Table 1 could be numbered, making the text associated with this table (lines 095-103) less difficult to ingest. Currently, readers need to count turns from the top when identifying references in the authors' description, and may wonder whether ""second"", ""third"", and ""last"" imply a side-specific or global enumeration.
2. Some reader confusion may be eliminated by explicitly defining what ""segment"" means in ""segment level"", as occurring on line 269. Previously, on line 129, this seemingly same thing was referred to as ""a sequence-sequence [similarity matrix]"". The two terms appear to be used interchangeably, but it is not clear what they actually mean, despite the text in section 3.3. It seems the authors may mean ""word subsequence"" and ""word subsequence to word subsequence"", where ""sub-"" implies ""not the whole utterance"", but not sure.
3. Currently, the variable symbol ""n"" appears to be used to enumerate words in an utterance (line 306), as well as utterances in a dialogue (line 389). The authors may choose two different letters for these two different purposes, to avoid confusing readers going through their equations.
4. The statement ""This indicates that a retrieval based chatbot with SMN can provide a better experience than the state-of-the-art generation model in practice."" at the end of section 5 appears to be unsupported. The two approaches referred to are deemed comparable in 555 out of 1000 cases, with the baseline better than the proposed method in 238 our of the remaining 445 cases.
The authors are encouraged to assess and present the statistical significance of this comparison. If it is weak, their comparison permits to at best claim that their proposed method is no worse (rather than ""better"") than the VHRED baseline.
5. The authors may choose to insert into Figure 1 the explicit ""first layer"", ""second layer"" and ""third layer"" labels they use in the accompanying text.
6. Their is a pervasive use of ""to meet"" as in ""a response candidate can meet each utterace"" on line 280 which is difficult to understand.
7. Spelling: ""gated recurrent unites""; ""respectively"" on line 133 should be removed; punctuation on line 186 and 188 is exchanged; ""baseline model over"" -> ""baseline model by""; ""one cannot neglects"".","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '4', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '3', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.,ARR_2022_338_review,ARR_2022,"- The unsupervised translation tasks are all quite superficial, taking existing datasets of similar languages (e.g. En-De Multi30k, En-Fr WMT) and editing them to an unsupervised MT corpus.
- Improvements on Multi30k are quite small (< 1 BLEU) and reported over single runs and measuring BLEU scores alone. It would be good to report averages over multiple runs and report some more modern metrics as well like COMET or BLEURT.
- It is initially quite unclear from the writing where the sentence-level representations come from. As they are explicitly modeled, they need supervision from somewhere. The constant comparison to latent variable models and calling these sentence representations latent codes does not add to the clarity of the paper. I hope this will be improved in a revision of the paper.
Some typos: - 001: ""The latent variables"" -> ""Latent variables"" - 154: ""efficiently to compute"" -> ""efficient to compute"" - 299: ""We denote the encoder and decoder for encoding and generating source-language sentences as the source encoder and decoder"" - unclear - 403: ""langauge"" -> ""language""","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.",ACL_2017_588_review,ACL_2017,"and the evaluation leaves some questions unanswered. - Strengths: The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.
- Weaknesses: 1) All the models evaluated, except the best performing model (HIERENC), do not have access to contextual information beyond a sentence. This does not seem sufficient to predict a missing entity. It is unclear whether any attempts at coreference and anaphora resolution have been made. It would generally help to see how well humans perform at the same task.
2) The choice of predictors used in all models is unusual. It is unclear why similarity between context embedding and the definition of the entity is a good indicator of the goodness of the entity as a filler.
3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary.
This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.
4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.
- Questions to the authors: 1) An important assumption being made is that d_e are good replacements for entity embeddings. Was this assumption tested?
2) Have you tried building a classifier that just takes h_i^e as inputs?
I have read the authors' responses. I still think the task+dataset could benefit from human evaluation. This task can potentially be a good benchmark for NLU systems, if we know how difficult the task is. The results presented in the paper are not indicative of this due to the reasons stated above. Hence, I am not changing my scores.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '1', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', 'X', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '1', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.",ARR_2022_123_review,ARR_2022,"1) it uses different experiment settings (e.g., a 32k batch size, a beam size of 5) and does not mention some details (e.g., the number of training steps), these different settings may contribute the performance and the comparisons in Table 2 may not be sufficiently reliable.
2) it only compares with some weak baselines in Tables 3, 4 and 6. Though the approach can surpass the sentence-level model baseline, the naive document-to-document translation model and Zheng et al. (2020), these baselines seem weak, for example, Voita et al. (2019) achieve 81.6, 58.1, 72.2 and 80.0 for deixis, lexical cohesion, ellipsis (infl.) and ellipsis (VP) respectively with the CADec model, while this work only gets 64.7, 46.3, 65.9 and 53.0. It seems that there is still a large gap with the presented approach in these linguistic evaluations.
3) the multi-resolutional data processing approach may somehow increase the instance weight of the document-level data, and how this affects the performance is not studied.
1) It's better to adopt experiment settings consistent with previous work.
2) There is still a large performance gap compared to Voita et al. (2019) in linguistic evaluations, while BLEU may not be able to reflect these document-level phenomena and linguistic evaluations are important. The issues for the performance gap shall be investigated and solved.
3) It's better to investigate whether the model really leverages document-level contexts correctly, probably refer to this paper: Do Context-Aware Translation Models Pay the Right Attention? In ACL 2021.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '1']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DVRTnFRi', 'boda'], 'labels': ['0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- ""While higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words"": it's true, but I was wondering whether sentiment words are rare in the corpus. If they are, those MT systems should obviously handle them (in addition to other rare words).",ACL_2017_96_review,ACL_2017,"lack statistics of the datsets (e.g. average length, vocabulary size) the baseline (Moses) is not proper because of the small size of the dataset the assumption ""sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word"" is not supported by the data. - General Discussion: This discussion gives more details about the weaknesses of the paper. Half of the paper is about the new dataset for sarcasm interpretation.
However, the paper doesn't show important information about the dataset such as average length, vocabulary size. More importantly, the paper doesn't show any statistical evidence to support their method of focusing on sentimental words. Because the dataset is small (only 3000 tweets), I guess that many words are rare. Therefore, Moses alone is not a proper baseline. A proper baseline should be a MT system that can handle rare words very well. In fact, using clustering and declustering (as in Sarcasm SIGN) is a way to handle rare words.
Sarcasm SIGN is built based on the assumption that ""sarcastic tweets often differ from their non sarcastic interpretations in as little as one sentiment word"". Table 1 however strongly disagrees with this assumption: the human interpretations are often different from the tweets at not only sentimental words. I thus strongly suggest the authors to give statistical evidence from the dataset that supports their assumption. Otherwise, the whole idea of Sarcasm SIGN is just a hack.
-------------------------------------------------------------- I have read the authors' response. I don't change my decision because of the following reasons: - the authors wrote that ""the Fiverr workers might not take this strategy"": to me it is not the spirit of corpus-based NLP. A model must be built to fit given data, not that the data must follow some assumption that the model is built on.
- the authors wrote that ""the BLEU scores of Moses and SIGN are above 60, which is generally considered decent in the MT literature"": to me the number 60 doesn't show anything at all because the sentences in the dataset are very short. And that, if we look at table 6, %changed of Moses is only 42%, meaning that even more than half of the time translation is simply copying, the BLUE score is more than 60.
- ""While higher scores might be achieved with MT systems that explicitly address rare words, these systems don't focus on sentiment words"": it's true, but I was wondering whether sentiment words are rare in the corpus. If they are, those MT systems should obviously handle them (in addition to other rare words).","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', '4']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The similarity/relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts. However, if I understand correctly, the actual judgements were made based on presenting phrases to the human annotators, and therefore they should be considered as phrase similarity datasets, and analyzed as such.",ACL_2017_178_review,ACL_2017,"- The evaluation reported in this paper includes only intrinsic tasks, mainly on similarity/relatedness datasets. As the authors note, such evaluations are known to have very limited power in predicting the utility of embeddings in extrinsic tasks. Accordingly, it has become recently much more common to include at least one or two extrinsic tasks as part of the evaluation of embedding models.
- The similarity/relatedness evaluation datasets used in the paper are presented as datasets recording human judgements of similarity between concepts. However, if I understand correctly, the actual judgements were made based on presenting phrases to the human annotators, and therefore they should be considered as phrase similarity datasets, and analyzed as such.
- The medical concept evaluation dataset, ‘mini MayoSRS’ is extremely small (29 pairs), and its larger superset ‘MayoSRS’ is only a little larger (101 pairs) and was reported to have a relatively low human annotator agreement. The other medical concept evaluation dataset, ‘UMNSRS’, is more reasonable in size, but is based only on concepts that can be represented as single words, and were represented as such to the human annotators. This should be mentioned in the paper and makes the relevance of this dataset questionable with respect to representations of phrases and general concepts. - As the authors themselves note, they (quite extensively) fine tune their hyperparameters on the very same datasets for which they report their results and compare them with prior work. This makes all the reported results and analyses questionable.
- The authors suggest that their method is superb to prior work, as it achieved comparable results while prior work required much more manual annotation. I don't think this argument is very strong because the authors also use large manually-constructed ontologies, and also because the manually annotated dataset used in prior work comes from existing clinical records that did not require dedicated annotations.
- In general, I was missing more useful insights into what is going on behind the reported numbers. The authors try to treat the relation between a phrase and its component words on one hand, and a concept and its alternative phrases on the other, as similar types of a compositional relation. However, they are different in nature and in my mind each deserves a dedicated analysis. For example, around line 588, I would expect an NLP analysis specific to the relation between phrases and their component words. Perhaps the reason for the reported behavior is dominant phrase headwords, etc. Another aspect that was absent but could strengthen the work, is an investigation of the effect of the hyperparameters that control the tradeoff between the atomic and compositional views of phrases and concepts.
General Discussion: Due to the above mentioned weaknesses, I recommend to reject this submission. I encourage the authors to consider improving their evaluation datasets and methodology before re-submitting this paper.
Minor comments: - Line 069: contexts -> concepts - Line 202: how are phrase overlaps handled?
- Line 220: I believe the dimensions should be |W| x d. Also, the terminology ‘negative sampling matrix’ is confusing as the model uses these embeddings to represent contexts in positive instances as well.
- Line 250: regarding ‘the observed phrase just completed’, it not clear to me how words are trained in the joint model. The text may imply that only the last words of a phrase are considered as target words, but that doesn’t make sense. - Notation in Equation 1 is confusing (using c instead of o) - Line 361: Pedersen et al 2007 is missing in the reference section.
- Line 388: I find it odd to use such a fine-grained similarity scale (1-100) for human annotations.
- Line 430: The newly introduced term ‘strings’ here is confusing. I suggest to keep using ‘phrases’ instead.
- Line 496: Which task exactly was used for the hyper-parameter tuning?
That’s important. I couldn’t find that even in the appendix.
- Table 3: It’s hard to see trends here, for instance PM+CL behaves rather differently than either PM or CL alone. It would be interesting to see development set trends with respect to these hyper-parameters.
- Line 535: missing reference to Table 5.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '1', '2']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '2', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"1. The proposed method is not compared with other CWS models. The baseline model (Bi-LSTM) is proposed in [1] and [2]. However, these model is proposed not for CWS but for POS tagging and NE tagging. The description ""In this paper, we employ the state-of-the-art architecture ..."" (in Section 2) is misleading.",ACL_2017_326_review,ACL_2017,"1. The proposed method is not compared with other CWS models. The baseline model (Bi-LSTM) is proposed in [1] and [2]. However, these model is proposed not for CWS but for POS tagging and NE tagging. The description ""In this paper, we employ the state-of-the-art architecture ..."" (in Section 2) is misleading.
2. The purpose of experiments in Section 6.4 is unclear. In Sec. 6.4, the purpose is that investigating ""datasets in traditional Chinese and simplified Chinese could help each other."" However, in the experimental setting, the model is separately trained on simplified Chinese and traditional Chinese, and the shared parameters are fixed after training on simplified Chinese. What is expected to fixed shared parameters?
- General Discussion: The paper should be more interesting if there are more detailed discussion about the datasets that adversarial multi-criteria learning does not boost the performance.
[1] Zhiheng Huang, Wei Xu, and Kai Yu. 2015. Bidirectional lstm-crf models for sequence tagging. arXiv preprint arXiv:1508.01991.
[2] Xuezhe Ma and Eduard Hovy. 2016. End-to-end sequence labeling via bi-directional lstm-cnns-crf. arXiv preprint arXiv:1603.01354 .","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', '2']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"1) The best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.",ACL_2017_768_review,ACL_2017,". First, the classification model used in this paper (concat + linear classifier) was shown to be inherently unable to learn relations in ""Do Supervised Distributional Methods Really Learn Lexical Inference Relations?"" ( Levy et al., 2015). Second, the paper makes superiority claims in the text that are simply not substantiated in the quantitative results. In addition, there are several clarity and experiment setup issues that give an overall feeling that the paper is still half-baked.
= Classification Model = Concatenating two word vectors as input for a linear classifier was mathematically proven to be incapable of learning a relation between words (Levy et al., 2015). What is the motivation behind using this model in the contextual setting?
While this handicap might be somewhat mitigated by adding similarity features, all these features are symmetric (including the Euclidean distance, since |L-R| = |R-L|). Why do we expect these features to detect entailment?
I am not convinced that this is a reasonable classification model for the task.
= Superiority Claims = The authors claim that their contextual representation is superior to context2vec. This is not evident from the paper, because: 1) The best result (F1) in both table 3 and table 4 (excluding PPDB features) is the 7th row. To my understanding, this variant does not use the proposed contextual representation; in fact, it uses the context2vec representation for the word type.
2) This experiment uses ready-made embeddings (GloVe) and parameters (context2vec) that were tuned on completely different datasets with very different sizes. Comparing the two is empirically flawed, and probably biased towards the method using GloVe (which was a trained on a much larger corpus).
In addition, it seems that the biggest boost in performance comes from adding similarity features and not from the proposed context representation. This is not discussed.
= Miscellaneous Comments = - I liked the WordNet dataset - using the example sentences is a nice trick.
- I don’t quite understand why the task of cross-lingual lexical entailment is interesting or even reasonable.
- Some basic baselines are really missing. Instead of the ""random"" baseline, how well does the ""all true"" baseline perform? What about the context-agnostic symmetric cosine similarity of the two target words?
- In general, the tables are very difficult to read. The caption should make the tables self-explanatory. Also, it is unclear what each variant means; perhaps a more precise description (in text) of each variant could help the reader understand?
- What are the PPDB-specific features? This is really unclear.
- I could not understand 8.1.
- Table 4 is overfull.
- In table 4, the F1 of ""random"" should be 0.25.
- Typo in line 462: should be ""Table 3"" = Author Response = Thank you for addressing my comments. Unfortunately, there are still some standing issues that prevent me from accepting this paper: - The problem I see with the base model is not that it is learning prototypical hypernyms, but that it's mathematically not able to learn a relation.
- It appears that we have a different reading of tables 3 and 4. Maybe this is a clarity issue, but it prevents me from understanding how the claim that contextual representations substantially improve performance is supported.
Furthermore, it seems like other factors (e.g. similarity features) have a greater effect.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', '1']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- Line 21-27 - the authors could have avoided this complicated structure for two simple sentences. Line 41 - Johnson et. al has SOTA on English-French and German-English. Line 77-79 there is no evidence provided as to why combination of multiple languages increases complexity. Please retract this statement or provide more evidence. Evidence in literature seems to suggest the opposite. Line 416-420 - The two lines here are repeated again. They were first mentioned in the previous paragraph. Line 577 - Figure 2 not 3!,ACL_2017_779_review,ACL_2017,"There were many sentences in the abstract and in other places in the paper where the authors stuff too much information into a single sentence. This could be avoided. One can always use an extra sentence to be more clear.
There could have been a section where the actual method used could be explained in a more detailed. This explanation is glossed over in the paper. It's non-trivial to guess the idea from reading the sections alone.
During test time, you need the source-pivot corpus as well. This is a major disadvantage of this approach. This is played down - in fact it's not mentioned at all. I could strongly encourage the authors to mention this and comment on it. - General Discussion: This paper uses knowledge distillation to improve zero-resource translation.
The techniques used in this paper are very similar to the one proposed in Yoon Kim et. al. The innovative part is that they use it for doing zero-resource translation. They compare against other prominent works in the field. Their approach also eliminates the need to do double decoding.
Detailed comments: - Line 21-27 - the authors could have avoided this complicated structure for two simple sentences.
Line 41 - Johnson et. al has SOTA on English-French and German-English.
Line 77-79 there is no evidence provided as to why combination of multiple languages increases complexity. Please retract this statement or provide more evidence. Evidence in literature seems to suggest the opposite.
Line 416-420 - The two lines here are repeated again. They were first mentioned in the previous paragraph.
Line 577 - Figure 2 not 3!","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors.",ACL_2017_494_review,ACL_2017,"- I was hoping to see some analysis of why the morph-fitted embeddings worked better in the evaluation, and how well that corresponds with the intuitive motivation of the authors. - The authors introduce a synthetic word similarity evaluation dataset, Morph-SimLex. They create it by applying their presumably semantic-meaning-preserving morphological rules to SimLex999 to generate many more pairs with morphological variability. They do not manually annotate these new pairs, but rather use the original similarity judgements from SimLex999.
The obvious caveat with this dataset is that the similarity scores are presumed and therefore less reliable. Furthermore, the fact that this dataset was generated by the very same rules that are used in this work to morph-fit word embeddings, means that the results reported on this dataset in this work should be taken with a grain of salt. The authors should clearly state this in their paper.
- (Soricut and Och, 2015) is mentioned as a future source for morphological knowledge, but in fact it is also an alternative approach to the one proposed in this paper for generating morphologically-aware word representations. The authors should present it as such and differentiate their work.
- The evaluation does not include strong morphologically-informed embedding baselines. General Discussion: With the few exceptions noted, I like this work and I think it represents a nice contribution to the community. The authors presented a simple approach and showed that it can yield nice improvements using various common embeddings on several evaluations and four different languages. I’d be happy to see it in the conference.
Minor comments: - Line 200: I found this phrasing unclear: “We then query … of linguistic constraints”.
- Section 2.1: I suggest to elaborate a little more on what the delta is between the model used in this paper and the one it is based on in Wieting 2015. It seemed to me that this was mostly the addition of the REPEL part.
- Line 217: “The method’s cost function consists of three terms” - I suggest to spell this out in an equation.
- Line 223: x and t in this equation (and following ones) are the vector representations of the words. I suggest to denote that somehow. Also, are the vectors L2-normalized before this process? Also, when computing ‘nearest neighbor’ examples do you use cosine or dot-product? Please share these details.
- Line 297-299: I suggest to move this text to Section 3, and make the note that you did not fine-tune the params in the main text and not in a footnote.
- Line 327: (create, creates) seems like a wrong example for that rule.
- I have read the author response","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '4', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '3', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
- Table 4: Caption should make it clear these are F1 scores as well as clarifying how the F1 score is weighted (e.g. micro/macro). This should also be made clear in the “evaluation metrics” section on page 6.,ACL_2017_727_review,ACL_2017,"Quantitative results are given only for the author's PSL model and not compared against any traditional baseline classification algorithms, making it unclear to what degree their model is necessary. Poor comparison with alternative approaches makes it difficult to know what to take away from the paper.
The qualitative investigation is interesting, but the chosen visualizations are difficult to make sense of and add little to the discussion. Perhaps it would make sense to collapse across individual politicians to create a clearer visual.
- General Discussion: The submission is well written and covers a topic which may be of interest to the ACL community. At the same time, it lacks proper quantitative baselines for comparison. Minor comments: - line 82: A year should be provided for the Boydstun et al. citation - It’s unclear to me why similar behavior (time of tweeting) should necessarily be indicative of similar framing and no citation was given to support this assumption in the model.
- The related work goes over quite a number of areas, but glosses over the work most clearly related (e.g. PSL models and political discourse work) while spending too much time mentioning work that is only tangential (e.g. unsupervised models using Twitter data).
- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if they used pre-trained embeddings.
- The authors give no intuition behind why unigrams are used to predict frames, while bigrams/trigrams are used to predict party.
- The authors note that temporal similarity worked best with one hour chunks, but make no mention of how important this assumption is to their results. If the authors are unable to provide full results for this work, it would still be worthwhile to give the reader a sense of what performance would look like if the time window were widened.
- Table 4: Caption should make it clear these are F1 scores as well as clarifying how the F1 score is weighted (e.g. micro/macro). This should also be made clear in the “evaluation metrics” section on page 6.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community.",ARR_2022_1_review,ARR_2022,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community.
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '4', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '4', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The writing is really poor. Many places are very confusing. The figures are not clearly separated from the text, and it is confusing that where I should look at. Many sentences use the past tense, while other sentences use the present tense. The only reason that I would like this paper to be accepted is because of the dataset. The writing itself is far from a solid paper, and I suggest authors go over the writing again.",ARR_2022_333_review,ARR_2022,"- The writing is really poor. Many places are very confusing. The figures are not clearly separated from the text, and it is confusing that where I should look at. Many sentences use the past tense, while other sentences use the present tense. The only reason that I would like this paper to be accepted is because of the dataset. The writing itself is far from a solid paper, and I suggest authors go over the writing again.
- This dataset is the first Thai N-NER dataset, and N-NER in Thai is a new task for the community, so it could be very insightful to know what specific challenges are in Thai, and what errors the models make. The paper provides an error analysis, but not deep enough. It would be insightful if the authors could list error patterns at a finer granularity.
It is also unclear to me that why syllable segmentation could be useful for the annotation. Many of the readers do not know Thai, so I think more explanation is necessary.
For the writing part, for example, Section 3 is mixed with the past tense and present tense. Figure 2 is hidden in the text. I suggest the authors put all the tables and figures at the top of the pages.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '1', '2']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response.",ACL_2017_71_review,ACL_2017,"-The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version.
-README file for the dataset [Authors committed to add README file] - General Discussion: - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper.
- In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.) - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure.
- Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response.
- In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '2', '4']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '4', '4']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself?,ARR_2022_187_review,ARR_2022,"1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty. 2. The writing is difficult to follow in many places and can be simplified.
1. Line 360-367 are occupying too much space than needed. 2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :) 3. Too many metrics used for evaluation. While I commend the paper’s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text. 4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM’s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant. 5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself? 6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better. 7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of “despite the weaknesses, NPRM is useful'' but it does not flesh out why it’s useful. 8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results. 10. Line 559: “lower performance on Vikidia-Fr compared to Newsela-Es …” – Why? These are different languages after all, so isn’t the performance difference in-comparable?","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
- line 82: A year should be provided for the Boydstun et al. citation - It’s unclear to me why similar behavior (time of tweeting) should necessarily be indicative of similar framing and no citation was given to support this assumption in the model.,ACL_2017_727_review,ACL_2017,"Quantitative results are given only for the author's PSL model and not compared against any traditional baseline classification algorithms, making it unclear to what degree their model is necessary. Poor comparison with alternative approaches makes it difficult to know what to take away from the paper.
The qualitative investigation is interesting, but the chosen visualizations are difficult to make sense of and add little to the discussion. Perhaps it would make sense to collapse across individual politicians to create a clearer visual.
- General Discussion: The submission is well written and covers a topic which may be of interest to the ACL community. At the same time, it lacks proper quantitative baselines for comparison. Minor comments: - line 82: A year should be provided for the Boydstun et al. citation - It’s unclear to me why similar behavior (time of tweeting) should necessarily be indicative of similar framing and no citation was given to support this assumption in the model.
- The related work goes over quite a number of areas, but glosses over the work most clearly related (e.g. PSL models and political discourse work) while spending too much time mentioning work that is only tangential (e.g. unsupervised models using Twitter data).
- Section 4.2 it is unclear whether Word2Vec was trained on their dataset or if they used pre-trained embeddings.
- The authors give no intuition behind why unigrams are used to predict frames, while bigrams/trigrams are used to predict party.
- The authors note that temporal similarity worked best with one hour chunks, but make no mention of how important this assumption is to their results. If the authors are unable to provide full results for this work, it would still be worthwhile to give the reader a sense of what performance would look like if the time window were widened.
- Table 4: Caption should make it clear these are F1 scores as well as clarifying how the F1 score is weighted (e.g. micro/macro). This should also be made clear in the “evaluation metrics” section on page 6.","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '3', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.,ARR_2022_311_review,ARR_2022,"__1. Lack of significance test:__ I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results.
__2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)?
__4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result?
__4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample?
1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation","{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,1,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['fZav2G06', 'DVRTnFRi', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': [], 'labels': []}",,
"2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).",ACL_2017_318_review,ACL_2017,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?","{'annotators': ['TxZsPCly', 'LbMNie2g', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '4', '1', '4', '2', '1', '2']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '4', '1', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '4', '4', '4', '2', '2', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '4', '3', '4', '3', '2', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '0', '1', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has.",ARR_2022_236_review,ARR_2022,"- My main criticism is that the ""mismatched"" image caption dataset is artificial and may not capture the kind of misinformation that is posted on platforms like Twitter. For instance, someone posting a fake image of a lockdown at a particular place may not just be about a mismatch between the image and the caption, but may rather require fact-checking, etc. Moreover, the in-the-wild datasets on which a complementary evaluation is conducted are also more about mismatched image-caption and not the real misinformation (lines 142-143). Therefore, the extent to which this dataset can be used for misinformation detection is limited. I would have liked to see this distinction between misinformation and mismatched image captions being clear in the paper.
- Also, since the dataset is artificially created, the dataset itself might have a lot of noise. For instance, the collected ""pristine"" set of tweets may not be pristine enough and might instead contain misinformation as well as out-of-context images. I would have liked to see more analysis around the quality of the collected dataset and the amount of noise it potentially has. - Since this is a new dataset, I would have liked to see evaluation of more models (other than just CLIP). But given that it is only a short paper, it is probably not critical (the paper makes enough contributions otherwise)
- Table 4 and Table 5: Are the differences statistically significant? ( especially important because the hNews and Twitter datasets are really small) - Lines 229-240: Are the differences between the topics statistically significant?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '3', '3', '4', '5', '3', '2', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '4', '4', '4', '5', '5', '2', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '4', '3', '4', '5', '5', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '3', '3', '5', '5', '5', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.,ACL_2017_554_review,ACL_2017,"1) The paper does not dig into the theory profs and show the convergence properties of the proposed algorithm.
2) The paper only shows the comparison between SG-MCMC vs RMSProp and did not conduct other comparison. It should explain more about the relation between pSGLD vs RMSProp other than just mentioning they are conterparts in two families.
2) The paper does not talk about the training speed impact with more details.
- General Discussion:","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '3', '2', '3', '1', '1', '2']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '2', '4', '2', '2', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '2', 'X', '1', '1', '1', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '3', '1', '1', '1', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['0', '0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn’t clear to me why the ones you chose were better options.",ACL_2017_516_review,ACL_2017,"Missing related work on anchor words Evaluation on 20 Newsgroups is not ideal Theoretical contribution itself is small - General Discussion: The authors propose a new method of interactive user specification of topics called Tandem Anchors. The approach leverages the anchor words algorithm, a matrix-factorization approach to learning topic models, by replacing the individual anchors inferred from the Gram-Schmidt algorithm with constructed anchor pseudowords created by combining the sparse vector representations of multiple words that for a topic facet. The authors determine that the use of a harmonic mean function to construct pseudowords is optimal by demonstrating that classification accuracy of document-topic distribution vectors using these anchors produces the most improvement over Gram-Schmidt. They also demonstrate that their work is faster than existing interactive methods, allowing interactive iteration, and show in a user study that the multiword anchors are easier and more effective for users.
Generally, I like this contribution a lot: it is a straightforward modification of an existing algorithm that actually produces a sizable benefit in an interactive setting. I appreciated the authors’ efforts to evaluate their method on a variety of scales. While I think the technical contribution in itself is relatively small (a strategy to assemble pseudowords based on topic facets) the thoroughness of the evaluation merited having it be a full paper instead of a short paper. It would have been nice to see more ideas as to how to build these facets in the absence of convenient sources like category titles in 20 Newsgroups or when initializing a topic model for interactive learning.
One frustration I had with this paper is that I find evaluation on 20 Newsgroups to not be great for topic modeling: the documents are widely different lengths, preprocessing matters a lot, users have trouble making sense of many of the messages, and naive bag-of-words models beat topic models by a substantial margin. Classification tasks are useful shorthand for how well a topic model corresponds to meaningful distinctions in the text by topic; a task like classifying news articles by section or reviews by the class of the subject of the review might be more appropriate. It would also have been nice to see a use case that better appealed to a common expressed application of topic models, which is the exploration of a corpus.
There were a number of comparisons I think were missing, as the paper contains little reference to work since the original proposal of the anchor word model.
In addition to comparing against standard Gram-Schmidt, it would have been good to see the method from Lee et. al. (2014), “Low-dimensional Embeddings for Interpretable Anchor-based Topic Inference”. I also would have liked to have seen references to Nguyen et. al. (2013), “Evaluating Regularized Anchor Words” and Nguyen et. al. (2015) “Is Your Anchor Going Up or Down? Fast and Accurate Supervised Topic Models”, both of which provide useful insights into the anchor selection process.
I had some smaller notes: - 164: …entire dataset - 164-166: I’m not quite sure what you mean here. I think you are claiming that it takes too long to do one pass? My assumption would have been you would use only a subset of the data to retrain the model instead of a full sweep, so it would be good to clarify what you mean.
- 261&272: any reason you did not consider the and operator or element-wise max? They seem to correspond to the ideas of union and intersection from the or operator and element-wise min, and it wasn’t clear to me why the ones you chose were better options.
- 337: Usenet should be capitalized - 338-340: Why fewer than 100 (as that is a pretty aggressive boundary)? Also, did you remove headers, footers, and/or quotes from the messages?
- 436-440: I would have liked to see a bit more explanation of what this tells us about confusion.
- 692: using tandem anchors Overall, I think this paper is a meaningful contribution to interactive topic modeling that I would like to see available for people outside the machine learning community to investigate, classify, and test hypotheses about their corpora.
POST-RESPONSE: I appreciate the thoughtful responses of the authors to my questions. I would maintain that for some of the complimentary related work that it's useful to compare to non-interactive work, even if it does something different.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '1', '5', '3', '1', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', '4', 'X', '5', '5', 'X', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '5', '2', '5', '5', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary. This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.",ACL_2017_588_review,ACL_2017,"and the evaluation leaves some questions unanswered. - Strengths: The proposed task requires encoding external knowledge, and the associated dataset may serve as a good benchmark for evaluating hybrid NLU systems.
- Weaknesses: 1) All the models evaluated, except the best performing model (HIERENC), do not have access to contextual information beyond a sentence. This does not seem sufficient to predict a missing entity. It is unclear whether any attempts at coreference and anaphora resolution have been made. It would generally help to see how well humans perform at the same task.
2) The choice of predictors used in all models is unusual. It is unclear why similarity between context embedding and the definition of the entity is a good indicator of the goodness of the entity as a filler.
3) The description of HIERENC is unclear. From what I understand, each input (h_i) to the temporal network is the average of the representations of all instantiations of context filled by every possible entity in the vocabulary.
This does not seem to be a good idea since presumably only one of those instantiations is correct. This would most likely introduce a lot of noise.
4) The results are not very informative. Given that this is a rare entity prediction problem, it would help to look at type-level accuracies, and analyze how the accuracies of the proposed models vary with frequencies of entities.
- Questions to the authors: 1) An important assumption being made is that d_e are good replacements for entity embeddings. Was this assumption tested?
2) Have you tried building a classifier that just takes h_i^e as inputs?
I have read the authors' responses. I still think the task+dataset could benefit from human evaluation. This task can potentially be a good benchmark for NLU systems, if we know how difficult the task is. The results presented in the paper are not indicative of this due to the reasons stated above. Hence, I am not changing my scores.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '1', '2', '5', '3', '3', '2']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '4', '5', '1', '5', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '2', 'X', 'X', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '2', '3', '3', '5', '5', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?",ARR_2022_23_review,ARR_2022,"The technical novelty is rather lacking. Although I believe this doesn't affect the contribution of this paper.
- You mention that you only select 10 answers from all correct answers, why do you do this? Does this affect the underestimation of the performances?
- Do you think generative PLMs that are pretrained on biomedical texts could be more suitable for solving the multi-token problem?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '1', '4', '1', '1', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '5', '1', '1', '2', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', 'X', 'X', '4', '2', 'X', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '5', '2', '3', '1', '3', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)’s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.",ARR_2022_65_review,ARR_2022,"1. The paper covers little qualitative aspects of the domains, so it is hard to understand how they differ in linguistic properties. For example, I think it is vague to say that the fantasy novel is more “canonical” (line 355). Text from a novel may be similar to that from news articles in that sentences tend to be complete and contain fewer omissions, in contrast to product comments which are casually written and may have looser syntactic structures. However, novel text is also very different from news text in that it contains unusual predicates and even imaginary entities as arguments. It seems that the authors are arguing that syntactic factors are more significant in SRL performance, and the experimental results are also consistent with this. Then it would be helpful to show a few examples from each domain to illustrate how they differ structurally.
2. The proposed dataset uses a new annotation scheme that is different from that of previous datasets, which introduces difficulties of comparison with previous results. While I think the frame-free scheme is justified in this paper, the compatibility with other benchmarks is an important issue that needs to be discussed. It may be possible to, for example, convert frame-based annotations to frame-free ones. I believe this is doable because FrameNet also has the core/non-core sets of argument for each frame. It would also be better if the authors can elaborate more on the relationship between this new scheme and previous ones. Besides eliminating the frame annotation, what are the major changes to the semantic role labels?
- In Sec. 3, it is a bit confusing why there is a division of source domain and target domain. Thus, it might be useful to mention explicitly that the dataset is designed for domain transfer experiments.
- Line 226-238 seem to suggest that the authors selected sentences from raw data of these sources, but line 242-244 say these already have syntactic information. If I understand correctly, the data selected is a subset of Li et al. (2019a)’s dataset. If this is the case, I think this description can be revised, e.g. mentioning Li et al. (2019a) earlier, to make it clear and precise.
- More information about the annotators would be needed. Are they all native Chinese speakers? Do they have linguistics background?
- Were pred-wise/arg-wise consistencies used in the construction of existing datasets? I think they are not newly invented. It is useful to know where they come from.
- In the SRL formulation (Sec. 5), I am not quite sure what is “the concerned word”. Is it the predicate? Does this formulation cover the task of identifying the predicate(s), or are the predicates given by syntactic parsing results?
- From Figure 3 it is not clear to me how ZX is the most similar domain to Source. Grouping the bars by domain instead of role might be better (because we can compare the shapes). It may also be helpful to leverage some quantitative measure (e.g. cross entropy).
- How was the train/dev/test split determined? This should be noted (even if it is simply done randomly).","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '4', '2', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', 'X', '5', '3', '5', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?,ARR_2022_93_review,ARR_2022,"1. From an experimental design perspective, the experimental design suggested by the authors has been used widely for open-domain dialogue systems with the caveat of it not being done in live interactive settings.
2. The authors have not referenced those works that use continuous scales in the evaluation and there is a large bunch of literature missing from the paper. Some of the references are provided in the comments section.
3. Lack of screenshots of the experimental interface
Comments: 1. Please add screenshots of the interface that was designed.
2. Repetition of the word Tables in Line 549 3. In Appendix A.3, the GLEU metric is reference as GLUE.
Questions: 1. In table 1, is there any particular reason for the reduction in pass rate % from free run 1 and free run2?
2. What is the purpose of the average duration reported in Table 1? There is no supporting explanation about it. Does it include time spent by the user waiting for the model to generate a response?
3. With regards to the model section, is there any particular reason that there was an emphasis on choosing retriever-based transformer models over generative models? Even if the models are based on ConvAI2, there are other language modeling GPT2 based techniques that could have been picked.
4. In figure 6, what are the models in the last two columns lan_model_p and lan_model?
Missing References: 1. Howcroft, David M., Anja Belz, Miruna-Adriana Clinciu, Dimitra Gkatzia, Sadid A. Hasan, Saad Mahamood, Simon Mille, Emiel van Miltenburg, Sashank Santhanam, and Verena Rieser. "" Twenty years of confusion in human evaluation: NLG needs evaluation sheets and standardised definitions."" In Proceedings of the 13th International Conference on Natural Language Generation, pp. 169-182. 2020.
2. Santhanam, S. and Shaikh, S., 2019. Towards best experiment design for evaluating dialogue system output. arXiv preprint arXiv:1909.10122.
3. Santhanam, S., Karduni, A. and Shaikh, S., 2020, April. Studying the effects of cognitive biases in evaluation of conversational agents. In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems (pp. 1-13).
4. Novikova, J., Dušek, O. and Rieser, V., 2018. RankME: Reliable human ratings for natural language generation. arXiv preprint arXiv:1803.05928.
5. Li, M., Weston, J. and Roller, S., 2019. Acute-eval: Improved dialogue evaluation with optimized questions and multi-turn comparisons. arXiv preprint arXiv:1909.03087.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '2', '1', '1', '2', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '2', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', '5', 'X', '5', '1', 'X', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '3', '3', '1', '3', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers? I thank the authors for their response.",ACL_2017_726_review,ACL_2017,"- Claims of being comparable to state of the art when the results on GeoQuery and ATIS do not support it. General Discussion: This is a sound work of research and could have future potential in the way semantic parsing for downstream applications is done. I was a little disappointed with the claims of “near-state-of-the-art accuracies” on ATIS and GeoQuery, which doesn’t seem to be the case (8 points difference from Liang et. al., 2011)). And I do not necessarily think that getting SOTA numbers should be the focus of the paper, it has its own significant contribution. I would like to see this paper at ACL provided the authors tone down their claims, in addition I have some questions for the authors.
- What do the authors mean by minimal intervention? Does it mean minimal human intervention, because that does not seem to be the case. Does it mean no intermediate representation? If so, the latter term should be used, being less ambiguous.
- Table 6: what is the breakdown of the score by correctness and incompleteness?
What % of incompleteness do these queries exhibit?
- What is expertise required from crowd-workers who produce the correct SQL queries? - It would be helpful to see some analysis of the 48% of user questions which could not be generated.
- Figure 3 is a little confusing, I could not follow the sharp dips in performance without paraphrasing around the 8th/9th stages. - Table 4 needs a little more clarification, what splits are used for obtaining the ATIS numbers?
I thank the authors for their response.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '5', '5', '1', '4', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '3', '5', '5', '3', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', 'X', 'X', '5', '1', 'X', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '4', '5', '5', '1', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"4) You perform ""on par or better"" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to ""on par"" and all the rest to ""better"". I think this wording should be corrected, but otherwise I'm fine with the experimental results.",ACL_2017_105_review,ACL_2017,"Maybe the model is just an ordinary BiRNN with alignments de-coupled.
Only evaluated on morphology, no other monotone Seq2Seq tasks.
- General Discussion: The authors propose a novel encoder-decoder neural network architecture with ""hard monotonic attention"". They evaluate it on three morphology datasets.
This paper is a tough one. One the one hand it is well-written, mostly very clear and also presents a novel idea, namely including monotonicity in morphology tasks. The reason for including such monotonicity is pretty obvious: Unlike machine translation, many seq2seq tasks are monotone, and therefore general encoder-decoder models should not be used in the first place. That they still perform reasonably well should be considered a strong argument for neural techniques, in general. The idea of this paper is now to explicity enforce a monotonic output character generation. They do this by decoupling alignment and transduction and first aligning input-output sequences monotonically and then training to generate outputs in agreement with the monotone alignments.
However, the authors are unclear on this point. I have a few questions: 1) How do your alignments look like? On the one hand, the alignments seem to be of the kind 1-to-many (as in the running example, Fig.1), that is, 1 input character can be aligned with zero, 1, or several output characters. However, this seems to contrast with the description given in lines 311-312 where the authors speak of several input characters aligned to 1 output character. That is, do you use 1-to-many, many-to-1 or many-to-many alignments?
2) Actually, there is a quite simple approach to monotone Seq2Seq. In a first stage, align input and output characters monotonically with a 1-to-many constraint (one can use any monotone aligner, such as the toolkit of Jiampojamarn and Kondrak). Then one trains a standard sequence tagger(!) to predict exactly these 1-to-many alignments. For example, flog->fliege (your example on l.613): First align as in ""f-l-o-g / f-l-ie-ge"". Now use any tagger (could use an LSTM, if you like) to predict ""f-l-ie-ge"" (sequence of length 4) from ""f-l-o-g"" (sequence of length 4). Such an approach may have been suggested in multiple papers, one reference could be [*, Section 4.2] below.
My two questions here are: 2a) How does your approach differ from this rather simple idea?
2b) Why did you not include it as a baseline?
Further issues: 3) It's really a pitty that you only tested on morphology, because there are many other interesting monotonic seq2seq tasks, and you could have shown your system's superiority by evaluating on these, given that you explicitly model monotonicity (cf. also [*]).
4) You perform ""on par or better"" (l.791). There seems to be a general cognitive bias among NLP researchers to map instances where they perform worse to ""on par"" and all the rest to ""better"". I think this wording should be corrected, but otherwise I'm fine with the experimental results.
5) You say little about your linguistic features: From Fig. 1, I infer that they include POS, etc. 5a) Where did you take these features from?
5b) Is it possible that these are responsible for your better performance in some cases, rather than the monotonicity constraints?
Minor points: 6) Equation (3): please re-write $NN$ as $\text{NN}$ or similar 7) l.231 ""Where"" should be lower case 8) l.237 and many more: $x_1\ldots x_n$. As far as I know, the math community recommends to write $x_1,\ldots,x_n$ but $x_1\cdots x_n$. That is, dots should be on the same level as surrounding symbols.
9) Figure 1: is it really necessary to use cyrillic font? I can't even address your example here, because I don't have your fonts.
10) l.437: should be ""these"" [*] @InProceedings{schnober-EtAl:2016:COLING, author = {Schnober, Carsten and Eger, Steffen and Do Dinh, Erik-L\^{a}n and Gurevych, Iryna}, title = {Still not there? Comparing Traditional Sequence-to-Sequence Models to Encoder-Decoder Neural Networks on Monotone String Translation Tasks}, booktitle = {Proceedings of COLING 2016, the 26th International Conference on Computational Linguistics: Technical Papers}, month = {December}, year = {2016}, address = {Osaka, Japan}, publisher = {The COLING 2016 Organizing Committee}, pages = {1703--1714}, url = {http://aclweb.org/anthology/C16-1160} } AFTER AUTHOR RESPONSE Thanks for the clarifications. I think your alignments got mixed up in the response somehow (maybe a coding issue), but I think you're aligning 1-0, 0-1, 1-1, and later make many-to-many alignments from these.
I know that you compare to Nicolai, Cherry and Kondrak (2015) but my question would have rather been: why not use 1-x (x in 0,1,2) alignments as in Schnober et al. and then train a neural tagger on these (e.g. BiLSTM). I wonder how much your results would have differed from such a rather simple baseline. ( A tagger is a monotone model to start with and given the monotone alignments, everything stays monotone. In contrast, you start out with a more general model and then put hard monotonicity constraints on this ...) NOTES FROM AC Also quite relevant is Cohn et al. (2016), http://www.aclweb.org/anthology/N16-1102 .
Isn't your architecture also related to methods like the Stack LSTM, which similarly predicts a sequence of actions that modify or annotate an input? Do you think you lose anything by using a greedy alignment, in contrast to Rastogi et al. (2016), which also has hard monotonic attention but sums over all alignments?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '3', '5', '5', '3', '4', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '2', '5', '5', '3', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '1', '5', '5', '4', '4', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '2', '5', '5', '3', '4', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.",ARR_2022_12_review,ARR_2022,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below).
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '1', '5', '1', '1', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '1', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', 'X', 'X', '5', '1', '1', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '2', '5', '1', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.",ARR_2022_311_review,ARR_2022,"__1. Lack of significance test:__ I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results.
__2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)?
__4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result?
__4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample?
1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '5', '5', '4', '4', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '4', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', 'X', '5', '2', '4', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '3', '4', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.",ACL_2017_818_review,ACL_2017,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '3', '5', '4', '4', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '3', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', '5', 'X', '5', '3', '5', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '5', '4', '5', '3', '3', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- Lack of novelty:- Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.,ARR_2022_317_review,ARR_2022,"- Lack of novelty: - Adversarial attacks by perturbing text has been done on many NLP models and image-text models. It is nicely summarized in related work of this paper. The only new effort is to take similar ideas and apply it on video-text models.
- Checklist (Ribeiro et. al., ACL 2020) had shown many ways to stress test NLP models and evaluate them. Video-text models could also be tested on some of those dimensions. For instance on changing NER.
- If you could propose any type of perturbation which is specific to video-text models (and probably not that important to image-text or just text models) will be interesting to see. Otherwise, this work, just looks like a using an already existing method on this new problem (video-text) which is just coming up.
- Is there a way to take any clue from the video to create harder negatives.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '1', '1', '1', '2', '1', '1']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '2', '1', '1', '2', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '1', '3', 'X', 'X', '2', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '2', '2', '1', '3', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by:",ACL_2017_31_review,ACL_2017,"] See below for details of the following weaknesses: - Novelties of the paper are relatively unclear.
- No detailed error analysis is provided.
- A feature comparison with prior work is shallow, missing two relevant papers.
- The paper has several obscure descriptions, including typos.
[General Discussion:] The paper would be more impactful if it states novelties more explicitly. Is the paper presenting the first neural network based approach for event factuality identification? If this is the case, please state that.
The paper would crystallize remaining challenges in event factuality identification and facilitate future research better if it provides detailed error analysis regarding the results of Table 3 and 4. What are dominant sources of errors made by the best system BiLSTM+CNN(Att)? What impacts do errors in basic factor extraction (Table 3) have on the overall performance of factuality identification (Table 4)? The analysis presented in Section 5.4 is more like a feature ablation study to show how useful some additional features are.
The paper would be stronger if it compares with prior work in terms of features. Does the paper use any new features which have not been explored before? In other words, it is unclear whether main advantages of the proposed system come purely from deep learning, or from a combination of neural networks and some new unexplored features. As for feature comparison, the paper is missing two relevant papers: - Kenton Lee, Yoav Artzi, Yejin Choi and Luke Zettlemoyer. 2015 Event Detection and Factuality Assessment with Non-Expert Supervision. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1643-1648.
- Sandeep Soni, Tanushree Mitra, Eric Gilbert and Jacob Eisenstein. 2014.
Modeling Factuality Judgments in Social Media Text. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 415-420.
The paper would be more understandable if more examples are given to illustrate the underspecified modality (U) and the underspecified polarity (u). There are two reasons for that. First, the definition of 'underspecified' is relatively unintuitive as compared to other classes such as 'probable' or 'positive'.
Second, the examples would be more helpful to understand the difficulties of Uu detection reported in line 690-697. Among the seven examples (S1-S7), only S7 corresponds to Uu, and its explanation is quite limited to illustrate the difficulties.
A minor comment is that the paper has several obscure descriptions, including typos, as shown below: - The explanations for features in Section 3.2 are somewhat intertwined and thus confusing. The section would be more coherently organized with more separate paragraphs dedicated to each of lexical features and sentence-level features, by: - (1) stating that the SIP feature comprises two features (i.e., lexical-level and sentence-level) and introduce their corresponding variables (l and c) *at the beginning*; - (2) moving the description of embeddings of the lexical feature in line 280-283 to the first paragraph; and - (3) presenting the last paragraph about relevant source identification in a separate subsection because it is not about SIP detection.
- The title of Section 3 ('Baseline') is misleading. A more understandable title would be 'Basic Factor Extraction' or 'Basic Feature Extraction', because the section is about how to extract basic factors (features), not about a baseline end-to-end system for event factuality identification.
- The presented neural network architectures would be more convincing if it describes how beneficial the attention mechanism is to the task.
- Table 2 seems to show factuality statistics only for all sources. The table would be more informative along with Table 4 if it also shows factuality statistics for 'Author' and 'Embed'.
- Table 4 would be more effective if the highest system performance with respect to each combination of the source and the factuality value is shown in boldface.
- Section 4.1 says, ""Aux_Words can describe the *syntactic* structures of sentences,"" whereas section 5.4 says, ""they (auxiliary words) can reflect the *pragmatic* structures of sentences."" These two claims do not consort with each other well, and neither of them seems adequate to summarize how useful the dependency relations 'aux' and 'mark' are for the task.
- S7 seems to be another example to support the effectiveness of auxiliary words, but the explanation for S7 is thin, as compared to the one for S6. What is the auxiliary word for 'ensure' in S7?
- Line 162: 'event go in S1' should be 'event go in S2'.
- Line 315: 'in details' should be 'in detail'.
- Line 719: 'in Section 4' should be 'in Section 4.1' to make it more specific.
- Line 771: 'recent researches' should be 'recent research' or 'recent studies'. 'Research' is an uncountable noun.
- Line 903: 'Factbank' should be 'FactBank'.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '4', '4', '3', '5', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '4', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '4', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space.",ACL_2017_779_review,ACL_2017,"However, there are many points that need to be address before this paper is ready for publication.
1) Crucial information is missing Can you flesh out more clearly how training and decoding happen in your training framework? I found out that the equations do not completely describe the approach. It might be useful to use a couple of examples to make your approach clearer.
Also, how is the montecarlo sampling done? 2) Organization The paper is not very well organized. For example, results are broken into several subsections, while they’d better be presented together.  The organization of the tables is very confusing. Table 7 is referred before table 6. This made it difficult to read the results.
3) Inconclusive results: After reading the results section, it’s difficult to draw conclusions when, as the authors point out in their comparisons, this can be explained by the total size of the corpus involved in their methods (621  ). 4) Not so useful information: While I appreciate the fleshing out of the assumptions, I find that dedicating a whole section of the paper plus experimental results is a lot of space. - General Discussion: Other: 578:  We observe that word-level models tend to have lower valid loss compared with sentence- level methods….
Is it valid to compare the loss from two different loss functions?
Sec 3.2, the notations are not clear. What does script(Y) means?
How do we get p(y|x)? this is never explained Eq 7 deserves some explanation, or better removed.
320: What approach did you use? You should talk about that here 392 : Do you mean 2016?
Nitty-gritty: 742  : import => important 772  : inline citation style 778: can significantly outperform 275: Assumption 2 needs to be rewritten … a target sentence y from x should be close to that from its counterpart z.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '2', '3', '3', '1', '1', '1', '2']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '4', '3', '4', '1', '1', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '4', '3', 'X', 'X', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '2', '3', '2', '1', '1', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '0', '1', '1', '1', '1', '1', '0']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.",ARR_2022_82_review,ARR_2022,"- In the “Updating Facts” section, although the results seem to show that modifying the neurons using the word embeddings is effective, the paper lacks a discussion on this. It is not intuitive to me that there is a connection between a neuron at a middle layer and the word embeddings (which are used at the input layer). - Using integrated gradients to measure the attribution has been studied in existing papers. The paper also proposes post-processing steps to filter out the “false-positive” neurons, however, the paper doesn’t show how important these post-processing steps are. I think an ablation study may be needed.
- The paper lacks details of experimental settings. For example, how are those hyperparameters ($t$, $p$, $\lambda_1$, etc.) tuned? In table 5, why do “other relations” have a very different scale of perplexity compared to “erased relation” before erasing? Are “other relations” randomly selected?
- The baseline method (i.e., using activation values as the attribution score) is widely used in previous studies. Although the paper empirically shows that the baseline is not as effective as the proposed method, - I expect more discussion on why using activation values is not a good idea.
- One limitation of this study is that the paper only focuses on single-word cloze queries (as discussed in the paper).
- Figure 3: The illustration is not clear to me. Why are there two “40%” in the figure?
- I was confused that the paper targets single-token cloze queries or multi-token ones. I did not see a clear clarification until reading the conclusion.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '5', '1', '3', '1', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '5', '1', '2', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '1', '3', '1', 'X', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '2', '4', '1', '4', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1. How will an antecedent be identified, when the prediction is a pronoun? The authors proposed a method by matching the head of noun phrases. It’s not clear how to handle the situation when the head word is not a pronoun.",ACL_2017_19_review,ACL_2017,"But I have a few questions regarding finding the antecedent of a zero pronoun: 1. How will an antecedent be identified, when the prediction is a pronoun? The authors proposed a method by matching the head of noun phrases. It’s not clear how to handle the situation when the head word is not a pronoun.
2. What if the prediction is a noun that could not be found in the previous contents?
3. The system achieves great results on standard data set. I’m curious is it possible to evaluate the system in two steps? The first step is to evaluate the performance of the model prediction, i.e. to recover the dropped zero pronoun into a word; the second step is to evaluate how well the systems works on finding an antecedent.
I’m also curious why the authors decided to use attention-based neural network. A few sentences to provide the reasons would be helpful for other researchers.
A minor comment: In figure 2, should it be s1, s2 … instead of d1, d2 ….? - General Discussion: Overall it is a great paper with innovative ideas and solid experiment setup.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '2', '2', '1', '1', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '3', '3', '2', '1', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', '3', '2', '4', '3', 'X', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '3', '2', '2', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '0', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.",ACL_2017_318_review,ACL_2017,"1. Presentation and clarity: important details with respect to the proposed models are left out or poorly described (more details below). Otherwise, the paper generally reads fairly well; however, the manuscript would need to be improved if accepted.
2. The evaluation on the word analogy task seems a bit unfair given that the semantic relations are explicitly encoded by the sememes, as the authors themselves point out (more details below).
- General Discussion: 1. The authors stress the importance of accounting for polysemy and learning sense-specific representations. While polysemy is taken into account by calculating sense distributions for words in particular contexts in the learning procedure, the evaluation tasks are entirely context-independent, which means that, ultimately, there is only one vector per word -- or at least this is what is evaluated. Instead, word sense disambiguation and sememe information are used for improving the learning of word representations. This needs to be clarified in the paper.
2. It is not clear how the sememe embeddings are learned and the description of the SSA model seems to assume the pre-existence of sememe embeddings. This is important for understanding the subsequent models. Do the SAC and SAT models require pre-training of sememe embeddings?
3. It is unclear how the proposed models compare to models that only consider different senses but not sememes. Perhaps the MST baseline is an example of such a model? If so, this is not sufficiently described (emphasis is instead put on soft vs. hard word sense disambiguation). The paper would be stronger with the inclusion of more baselines based on related work.
4. A reasonable argument is made that the proposed models are particularly useful for learning representations for low-frequency words (by mapping words to a smaller set of sememes that are shared by sets of words). Unfortunately, no empirical evidence is provided to test the hypothesis. It would have been interesting for the authors to look deeper into this. This aspect also does not seem to explain the improvements much since, e.g., the word similarity data sets contain frequent word pairs.
5. Related to the above point, the improvement gains seem more attributable to the incorporation of sememe information than word sense disambiguation in the learning procedure. As mentioned earlier, the evaluation involves only the use of context-independent word representations. Even if the method allows for learning sememe- and sense-specific representations, they would have to be aggregated to carry out the evaluation task.
6. The example illustrating HowNet (Figure 1) is not entirely clear, especially the modifiers of ""computer"".
7. It says that the models are trained using their best parameters. How exactly are these determined? It is also unclear how K is set -- is it optimized for each model or is it randomly chosen for each target word observation? Finally, what is the motivation for setting K' to 2?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '3', '2', '3', '1', '5', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '3', '2', '2', '5', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '3', '1', '2', '5', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '2', '1', '1', '5', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '0']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The abstract is written well and invokes intrigue early - could potentially be made even better if, for ""evaluating with gold answers is inconsistent with human evaluation"" - an example of the inconsistency, such as models get ranked differently is also given there.",ARR_2022_227_review,ARR_2022,"1. The case made for adopting the proposed strategy for a new automated evaluation paradigm - auto-rewrite (where the questions that are not valid due to a coreference resolution failure in terms of the previous answer get their entity replaced to be made consistent with the gold conversational history) - seems weak. While the proposed strategy does seem to do better in terms of being closer to how humans evaluated the 4 models (all in the context of one specific English dataset), it is not clear how the proposed strategy - a) does better than the previously proposed strategy of using model-predicted history (auto-pred). Looking at the comparison results for different evaluations - in terms of table 1, there definitely does not seem to be much difference between the two strategies (auto-rewrite and auto-pred). In fig 5, for some (2/6) pairs, the pred-history strategy has higher agreement than the proposed auto-rewrite strategy while they are all at the same agreement for 1/6 pairs. b) gets to the fundamental problem with automated evaluation raised in the paper, which is that ""when placed in realistic settings, the models never have access to the ground truth (gold answers) and are only exposed to the conversational history and the passage."" The proposed strategy seems to need gold answers as well, which is incompatible with the real-world use case. The previously proposed auto-pred strategy, however, uses only the questions and the model's own predictions to form the conversational history - which seems to be more compatible with the real-world use case. In summary, it is not clear why the proposed new way of automatically evaluating CQA systems is better or should be adopted as opposed to the previously proposed automated evaluation method of using a model's predictions as the conversational history (auto-pred), and the comparison between the results for these two automated strategies seems to be a missing exploration and discussion.
Questions to the authors (which also act as suggestions): Q1. - Line 151: ""four representative CQA models"" - what does representative mean here? representative in what sense? In terms of types or architectures of models? This needs clarification and takes on importance because the discrepancy, in terms of how models get evaluated on human vs automated evaluation, depends on these four models in a sense. Q2. Line 196: ""We noticed that the annotators are biased when evaluating the correctness of answers"" - are any statistics on this available? Q3. Section 3.1: For Mechanical Turk crowdsourcing work, what was the compensation rate for the annotators? This should be mentioned, if not in the main text, then add to appendix and point to it in the main text. Also, following the work in Card et al (2020) (""With Little Power Comes Great Responsibility."") - were there any steps taken to ensure the human annotation collection study was appropriately powered? ( If not, consider noting or discussing this somewhere in the paper as it helps with understanding the validity of human experiments) Q4. Lines 264-265: ""The gap between HAM and ExCorD is significant in Auto-Gold"" - how is significance measured here?
Q5. Lines 360-364: ""We determine whether e∗_j = e_j by checking if F1(s∗_{j,1}, s_{j,1}) > 0 .... .... as long as their first mentions have word overlap."" Two questions here - 5a. It is not clear why word overlap was used and not just an exact match here? What about cases where there is some word overlap but the two entities are indeed different, and therefore, the question is invalid (in terms of coreference resolution) but deemed valid?
5b. How accurate is this invalid question detection strategy? In case this has not already been measured, perhaps a sample of instances where predicted history invalidates questions via unresolved coreference (marked by humans) can be used to then detect if the automated method catches these instances accurately. Having some idea of how well invalid question detection happens is needed to get a sense of if or how many of the invalid questions will get rewritten. Comments, suggestions, typos: - Line 031: ""has the promise to revolutionize"" - this should be substantiated further, seems quite vague. - Line 048: ""extremely competitive performance of"" - what is 'performance' for these systems? ideally be specific since, at this point in the paper, we do not know what is being measured, and 'extremely competitive' is also quite vague. - The abstract is written well and invokes intrigue early - could potentially be made even better if, for ""evaluating with gold answers is inconsistent with human evaluation"" - an example of the inconsistency, such as models get ranked differently is also given there. - Line 033: ""With recent development of large-scale datasets"" -> the* recent development, but more importantly - which languages are these datasets in? And for this overall work on CQA, the language which is focused on should be mentioned early on in the introduction and ideally in the abstract itself. - Line 147: ""more modeling work has been done than in free-form question answering"" - potential typo, maybe it should be ""maybe more modeling work has been done 'in that'"" - where that refers to extractive QA?
- Line 222: ""In total, we collected 1,446 human-machine con- versations and 15,059 question-answer pairs"" - suggestion: It could be reasserted here that this dataset will be released as this collection of conversations is an important resource and contribution and does not appear to have been highlighted as much as it could. - Figure 2: It is a bit unintuitive and confusing to see the two y-axes with different ranges and interpret what it means for the different model evaluations. Can the same ranges on the y-axes be used at least even if the two metrics are different? Perhaps the F1 can use the same range as Accuracy - it would mean much smaller gold bars but hopefully, still get the point across without trying to keep two different ranges in our head? Still, the two measures are different - consider making two side-by-side plots instead if that is feasible instead of both evaluations represented in the same chart. - Lines 250-252: ""the absolute numbers of human evaluation are much higher than those of automatic evaluations"" - saying this seems a bit suspect - what does absolute accuracy numbers being higher than F1 scores mean? They are two different metrics altogether and should probably not be compared in this manner. Dropping this, the other implications still hold well and get the point across - the different ranking of certain models, and Auto-Gold conveying a gap between two models where Human Eval does not. - Line 348: ""background 4, S∗ - latex styling suggestion, add footnote marker only right after the punctuation for that renders better with latex, so - ""background,\footnote{} ..."" in latex.
- Footnote 4: 'empirically helpful' - should have a cite or something to back that there. - Related Work section: a suggestion that could make this section but perhaps also the broader work stronger and more interesting to a broader audience is making the connection to how this work fits with other work looking at different NLP tasks that looks at failures of the popular automated evaluation strategy or metrics failing to capture or differing significantly from how humans would evaluate systems in a real-world setting.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '3', '2', '2', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '2', '2', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', '4', '1', '4', '3', '2', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '5', '2', '5', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.,ACL_2017_818_review,ACL_2017,"- I would have liked to see more examples of objects pairs, action verbs, and predicted attribute relations. What are some interesting action verbs and corresponding attributes relations? The paper also lacks analysis/discussion on what kind of mistakes their model makes.
- The number of object pairs (3656) in the dataset is very small. How many distinct object categories are there? How scalable is this approach to larger number of object pairs?
- It's a bit unclear how the frame similarity factors and attributes similarity factors are selected.
General Discussion/Suggestions: - The authors should discuss the following work and compare against mining attributes/attribute distributions directly and then getting a comparative measure. What are the advantages offered by the proposed method compared to a more direct approach?
Extraction and approximation of numerical attributes from the Web Dmitry Davidov, Ari Rappoport ACL 2010 Minor typos: 1. In the abstract (line 026), the authors mention 'six' dimensions, but in the paper, there is only five.
2. line 248: Above --> The above 3. line 421: object first --> first 4. line 654: more skimp --> a smaller 5. line 729: selctional --> selectional","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '2', '4', '1', '1', '1', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '1', '2', '5', '1', '1', '2', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '1', '1', '1', '1', 'X', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '3', '1', '5', '1', '1', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '0', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.",ACL_2017_699_review,ACL_2017,"1. Some discussions are required on the convergence of the proposed joint learning process (for RNN and CopyRNN), so that readers can understand, how the stable points in probabilistic metric space are obtained? Otherwise, it may be tough to repeat the results.
2. The evaluation process shows that the current system (which extracts 1.
Present and 2. Absent both kinds of keyphrases) is evaluated against baselines (which contains only ""present"" type of keyphrases). Here there is no direct comparison of the performance of the current system w.r.t. other state-of-the-arts/benchmark systems on only ""present"" type of key phrases. It is important to note that local phrases (keyphrases) are also important for the document. The experiment does not discuss it explicitly. It will be interesting to see the impact of the RNN and Copy RNN based model on automatic extraction of local or ""present"" type of key phrases.
3. The impact of document size in keyphrase extraction is also an important point. It is found that the published results of [1], (see reference below) performs better than (with a sufficiently high difference) the current system on Inspec (Hulth, 2003) abstracts dataset. 4. It is reported that current system uses 527,830 documents for training, while 40,000 publications are held out for training baselines. Why are all publications not used in training the baselines? Additionally, The topical details of the dataset (527,830 scientific documents) used in training RNN and Copy RNN are also missing. This may affect the chances of repeating results.
5. As the current system captures the semantics through RNN based models. So, it would be better to compare this system, which also captures semantics. Even, Ref-[2] can be a strong baseline to compare the performance of the current system.
Suggestions to improve: 1. As, per the example, given in the Figure-1, it seems that all the ""absent"" type of key phrases are actually ""Topical phrases"". For example: ""video search"", ""video retrieval"", ""video indexing"" and ""relevance ranking"", etc.
These all define the domain/sub-domain/topics of the document. So, In this case, it will be interesting to see the results (or will be helpful in evaluating ""absent type"" keyphrases): if we identify all the topical phrases of the entire corpus by using tf-idf and relate the document to the high-ranked extracted topical phrases (by using Normalized Google Distance, PMI, etc.). As similar efforts are already applied in several query expansion techniques (with the aim to relate the document with the query, if matching terms are absent in document).
Reference: 1. Liu, Zhiyuan, Peng Li, Yabin Zheng, and Maosong Sun. 2009b. Clustering to find exemplar terms for keyphrase extraction. In Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 257–266.
2. Zhang, Q., Wang, Y., Gong, Y., & Huang, X. (2016). Keyphrase extraction using deep recurrent neural networks on Twitter. In Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (pp. 836-845).","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '4', '4', '5', '4', '4', '1', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '3', '5', '4', '2', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', 'X', '4', '4', '2', 'X', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '4', '4', '5', '3', '4', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?",ACL_2017_818_review,ACL_2017,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '4', '5', '5', '4', '3', '5', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '3', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', 'X', '1', '4', '4', '2', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '4', '4', '4', '3', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The sentence in line 212 (""We train a GRU that encodes a source sentence into a single vector"") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.",ACL_2017_49_review,ACL_2017,"As always, more could be done in the experiments section to strengthen the case for chunk-based models. For example, Table 3 indicates good results for Model 2 and Model 3 compared to previous papers, but a careful reader will wonder whether these improvements come from switching from LSTMs to GRUs. In other words, it would be good to see the GRU tree-to-sequence result to verify that the chunk-based approach is still best.
Another important aspect is the lack of ensembling results. The authors put a lot of emphasis is claiming that this is the best single NMT model ever published. While this is probably true, in the end the best WAT system for Eng-Jap is at 38.20 (if I'm reading the table correctly) - it's an ensemble of 3. If the authors were able to report that their 3-way chunk-based ensemble comes top of the table, then this paper could have a much stronger impact.
Finally, Table 3 would be more interesting if it included decoding times. The authors mention briefly that the character-based model is less time-consuming (presumably based on Eriguchi et al.'16), but no cite is provided, and no numbers from chunk-based decoding are reported either. Is the chunk-based model faster or slower than word-based? Similar? Who know... Adding a column to Table 3 with decoding times would give more value to the paper.
- General Discussion: Overall I think the paper is interesting and worth publishing. I have minor comments and suggestions to the authors about how to improve their presentation (in my opinion, of course). - I think they should clearly state early on that the chunks are supplied externally - in other words, that the model does not learn how to chunk. This only became apparent to me when reading about CaboCha on page 6 - I don't think it's mentioned earlier, and it is important.
- I don't see why the authors contrast against the char-based baseline so often in the text (at least a couple of times they boast a +4.68 BLEU gain). I don't think readers are bothered... Readers are interested in gains over the best baseline.
- It would be good to add a bit more detail about the way UNKs are being handled by the neural decoder, or at least add a citation to the dictionary-based replacement strategy being used here.
- The sentence in line 212 (""We train a GRU that encodes a source sentence into a single vector"") is not strictly correct. The correct way would be to say that you do a bidirectional encoder that encodes the source sentence into a set of vectors... at least, that's what I see in Figure 2.
- The motivating example of lines 69-87 is a bit weird. Does ""you"" depend on ""bite""? Or does it depend on the source side? Because if it doesn't depend on ""bite"", then the argument that this is a long-dependency problem doesn't really apply.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '4', '5', '5', '5', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '1', '4', '5', '5', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '5', '5', '5', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) -,ACL_2017_494_review,ACL_2017,"- fairly straightforward extension of existing retrofitting work - would be nice to see some additional baselines (e.g. character embeddings) - General Discussion: The paper describes ""morph-fitting"", a type of retrofitting for vector spaces that focuses specifically on incorporating morphological constraints into the vector space. The framework is based on the idea of ""attract"" and ""repel"" constraints, where attract constraints are used to pull morphological variations close together (e.g. look/looking) and repel constraints are used to push derivational antonyms apart (e.g. responsible/irresponsible). They test their algorithm on multiple different vector spaces and several language, and show consistent improvements on intrinsic evaluation (SimLex-999, and SimVerb-3500). They also test on the extrinsic task of dialogue state tracking, and again demonstrate measurable improvements over using morphologically-unaware word embeddings.
I think this is a very nice paper. It is a simple and clean way to incorporate linguistic knowledge into distributional models of semantics, and the empirical results are very convincing. I have some questions/comments below, but nothing that I feel should prevent it from being published.
- Comments for Authors 1) I don't really understand the need for the morph-simlex evaluation set. It seems a bit suspect to create a dataset using the same algorithm that you ultimately aim to evaluate. It seems to me a no-brainer that your model will do well on a dataset that was constructed by making the same assumptions the model makes. I don't think you need to include this dataset at all, since it is a potentially erroneous evaluation that can cause confusion, and your results are convincing enough on the standard datasets.
2) I really liked the morph-fix baseline, thank you for including that. I would have liked to see a baseline based on character embeddings, since this seems to be the most fashionable way, currently, to side-step dealing with morphological variation. You mentioned it in the related work, but it would be better to actually compare against it empirically.
3) Ideally, we would have a vector space where morphological variants are just close together, but where we can assign specific semantics to the different inflections. Do you have any evidence that the geometry of the space you end with is meaningful. E.g. does ""looking"" - ""look"" + ""walk"" = ""walking""? It would be nice to have some analysis that suggests the morphfitting results in a more meaningful space, not just better embeddings.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '1', '4', '3', '2', '3', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '3', '1', '4', '1', '2', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '1', '3', '1', '2', 'X', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '4', '1', '4', '1', '3', '3', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8. Line 277: “The may be attributed…” -> “This may be attributed…",ARR_2022_68_review,ARR_2022,"1. Despite the well-motivated problem formulation, the simulation is not realistic. The author does not really collect feedback from human users but derives them from labeled data. One can imagine users can find out that returned answers are contrastive to commonsense. For instance, one can know that “Tokyo” is definitely a wrong answer to the question “What is the capital of South Africa?”. However, it is not very reasonable to assume that the users are knowledgeable enough to provide both positive and negative feedback. If so, why do they need to ask QA models? And what is the difference between collecting feedback and labeling data?
In conclusion, it would be more realistic to assume that only a small portion of the negative feedback is trustworthy, and there may be little or no reliable positive feedback. According to the experiment results, however, 20% of feedback perturbation makes the proposed method fail.
Therefore, the experiment results cannot support the claim made by authors.
2. There is a serious issue of missing related work. As mentioned above, Campos et al. (2020) has already investigated using user feedback to fine-tune deployed QA models. They also derive feedback from gold labels and conduct experiments with both in-domain and out-of-domain evaluation. The proposed methods are also similar: upweighting or downweighting the likelihood of predicted answer according to the feedback.
Moreover, Campos et al. (2020) has a more reasonable formulation, where there could be multiple feedback for a certain pair of questions and predicted answers. 3. The adopted baseline models are weak. First of all, the author does not compare to Campos et al. (2020), which also uses feedback in QA tasks. Second, they also do no comparison with other domain adaptation methods, such as those work cited in Section 8.
Line 277: “The may be attributed…” -> “This may be attributed…","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '4', '1', '3', '1', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '5', '1', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '5', 'X', '5', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '5', '3', '3', '3', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '0', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- In figure 5, the y-axis label may use ""Exact Match ratio"" directly.",ARR_2022_113_review,ARR_2022,"The methodology part is a little bit unclear. The author could describe clearly how the depth-first path completion really works using Figure 3. Also, I'm not sure if the ZIP algorithm is proposed by the authors and also confused about how the ZIP algorithm handles multiple sequence cases.
- Figure 2, it is not clear about ""merge target"". If possible, you may use a shorter sentence.
- Line 113 (right column), will the lattice graph size explode? For a larger dataset, it may impossible to just get the lattice graph, am I right? How should you handle that case?
- Algorithm 1 step 4 and 5, you may need to give the detailed steps of *isRecomb* and *doRecomb* in the appendix.
- Line 154 left, ""including that it optimizes for the wrong objective"". Can you clearly state what objective? why the beam search algorithm is wrong? Beam search is a greedy algorithm that can recover the best output with high probability.
- For the ZIP method, one thing unclear to me is how you combine multiple sequences by if they have different lengths of shared suffixes?
- Line 377, is BFSZIP an existing work? If so, you need to cite their work. - In figure 5, the y-axis label may use ""Exact Match ratio"" directly.
- Line 409, could you cite the ""R2"" metric?
- Appendix A, the authors state ""better model score cannot result in better hypothesis"". You'd better state clearly what idea hypothesis you want. "" a near-optimal model score"" this sentence is unclear to me, could you explain in detail?
- In line 906, it is clear from the previous papers that Beam search results lack diversity and increase the beam size does not work. Can you simplify the paragraph?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '4', '5', '1', '3', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '3', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '1', 'X', '5', '5', 'X', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '4', '5', '5', '3', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction. Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig.",ARR_2022_149_review,ARR_2022,"- The attribute-based approach can be useful if the attribute is given. This limits the application of the proposed approach if there is no attribute given but the text is implicitly offensive.
- It is not described if the knowledge bases that are inserted in are free from societal biases, or the issue is not affected by such restriction.
Comments - I like attacking implicit offensive texts with reasoning chains, but not yet convinced with the example of Fig. 1. If other contexts such as 'S1 is fat/poor' is not given, then the conversation between S1 and S2 seems quite natural. The addressee may ask if bookclubs provide free food without offensive intention. If the word of S2 is to be decided as 'implicitly offensive', then one of the reasoning chain, such as 'you are fat/poor', should be provided as a context. If I correctly understood, I recommend the authors to change the example to more clear-cut and non-ambiguous one.
- I found some issues in the provided example; i) AIR is still written as ASR, and ii) there are some empty chains in the full file. Hope this to be checked in the final release.
Suggestions - It would be nice if the boundary between explicit and implicit offensive texts is stated clearly. Is it decided as explicit if the statement contains specific terms and offensive? Or specific expression / sentence structure?
- Please be more specific on the 'Chain of Reasoning' section, especially line 276.
- Please describe more on MNLI corpus, on the reason why the dataset is utilized in the training of entailment system.
Typos - TweetEval << two 'L's in line 349","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '4', '2', '4', '1', '1', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '4', '4', '2', '2', '5', '2', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '3', 'X', '2', '4', '2', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '3', '2', '2', '4', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '0', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '1', '0', '0', '1', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.",ACL_2017_365_review,ACL_2017,"1) Instead of arguing that the MTL approach replaces the attention mechanism, I think the authors should investigate why attention did not work on MTL, and perhaps modify the attention mechanism so that it would not harm performance.
2) I think the authors should reference past seq2seq MTL work, such as [2] and [3]. The MTL work in [2] also worked on non-attention seq2seq models.
3) This paper only tested on one German historical text data set of 44 documents. It would be interesting if the authors can evaluate the same approach in another language or data set.
References: [1] Allen Schmaltz, Yoon Kim, Alexander M. Rush, and Stuart Shieber. 2016.
Sentence-level grammatical error identification as sequence-to-sequence correction. In Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications.
[2] Minh-Thang Luong, Ilya Sutskever, Quoc V. Le, Oriol Vinyals, and Lukasz Kaiser. Multi-task Sequence to Sequence Learning. ICLR’16. [3] Dong, Daxiang, Wu, Hua, He, Wei, Yu, Dianhai, and Wang, Haifeng.
Multi-task learning for multiple language translation. ACL'15 --------------------------- Here is my reply to the authors' rebuttal: I am keeping my review score of 3, which means I do not object to accepting the paper. However, I am not raising my score for 2 reasons: - the authors did not respond to my questions about other papers on seq2seq MTL, which also avoided using attention mechanism. So in terms of novelty, the main novelty lies in applying it to text normalization.
- it is always easier to show something (i.e. attention in seq2seq MTL) is not working, but the value would lie in finding out why it fails and changing the attention mechanism so that it works.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '4', '3', '3', '1', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '3', '2', '1', '2', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', 'X', '2', '1', '2', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '4', '1', '3', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?",ARR_2022_202_review,ARR_2022,"1. The write-up has many typos and some formulas/explanations are confusing.
2. The technical innovation of the proposed method is limited. The proposed objective function is basically a combination of two related works with tiny changes.
3. Reproductivity is not ideal, as some essential parts are not addressed in the paper, such as training data.
4. More strong baselines should be included/discussed in the experiments.
Comments 1. Line 18: it’s better to specify the exact tasks rather than stating several tasks in the abstract 2. Line 69: “current” -> “currently” 3. Line 112: margin-based loss can use one positive with MULTIPLE negatives.
4. Figure 1: the relation types are not explicitly modeled in this work so the figure is kind of confusing.
5. Eq. 1: what is z_p 6. Eq. 2: how do you convert BERT token embeddings into event embeddings? Concatenation? Any pooling?
7. Line 236-237: “conciser” -> “consider” 8. Line 209-211: I can understand what you mean but this part should be re-written. For example, “given an anchor event, we generate 3 positive samples with different dropout masks.”
9. Table 1: needs to include a random baseline to show the task difficulty 10. Experiments: what training data you use for pre-training?
11. Table 1: do the baselines and the proposed method use the same training data for pre-training? How did you get the results for the baselines? Did you have your own implementation, or directly use their released embeddings/code?
12. Line 450: L_{pc} or L_{cp}| in Eq. 7?
13. Table 2: what’s the difference between “SWCC w/o Prototype-based Clustering” and “BERT(InfoNCE)”?
14. Table 3: MCNC should have many strong baselines that are not compared here, such as the baselines in [1]. Can you justify the reason?
15. Can you provide an analysis on the impact of the number of augmented samples (e.g., z_{a1}, z_{a2}) here?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '4', '5', '3', '5', '3', '2', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '5', '5', '2', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '5', '5', '2', '1', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '4', '5', '3', '2', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.,ARR_2022_303_review,ARR_2022,"- Citation type recognition is limited to two types –– dominant and reference –– which belies the complexity of the citation function, which is a significant line of research by other scholars. However this is more of a choice of the research team in limiting the scope of research.
- Relies on supplemental space to contain the paper. The paper is not truly independent given this problem (esp. S3.1 reference to Sup. Fig. 6) and again later as noted with the model comparison and other details of the span vs. sentence investigation.
- The previous report of SciBERT were removed, but this somewhat exacerbates the earlier problem in v1 where the analyses of the outcomes of the models was too cursory and unsupported by deeper analyses. However, this isn't very fair to write as a weakness because the current paper just simply doesn't mention this.
- Only having two annotators for the dataset is a weakness, since it's not clear how the claims might generalise, given such a small sample.
- A summative demographics is inferrable but not mentioned in the text. Table 1's revised caption mentions 2.9K paragraphs as the size.
This paper is a differential review given that I previously reviewed the work in the Dec 2021 version submitted to ARR.
There are minor changes to the introduction section, lengthening the introduction and moving the related work section to the more traditional position, right after the introduction.
There are no rebuttals nor notes from the authors to interpret what has been changed from the previous submission, which could have been furnished to ease reviewer burden in checking (I had to read both the new and old manuscripts side by side and align them myself) Many figures could be wider given the margins for the column. I understand you want to preserve space to make up for the new additions into your manuscript, but the wider margins would help for legibility.
Minor changes were made S3.3 to incorporate more connection to prior work. S4.1 Model design was elaborated into subsections, S5.2.1 adds an introduction to LED.
462 RoBERTa-base","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '1', '3', '4', '2', '1', '1']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '3', '2', '5', '1', '2', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '1', '5', '4', '3', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '2', '3', '5', '1', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['0', '0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.) - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure.",ACL_2017_71_review,ACL_2017,"-The explanation of methods in some paragraphs is too detailed and there is no mention of other work and it is repeated in the corresponding method sections, the authors committed to address this issue in the final version.
-README file for the dataset [Authors committed to add README file] - General Discussion: - Section 2.2 mentions examples of DBpedia properties that were used as features. Do the authors mean that all the properties have been used or there is a subset? If the latter please list them. In the authors' response, the authors explain in more details this point and I strongly believe that it is crucial to list all the features in details in the final version for clarity and replicability of the paper.
- In section 2.3 the authors use Lample et al. Bi-LSTM-CRF model, it might be beneficial to add that the input is word embeddings (similarly to Lample et al.) - Figure 3, KNs in source language or in English? ( since the mentions have been translated to English). In the authors' response, the authors stated that they will correct the figure.
- Based on section 2.4 it seems that topical relatedness implies that some features are domain dependent. It would be helpful to see how much domain dependent features affect the performance. In the final version, the authors will add the performance results for the above mentioned features, as mentioned in their response.
- In related work, the authors make a strong connection to Sil and Florian work where they emphasize the supervised vs. unsupervised difference. The proposed approach is still supervised in the sense of training, however the generation of training data doesn’t involve human interference","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '4', '5', '4', '5', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', 'X', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '4', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.",ARR_2022_311_review,ARR_2022,"- The main weaknesses of the paper are the experiments, which is understandable for a short paper but I'd still expect it to be stronger. First, the setting is only on extremely low-resource regime, which is not the only case we want to use data augmentation in real-world applications. Also, sentence classification is an easier task. I feel like the proposed augmentation method has potential to be used on more NLP tasks, which was unfortunately not shown.
- The proposed mixup strategy is very simple (Equation 5), I wonder if the authors have tried other ways to interpolate the one-hot vector with the MLM smoothed vector.
- How does \lambda influence the performances?
- How does the augmentation method compare to other baselines with more training data?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '3', '3', '1', '5', '3', '2']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '3', '5', '1', '5', '3', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '2', '4', '3', '5', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '2', '3', '2', '5', '3', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1) Is it necessary to treat concept map extraction as a separate task? On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.",ACL_2017_331_review,ACL_2017,"The document-independent crowdsourcing annotation is unreliable. - General Discussion: This work creates a new benchmark corpus for concept-map-based MDS. It is well organized and written clearly. The supplement materials are sufficient. I have two questions here.
1) Is it necessary to treat concept map extraction as a separate task?
On the one hand, many generic summarization systems build a similar knowledge graph and then generate summaries accordingly. On the other hand, with the increase of the node number, the concept map becomes growing hard to distinguish. Thus, the general summaries should be more readable.
2) How can you determine the importance of a concept independent of the documents? The definition of summarization is to reserve the main concepts of documents. Therefore, the importance of a concept highly depends on the documents. For example, in the given topic of coal mining accidents, assume there are two concepts: A) an instance of coal mining accidents and B) a cause of coal mining accidents. Then, if the document describes a series of coal mining accidents, A is more important than B. In comparison, if the document explores why coal mining accidents happen, B is more significant than A. Therefore, just given the topic and two concepts A&B, it is impossible to judge their relative importance.
I appreciate the great effort spent by authors to build this dataset. However, this dataset is more like a knowledge graph based on common sense rather than summary.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '2', '1', '3', '1', '1', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', '2', '4', '2', '4', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', '3', '4', '3', '3', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '4', '3', '2', '3', '4', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?",ARR_2022_112_review,ARR_2022,"- The paper does not discuss much about linguistic aspect of the dataset. While their procedures are thoroughly described, analyses are quite limited in that they do not reveal much about linguistic challenges in the dataset as compared to, for example, information extraction. The benefit of pretraining on the target domain seems to be the only consistent finding in their paper and I believe this argument holds in majority of datasets.
- Relating to the first point, authors should describe more about the traits of the experts and justify why annotation must be carried out by the experts, outside its commercial values. Were the experts linguistic experts or domain experts? Was annotation any different from what non-experts would do? Did it introduce any linguistic challenges?
- The thorough description of the processes is definitely a strength, it might be easier to follow if the authors moved some of the details to appendix.
L23: I was not able to find in the paper that single-task models consistently outperformed multi-task models. Could you elaborate a bit more about this?
Table 1: It would be easier if you explain about ""Type of Skills"" in the caption. It might also be worth denoting number of sentences for your dataset, as Jia et al (2018) looks larger than SkillSpan at a first glance.
Section 3: This section can be improved to better explain which of ""skill"", ""knowledge"" and ""attitude"" correspond to ""hard"" and ""soft"" knowledge. Soft skills are referred as attitudes (L180) but this work seems to only consider ""skill"" and ""knowledge"" (L181-183 and Figure 1)? This contradicts with the claim that SkillSpan incorporates both hard and soft knowledge.
L403: I don't think it is the field's norm to call it multi-task learning when it is merely solving two sequential labeling problems at a same time.
L527: Is there any justification as to why you suspected that domain-adaptive pre-raining lead to longer spans?
L543: What is continuous pretraining?
L543: ""Pre-training"" and ""pretraining"" are not spelled consistently.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '4', '3', '1', '5', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '5', '5', '4', '2', '5', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', 'X', 'X', '4', '1', '5', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '5', '3', '3', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1) Lines 102-106 is misleading. While intersection and probs are true, ""such distribution"" cannot refer to the discussion in the above.",ACL_2017_503_review,ACL_2017,"Reranking use is not mentioned in the introduction.
It would be a great news in NLP context if an Earley parser would run in linear time for NLP grammars (unlike special kinds of formal language grammars).
Unfortunately, this result involves deep assumptions about the grammar and the kind of input. Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
To me, the paper should be more clear in this as a random reader may miss the difference between semantic parsing (from strings) and parsing of semantic parses (the current work).
There does not seem to be any control of the linear order of 0-arity edges. It might be useful to mention that if the parser is extended to string inputs with the aim to find the (best?) hypergraph for a given external nodes, then the item representations of the subgraphs must also keep track of the covered 0-arity edges. This makes the string-parser variant exponential. - Easily correctable typos or textual problems: 1) Lines 102-106 is misleading. While intersection and probs are true, ""such distribution"" cannot refer to the discussion in the above.
2) line 173: I think you should rather talk about validation or recognition algorithms than parsing algorithms as ""parsing"" in NLP means usually completely different thing that is much more challenging due to the lexical and structural ambiguity.
3) lines 195-196 are unclear: what are the elements of att_G; in what sense they are pairwise distinct. Compare Example 1 where ext_G and att_G(e_1) are not disjoint sets.
4) l.206. Move *rank* definition earlier and remove redundancy.
5) l. 267: rather ""immediately derives"", perhaps.
6) 279: add ""be"" 7) l. 352: give an example of a nontrivial internal path.
8) l. 472: define a subgraph of a hypergraph 9) l. 417, l.418: since there are two propositions, you may want to tell how they contribute to what is quoted.
10) l. 458: add ""for"" Table: Axiom: this is only place where this is introduced as an axiom. Link to the text that says it is a trigger.
- General Discussion: It might be useful to tell about MSOL graph languages and their yields, which are context-free string languages. What happens if the grammar is ambiguous and not top-down deterministic?
What if there are exponential number of parses even for the input graph due to lexical ambiguity or some other reasons. How would the parser behave then?
Wouldn't the given Earley recogniser actually be strictly polynomial to m or k ?
Even a synchronous derivation of semantic graphs can miss some linguistic phenomena where a semantic distinction is expressed by different linguistic means. E.g. one language may add an affix to a verb when another language may express the same distinction by changing the object. I am suggesting that although AMR increases language independence in parses it may have such cross-lingual challenges.
I did not fully understand the role of the marker in subgraphs. It was elided later and not really used.
l. 509-510: I already started to miss the remark of lines 644-647 at this point.
It seems that the normal order is not unique. Can you confirm this?
It is nice that def 7, cond 1 introduces lexical anchors to predictions.
Compare the anchors in lexicalized grammars.
l. 760. Are you sure that non-crossing links do not occur when parsing linearized sentences to semantic graphs?
- Significant questions to the Authors: Linear complexity of parsing of an input graph seem right for a top-down deterministic grammars but the paper does not recognise the fact that an input string in NLP usually gives rise to an exponential number of graphs. In other words, the parsing complexity result must be interpreted in the context of graph validation or where one wants to find out a derivation of the graph, for example, for the purposes of graph transduction via synchronous derivations.
What would you say about parsing complexity in the case the RGG is a non-deterministic, possibly ambiguous regular tree grammar, but one is interested to use it to assign trees to frontier strings like a context-free grammar? Can one adapt the given Earley algorithm to this purpose (by guessing internal nodes and their edges)?
Although this question might seem like a confusion, it is relevant in the NLP context.
What prevents the RGGs to generate hypergraphs whose 0-arity edges (~words) are then linearised? What principle determines how they are linearised? Is the linear order determined by the Earley paths (and normal order used in productions) or can one consider an actual word order in strings of a natural language? There is no clear connection to (non)context-free string languages or sets of (non)projective dependency graphs used in semantic parsing. What is written on lines 757-758 is just misleading: Lines 757-758 mention that HRGs can be used to generate non-context-free languages. Are these graph languages or string languages? How an NLP expert should interpret the (implicit) fact that RGGs generate only context-free languages? Does this mean that the graphs are noncrossing graphs in the sense of Kuhlmann & Jonsson (2015)?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '4', '2', '5', '4', '2', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '4', '3', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '1', '4', '3', '2', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '4', '3', '1', '4', '3', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '0', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021)",ARR_2022_233_review,ARR_2022,"Additional details regarding the creation of the dataset would be helpful to solve some doubts regarding its robustness. It is not stated whether the dataset will be publicly released.
1) Additional reference regarding explainable NLP Datasets: ""Detecting and explaining unfairness in consumer contracts through memory networks"" (Ruggeri et al 2021) 2) Some aspects of the creation of the dataset are unclear and the authors must address them. First of all, will the author release the dataset or will it remain private?
Are the guidelines used to train the annotators publicly available?
Having a single person responsible for the check at the end of the first round may introduce biases. A better practice would be to have more than one checker for each problem, at least on a subset of the corpus, to measure the agreement between them and, in case of need, adjust the guidelines.
It is not clear how many problems are examined during the second round and the agreement between the authors is not reported.
It is not clear what is meant by ""accuracy"" during the annotation stages.
3) Additional metrics that may be used to evaluate text generation: METEOR (http://dx.doi.org/10.3115/v1/W14-3348), SIM(ile) (http://dx.doi.org/10.18653/v1/P19-1427).
4) Why have the authors decided to use the colon symbol rather than a more original and less common symbol? Since the colon has usually a different meaning in natural language, do they think it may have an impact?
5) How much are these problems language-dependent? Meaning, if these problems were perfectly translated into another language, would they remain valid? What about the R4 category? Additional comments about these aspects would be beneficial for future works, cross-lingual transfers, and multi-lingual settings.
6) In Table 3, it is not clear whether the line with +epsilon refers to the human performance when the gold explanation is available or to the roberta performance when the golden explanation is available?
In any case, both of these two settings would be interesting to know, so I suggest, if it is possible, to include them in the comparison if it is possible.
7) The explanation that must be generated for the query, the correct answer, and the incorrect answers could be slightly different. Indeed, if I am not making a mistake, the explanation for the incorrect answer must highlight the differences w.r.t. the query, while the explanation for the answer must highlight the similarity. It would be interesting to analyze these three categories separately and see whether if there are differences in the models' performances.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '5', '2', '4', '1', '3', '1']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '5', '3', '5', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['X', '5', 'X', '4', '5', 'X', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '5', '2', '5', '1', '2', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).,ARR_2022_98_review,ARR_2022,"1. Human evaluations were not performed. Given the weaknesses of SARI (Vásquez-Rodríguez et al. 2021) and FKGL (Tanprasert and Kauchak, 2021), the lack of human evaluations severely limits the potential impact of the results, combined with the variability in the results on different datasets.
2. While the authors explain the need to include text generation models into the framework of (Kumar et al., 2020), it is not clear as to why only the delete operation was retained from the framework, which used multiple edit operations (reordering, deletion, lexical simplification, etc.). Further, it is not clear how including those other operations will affect the quality and performance of the system.
3. ( minor) It is unclear how the authors arrived at the different components of the ""scoring function,"" nor is it clear how they arrived at the different threshold values/ranges.
4. Finally, one might wonder that the performance gains on Newsela are due to a domain effect, given that the system was explicitly tuned for deletion operations (that abound in Newsela) and that performance is much lower on the ASSET test corpus. It is unclear how the system would generalize to new datasets with varying levels of complexity, and peripheral content.
1. Is there any reason why 'Gold Reference' was not reported for Newsela? It makes it hard to assess the performance of the existing system. 2. Similarly, is there a reason why the effect of linguistic acceptability was not analyzed (Table 3 and Section 4.6)?
3. It will be nice to see some examples of the system on actual texts (vs. other components & models).
4. What were the final thresholds that were used for the results? It will also be good for reproducibility if the authors can share the full set of hyperparameters as well.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '2', '4', '4', '5', '1', '1', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '2', '5', '3', '1', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', 'X', 'X', '2', '1', '1', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '2', '3', '5', '3', '1', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '0', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- A number of claims from this paper would benefit from more in-depth analysis.,ARR_2022_232_review,ARR_2022,"- A number of claims from this paper would benefit from more in-depth analysis.
- There are still some methodological flaws that should be addressed.
### Main questions/comments Looking at the attached dataset files, I cannot work out whether the data is noisy or if I don't understand the format. The 7th example in the test set has three parts separated by [SEP] which I thought corresponded to the headlines from the three sides (left, center, right). However, the second and third headlines don't make sense as stand-alone texts. Especially the third one which states ""Finally, borrowers will receive relief."" seems like a continuation of the previous statements. In addition to the previous question, I cannot find the titles, any meta-information as stated in lines 187-189, nor VAD scores which I assumed would be included.
Part of the motivation is that scaling the production of neutral (all-sides) summaries is difficult. It would be good to quantify this if possible, as a counterargument to that would be that not all stories are noteworthy enough to require such treatment (and not all stories will appear on all sides).
Allsides.com sometimes includes more than one source from the same side -- e.g. https://www.allsides.com/story/jan-6-panel-reportedly-finds-gaps-trump-white-house-phone-records has two stories from Center publishers (CNBC and Reuters) and none from the right. Since the inputs are always one from each side (Section 3.2), are such stories filtered out of the dataset? Of the two major insights that form the basis of this work (polarity is a proxy for framing bias and titles are good indicators of framing bias) only first one is empirically tested with the human evaluation presented in Section 5.1.3. Even then, we are missing any form of analysis of disagreements or low correlation cases that would help solidify the argument. The only evidence we have for the second insight are the results from the NeuS-Title system (compared to the NeuSFT model that doesn't explicitly look at the titles), but again, the comparison is not systematic enough (e.g. no ablation study) to give us concrete evidence to the validity of the claim.
Related to the previous point, it's not clear what the case study mentioned in Section 4.2 actually involved. The insights gathered aren't particularly difficult to arrive at by reading the related literature and the examples in Table 1, while indicative of the arguments don't seem causally critical. It would be good to have more details about this study and how it drove the decisions in the rest of the paper. In particular, I would like to know if there were any counterexamples to the main points (e.g. are there titles that aren't representative of the type of bias displayed in the main article?).
The example in Table 3 shows that the lexicon-based approach (VAD dataset) suffers from lack of context sensitivity (the word ""close"" in this example is just a marker of proximity). This is a counterpoint to the advantages of such approaches presented in Section 5.1.1 and it would be interesting to quantify it (e.g. by looking at the human-annotated data from Section 5.1.3) beyond the safeguard introduced in the second paragraph (metric calibration).
For the NeuS-Title model, does that order of the input matter? It would be interesting to rerun the same evaluation with different permutations (e.g. center first, or right first). Is there a risk of running into the token limit of the encoder?
From the examples shared in Table 3, it appears that both the NeuSFT and NeuS-Title models stay close to a single target article. What strikes me as odd is that neither chose the center headline (the former is basically copying the Right headline, the latter has done some paraphrasing both mostly based on the Left headline). Is there a particular reason for this? Is the training objective discouraging true multi-headline summarisation since the partisan headlines will always contain more biased information/tokens?
### Minor issues I was slightly confused by the word ""headline"" to refer to the summary of the article. I think of headlines and titles as fundamentally the same thing: short (one sentence) high-level descriptions of the article to come. It would be helpful to refer to longer text as a summary or a ""headline roundup"" as allsides.com calls it. Also there is a discrepancy between the input to the NeuS-Title model (lines 479-486) and the output shown in Table 3 (HEADLINE vs ARTICLE).
Some citations have issues. Allsides.com is cited as (all, 2021) and (Sides, 2018); the year is missing for the citations on lines 110 and 369; Entman (1993) and (2002) are seemingly the same citation (and the 1993 is missing any publication details); various capitatlisation errors (e.g. ""us"" instead of ""US"" on line 716) It's not clear what the highlighting in Table 1 shows (it is implicitly mentioned later in the text). I would recommend colour-coding the different types of bias and providing the details in the caption.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '1', '2', '1', '1', '1', '3']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', 'X', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '2', '1', '2', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '0', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.",ACL_2017_483_review,ACL_2017,"- 071: This formulation of argumentation mining is just one of several proposed subtask divisions, and this should be mentioned. For example, in [1], claims are detected and classified before any supporting evidence is detected.
Furthermore, [2] applied neural networks to this task, so it is inaccurate to say (as is claimed in the abstract of this paper) that this work is the first NN-based approach to argumentation mining.
- Two things must be improved in the presentation of the model: (1) What is the pooling method used for embedding features (line 397)? and (2) Equation (7) in line 472 is not clear enough: is E_i the random variable representing the *type* of AC i, or its *identity*? Both are supposedly modeled (the latter by feature representation), and need to be defined. Furthermore, it seems like the LHS of equation (7) should be a conditional probability.
- There are several unclear things about Table 2: first, why are the three first baselines evaluated only by macro f1 and the individual f1 scores are missing?
This is not explained in the text. Second, why is only the ""PN"" model presented? Is this the same PN as in Table 1, or actually the Joint Model? What about the other three?
- It is not mentioned which dataset the experiment described in Table 4 was performed on.
General Discussion: - 132: There has to be a lengthier introduction to pointer networks, mentioning recurrent neural networks in general, for the benefit of readers unfamiliar with ""sequence-to-sequence models"". Also, the citation of Sutskever et al. (2014) in line 145 should be at the first mention of the term, and the difference with respect to recursive neural networks should be explained before the paragraph starting in line 233 (tree structure etc.).
- 348: The elu activation requires an explanation and citation (still not enough well-known).
- 501: ""MC"", ""Cl"" and ""Pr"" should be explained in the label.
- 577: A sentence about how these hyperparameters were obtained would be appropriate.
- 590: The decision to do early stopping only by link prediction accuracy should be explained (i.e. why not average with type accuracy, for example?).
- 594: Inference at test time is briefly explained, but would benefit from more details.
- 617: Specify what the length of an AC is measured in (words?).
- 644: The referent of ""these"" in ""Neither of these"" is unclear.
- 684: ""Minimum"" should be ""Maximum"".
- 694: The performance w.r.t. the amount of training data is indeed surprising, but other models have also achieved almost the same results - this is especially surprising because NNs usually need more data. It would be good to say this.
- 745: This could alternatively show that structural cues are less important for this task.
- Some minor typos should be corrected (e.g. ""which is show"", line 161).
[1] Rinott, Ruty, et al. ""Show Me Your Evidence-an Automatic Method for Context Dependent Evidence Detection."" EMNLP. 2015.
[2] Laha, Anirban, and Vikas Raykar. "" An Empirical Evaluation of various Deep Learning Architectures for Bi-Sequence Classification Tasks."" COLING. 2016.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', 'X', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent.",ARR_2022_215_review,ARR_2022,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent. 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability. - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column). - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '4', '4', '3', '5', '3', '3', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '5', '5', '5', '5', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '5', '5', '5', '5', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '4', '5', '5', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.,ARR_2022_121_review,ARR_2022,"1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)).
Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays. Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '5', '3', '5', '3', '4', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '3', '5', '3', '4', '3', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '4', '5', '5', '4', '5', '5', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '4', '5', '5', '5', '5', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used?",ARR_2022_186_review,ARR_2022,"- it is not clear what's the goal of the paper. Is the release of a challenging dataset or proposing an analysis of augmenting models with expert guided adversarial examples. If it is the first, ok, but the paper misses a lot of important information, and data analysis to give a sense of the quality and usefulness of such a dataset. If it is the second, it is not clear what's the novelty.
- In general, it seems the authors want to propose a way of create a challenging set. However, what they describe seems very specific and not scalable.
- The paper structure and writing is not sufficient
My main concern is that it's not clear what's the goal of the paper. Also, the structure and writing should greatly improve. I believe also that the choice to go for a short paper was penalizing the authors, as it seems clear that they cut out some information that could've been useful to better understand the paper (also given the 5 pages appendix).
Detailed comments/questions: - Line 107 data, -> data.
- Line 161-162: this sentence is not clear.
- Table 1: are these all the rules you defined? How the rule is applied? When you decide to make small changes to the context? For example, when you decide to add ""and her team"" as in the last example of Table 1? - Also, it seems that all the rules change a one-token entity to a multi-token one or vice-versa. Will models be biased by this?
- Line 183-197: not clear what you're doing here. Details cannot be in appendix.
- What is not clear also to me is how is used the Challenge Set. If I understood correctly, the CS is created by the linguistic experts and it's used for evaluation purposes. Is this used also to augment the training material? If yes, what is the data split you used? - Line 246-249: this sentence lacks the conclusion - Line 249: What are eligible and not eligible examples?
- Line 251: what is p?
- Line 253: The formula doesn't depend on p, so why the premise is ""if p=100% of the eligible example""?
- Line 252: Not clear what is the subject of this sentence.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '5', '2', '1', '3', '3', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '5', '5', '1', '2', '4', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['X', '5', 'X', 'X', '3', '3', 'X', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '5', '3', '3', '1', '3', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here. Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.",ACL_2017_128_review,ACL_2017,"----- I'm not very convinced by the empirical results, mostly due to the lack of details of the baselines. Comments below are ranked by decreasing importance.
- The proposed model has two main parts: sentence embedding and substructure embedding. In Table 1, the baseline models are TreeRNN and DCNN, they are originally used for sentence embedding but one can easily take the node/substructure embedding from them too. It's not clear how they are used to compute the two parts.
- The model uses two RNNs: a chain-based one and a knowledge guided one. The only difference in the knowledge-guided RNN is the addition of a ""knowledge"" vector from the memory in the RNN input (Eqn 5 and 8). It seems completely unnecessary to me to have separate weights for the two RNNs. The only advantage of using two is an increase of model capacity, i.e. more parameters.
Furthermore, what are the hyper-parameters / size of the baseline neural networks? They should have comparable numbers of parameters.
- I also think it is reasonable to include a baseline that just input additional knowledge as features to the RNN, e.g. the head of each word, NER results etc.
- Any comments / results on the model's sensitivity to parser errors?
Comments on the model: - After computing the substructure embeddings, it seems very natural to compute an attention over them at each word. Is there any reason to use a static attention for all words? I guess as it is, the ""knowledge"" is acting more like a filter to mark important words. Then it is reasonable to include the baseline suggest above, i.e. input additional features.
- Since the weight on a word is computed by inner product of the sentence embedding and the substructure embedding, and the two embeddings are computed by the same RNN/CNN, doesn't it means nodes / phrases similar to the whole sentence gets higher weights, i.e. all leaf nodes?
- The paper claims the model generalizes to different knowledge but I think the substructure has to be represented as a sequence of words, e.g. it doesn't seem straightforward for me to use constituent parse as knowledge here.
Finally, I'm hesitating to call it ""knowledge"". This is misleading as usually it is used to refer to world / external knowledge such as a knowledge base of entities, whereas here it is really just syntax, or arguably semantics if AMR parsing is used.
-----General Discussion----- This paper proposes a practical model which seems working well on one dataset, but the main ideas are not very novel (see comments in Strengths). I think as an ACL paper there should be more takeaways. More importantly, the experiments are not convincing as it is presented now. Will need some clarification to better judge the results.
-----Post-rebuttal----- The authors did not address my main concern, which is whether the baselines (e.g. TreeRNN) are used to compute substructure embeddings independent of the sentence embedding and the joint tagger. Another major concern is the use of two separate RNNs which gives the proposed model more parameters than the baselines. Therefore I'm not changing my scores.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '4', '3', '1', '5', '3', '1', '1']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '2', '3', '1', '2', '5', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '4', '3', '4', '2', '1', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '3', '2', '1', '3', '2', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?,ACL_2017_108_review,ACL_2017,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['4', '3', '5', '4', '4', '3', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['4', '3', '5', '5', '4', '2', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '5', '4', 'X', '4', '3', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['3', '3', '5', '5', '4', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '1', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.",ACL_2017_614_review,ACL_2017,"- I don't understand effectiveness of the multi-view clustering approach.
Almost all across the board, the paraphrase similarity view does significantly better than other views and their combination. What, then, do we learn about the usefulness of the other views? There is one empirical example of how the different views help in clustering paraphrases of the word 'slip', but there is no further analysis about how the different clustering techniques differ, except on the task directly. Without a more detailed analysis of differences and similarities between these views, it is hard to draw solid conclusions about the different views. - The paper is not fully clear on a first read. Specifically, it is not immediately clear how the sections connect to each other, reading more like disjoint pieces of work. For instance, I did not understand the connections between section 2.1 and section 4.3, so adding forward/backward pointer references to sections should be useful in clearing up things. Relatedly, the multi-view clustering section (3.1) needs editing, since the subsections seem to be out of order, and citations seem to be missing (lines 392 and 393).
- The relatively poor performance on nouns makes me uneasy. While I can expect TWSI to do really well due to its nature, the fact that the oracle GAP for PPDBClus is higher than most clustering approaches is disconcerting, and I would like to understand the gap better. This also directly contradicts the claim that the clustering approach is generalizable to all parts of speech (124-126), since the performance clearly isn't uniform.
- General Discussion: The paper is mostly straightforward in terms of techniques used and experiments. Even then, the authors show clear gains on the lexsub task by their two-pronged approach, with potentially more to be gained by using stronger WSD algorithms.
Some additional questions for the authors : - Lines 221-222 : Why do you add hypernyms/hyponyms?
- Lines 367-368 : Why does X^{P} need to be symmetric?
- Lines 387-389 : The weighting scheme seems kind of arbitrary. Was this indeed arbitrary or is this a principled choice?
- Is the high performance of SubstClus^{P} ascribable to the fact that the number of clusters was tuned based on this view? Would tuning the number of clusters based on other matrices affect the results and the conclusions?
- What other related tasks could this approach possibly generalize to? Or is it only specific to lexsub?","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '1', '2', '5', '2', '1', '2']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '5', '5', '3', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '2', 'X', '5', '2', 'X', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '5', '3', '3', '5', '2', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures?,ACL_2017_108_review,ACL_2017,"Clarification is needed in several places.
1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.
2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?
3. section 5.1 does not seem to provide useful info regarding why the new model is superior.
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures? - General Discussion: The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '4', '5', '5', '2', '3', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '2', '5', '4', '5', '4', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '2', '3', '4', '3', '1', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['4', '5', '2', '5', '4', '1', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.",ARR_2022_114_review,ARR_2022,"By showing that there is an equivalent graph in the rank space on which message passing is equivalent to message passing in the original joint state and rank space, this work exposes the fact that these large structured prediction models with fully decomposable clique potentials (Chiu et al 2021 being an exception) are equivalent to a smaller structured prediction model (albeit with over-parameterized clique potentials). For example, looking at Figure 5 (c), the original HMM is equivalent to a smaller MRF with state size being the rank size (which is the reason why inference complexity does not depend on the original number of states at all after calculating the equivalent transition and emission matrices). One naturally wonders why not simply train a smaller HMM, and where does the performance gain of this paper come from in Table 3.
As another example, looking at Figure 4 (a), the original PCFG is equivalent to a smaller PCFG (with fully decomposable potentials) with state size being the rank size. This smaller PCFG is over-parameterized though, e.g., its potential $H\in \mathcal{R}^{r \times r}$ is parameterized as $V U^T$ where $U,V\in \mathcal{R}^{r \times m}$ and $r < m$, instead of directly being parameterized as a learned matrix of $\mathcal{R}^{r \times r}$. That being said, I don't consider this a problem introduced by this paper since this should be a problem of many previous works as well, and it seems an intriguing question why large state spaces help despite the existence of these equivalent small models. Is it similar to why overparameterizing in neural models help? Is there an equivalent form of the lottery ticket hypothesis here?
In regard to weakness #1, I think this work would be strengthened by adding the following baselines: 1. For each PCFG with rank r, add a baseline smaller PCFG with state size being r, but where $H, I, J, K, L$ are directly parameterized as learned matrices of $\mathcal{R}^{r \times r}$, $\mathcal{R}^{r \times o}$, $\mathcal{R}^{r}$, etc. Under this setting, parsing F-1 might not be directly comparable, but perplexity can still be compared.
2. For each HMM with rank r, add a baseline smaller HMM with state size being r.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '4', '5', '5', '5', '3', '5', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '3', '5', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['3', '5', '3', '4', '5', '3', '5', 'X']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '4', '5', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
- 241: It would also be good to state the maximum number of tasks done by any annotator.,ARR_2022_215_review,ARR_2022,"1. The paper raises two hypotheses in lines 078-086 about multilinguality and country/language-specific bias. While I don't think the hypotheses are phrased optimally (could they be tested as given?), their underlying ideas are valuable. However, the paper actually does not really study these hypotheses (nor are they even mentioned/discussed again). I found this not only misleading, but I would have also liked the paper to go deeper into the respective topics, at least to some extent. 2. It seemed a little disappointing to me that the 212 new pairs have _not_ been translated to English (if I'm not mistaken). To really make this dataset a bilingual resource, it would be good to have all pairs in both languages. In the given way, it seems that ultimately only the French version was of interest to the study - unlike it is claimed initially.
3. Almost no information about the reliability of the translations and the annotations is given (except for the result of the translation checking in line 285), which seems unsatisfying to me. To assess the translations, more information about the language/translation expertise of the authors would be helpful (I don't think this violates anonymity). For the annotations, I would expect some measure of inter-annotator agreement.
4. The metrics in Tables 4 and 5 need explanation, in order to make the paper self-contained. Without going to the original paper on CrowS-pairs, the values are barely understandable. Also, information on the values ranges should be given as well as whether higher or lower values are better.
- 066: social contexts >> I find this term misleading here, since the text seems to be about countries/language regions.
- 121: Deviding 1508 into 16*90 = 1440 cases cannot be fully correct. What about the remaining 68 cases?
- 241: It would also be good to state the maximum number of tasks done by any annotator.
- Table 3: Right-align the numeric columns.
- Table 4 (1): Always use the same number of decimal places, for example 61.90 instead of 61.9 to match the other values. This would increase readability. - Table 4 (2): The table exceeds the page width; that needs to be fixed.
- Tables 4+5 (1): While I undersand the layout problem, the different approaches would be much easier to compare if tables and columns were flipped (usually, one approach per row, one metric per column). - Tables 4+5 (2): What's the idea of showing the run-time? I didn't see for what this is helpful.
- 305/310: Marie/Mary >> I think these should be written the same.
- 357: The text speaks of ""53"", but I believe the value ""52.9"" from Table 4 is meant. In my view, such rounding makes understanding harder rather than helping.
- 575/577: ""1/"" and ""2/"" >> Maybe better use ""(1)"" and ""(2)""; confused me first.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '4', '5', '5', '5', '3', '4', '5']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '4', '2', '5']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '5', '4', 'X', '5', '2', '3', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '5', '5', '5', '2', '4', '2']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
"5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?",ACL_2017_489_review,ACL_2017,"1) The main weakness for me is the statement of the specific hypothesis, within the general research line, that the paper is probing: I found it very confusing. As a result, it is also hard to make sense of the kind of feedback that the results give to the initial hypothesis, especially because there are a lot of them and they don't all point in the same direction.
The paper says: ""This paper pursues the hypothesis that an accurate model of referential word meaning does not need to fully integrate visual and lexical knowledge (e.g. as expressed in a distributional vector space), but at the same time, has to go beyond treating words as independent labels.""
The first part of the hypothesis I don't understand: What is it to fully integrate (or not to fully integrate) visual and lexical knowledge? Is the goal simply to show that using generic distributional representation yields worse results than using specific, word-adapted classifiers trained on the dataset?
If so, then the authors should explicitly discuss the bounds of what they are showing: Specifically, word classifiers must be trained on the dataset itself and only word classifiers with a sufficient amount of items in the dataset can be obtained, whereas word vectors are available for many other words and are obtained from an independent source (even if the cross-modal mapping itself is trained on the dataset); moreover, they use the simplest Ridge Regression, instead of the best method from Lazaridou et al. 2014, so any conclusion as to which method is better should be taken with a grain of salt. However, I'm hoping that the research goal is both more constructive and broader. Please clarify. 2) The paper uses three previously developed methods on a previously available dataset. The problem itself has been defined before (in Schlangen et al.). In this sense, the originality of the paper is not high. 3) As the paper itself also points out, the authors select a very limited subset of the ReferIt dataset, with quite a small vocabulary (159 words). I'm not even sure why they limited it this way (see detailed comments below).
4) Some aspects could have been clearer (see detailed comments).
5) The paper contains many empirical results and analyses, and it makes a concerted effort to put them together; but I still found it difficult to get the whole picture: What is it exactly that the experiments in the paper tell us about the underlying research question in general, and the specific hypothesis tested in particular? How do the different pieces of the puzzle that they present fit together?
- General Discussion: [Added after author response] Despite the weaknesses, I find the topic of the paper very relevant and also novel enough, with an interesting use of current techniques to address an ""old"" problem, REG and reference more generally, in a way that allows aspects to be explored that have not received enough attention. The experiments and analyses are a substantial contribution, even though, as mentioned above, I'd like the paper to present a more coherent overall picture of how the many experiments and analyses fit together and address the question pursued.
- Detailed comments: Section 2 is missing the following work in computational semantic approaches to reference: Abhijeet Gupta, Gemma Boleda, Marco Baroni, and Sebastian Pado. 2015. Distributional vectors encode referential attributes.
Proceedings of EMNLP, 12-21 Aurelie Herbelot and Eva Maria Vecchi. 2015.
Building a shared world: mapping distributional to model-theoretic semantic spaces. Proceedings of EMNLP, 22–32.
142 how does Roy's work go beyond early REG work?
155 focusses links 184 flat ""hit @k metric"": ""flat""?
Section 3: please put the numbers related to the dataset in a table, specifying the image regions, number of REs, overall number of words, and number of object names in the original ReferIt dataset and in the version you use. By the way, will you release your data? I put a ""3"" for data because in the reviewing form you marked ""Yes"" for data, but I can't find the information in the paper.
229 ""cannot be considered to be names"" ==> ""image object names"" 230 what is ""the semantically annotated portion"" of ReferIt?
247 why don't you just keep ""girl"" in this example, and more generally the head nouns of non-relational REs? More generally, could you motivate your choices a bit more so we understand why you ended up with such a restricted subset of ReferIt?
258 which 7 features? ( list) How did you extract them?
383 ""suggest that lexical or at least distributional knowledge is detrimental when learning what a word refers to in the world"": How does this follow from the results of Frome et al. 2013 and Norouzi et al. 2013? Why should cross-modal projection give better results? It's a very different type of task/setup than object labeling.
394-395 these numbers belong in the data section Table 1: Are the differences between the methods statistically significant?
They are really numerically so small that any other conclusion to ""the methods perform similarly"" seems unwarranted to me. Especially the ""This suggests..."" part (407). Table 1: Also, the sim-wap method has the highest accuracy for hit @5 (almost identical to wac); this is counter-intuitive given the @1 and @2 results. Any idea of what's going on?
Section 5.2: Why did you define your ensemble classifier by hand instead of learning it? Also, your method amounts to majority voting, right? Table 2: the order of the models is not the same as in the other tables + text.
Table 3: you report cosine distances but discuss the results in terms of similarity. It would be clearer (and more in accordance with standard practice in CL imo) if you reported cosine similarities.
Table 3: you don't comment on the results reported in the right columns. I found it very curious that the gold-top k data similarities are higher for transfer+sim-wap, whereas the results on the task are the same. I think that you could squeeze more information wrt the phenomenon and the models out of these results.
496 format of ""wac"" Section 6 I like the idea of the task a lot, but I was very confused as to how you did and why: I don't understand lines 550-553. What is the task exactly? An example would help. 558 ""Testsets"" 574ff Why not mix in the train set examples with hypernyms and non-hypernyms?
697 ""more even"": more wrt what?
774ff ""Previous cross-modal mapping models ... force..."": I don't understand this claim.
792 ""larger test sets"": I think that you could even exploit ReferIt more (using more of its data) before moving on to other datasets.","{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '3', '4', '3', '3', '1', '4']}",,,2,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['5', '5', '2', '3', '2', '1', '1', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '3', '1', 'X', '1', '3', '1', '4']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['2', '3', '3', '5', '2', '3', '1', '3']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV', 'boda'], 'labels': ['1', '1', '1', '1', '1', '1', '1', '1']}",,,"{'annotators': ['TxZsPCly', 'LbMNie2g', 'ZGOQesrg', '2B7nNvBS', '3O1LNSAk', '9BW3mEvI', 'pGkwDdmV'], 'labels': ['0', '0', '0', '0', '0', '0', '0']}",,,"{'annotators': [], 'labels': []}",,
2. It would be nice to include the hard prompt baseline in Table 1 to see the increase in performance of each method.,gybvlVXT6z,EMNLP_2023,"1. I feel that paper has insufficiant baseline. For example, CoCoOp (https://arxiv.org/abs/2203.05557) is a widely used baseline for prompt tuning research in CLIP. Moreover, it would be nice to include the natural data shift setting as in most other prompt tuning papers for CLIP.
2. It would be nice to include the hard prompt baseline in Table 1 to see the increase in performance of each method.
3. I think the performance drop seen with respect to the prompt length (Figure 4) is a major limitation of this approach. For example, this phenomenon might make it so that using just a general hard prompt of length 4 ('a photo of a') would outperform the CBBT with length 4 or even CBBT with length 1.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
1. Lack of numerical results. The reviewer is curious about how to apply it to some popular algorithms and their performance compared with existing DP algorithms.,NIPS_2022_670,NIPS_2022,1. Lack of numerical results. The reviewer is curious about how to apply it to some popular algorithms and their performance compared with existing DP algorithms. 2. The presentation of this paper is hard to follow for the reviewer.,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '3', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '4', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '1', '3', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones.,NIPS_2020_295,NIPS_2020,"1. The experimental comparisons are not enough. Some methods like MoCo and SimCLR also test the results with wider backbones like ResNet50 (2×) and ResNet50 (4×). It would be interesting to see the results of proposed InvP with these wider backbones. 2. Some methods use epochs and pretrain epochs as 200, while the reported InvP uses 800 epochs. What are the results of InvP with epochs as 200? It would be more clear after adding these results into the tables. 3. The proposed method adopts memory bank to update vi, as detailed in the beginning of Sec.3. What the results would be when adopting momentum queue and current batch of features? As the results of SimCLR and MoCo are better than InsDis, it would be nice to have those results.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.",NIPS_2017_351,NIPS_2017,"- As I said above, I found the writing / presentation a bit jumbled at times.
- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).
- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.
- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.
- Figure 3 is never referenced unless I missed it.
Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.
- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?
- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '2', '3', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '2', '2', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '3', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.",NIPS_2016_9,NIPS_2016,"Weakness: The authors do not provide any theoretical understanding of the algorithm. The paper seems to be well written. The proposed algorithm seems to work very all on the experimental setup, using both synthetic and real-world data. The contributions of the papers are enough to be considered for a poster presentation. The following concerns if addressed properly could raise to the level of oral presentation: 1. The paper does not provide an analysis on what type of data the algorithm work best and on what type of data the algorithm may not work well. 2. The first claimed contribution of the paper is that unlike other existing algorithms, the proposed algorithm does not take as many points or does not need apriori knowledge about dimensions of subspaces. It would have been better if there were some empirical justification about this. 3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', 'X', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The applicability of the robust training scheme seems unlikely to scale to practical datasets, particularly those supported on high-dimensional domains. It seems like the accuracy would scale unfavorably unless the size of V scales exponentially with the dimension.",NIPS_2020_916,NIPS_2020,"My major complaints can be characterized into the following bullet points: - Robustness is argued only indirectly by way of Lipschitz constants. While the authors present a novel formulation (i.e., differing from the standard low-lipschitz=>robustness claims in the ML literature) of how controlling the lipschitz function of the classifier controls sensitivity to adversarial perturbations, this setting differs slightly from standard classification settings. In the standard classification setting, where labels are 1-hot vectors in R^dim(Y), a classifier typically returns as a label the argmax of the vector-valued function. Robustness then is usually considered as whether the argmax returns the right label, rather than a strictly-convex loss applied to the one-hot-label: this work incorporates 'confidence' into the robustness evaluation. - The applicability of the robust training scheme seems unlikely to scale to practical datasets, particularly those supported on high-dimensional domains. It seems like the accuracy would scale unfavorably unless the size of V scales exponentially with the dimension. - The example data distribution could be better chosen. As it stands, a classifier with (true) loss would require a Lipschitz constant that tends to infinity. A more standard/practical dataset would admit a perfect classifier that has a valid Lipschitz constant.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '5', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '3', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '2', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The choice of datasets: is 4 years sufficient period to study style shifts? what kind of style shifts happen in such a time? Without these answers, it is hard to appreciate what the model is capturing.",VnMfQuDSgG,EMNLP_2023,"- The paper is entirely statistical. For a task like this, it is important to show the linguistic nuance that is captured by the metrics.
- The choice of datasets: is 4 years sufficient period to study style shifts? what kind of style shifts happen in such a time? Without these answers, it is hard to appreciate what the model is capturing.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '4', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', 'X', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2) The paper provides some interesting results: i) (Section 3.1) Knowing the level of misspecification ε is a key ingredient, as not knowing the same would yield sample complexity bounds which are no better than the bound obtainable from unstructured ( ε = ∞ ) stochastic bandits. ii) A single no-regret learner is used for the sampling strategy instead of assigning a learner for each of the (N choose k) answers, thus exponentially reducing the number of online learners. iii) The proposed decision rules are shown to match the prescribed lower bound asymptotically.",NIPS_2021_442,NIPS_2021,"of the paper:
Strengths: 1) To the best of my knowledge, the problem investigated in the paper is original in the sense that top-m identification has not been studied in the misspecified setting. 2) The paper provides some interesting results:
i) (Section 3.1) Knowing the level of misspecification ε
is a key ingredient, as not knowing the same would yield sample complexity bounds which are no better than the bound obtainable from unstructured ( ε = ∞
) stochastic bandits. ii) A single no-regret learner is used for the sampling strategy instead of assigning a learner for each of the (N choose k) answers, thus exponentially reducing the number of online learners. iii) The proposed decision rules are shown to match the prescribed lower bound asymptotically. 3) Sufficient experimental validation is provided to showcase the empirical performance of the prescribed decision rules.
Weaknesses: Some of the explanations provided by the authors are a bit unclear to me. Specifically, I have the following questions: 1) IMO, a better explanation of investigating top-m identification in this setting is required. Specifically, in this setting, we could readily convert the problem to the general top-m identification by appending the constant 1 to the features (converting them into d + 1
dimensional features) and trying to estimate the misspecifications η
in the higher dimensional space. Why is that disadvantageous?
Can the authors explain how the lower bound in Theorem 1 explicitly captures the effect of the upper bound on misspecification ε
? The relationship could be shown, for instance, by providing an example of a particular bandit environment (say, Gaussian bandits) ala [Kaufmann2016].
Sample complexity: Theorem 2 states the sample complexity in a very abstract way; it provides an equation which needs to be solved in order to get an explicit expression of the sample complexity. In order to make a comparison, the authors then mention that the unstructured confidence interval β t , δ u n s
is approximately log ⁡ ( 1 δ )
in the limit of δ → 0
, which is then used to argue that the sample complexity of MISLID is asymptotically optimal. However, β t , δ u n s
also depends on t
. In fact, my understanding is that as δ
goes to 0
, the stopping time t
goes to infinity, where it is not clear as to what value the overall expression β t , δ u n s
converges. Overall, I feel that the authors need to explicate the sample complexity a bit more. My suggestions are: can the authors find a solution to equation (5) (or at least an upper bound on the solution for different regimes of ε
)? Using such an upper bound, even if the authors could give an explicit expression of the (asymptotic) sample complexity and show how it compares to the lower bound, it would be a great contribution.
Looking at Figure 1A (the second figure from the left, for the case of ε = 2
), it looks like LinGapE outperforms MISLID in terms of average sample complexity. Please correct me if I’m missing something, but if what I understand is correct, then why use MISLID and not LinGapE?
Probable typo: Line 211: Should it be θ
instead of θ t
for the self-normalized concentration?
The authors have explained the limitations of the investigation in Section 6.","{'annotators': ['tpuEIMI7'], 'labels': ['1']}",,,3,"{'annotators': ['tpuEIMI7'], 'labels': ['5']}",,,"{'annotators': ['tpuEIMI7'], 'labels': ['5']}",,,"{'annotators': ['tpuEIMI7'], 'labels': ['1']}",,,"{'annotators': ['tpuEIMI7'], 'labels': ['1']}",,,"{'annotators': ['tpuEIMI7'], 'labels': ['1']}",,,"{'annotators': [], 'labels': []}",,
"2: the callout to table 5 should go to table 3, instead. Page 7, section 5, last par.: figure 6 callout is not directing properly",ICLR_2023_977,ICLR_2023,"the evaluation section has 2 experiments, but only 2 very insightful detailed examples. The paper can use a few more examples to illustrate more differences of the output sequences. This would allow the reader to internalize how the non-monotonicity in a deeper way.
Questions: In details, how does the decoding algorithm actually avoid repetitions? In other way, how does other models actually degrade validation perplexity using their decoding algorithm?
Typos, Grammar, etc.: Page 7, section 4.2, par. 2: the callout to table 5 should go to table 3, instead. Page 7, section 5, last par.: figure 6 callout is not directing properly","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"3. Experiments. Probably the biggest concern with the paper is with the experiments. The paper reports only self comparisons. The paper also doesn't explain why this is so, which adds to the poor motivation problem. In a generative setting comparisons with SketchRNN could be performed.",NIPS_2020_1371,NIPS_2020,"When reading the paper, I've got the impression that the paper is not finished with couple of key experiments missing. Some parts of the paper lack motivation. Terminology is sometimes unclear and ambiguous. 1. Terminology. The paper uses terms ""animation"", ""generative"", ""interpolation"". See contribution 1 in L40-42. While the paper reported some interpolation experiments, I haven't found any animation or generation experiments. It seems the authors equate interpolation and animation (Section 4.2) which is not correct. I consider animation is a physically plausible motion. Like a person opens a mouth, car moves, while interpolation is just warping one image into the other. Fig 7 shows exactly this with the end states being plausible states of the system. The authors should fix the ambiguity to avoid misunderstanding. The authors also don't report any generation results. Can I sample a random shape from the learnt distribution? If not the I don't think it's correct to say the model is generative. 2. Motivation. It's not clear why the problem is important from practical standpoint? Why one would like to interpolate between two icons? Motivation behind animation is more clear, but in my opinion, the paper doesn't do animation. I believe from a practical standpoint letting the user to input text and be able to generate an icon would also be important. Again, I have hard time understanding why shape autoencoding and interpolation is interesting. 3. Experiments. Probably the biggest concern with the paper is with the experiments. The paper reports only self comparisons. The paper also doesn't explain why this is so, which adds to the poor motivation problem. In a generative setting comparisons with SketchRNN could be performed.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.",ICLR_2022_1794,ICLR_2022,"1 Medical imaging are often obtained in 3D volumes, not only limited to 2D images. So experiments should include the 3D volume data as well for the general community, rather than all on 2D images. And the lesion detection is another important task for the medical community, which has not been studied in this work.
2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.
3 On the segmentation mask involved with cancer on CSAW-S, the segmentation results of DEEPLAB3-DEIT-S cannot be concluded as better than DEEPLAB3-RESNET50. The implication that ViTs outperform CNNs in this segmentation task cannot be validly drawn from an 0.2% difference with larger variance.
Questions: 1 For the grid search of learning rate, is it done on the validation set?
Minor problems: 1 The n number for Camelyon dataset in Table 1 is not consistent with the descriptions in the text in Page 4.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '2', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '2', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).",ICLR_2022_3352,ICLR_2022,"+ The problem studied in this paper is definitely important in many real-world applications, such as robotics decision-making and autonomous driving. Discovering the underlying causation is important for agents to make reasonable decisions, especially in dynamic environments.
+ The method proposed in this paper is interesting and technically correct. Intuitively, using GRU to extract sequential information helps capture the changes of causal graphs.
- The main idea of causal discovery by sampling intervention set and causal graphs for masking is similar to DCDI [1]. This paper is more like using DCDI in dynamic environments, which may limit the novelty of this paper.
- The paper is not difficult to follow, but there are several places that are may cause confusion. (listed in point 3).
- The contribution of this paper is not fully supported by experiments.
Main Questions
(1) During the inference stage, why use samples instead of directly taking the argmax of Bernoulli distribution? How many samples are required? Will this sampling cause scalability problems?
(2) In the experiment part, the authors only compare with one method (V-CDN). Is it possible to compare DYNOTEARS with the proposed method?
(3) The authors mention that there is no ground truth to evaluate the causal discovery task. I agree with this opinion since the real world does not provide us causal graphs. However, the first experiment is conducted on a synthetic dataset, where I believe it is able to obtain the causation by checking collision conditions. In other words, I am not convinced only by the prediction results. Could the author provide the learned causal graphs and intervention sets and compare them with ground truth even on a simple synthetic dataset?
Clarification questions
(1) It seems the citation of NOTEARS [2] is wrongly used for DYNOTEARS [3]. This citation is important since DYNOTEARS is one of the motivations of this paper.
(2) ICM part in Figure 3 is not clear. How is the Intervention set I is used? If I understand correctly, function f
is a prediction model conditioned on history frames.
(3) The term “Bern” in equation (3) is not defined. I assume it is the Bernoulli distribution. Then what does the symbol B e r n ( α t , β t ) mean?
(4) According to equation (7), each node j
has its own parameters ϕ j t and ψ j t
. Could the authors explain why the parameters are related to time?
(5) In equation (16), the authors mention the term “ secondary optimization”. I can’t find any reference for it. Could the author provide more information?
Minor things:
(1) In the caption of Figure 2, the authors say “For nonstationary causal models, (c)….”. But in figure (c) belongs to stationary methods.
[1] Brouillard P, Lachapelle S, Lacoste A, et al. Differentiable causal discovery from interventional data[J]. arXiv preprint arXiv:2007.01754, 2020.
[2] Zheng X, Aragam B, Ravikumar P, et al. Dags with no tears: Continuous optimization for structure learning[J]. arXiv preprint arXiv:1803.01422, 2018.
[3] Pamfil R, Sriwattanaworachai N, Desai S, et al. Dynotears: Structure learning from time-series data[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2020: 1595-1605.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['X', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '1']}",,
"1) ""However, there is no corresponding set of tools for the reinforcement learning setting."" - This is false. See references below (also some in the submitted paper).",ICLR_2021_1948,ICLR_2021,"a. Anonymisation Failure in References
i. A reference uncited in the manuscript body contains a non-anonymised set of author names to a paper with the same title as the system presented in this paper. This was not detected during initial review. ""Shuby Deshpande and Jeff Schneider. Vizarel: A System to Help Better Understand RL Agents.arXiv:2007.05577.""
b. Citations
i. There is egregiously missing information in almost all citations. This is quite obvious in all the missing dates in the manuscript text.
c. Clarity
i. There are a few unclear or misleadingly worded statements made as below:
1) ""However, there is no corresponding set of tools for the reinforcement learning setting."" - This is false. See references below (also some in the submitted paper).
2) ""stronger feedback loop between the researcher and the agent"" - This is at least confusing. In any learning setting, there is a strong interaction loop between experimentation by the researcher and resulting outcomes for the trained model.
3) ""To the best of our knowledge, there do not exist visualization systems built for interpretable reinforcement learning that effectively address the broader goals we have identified"" - It isn't clear what these broader goals are that have been identified. Therefore it isn't possible to evaluate this claim.
4) ""For multi-dimensional action spaces, the viewport could be repurposed to display the variance of the action distribution, plot different projections of the action distribution, or use more sophisticated techniques (Huber)."" - It would be clearer to actually state what the sophisticated techniques from Huber are here.
ii. The framework could be clearer that it applies most directly as described on single agent RL. The same approach could be used with multi-agent RL but the observation state and visualisations around that get more confusing when there are multiple potentially different sets of observations. Is this not the case, please clarify?
d. Experimental rigour
i. The paper does include 3 extremely brief examples of how the tool might be used. However, it does not include any experiments to suggest that this tool would actually improve the debugging process for training RL in a real user study. Not every paper requires a user study, however, the contributions proposed by this particular manuscript require validation at some level from actual RL users (even a case study with some feedback from users would address this to some extent).
e. Novelty in Related Work
i. The manuscript contains references to several relevant publications that can be compared to the current work. However, the paper is also missing references to many related and relevant works in the space of debugging reinforcement learning using visualisations, especially with an eye towards explainable reinforcement learning. See a sampling below.
1) @inproceedings{ Rupprecht2020Finding, title={Finding and Visualizing Weaknesses of Deep Reinforcement Learning Agents}, author={Christian Rupprecht and Cyril Ibrahim and Christopher J. Pal}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rylvYaNYDH} }
2) @inproceedings{ Atrey2020Exploratory, title={Exploratory Not Explanatory: Counterfactual Analysis of Saliency Maps for Deep Reinforcement Learning}, author={Akanksha Atrey and Kaleigh Clary and David Jensen}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=rkl3m1BFDB} }
3) @inproceedings{ Puri2020Explain, title={Explain Your Move: Understanding Agent Actions Using Specific and Relevant Feature Attribution}, author={Nikaash Puri and Sukriti Verma and Piyush Gupta and Dhruv Kayastha and Shripad Deshmukh and Balaji Krishnamurthy and Sameer Singh}, booktitle={International Conference on Learning Representations}, year={2020}, url={https://openreview.net/forum?id=SJgzLkBKPB} }
4) @article{reddy2019learning, title={Learning human objectives by evaluating hypothetical behavior}, author={Reddy, Siddharth and Dragan, Anca D and Levine, Sergey and Legg, Shane and Leike, Jan}, journal={arXiv preprint arXiv:1912.05652}, year={2019} }
5) @inproceedings{mcgregor2015facilitating, title={Facilitating testing and debugging of Markov Decision Processes with interactive visualization}, author={McGregor, Sean and Buckingham, Hailey and Dietterich, Thomas G and Houtman, Rachel and Montgomery, Claire and Metoyer, Ronald}, booktitle={2015 IEEE Symposium on Visual Languages and Human-Centric Computing (VL/HCC)}, pages={53--61}, year={2015}, organization={IEEE} }
6) @article{puiutta2020explainable, title={Explainable Reinforcement Learning: A Survey}, author={Puiutta, Erika and Veith, Eric}, journal={arXiv preprint arXiv:2005.06247}, year={2020} }
7) @book{calvaresi2019explainable, title={Explainable, Transparent Autonomous Agents and Multi-Agent Systems: First International Workshop, EXTRAAMAS 2019, Montreal, QC, Canada, May 13--14, 2019, Revised Selected Papers}, author={Calvaresi, Davide and Najjar, Amro and Schumacher, Michael and Fr{\""a}mling, Kary}, volume={11763}, year={2019}, publisher={Springer Nature} }
8) @inproceedings{juozapaitis2019explainable, title={Explainable reinforcement learning via reward decomposition}, author={Juozapaitis, Zoe and Koul, Anurag and Fern, Alan and Erwig, Martin and Doshi-Velez, Finale}, booktitle={IJCAI/ECAI Workshop on Explainable Artificial Intelligence}, year={2019} }
9) @misc{sundararajan2020shapley, title={The many Shapley values for model explanation}, author={Mukund Sundararajan and Amir Najmi}, year={2020}, eprint={1908.08474}, archivePrefix={arXiv}, primaryClass={cs.AI} }
10) @misc{madumal2020distal, title={Distal Explanations for Model-free Explainable Reinforcement Learning}, author={Prashan Madumal and Tim Miller and Liz Sonenberg and Frank Vetere}, year={2020}, eprint={2001.10284}, archivePrefix={arXiv}, primaryClass={cs.AI} }
11) @article{Sequeira_2020, title={Interestingness elements for explainable reinforcement learning: Understanding agents’ capabilities and limitations}, volume={288}, ISSN={0004-3702}, url={http://dx.doi.org/10.1016/j.artint.2020.103367}, DOI={10.1016/j.artint.2020.103367}, journal={Artificial Intelligence}, publisher={Elsevier BV}, author={Sequeira, Pedro and Gervasio, Melinda}, year={2020}, month={Nov}, pages={103367} }
12) @article{Fukuchi_2017, title={Autonomous Self-Explanation of Behavior for Interactive Reinforcement Learning Agents}, ISBN={9781450351133}, url={http://dx.doi.org/10.1145/3125739.3125746}, DOI={10.1145/3125739.3125746}, journal={Proceedings of the 5th International Conference on Human Agent Interaction}, publisher={ACM}, author={Fukuchi, Yosuke and Osawa, Masahiko and Yamakawa, Hiroshi and Imai, Michita}, year={2017}, month={Oct} }
4. Recommendation
a. I recommend this paper for rejection as the degree of change needed to validate the contribution through a user study or other validation is likely not feasible in the time and space needed. However, the contribution is potentially valuable to RL, so with the inclusion of this missing evaluation and additions to related work/contextualisation of the contribution, I would consider increasing my score and changing my recommendation.
5. Minor Comments/Suggestions
a. It is recommended to use the TensorFlow whitepaper citation for TensorBoard (https://arxiv.org/abs/1603.04467). This is the official response (https://github.com/tensorflow/tensorboard/issues/3437).","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '5', '5', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The results, while mostly based on ""standard"" techniques, are not obvious a priori, and require a fair degree of technical competency (i.e., the techniques are really only ""standard"" to a small group of experts).",NIPS_2016_370,NIPS_2016,", and while the scores above are my best attempt to turn these strengths and weaknesses into numerical judgments, I think it's important to consider the strengths and weaknesses holistically when making a judgment. Below are my impressions. First, the strengths: 1. The idea to perform improper unsupervised learning is an interesting one, which allows one to circumvent certain NP hardness results in the unsupervised learning setting. 2. The results, while mostly based on ""standard"" techniques, are not obvious a priori, and require a fair degree of technical competency (i.e., the techniques are really only ""standard"" to a small group of experts). 3. The paper is locally well-written and the technical presentation flows easily: I can understand the statement of each theorem without having to wade through too much notation, and the authors do a good job of conveying the gist of the proofs. Second, the weaknesses: 1. The biggest weakness is some issues with the framework itself. In particular: 1a. It is not obvious that ""k-bit representation"" is the right notion for unsupervised learning. Presumably the idea is that if one can compress to a small number of bits, one will obtain good generalization performance from a small number of labeled samples. But in reality, this will also depend on the chosen model class used to fit this hypothetical supervised data: perhaps there is one representation which admits a linear model, while another requires a quadratic model or a kernel. It seems more desirable to have a linear model on 10,000 bits than a quadratic model on 1,000 bits. This is an issue that I felt was brushed under the rug in an otherwise clear paper. 1b. It also seems a bit clunky to work with bits (in fact, the paper basically immediately passes from bits to real numbers). 1c. Somewhat related to 1a, it wasn't obvious to me if the representations implicit in the main results would actually lead to good performance if the resulting features were then used in supervised learning. I generally felt that it would be better if the framework was (a) more tied to eventual supervised learning performance, and (b) a bit simpler to work with. 2. I thought that the introduction was a bit grandiose in comparing itself to PAC learning. 3. The main point (that improper unsupervised learning can overcome NP hardness barriers) didn't come through until I had read the paper in detail. When deciding what papers to accept into a conference, there are inevitably cases where one must decide between conservatively accepting only papers that are clearly solid, and taking risks to allow more original but higher-variance papers to reach a wide audience. I generally favor the latter approach, I think this paper is a case in point: it's hard for me to tell whether the ideas in this paper will ultimately lead to a fruitful line of work, or turn out to be flawed in the end. So the variance is high, but the expected value is high as well, and I generally get the sense from reading the paper that the authors know what they are doing. So I think it should be accepted. Some questions for the authors (please answer in rebuttal): -Do the representations implicit in Theorems 3.2 and Theorem 4.1 yield features that would be appropriate for subsequent supervised learning of a linear model (i.e., would linear combinations of the features yield a reasonable model family)? -How easy is it to handle e.g. manifolds defined by cubic constraints with the spectral decoding approach?","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,
"2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't.",ARR_2022_223_review,ARR_2022,"The majority of the weaknesses in the paper seem to stem from confusion and inconsistencies between some of the prose and the results.
1. Figure 2, as it is, isn't totally convincing there is a gap in convergence times. The x-axis of the graph is time, when it would have been more convincing using steps. Without an efficient, factored attention for prompting implementation a la [He et al. (2022)](https://arxiv.org/abs/2110.04366) prompt tuning can cause slow downs from the increased sequence length. With time on the x-axis it is unclear if prompt tuning requires more steps or if each step just takes more time. Similarly, this work uses $0.001$ for the learning rate. This is a lot smaller than the suggested learning rate of $0.3$ in [Lester et al (2021)](https://aclanthology.org/2021.emnlp-main.243/), it would have been better to see if a larger learning rate would have closed this gap. Finally, this gap with finetuning is used as a motivating examples but the faster convergence times of things like their initialization strategy is never compared to finetuning.
2. Confusion around output space and label extraction. In the prose (and Appendix A.3) it is stated that labels are based on the predictions at `[MASK]` for RoBERTa Models and the T5 Decoder for generation. Scores in the paper, for example the random vector baseline for T5 in Table 2 suggest that the output space is restricted to only valid labels as a random vector of T5 generally produces nothing. Using this rank classification approach should be stated plainly as direct prompt reuse is unlikely to work for actual T5 generation.
3. The `laptop` and `restaurant` datasets don't seem to match their descriptions in the appendix. It is stated that they have 3 labels but their random vector performance is about 20% suggesting they actually have 5 labels?
4. Some relative performance numbers in Figure 3 are really surprising, things like $1$ for `MRPC` to `resturant` transfer seem far too low, `laptop` source to `laptop` target on T5 doesn't get 100, Are there errors in the figure or is where something going wrong with the datasets or implementation?
5. Prompt similarities are evaluated based on correlation with zero-shot performance for direct prompt transfer. Given that very few direct prompt transfers yield gain in performance, what is actually important when it comes to prompt transferability is how well the prompt works as an initialization and does that boost performance. Prompt similarity tracking zero-shot performance will be a good metric if that is in turn correlated with transfer performance. The numbers from Table 1 generally support that this as a good proxy method as 76% of datasets show small improvements when using the best zero-shot performing prompt as initialization when using T5 (although only 54% of datasets show improvement for RoBERTa). However Table 2 suggests that this zero-shot performance isn't well correlated with transfer performance. In only 38% of datasets does the best zero-shot prompt match the best prompt to use for transfer (And of these 5 successes 3 of them are based on using MNLI, a dataset well known for giving strong transfer results [(Phang et al., 2017)](https://arxiv.org/abs/1811.01088)). Given that zero-shot performance doesn't seem to be correlated with transfer performance (and that zero-shot transfer is relatively easy to compute) it seems like _ON_'s strong correlation would not be very useful in practice.
6. While recent enough that it is totally fair to call [Vu et al., (2021)](https://arxiv.org/abs/2110.07904) concurrent work, given the similarity of several approaches there should be a deeper discussion comparing the two works. Both the prompt transfer via initialization and the prompt similarity as a proxy for transferability are present in that work. Given the numerous differences (Vu et al transfer mostly focuses on large mixtures transferring to tasks and performance while this work focuses on task to task transfer with an eye towards speed. _ ON_ as an improvement over the Cosine similarities which are also present in Vu et al) it seems this section should be expanded considering how much overlap there is.
7. The majority of Model transfer results seem difficult to leverage. Compared to cross-task transfer, the gains are minimal and the convergence speed ups are small. Coupled with the extra time it takes to train the projector for _Task Tuning_ (which back propagation with the target model) it seems hard to imagine situations where this method is worth doing (that knowledge is useful). Similarly, the claim on line 109 that model transfer can significantly accelerate prompt tuning seems lie an over-claim.
8. Line 118 claims `embedding distances of prompts do not well indicate prompt transferability` but Table 4 shows that C$_{\text{average}}$ is not far behind _ON_. This claim seems over-reaching and should instead be something like ""our novel method of measuring prompt similarity via model activations is better correlated with transfer performance than embedding distance based measures""
1. Line 038: They state that GPT-3 showed extremely large LM can give remarkable improvements. I think it would be correct to have one of their later citations on continually developed LM as the one that showed that. GPT-3 mostly showed promise for Few-Shot evaluation, not that it get really good performance on downstream tasks.
2. Line 148: I think it would make sense to make a distinction between hard prompt work updates the frozen model (Schick and Schütez, etc) from ones that don't.
3. Line 153: I think it makes sense to include [_Learning How to Ask: Querying LMs with Mixtures of Soft Prompts_ (Qin and Eisner, 2021)](https://aclanthology.org/2021.naacl-main.410.pdf) in the citation list for work on soft prompts.
4. Figure 3: The coloring of the PI group makes the text very hard to read in Black and White.
5. Table 1: Including the fact that the prompt used for initialization is the one that performed best in direct transfer in the caption as well as the prose would make the table more self contained.
6. Table 2: Mentioning that the prompt used as cross model initialization is from _Task Tuning_ in the caption would make the table more self contained.
7. Line 512: It is mentioned that _ON_ has a drop when applied to T$5_{\text{XXL}}$ and it is suggested this has to do with redundancy as the models grow. I think this section could be improved by highlighting that the Cosine based metrics have a similar drop (suggesting this is a fact of the model rather than the fault of the _ON_ method). Similarly, Figure 4 shows the dropping correlation as the model grows. Pointing out the that the _ON_ correlation for RoBERTA$_{\text{large}}$ would fit the tend of correlation vs model size (being between T5 Base and T5 Large) also strengths the argument but showing it isn't an artifact of _ON_ working poorly on encoder-decoder models. I think this section should also be reordered to show that this drop is correlated with model size. Then the section can be ended with hypothesizing and limited exploration of model redundancy.
8. Figure 6. It would have been interesting to see how the unified label space worked for T5 rather than RoBERTAa as the generative nature of T5's decoding is probably more vulnerable to issue stemming from different labels.
9. _ ON_ could be pushed farther. An advantage of prompt tuning is that the prompt is transformed by the models attention based on the value of the prompt. Without having an input to the model, the prompts activations are most likely dissimilar to the kind of activations one would expect when actually using the prompt.
10. Line 074: This sentence is confusing. Perhaps something like ""Thus"" over ""Hence only""?
11. Line 165: Remove ""remedy,""","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).",CEPkRTOlut,EMNLP_2023,"No ethics section, but there are ethical issues that deserve discussion (see the ethics section).
Also a few, mostly minor points:
- When the corpus was created, participants were told to speak in such a way to make the intent of the speech unambiguous. This may lead to over-emphasis compared with natural speech. There was no mention of any evaluation of the data to avoid this.
- The corpus was created with only ambiguous sentences, and the non-ambiguous content was taken from another source. There is a chance that different recording qualities between news (LSCVR) and crowdsourced data could artificially raise the ability of the model to distinguish between ambiguous (tag 1 or 2) and non-ambiguous (tag 0) sentences.
- The amount of data used to train the text disambiguation model was significantly lower than the data used for training the end-to-end system. Given that the difference between the two proposed systems is only a few percentage points, it brings into question the conclusion that the direct model is clearly the better of the two (but they still are both demonstrably superior to the baseline).
- It would be hard to reproduce the fine tuning of the IndoBART model without a little more information. Was it fine-tuned for a certain number of steps, for example?","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '2', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"3. The paper does not clearly motivate GaRare; it lacks evidence or justification for GaRare's advantages over GaLore based on theoretical analysis. Additionally, a more detailed algorithmic presentation is needed, particularly to clarify the process of recovering updated parameters from projected gradients, which would enhance understanding.",pxclAomHat,ICLR_2025,"1. The paper does not explicitly link the quality of the optimization landscape to convergence speed or generalization performance, which undermines its stated goal of theoretically explaining the performance of LoRA and GaLore. Many claims also lack this connection (e.g., lines 60-61, 289-291, and 301-304), making them appear weaker and less convincing.
2. The theoretical results in this paper are derived from analyses of MLPs, whereas LLMs, with their attention mechanisms, layer normalization, etc., are more complex. Since there is a gap between MLPs and LLMs, and the paper does not directly validate its theoretical results on LLMs (instead of relying only on the convergence and performance outcomes), the theoretical conclusions are less convincing.
3. The paper does not clearly motivate GaRare; it lacks evidence or justification for GaRare's advantages over GaLore based on theoretical analysis. Additionally, a more detailed algorithmic presentation is needed, particularly to clarify the process of recovering updated parameters from projected gradients, which would enhance understanding.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.",NIPS_2017_356,NIPS_2017,"]
My major concerns about this paper is the experiment on visual dialog dataset. The authors only show the proposed model's performance on discriminative setting without any ablation studies. There is not enough experiment result to show how the proposed model works on the real dataset. If possible, please answer my following questions in the rebuttal.
1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters?
2: There are two test settings in visual dialog, while the Table 1 only shows the result on discriminative setting. It's known that discriminative setting can not apply on real applications, what is the result on generative setting?
3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
2. Missing link to similar work on Continuous Conditional Random Fields [Ristovski 2013] and Continuous Conditional Neural Fields [Baltrusaitis 2014] that has a similar structure of the CRF and ability to perform exact inference.,NIPS_2019_220,NIPS_2019,"1. Unclear experimental methodology. The paper states that 300W-LP is used to train the model, but later it is claimed same procedure is used as was used for baselines. Most baselines do not use 300W-LP dataset in their training. Is 300W-LP used in all experiments or just some? If it is used in all this would provide an unfair advantage to the proposed method. 2. Missing link to similar work on Continuous Conditional Random Fields [Ristovski 2013] and Continuous Conditional Neural Fields [Baltrusaitis 2014] that has a similar structure of the CRF and ability to perform exact inference. 3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer)","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '5', '5', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. It would be interesting to try to explain why WPA works -- with np.ones input, what is the model predicting? Would any input serve as white paper? Figure 2 seems to suggest that Gaussian noise input does not work as well as WPA. Why? Authors spend lot of time showing WPA improves the test performance of the original model, but fails to provide useful insights on how WPA works -- this is particularly important because it can spark future research directions.",ICLR_2022_2470,ICLR_2022,"Weakness:
The idea is a bit simple -- which in of itself is not a true weakness. ResNet as an idea is not complicated at all. I find it disheartening that the paper did not really tell readers how to construct a white paper in section 3 (if I simply missed it, please let me know). However, the code in the supplementary materials helped. White paper is constructed as follow:
white_paper_gen = torch.ones(args.train_batch, 3, 32, 32)
It offers another way of constructing white paper, which is
white_paper_gen = 255 * np.ones((32, 32, 3), dtype=np.uint8)
white_paper_gen = Image.fromarray(white_paper_gen)
white_paper_gen = transforms.ToTensor()(white_paper_gen)
white_paper_gen = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))(white_paper_gen)
The code states that either version works similarly and does not affect the performance. I wonder if there are other white papers as well, for example np.zeros((32, 32, 3)) -- most CNN models add explicit bias terms in their CNN kernel. Would a different white paper reveal different bias in the model? I don't think the paper answers this question or discusses it. 2. Section 4 ""Is white paper training harmful to the model?"" -- the evidences do not seem to support the claim. The evidences are 1). Only projection head (CNN layers) are affected but not classification head (FCN layer); 2). Parameter changes are small. None of these constitute as a direct support that the training is not ""harmful"" to the model. This point can simply be illustrated by the experimental results 3. Section 5.1 and 5.2 mainly build the narrative that WPA improves the test performance (generalization performance), but they are indirect evidence to support that WPA does in fact alleviate shortcut learning. Only Section 5.3 and Table 6 directly show whether WPA does what it's designed to do. A suggestion is to discuss the result of Section 5.3 more. 4. It would be interesting to try to explain why WPA works -- with np.ones input, what is the model predicting? Would any input serve as white paper? Figure 2 seems to suggest that Gaussian noise input does not work as well as WPA. Why? Authors spend lot of time showing WPA improves the test performance of the original model, but fails to provide useful insights on how WPA works -- this is particularly important because it can spark future research directions.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', 'X', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
1.It seems that the method part is very similar to the related work cited in the paper: Generating Adversarial Disturbances for Controller Verification. Could the author provide more clarification on this?,ICLR_2023_1979,ICLR_2023,"Weakness: 1.It seems that the method part is very similar to the related work cited in the paper: Generating Adversarial Disturbances for Controller Verification. Could the author provide more clarification on this? 2.Experimental comparison to RRT* seems not good: Even though the RRT* baseline is an oracle without partial observability, the visible region is still very large, which covers more than half of the obstacles. In this case, the naive RRT* (as mentioned in supp C1) can still outperform the proposed method by a large margin on the first task.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '3', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The proposed method adopts a proposal generator pretrained on MSCOCO which aggregates more information. Is it fair to compared with other methods? Besides, could the proposed technique propmotes existing Class incremental semantic segmentation methods. The authors adequately addressed the limitations and potential negative societal impact of their work.",NIPS_2022_2513,NIPS_2022,"Weakness:
The claim in Line 175~176 is not validated which it is valuable to see whether the proposed method could prevents potential classes from being incorrectly classified into historical classed.
In Tab. 1, for VOC 2-2 (10 tasks) and VOC 19-1 (2 tasks) MicroSeg gets inferior performance compared with SSUL, the reason should be explained. It also appear in ADE 100-50 (2 tasks) in Tab. 2.
The proposed method adopts a proposal generator pretrained on MSCOCO which aggregates more information. Is it fair to compared with other methods? Besides, could the proposed technique propmotes existing Class incremental semantic segmentation methods.
The authors adequately addressed the limitations and potential negative societal impact of their work.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '3', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '2', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. End of Sec.2., there are two important parameters/thresholds to set. One is the minimum cluster size and the other is the conductance threshold. However, the experimental section (Sec. 3) did not mention or discuss how these parameters are set and how sensitive the performance is with respect to these parameters.",NIPS_2016_287,NIPS_2016,"weakness, however, is the experiment on real data where no comparison against any other method is provided. Please see the details comments below.1. While [5] is a closely related work, it is not cited or discussed at all in Section 1. I think proper credit should be given to [5] in Sec. 1 since the spacey random walk was proposed there. The difference between the random walk model in this paper and that in [5] should also be clearly stated to clarify the contributions. 2. The AAAI15 paper titled ""Spectral Clustering Using Multilinear SVD: Analysis, Approximations and Applications"" by Ghoshdastidar and Dukkipati seems to be a related work missed by the authors. This AAAI15 paper deals with hypergraph data with tensors as well so it should be discussed and compared against to provide a better understanding of the state-of-the-art. 3. This work combines ideas from [4], [5], and [14] so it is very important to clearly state the relationships and differences with these earlier works. 4. End of Sec. 2., there are two important parameters/thresholds to set. One is the minimum cluster size and the other is the conductance threshold. However, the experimental section (Sec. 3) did not mention or discuss how these parameters are set and how sensitive the performance is with respect to these parameters. 5. Sec. 3.2 and Sec. 3.3: The real data experiments study only the proposed method and there is no comparison against any existing method on real data. Furthermore, there is only some qualitative analysis/discussion on the real data results. Adding some quantitative studies will be more helpful to the readers and researchers in this area. 6. Possible Typo? Line 131: ""wants to transition"".","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.,NIPS_2017_330,NIPS_2017,"- Section 4 is very tersely written (maybe due to limitations in space) and could have benefitted with a slower development for an easier read.
- Issues of convergence, especially when applying gradient descent over a non-Euclidean space, is not addressed
In all, a rather thorough paper that derives an efficient way to compute gradients for optimization on LDSs modeled using extended subspaces and kernel-based similarity. At one hand, this leads to improvements over some competing methods. Yet, at its core, the paper avoids handling of the harder topics including convergence and any analysis of the proposed optimization scheme. None the less, the derivation of the gradient computations is interesting by itself. Hence, my recommendation.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '5', '3', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['X', '4', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
* This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.,NIPS_2020_257,NIPS_2020,"* In terms of novelty, note that both the motivation for the model as well as the initial parts of it hold similarities to some prior works. See detailed description in the relation to prior work section. * I would be happy to see results about generalization not only for the CLEVR dataset, but also for natural images datasets where there is larger variance both in the language and visual complexity. There are multiple datasets for generalization in VQA that can be used for that such as CP-VQA and also some splits of GQA. For the CLEVR dataset, the model is basically based on using an object detector to recognize the objects and their properties and build a semantic graph that represents the image. While other approaches that are compared to for this task use object detectors as well, there are many approaches for CLEVR (such as the Neural Module Network, Relation Network, MAC and FiLM) that do not use such strong supervision and therefore the comparison between these approaches in the experimental section is not completely valid. For better comparability, I would be interested to see generalization results when these models are also being fed with at least object-based bounding-boxes representations instead of the earlier commonly used spatial features, as is very common in VQA in the last years (see bottom-up attention networks). * This can be open for debate but I personally believe that the need for reinforcement learning for a static VQA task may be a potential weakness making the approach less data efficient and harder to train the models that use gradient descent.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '1', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '1', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.",BTr3PSlT0T,ICLR_2025,"- I express skepticism about whether the number of videos in the benchmark can achieve a robust assessment. The CVRR-ES benchmark includes only 214 videos, with the shortest video being just 2 seconds. Upon reviewing several videos from the anonymous link, I noticed a significant proportion of short videos. I question whether such short videos can adequately cover 11 categories. Moreover, current work that focuses solely on designing Video-LLMs, without specifically constructing evaluation benchmarks, provides a much larger number of assessment videos than the 214 included in CVRR-ES, for example, Tarsier [1].
- As mentioned in the previous question, the distribution of videos of different lengths within the benchmark is crucial for the assessment of reasoning ability and robustness, and the paper does not provide relevant explanations. The authors should include a table showing the distribution of video lengths across the dataset, and explain how they ensured a balanced representation of different video lengths across the 11 categories.
- In the motivation, it is mentioned that the goal is to build human-centric AI systems. Does the paper's reflection on this point merely consist of providing a human baseline? I think that offering more fine-grained visual examples would be more helpful for human-AI comparisons.
- I think that the contribution of the DSCP is somewhat overstated and lacks novelty. Such prompt engineering-based methods have already been applied in many works for data generation, model evaluation, and other stages. The introduction and ablation experiments of this technology in the paper seem redundant.
- The discussion on DSCP occupies a significant portion of the experimental analysis. I think that the current analysis provided in the paper lacks insight and does not fully reflect the value of CVRR-ES, especially in terms of human-machine comparison.
- The phrase should be ""there exist a few limitations"" instead of ""there exist few limitations"" in line 520.
- The paper does not provide prompt templates for all the closed-source and open-source Video-LLMs used, which will influence the reproducibility.
The problems discussed in this paper are valuable, but the most crucial aspects of benchmark construction and evaluation are not entirely convincing. Instead, a significant amount of space is dedicated to introducing the DSCP method. I don't think it meets the acceptance standards of ICLR yet. I will consider modifying the score based on the feedback from other reviewers and the authors' responses. ***
[1] Wang J, Yuan L, Zhang Y. Tarsier: Recipes for Training and Evaluating Large Video Description Models[J]. arXiv preprint arXiv:2407.00634, 2024.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '2', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
8.L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case? Minor Points:,NIPS_2017_53,NIPS_2017,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.",OvoRkDRLVr,ICLR_2024,"1. The paper proposes a multimodal framework built atop a frozen Large Language Model (LLM) aimed at seamlessly integrating and managing various modalities. However, this approach seems to be merely an extension of the existing InstructBLIP.
2. Additionally, the concept of extending to multiple modalities, such as the integration of audio and 3D modalities, has already been proposed in prior works like PandaGPT. Therefore, the paper appears to lack sufficient novelty in both concept and methodology.
3. In Table 1, there is a noticeable drop in performance for X-InstructBLIP. Could you please clarify the reason behind this? If this drop is due to competition among different modalities, do you propose any solutions to mitigate this issue?
4. The promised dataset has not yet been made publicly available, so a cautious approach should be taken regarding this contribution until the dataset is openly accessible.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '5', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['X', '3', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '4', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"1. The novelty is limited. The proposed method is too similar to other attentional modules proposed in previous works [1, 2, 3]. The group attention design seems to be related to ResNeSt [4] but it is not discussed in the paper. Although these works did not evaluate their performance on object detection and instance segmentation, the overall structures between these modules and the one that this paper proposed are pretty similar.",ICLR_2023_3203,ICLR_2023,"1. The novelty is limited. The proposed method is too similar to other attentional modules proposed in previous works [1, 2, 3]. The group attention design seems to be related to ResNeSt [4] but it is not discussed in the paper. Although these works did not evaluate their performance on object detection and instance segmentation, the overall structures between these modules and the one that this paper proposed are pretty similar.
2. Though the improvement is consistent for different frameworks and tasks, the relative gains are not very strong. For most of the baselines, the proposed methods can only achieve just about 1% gain on a relative small backbone ResNet-50. As the proposed method introduces global pooling into its structure, it might be easy to improve a relatively small backbone since it is with a smaller receptive field. I suspect whether the proposed method still works well on large backbone models like Swin-B or Swin-L.
3. Some of the baseline results do not matched with their original paper. I roughly checked the original Mask2former paper but the performance reported in this paper is much lower than the one reported in the original Mask2former paper. For example, for panoptic segmentation, Mask2former reported 51.9 but in this paper it's 50.4, and the AP for instance segmentation reported in the original paper is 43.7 but here what reported is 42.4.
Meanwhile, there are some missing references about panoptic segmentation that should be included in this paper [5, 6]. Reference
[1] Chen, Yunpeng, et al. ""A^ 2-nets: Double attention networks."" NeurIPS 2018.
[2] Cao, Yue, et al. ""Gcnet: Non-local networks meet squeeze-excitation networks and beyond."" T-PAMI 2020
[3] Yinpeng Chen, et al. Dynamic convolution: Attention over convolution kernels. CVPR 2020.
[4] Zhang, Hang, et al. ""Resnest: Split-attention networks."" CVPR workshop 2022.
[5] Zhang, Wenwei, et al. ""K-net: Towards unified image segmentation."" Advances in Neural Information Processing Systems 34 (2021): 10326-10338.
[6] Wang, Huiyu, et al. ""Max-deeplab: End-to-end panoptic segmentation with mask transformers."" CVPR 2021","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '2', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '1', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"1. Fig. 3 e. Since the preactivation values of two networks are the same membrane potentials, their output cosine similarity will be very high. Why not directly illustrate the results of the latter loss term of Eqn 13?",ICLR_2023_2283,ICLR_2023,"1. 1. The symbols in Section 4.3 are not very clearly explained. 2. This paper only experiments on the very small time steps (e.g.1、2) and lack of some experiments on slightly larger time steps (e.g. 4、6) to make better comparisons with other methods. I think it is necessary to analyze the impact of the time step on the method proposed in this paper. 3. Lack of experimental results on ImageNet to verify the method.
Questions: 1. Fig. 3 e. Since the preactivation values of two networks are the same membrane potentials, their output cosine similarity will be very high. Why not directly illustrate the results of the latter loss term of Eqn 13? 2. Is there any use of recurrent connections in the experiments in this paper? Apart from appendix A.5, I do not see the recurrent connections.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.",eI6ajU2esa,ICLR_2024,"- This paper investigates the issue of robustness in video action recognition, but it lacks comparison with test-time adaptation (TTA) methods, such as [A-B]. These TTA methods also aim to adapt to out-of-distribution data when the input data is disturbed by noise. Although these TTA methods mainly focus on updating model parameters, and this paper primarily focuses on adjusting the input data, how to prove that data processing is superior to model parameter adjustment? I believe a comparison should be made based on experimental results.
- Under noisy conditions, many TTA methods can achieve desirable results, while the improvement brought by this paper's method is relatively low.
- In appendix A.2.1, under noisy conditions, the average performance improvement brought by this paper's method is very low and can even be counterproductive under certain noise conditions. Does this indicate an issue with the approach of changing input data?
- How to verify the reliability of the long-range photometric consistency in section 3.3? Are there any ablation study results reflecting the performance gain brought by each part?
- The explanation of the formula content in Algorithm 1 in the main body is not clear enough.
[A] Temporal Coherent Test-Time Optimization for Robust Video Classification. ICLR23
[B] Video Test-Time Adaptation for Action Recognition. CVPR23","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. Section 3.2.1: The first expression for J ( θ ) is incorrect, which should be Q ( s t 0 , π θ ( s t 0 ) ) .",ICLR_2021_863,ICLR_2021,"Weakness 1. The presentation of the paper should be improved. Right now all the model details are placed in the appendix. This can cause confusion for readers reading the main text. 2. The necessity of using techniques includes Distributional RL and Deep Sets should be explained more thoroughly. From this paper, the illustration of Distributional RL lacks clarity. 3. The details of state representation are not explained clear. For an end-to-end method like DRL, it is crucial for state representation for training a good agent, as for network architecture. 4. The experiments are not comprehensive for validating that this algorithm works well in a wide range of scenarios. The efficiency, especially the time efficiency of the proposed algorithm, is not shown. Moreover, other DRL benchmarks, e.g., TD3 and DQN, should also be compared with. 5. There are typos and grammar errors.
Detailed Comments 1. Section 3.1, first paragraph, quotation mark error for ""importance"". 2. Appendix A.2 does not illustrate the state space representation of the environment clearly. 3. The authors should state clearly as to why the complete state history is enough to reduce POMDP for the no-CSI case. 4. Section 3.2.1: The first expression for J ( θ )
is incorrect, which should be Q ( s t 0 , π θ ( s t 0 ) )
. 5. The paper did not explain Figure 2 clearly. In particular, what does the curve with the label ""Expected"" in Fig. 2(a) stand for? Not to mention there are multiple misleading curves in Fig. 2(b)&(c). The benefit of introducing distributional RL is not clearly explained. 6. In Table 1, only 4 classes of users are considered in the experiment sections, which might not be in accordance with practical situations, where there can be more classes of users in the real system and more user numbers. 7. In the experiment sections, the paper only showed the Satisfaction Probability of the proposed method is larger than conventional methods. The algorithm complexity, especially the time complexity of the proposed method in an ultra multi-user scenario, is not shown. 8. There is a large literature on wireless scheduling with latency guarantees from the networking community, e.g., Sigcomm, INFOCOM, Sigmetrics. Representative results there should also be discussed and compared with.
====== post rebuttal: My concern regarding the experiments remains. I will keep my score unchanged.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"8: s/expensive approaches2) allows/expensive approaches,2) allows/ p.8: s/estimates3) is/estimates, and3) is/ In the references: Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers. Dusenberry et al. (2020) was published in ICML 2020 Osawa et al. (2019) was published in NeurIPS 2019 Swiatkowski et al. (2020) was published in ICML 2020 p. 13, supplement, Fig.",ICLR_2021_872,ICLR_2021,"The authors push on the idea of scalable approximate inference, yet the largest experiment shown is on CIFAR-10. Given this focus on scalability, and the experiments in recent literature in this space, I think experiments on ImageNet would greatly strengthen the paper (though I sympathize with the idea that this can a high bar from a resources standpoint).
As I noted down below, the experiments currently lack results for the standard variational BNN with mean-field Gaussians. More generally, I think it would be great to include the remaining models from Ovadia et al. (2019). More recent results from ICML could also useful to include (as referenced in the related works sections). Recommendation
Overall, I believe this is a good paper, but the current lack of experiments on a dataset larger than CIFAR-10, while also focusing on scalability, make it somewhat difficult to fully recommend acceptance. Therefore, I am currently recommending marginal acceptance for this paper.
Additional comments
p. 5-7: Including tables of results for each experiment (containing NLL, ECE, accuracy, etc.) in the main text would be helpful to more easily assess
p. 7: For the MNIST experiments, in Ovadia et al. (2019) they found that variational BNNs (SVI) outperformed all other methods (including deep ensembles) on all shifted and OOD experiments. How does your proposed method compare? I think this would be an interesting experiment to include, especially since the consensus in Ovadia et al. (2019) (and other related literature) is that full variational BNNs are quite promising but generally methodologically difficult to scale to large problems, with relative performance degrading even on CIFAR-10. Minor
p. 6: In the phrase ""for 'in-between' uncertainty"", the first quotation mark on 'in-between' needs to be the forward mark rather than the backward mark (i.e., ‘ i n − b e t w e e n ′ ).
p. 7: s/out of sitribution/out of distribution/
p. 8: s/expensive approaches 2) allows/expensive approaches, 2) allows/
p. 8: s/estimates 3) is/estimates, and 3) is/
In the references:
Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers.
Dusenberry et al. (2020) was published in ICML 2020
Osawa et al. (2019) was published in NeurIPS 2019
Swiatkowski et al. (2020) was published in ICML 2020
p. 13, supplement, Fig. 5: error bar regions should be upper and lowered bounded by [0, 1] for accuracy.
p. 13, Table 2: Splitting this into two tables, one for MNIST and one for CIFAR-10, would be easier to read.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', '5', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
3. I didn't find all parameter values. What are the model parameters for task 1? What lambda was chosen for the Boltzmann policy. But more importantly: How were the parameters chosen? Maximum likelihood estimates?,NIPS_2016_339,NIPS_2016,"weakness of the model. How would the values in table 1 change without this extra assumption? 3. I didn't find all parameter values. What are the model parameters for task 1? What lambda was chosen for the Boltzmann policy. But more importantly: How were the parameters chosen? Maximum likelihood estimates? 4. An answer to this point may be beyond the scope of this work, but it may be interesting to think about it. It is mentioned (lines 104-106) that ""the examples [...] should maximally disambiguate the concept being taught from other possible concepts"". How is disambiguation measured? How can disambiguation be maximized? Could there be an information theoretic approach to these questions? Something like: the teacher chooses samples that maximally reduce the entropy of the assumed posterior of the student. Does the proposed model do that? Minor points: â¢ line 88: The optimal policy is deterministic. Hence I'm a bit confused by ""the stochastic optimal policy"". Is above defined ""the Boltzmann policy"" meant? â¢ What is d and h in equation 2? â¢ line 108: ""to calculate this ..."" What is meant by ""this""? â¢ Algorithm 1: Require should also include epsilon. Does line 1 initialize the set of policies to an empty set? Are the policies in line 4 added to this set? Does calculateActionValues return the Q* defined in line 75? What is M in line 6? How should p_min be chosen? Why is p_min needed anyway? â¢ Experiment 2: Is the reward 10 points (line 178) or 5 points (line 196)? â¢ Experiment 2: Is 0A the condition where all tiles are dangerous? Why are the likelihoods so much larger for 0A? Is it reasonable to average over likelihoods that differ by more than an order of magnitude (0A vs 2A-C)? â¢ Text and formulas should be carefully checked for typos (e.g. line 10 in Algorithm 1: delta > epsilon; line 217: 1^-6;)","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods.",NIPS_2016_313,NIPS_2016,"Weakness: 1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim. 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc. 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods. 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work. 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '4', '3', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,
"4. Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?",NIPS_2017_631,NIPS_2017,"1.	The main contribution of the paper is CBN. But the experimental results in the paper are not advancing the state-of-art in VQA (on the VQA dataset which has been out for a while and a lot of advancement has been made on this dataset), perhaps because the VQA model used in the paper on top of which CBN is applied is not the best one out there. But in order to claim that CBN should help even the more powerful VQA models, I would like the authors to conduct experiments on more than one VQA model â favorably the ones which are closer to state-of-art (and whose codes are publicly available) such as MCB (Fukui et al., EMNLP16), HieCoAtt (Lu et al., NIPS16). It could be the case that these more powerful VQA models are already so powerful that the proposed early modulating does not help. So, it is good to know if the proposed conditional batch norm can advance the state-of-art in VQA or not.
2.	L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to?
3.	In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.
4.	Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?
5.	Figure 4 visualization: the visualization in figure (a) is from ResNet which is not finetuned at all. So, it is not very surprising to see that there are not clear clusters for answer types. However, the visualization in figure (b) is using ResNet whose batch norm parameters have been finetuned with question information. So, I think a more meaningful comparison of figure (b) would be with the visualization from Ft BN ResNet in figure (a).
6.	The first two bullets about contributions (at the end of the intro) can be combined together.
7.	Other errors/typos:
a.	L14 and 15: repetition of word âimagineâ
b.	L42: missing reference
c.	L56: impact -> impacts
Post-rebuttal comments:
The new results of applying CBN on the MRN model are interesting and convincing that CBN helps fairly developed VQA models as well (the results have not been reported on state-of-art VQA model). So, I would like to recommend acceptance of the paper.
However I still have few comments --
1. It seems that there is still some confusion about test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the performance of the MCB model to be 62.5% on test-standard split. However, 62.5% seems to be the performance of the MCB model on the test-dev split as per table 1 in the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).
2. The reproduced performance reported on MRN model seems close to that reported in the MRN paper when the model is trained using VQA train + val data. I would like the authors to clarify in the final version if they used train + val or just train to train the MRN and MRN + CBN models. And if train + val is being used, the performance can't be compared with 62.5% of MCB because that is when MCB is trained on train only. When MCB is trained on train + val, the performance is around 64% (table 4 in MCB paper).
3. The citation for the MRN model (in the rebuttal) is incorrect. It should be -- @inproceedings{kim2016multimodal,
title={Multimodal residual learning for visual qa},
author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle={Advances in Neural Information Processing Systems}, pages={361--369}, year={2016} }
4. As AR2 and AR3, I would be interested in seeing if the findings from ResNet carry over to other CNN architectures such as VGGNet as well.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '3', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- Lack of comparison with a highly relevant method. [1] also proposes to utilize the previous knowledge with ‘inter-task ensemble’, while enhancing the current task’s performance with ‘intra-task’ ensemble. Yet, the authors didn’t include the method comparison or performance comparison.",yIv4SLzO3u,ICLR_2024,"- Lack of comparison with a highly relevant method. [1] also proposes to utilize the previous knowledge with ‘inter-task ensemble’, while enhancing the current task’s performance with ‘intra-task’ ensemble. Yet, the authors didn’t include the method comparison or performance comparison.
- Novelty is limited. From my perspective, the submission simply applies the existing weight averaging and the bounded update to the class incremental learning problems.
- No theoretical justification or interpretation. Is there any theoretical guarantee that to what extent inter-task weight averaging or bounded update counter catastrophic forgetting?
- Though it shows in Table 3 that bounded model update mitigates the forgetting, the incorporation of bounded model update doesn’t seem to have enough motivation from the methodological perspective. The inter-task weight average is designed to incorporate both old and new knowledge, which is by design enough to tackle forgetting.
##### References:
1.Miao, Z., Wang, Z., Chen, W., & Qiu, Q. (2021, October). Continual learning with filter atom swapping. In International Conference on Learning Representations.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
- the required implicit call to the Witness oracle is confusing.,NIPS_2018_914,NIPS_2018,"of the paper are (i) the presentation of the proposed methodology to overcome that effect and (ii) the limitations of the proposed methods for large-scale problems, which is precisely when function approximation is required the most. While the intuition behind the two proposed algorithms is clear (to keep track of partitions of the parameter space that are consistent in successive applications of the Bellman operator), I think the authors could have formulated their idea in a more clear way, for example, using tools from Constraint Satisfaction Problems (CSPs) literature. I have the following concerns regarding both algorithms: - the authors leverage the complexity of checking on the Witness oracle, which is ""polynomial time"" in the tabular case. This feels like not addressing the problem in a direct way. - the required implicit call to the Witness oracle is confusing. - what happens if the policy class is not realizable? I guess the algorithm converges to an \empty partition, but that is not the optimal policy. minor: line 100 : ""a2 always moves from s1 to s4 deterministically"" is not true line 333 : ""A number of important direction"" -> ""A number of important directions"" line 215 : ""implict"" -> ""implicit"" - It is hard to understand the figure where all methods are compared. I suggest to move the figure to the appendix and keep a figure with less curves. - I suggest to change the name of partition function to partition value. [I am satisfied with the rebuttal and I have increased my score after the discussion]","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '2', '5', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '3', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
2) it loses the guarantee for finding the whole Pareto front. They have been properly discussed in the paper (see remark at the end of Section 4.1 and Conclusion). I do not see any potential negative societal impact of this work.,NIPS_2022_532,NIPS_2022,"1. Imitation Learning: The proposed method needs to be trained by behavioral cloning, which means 1) it requires a carefully well-designed algorithm (e.g., ODA-T/B/K) to generate the supervised data set. 2) More importantly, the data generated by ODA with a time limit L is indeed not a perfect teacher for behavioral cloning. Since all the ODAs are not designed and optimized for bounded time performance, their behavior (the state-action pairs) for the first L time is not the optimal policy under time limit L.
Since the generic algorithm procedure of ODA can be encoded as a MDP, is it possible to use reinforcement learning to train the model? Would the RL-based approach find a better policy for bounded time performance that is aware of the time limit T？
2. Performance with Different Time Limits: It seems that the proposed method is both trained and tested with a single fixed time limit T = 1000s for all problems. However, in practice, the applications could have very different run time limits. Will the proposed method generalize well to different time limits (such as 1/10/100/2000/5000s)?
3. Comparison with PMOCO: PMOCO is the only learning-based approach in the comparison, but it has a very small cardinality (feasible points) for most problems. Its approximated Pareto front is also relatively sparse (which means a small set of solutions) in Figure A.3. This result is a bit counter-intuitive.
In my understanding, PMOCO is a construction-based neural combinatorial optimization algorithm, of which one important advantage is the very fast run time. The PMOCO paper [1] reports it can generate 101 solutions for 100 MOKP(2-100) instances (hence 10,100 solutions in total) in only 15s, and 10,011 solutions for 100 MOTSP(3-100) instances (hence 1,001,100 solutions in total) in 33 minutes (~2000s). In this work, with a large time limit of 1,000s, I think POMO should be able to generate a dense set of solutions for each instance.
In addition, while a dataset with 100 instances could provide enough supervised ODA state-action pairs (with a time limit L = 1000s for each instance) for imitation learning, it is far from enough for PMOCO's RL-based training. Since PMOCO does not require any supervised data and the MOKP instances can be easily generated on the fly, is it more suitable to train PMOCO under the same wall-clock time with the proposed method?
[1] Pareto set learning for neural multi-objective combinatorial optimization. ICLR 2022.
The limitations of this work are 1) the requirement of ODAs and a well-designed IP solver; 2) it loses the guarantee for finding the whole Pareto front. They have been properly discussed in the paper (see remark at the end of Section 4.1 and Conclusion).
I do not see any potential negative societal impact of this work.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['2', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw'], 'labels': ['1', '1']}",,
"1) The proposed method cannot handle the headpose. While this paper defers this problem to a future work, a previous work (e.g., Gafni et al. ICCV 2021) is already able to control both facial expression and headpose. Why is it not possible to condition the headpose parameters in the NeRF beyond the facial expression similar to [Gafni et al. ICCV 2021]?",ICLR_2022_1420,ICLR_2022,"Weakness:
Lack of novelty. The key idea, i.e., combining foreground masks to remove the artifacts from the background, is not new. Separate handling of foreground from background is a common practice for dynamic scene novel view synthesis, and many recent methods do not even require the foreground masks for modeling dynamic scenes (they jointly model the foreground region prediction module, e.g., Tretschk et al. 2021).
Lack of controllability. 1) The proposed method cannot handle the headpose. While this paper defers this problem to a future work, a previous work (e.g., Gafni et al. ICCV 2021) is already able to control both facial expression and headpose. Why is it not possible to condition the headpose parameters in the NeRF beyond the facial expression similar to [Gafni et al. ICCV 2021]? 2) Even for controlling facial expression, it is highly limited to the mouth region only. From overall qualitative results and demo video, it is not clear the method indeed can handle overall facial expression including eyes, nose, and wrinkle details, and the diversity in mouth shape that the model can deliver is significantly limited.
Low quality. The results made by the proposed method are of quite low quality. 1) Low resolution: While many previous works introduce high-quality view synthesis results with high-resolution (512x512 or more), this paper shows low resolution results (256x256) for some reasons. Simply saying the problem of resources is not a convincing argument since many existing works already proved the feasibility of high resolution image synthesis using implicit function. Due to this low-resolution nature, many high-frequency details (e.g., facial wrinkles), which are the key to enabling photorealistic face image synthesis, are washed out. 2) In many cases, the conditioning facial expressions do not match that from the synthesized image. From the demo video, while mouth opening or closing are somehow synchronized with conditioning videos, there exists a mismatch in the style of the detailed mouth shape.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '2', '1', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '1', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.",ICLR_2023_3449,ICLR_2023,"1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '1']}",,
"2. Structural optimization seems one of the main components and it has been emphasized several times. However, it seems the optimization algorithm is directly from some previous works. That is a little bit confusing and reduces the contribution.",ICLR_2022_3332,ICLR_2022,"Weakness: 1. The writing needs a lot of improvement. Many of the concepts or notations are not explained. For example, what do “g_\alpha” and “vol(\alpha)” mean? What is an “encoding tree”(I believe it is not a common terminology)? Why can the encoding tree be used a tree kernel? Other than complexity, what is the theoretic advantage of using these encoding trees? 2. Structural optimization seems one of the main components and it has been emphasized several times. However, it seems the optimization algorithm is directly from some previous works. That is a little bit confusing and reduces the contribution. 3. From my understanding, the advantage over WL-kernel is mainly lower complexity, but compared to GIN or other GNNs, the complexity is higher. That is also not convincing enough. Of course, the performance is superior, so I do not see it as a major problem, but I would like to see more explanations of the motivation and advantages. 4. If we do not optimize the structural entropy but just use a random encoding tree, how does it perform? I think the authors need this ablation study to demonstrate the necessity of structural optimization.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '4', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
- This pipeline style method including two models does not give better average results for both XVNLI and MaRVL. Baseline models in the experiments are not well introduced.,4kuLaebvKx,EMNLP_2023,"- The chained impacts of image captioning and multilingual understanding model in the proposed pipeline. If the Image caption gives worse results and the final results could be worse. So The basic performance of the image caption model and multilingual language mode depends on the engineering model choice when it applies to zero-shot.
- This pipeline style method including two models does not give better average results for both XVNLI and MaRVL. Baseline models in the experiments are not well introduced.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '3', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', 'X', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,
"- The authors have reproduced a well-known result in the literature--left political bias in ChatGPT and in LLMs in general--using the ""coarse"" (their description) methodology of passing a binary stance classifier over ChatGPT's output. The observation that language models reproduce the biases of the corpora on which they're trained has been made at each step of the evolution of these models, from word2vec to BERT to ChatGPT, and so it's unclear why this observation needs to once again be made using the authors ""coarse"" methodology.",GeFFYOCkvS,EMNLP_2023,"- The authors have reproduced a well-known result in the literature--left political bias in ChatGPT and in LLMs in general--using the ""coarse"" (their description) methodology of passing a binary stance classifier over ChatGPT's output. The observation that language models reproduce the biases of the corpora on which they're trained has been made at each step of the evolution of these models, from word2vec to BERT to ChatGPT, and so it's unclear why this observation needs to once again be made using the authors ""coarse"" methodology.
- The authors' decision to filter by divisive topic using introduces an unacknowledged prior: for most of the topics given in the appendix (immigration, the death penalty, etc.), the choice to bother writing about the topic itself introduces bias. This choice will be reflected in both the frequency with which such articles appear and in the language used in those articles. The existence of the death penalty, for example, might be considered unproblematic on the right and, for that reason, one will see fewer articles on the subject in right-leaning newspapers. When they do appear, neutral language will be employed. The opposite is likely true for a left-leaning publication, for whom the existence of the death penalty is a problem: the articles will occur with more frequency and will employ less neutral language. For these reasons, it's not surprising that the authors' classifier, which was annotated via distant supervision using political slant tags assigned by Wikipedia, will tend to score most content generated by an LLM as left-leaning.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.",NIPS_2017_53,NIPS_2017,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.",NIPS_2017_645,NIPS_2017,"- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.
- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.
- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,
"1. The authors should make clear the distinction of when the proposed method is trained using only weak supervision and when it is semi-supervised trained. For instance, in Table 1, I think the proposed framework row refers to the semi-supervised version of the method, thus the authors should rename the column to ‘Fully supervised’ from ‘Supervised’. Maybe a better idea is to specify the data used to train ALL the parts of each model and have two big columns ‘Mixture training data’ and ‘Single source data’ which will make it much more prevalent of what is which.",4N97bz1sP6,ICLR_2024,"1. The authors should make clear the distinction of when the proposed method is trained using only weak supervision and when it is semi-supervised trained. For instance, in Table 1, I think the proposed framework row refers to the semi-supervised version of the method, thus the authors should rename the column to ‘Fully supervised’ from ‘Supervised’. Maybe a better idea is to specify the data used to train ALL the parts of each model and have two big columns ‘Mixture training data’ and ‘Single source data’ which will make it much more prevalent of what is which.
2. Building upon my previous argument, I think that when one is using these large pre-trained networks on single-source data like CLAP, the underlying method becomes supervised in a sense, or to put it more specifically supervised with unpaired data. The authors should clearly explain these differences throughout the manuscript.
3. I think the authors should include stronger text-based sound separation baselines like the model and ideally the training method that uses heterogeneous conditions to train the separation model in [A] which has already shown to outperform LASS-Net (Liu et. al 2022) which is almost always the best performing baseline in this paper.
I would be more than happy to increase my score if all the above weaknesses are addressed by the authors.
[A] Tzinis, E., Wichern, G., Smaragdis, P. and Le Roux, J., 2023, June. Optimal Condition Training for Target Source Separation. In ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (pp. 1-5). IEEE.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice.",NIPS_2020_1454,NIPS_2020,"- Small contributions over previous methods (NCNet [6] and Sparse NCNet [21]). Mostly (good) engineering. And despite that it seems hard to differentiate it from its predecessors, as it performs very similarly in practice. - Claims to be SOTA on three datasets, but this does not seem to be the case. Does not evaluate on what it trains on (see ""additional feedback"").","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '2', '1', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"- ""semantic"" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper.",NIPS_2018_25,NIPS_2018,"- My understanding is that R,t and K (the extrinsic and intrinsic parameters of the camera) are provided to the model at test time for the re-projection layer. Correct me in the rebuttal if I am wrong. If that is the case, the model will be very limited and it cannot be applied to general settings. If that is not the case and these parameters are learned, what is the loss function? - Another issue of the paper is that the disentangling is done manually. For example, the semantic segmentation network is the first module in the pipeline. Why is that? Why not something else? It would be interesting if the paper did not have this type of manual disentangling, and everything was learned. - ""semantic"" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper. - During evaluation at test time, how is the 3D alignment between the prediction and the groundtruth found? - Please comment on why the performance of GTSeeNet is lower than that of SeeNetFuse and ThinkNetFuse. The expectation is that groundtruth 2D segmentation should improve the results. - line 180: Why not using the same amount of samples for SUNCG-D and SUNCG-RGBD? - What does NoSeeNet mean? Does it mean D=1 in line 96? - I cannot parse lines 113-114. Please clarify.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.",NIPS_2021_2168,NIPS_2021,"1.The motivation to investigate a graph structured model is to capture the global dependency structure in the sentence which different from existing sequence models that tend to focus on the dependency between each word and its close preceding words. However,the encoder and decoder is based on Transformer，which can draw global dependencies in sentence. Therefore, I am a bit confused about the complex approach of this article.
2.Table3 does not use the C3D feature to do the experiment, I think this paper should compare the result of C3D features in MSR-VTT with [1].On the MSR-VTT dataset, the performance of the method has not improved much on CIDEr, within 0.3%.
3.In the ablation experiment, the performance without reinforcement learning dropped lower than without dependency tree.The two tables do not list the cases where dependency tree and RL are not used.
4.Constructing the generation tree, constructing the ground-truth dependency trees, calculating loss and reward all consume many computing resources. Is the time cost of the training process large?
[1] Zhang at al.Object Relational Graph with Teacher-Recommended Learning for Video Captioning.CVPR'2020 Typos:e.g.The last two rows of table3 are wrongly blackened on the METEOR on dataset MSR-VTT.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses).",ICLR_2022_562,ICLR_2022,"1. The main weakness of this paper is the experiments section. The results are presented only on CIFAR-10 dataset and do not consider many other datasets from Federated learning benchmarks (e.g., LEAF https://leaf.cmu.edu/). The authors should see relevant works like (FedProx https://arxiv.org/abs/1812.06127) and (FedMAX, https://arxiv.org/abs/2004.03657 (ECML 2020)) for details on different datasets and model types. If the experimental evaluation was comprehensive enough, this would be a very good paper (given the interesting and important problem it addresses). 2. One other thing (although this is not the main focus of this paper), the authors should provide comparisons between strategies that result in fast convergence (without sparsity) vs. sparse methods? For example, do non-sparse, fast convergence methods (like FedProx, FedMAX, and others) result in small enough number of epochs compared to sparse methods? Can the fast convergence methods be augmented with sparisity ideas successfully without resulting in significant loss of accuracy? Some discussion and possibly performance numbers are needed here.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. Does the claim ""It can be seen from the table that our proposed modules improve in both accuracy and completeness"" really hold? Why not use another dataset for the ablation study, e.g., the training set of Tanks & Temples or ETH3D?",NIPS_2022_738,NIPS_2022,"W1) The paper states that ""In order to introduce epipolar constraints into attention-based feature matching while maintaining robustness to camera pose and calibration inaccuracies, we develop a Window-based Epipolar Transformer (WET), which matches reference pixels and source windows near the epipolar lines."" It claims that it introduces ""a window-based epipolar Transformer (WET) for enhancing patch-to-patch matching between the reference feature and corresponding windows near epipolar lines in source features"". To me, taking a window around the epipolar line into account seems like an approximation to estimating the uncertainty region around the epipolar lines caused by inaccuracies in calibration and camera pose and then searching within this region (see [Förstner & Wrobel, Photogrammetric Computer Vision, Springer 2016] for a detailed derivation of how to estimate uncertainties). Is it really valid to claim this part of the proposed approach as novel?
W2) I am not sure how significant the results on the DTU dataset are: a) The difference with respect to the best performing methods is less than 0.1 mm (see Tab. 1). Is the ground truth sufficiently accurate enough that such a small difference is actually noticeable / measurable or is the difference due to noise or randomness in the training process? b) Similarly, there is little difference between the results reported for the ablation study in Tab. 4. Does the claim ""It can be seen from the table that our proposed modules improve in both accuracy and completeness"" really hold? Why not use another dataset for the ablation study, e.g., the training set of Tanks & Temples or ETH3D?
W3) I am not sure what is novel about the ""novel geometric consistency loss (Geo Loss)"". Looking at Eq. 10, it seems to simply combine a standard reprojection error in an image with a loss on the depth difference. I don't see how Eq. 10 provides a combination of both losses.
W4) While the paper discusses prior work in Sec. 2, there is mostly no mentioning on how the paper under review is related to these existing works. In my opinion, a related work section should explain the relation of prior work to the proposed approach. This is missing.
W5) There are multiple parts in the paper that are unclear to me: a) What is C in line 106? The term does not seem to be introduced. b) How are the hyperparameters in Sec. 4.1 chosen? Is their choice critical? c) Why not include UniMVSNet in Fig. 5, given that UniMVSNet also claims to generate denser point clouds (as does the paper under review)? d) Why use only N=5 images for DTU and not all available ones? e) Why is Eq. 9 a reprojection error? Eq. 9 measures the depth difference as a scalar and no projection into the image is involved. I don't see how any projection is involved in this loss.
Overall, I think this is a solid paper that presents a well-engineered pipeline that represents the current state-of-the-art on a challenging benchmark. While I raised multiple concerns, most of them should be easy to address. E.g., I don't think that removing the novelty claim from W1 would make the paper weaker. The main exception is the ablation study, where I believe that the DTU dataset is too easy to provide meaningful comparisons (the relatively small differences might be explained by randomness in the training process.
The following minor comments did not affect my recommendation:
References are missing for Pytorch and the Adam optimizer.
Post-rebuttal comments
Thank you for the detailed answers. Here are my comments to the last reply:
Q: Relationship to prior work.
Thank you very much, this addresses my concern.
A: Fig. 5 is not used to claim our method achieves the best performance among all the methods in terms of completeness, it actually indicates that our proposed method could help reconstruct complete results while keeping high accuracy (Tab. 1) compared with our baseline network [7] and the most relevant method [3]. In that context, we not only consider the quality of completeness but also the relevance to our method to perform comparison in Fig. 5.
As I understand lines 228-236 in the paper, in particular ""The quantitative results of DTU evaluation set are summarized in Tab. 1, where Accuracy and Completeness are a pair of official evaluation metrics. Accuracy is the percentage of generated point clouds matched in the ground truth point clouds, while Completeness measures the opposite. Overall is the mean of Accuracy and Completeness. Compared with the other methods, our proposed method shows its capability for generating denser and more complete point clouds on textureless regions, which is visualized in Fig. 5."", the paper seems to claim that the proposed method generates denser point clouds. Maybe this could be clarified?
A: As a) nearly all the learning-based MVS methods (including ours) take the DTU as an important dataset for evaluation, b) the GT of DTU is approximately the most accurate GT we can obtain (compared with other datasets), c) the final results are the average across 22 test scans, we think that fewer errors could indicate better performance. However, your point about the accuracy of DTU GT is enlightening, and we think it's valuable future work.
This still does not address my concern. My question is whether the ground truth is accurate enough that we can be sure that the small differences between the different components really comes from improvements provided by adding components. In this context, stating that ""the GT of DTU is approximately the most accurate GT we can obtain (compared with other datasets)"" does not answer this question as, even though DTU has the most accurate GT, it might not be accurate enough to measure differences at this level of accuracy (0.05 mm difference). If the GT is not accurate enough to differentiate in the 0.05 mm range, then averaging over different test scans will not really help. That ""nearly all the learning-based MVS methods (including ours) take the DTU as an important dataset for evaluation"" does also not address this question. Since the paper claims improvements when using the different components and uses the results to validate the components, I do not think that answering the question whether the ground truth is accurate enough to make these claims in future work is really an option. I think it would be better to run the ablation study on a dataset where improvements can be measured more clearly.
Final rating
I am inclined to keep my original rating (""6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.""). I still like the good results on the Tanks & Temples dataset and believe that the proposed approach is technically sound. However, I do not find the authors' rebuttals particularly convincing and thus do not want to increase my rating. In particular, I still have concerns about the ablation study as I am not sure whether the ground truth of the DTU dataset is accurate enough that it makes sense to claim improvements if the difference is 0.05 mm or smaller. Since this only impacts the ablation study, it is also not a reason to decrease my rating.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- vulnerability discovery methodology is also questionable. The authors consider a single vulnerability at a time, and while they acknowledge and address the data imbalance issue, I am not sure about the ecological validity of such a study. Previous work has considered multiple CVEs or CWEs at a time, and report whether or not the code contains any such vulnerability. Are the authors arguing that identifying one vulnerability at a time is an intended use case? In any case, the results are difficult to interpret (or are marginal improvements at best).",5EHI2FGf1D,EMNLP_2023,"- no comparison against baselines. The functionality similarity comparison study reports only accuracy across optimization levels of binaries, but no baselines are considered. This is a widely-understood binary analysis application and many papers have developed architecture-agnostic similarity comparison (or often reported as codesearch, which is a similar task).
- rebuttal promises to add this evaluation
- in addition, the functionality similarity comparison methodology is questionable. The authors use cosine similarity with respect to embeddings, which to me makes the experiment rather circular. In contrast, I might have expected some type of dynamic analysis, testing, or some other reasoning to establish semantic similarity between code snippets.
- rebuttal addresses this point.
- vulnerability discovery methodology is also questionable. The authors consider a single vulnerability at a time, and while they acknowledge and address the data imbalance issue, I am not sure about the ecological validity of such a study. Previous work has considered multiple CVEs or CWEs at a time, and report whether or not the code contains any such vulnerability. Are the authors arguing that identifying one vulnerability at a time is an intended use case? In any case, the results are difficult to interpret (or are marginal improvements at best).
- addressed in rebuttal
- This paper is very similar to another accepted at Usenix 2023: Can a Deep Learning Model for One Architecture Be Used for Others?
Retargeted-Architecture Binary Code Analysis. In comparison to that paper, I do not quite understand the novelty here, except perhaps for a slightly different evaluation/application domain. I certainly acknowledge that this submission was made slightly before the Usenix 2023 proceedings were made available, but I would still want to understand how this differs given the overall similarity in idea (building embeddings that help a model target a new ISA).
- addressed in rebuttal
- relatedly, only x86 and ARM appear to be considered in the evaluation (the authors discuss building datasets for these ISAs). There are other ISAs to consider (e.g., PPC), and validating the approach against other ISAs would be important if claiming to build models that generalize to across architectures.
- author rebuttal promises a followup evaluation","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '1', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- CARAFE: Content-Aware ReAssembly of Features. The paper is in ICCV 2019, not CVPR 2019 In sum, I think the idea of the spatially adaptive upsampling kernel is technically sound. I also like the extensive evaluation in this paper. However, I have concerns about the high degree of similarity with the prior method and the lack of comparison with CARAFE. ** After discussions ** I have read other reviewers' comments. Many of the reviewers share similar concerns regarding the technical novelty of this work. I don't find sufficient ground to recommend acceptance of this paper.",ICLR_2021_2506,ICLR_2021,"Weakness
== Exposition ==
The exposition of the proposed method can be improved. For examples, - it’s unclear how the “Semantic Kernel Generation” is implemented. I can probably guess this step is essentially a 1 x 1 convolution, but it would be better to fill in the details. - In the introduction, the paper mentioned that the translated images often contain artifacts due to the nearest-neighbor upsampling. Yet, in the Feature Spatial Expansion step it also used nearest-neighbor interpolation. This needs some more justification. - For Figure 3, it’s unclear what the learned semantically adaptive kernel visualization means. At each upsampling layer, the kernel is only K x K. - At the end of Section 2, I do not understand what it means by “the semantics of the upsampled feature map can be stronger than the original one”. - The proposed upsampling layer is called “semantically aware”. However, I do not see anything that’s related to the semantics other than the fact that the input is a semantic map. I would suggest that this should be called “content aware” instead.
== Technical novelty == - My main concern about the paper lies in its technical novelty. There have been multiple papers that proposed content aware filter. As far as I know, the first one is [Xu et al. 2016]. Jia, Xu, et al. ""Dynamic filter networks."" Advances in neural information processing systems. 2016.
The work most relevant to this paper is the CARAFE [Wang et al. ICCV 2019], which can be viewed as a special case of the dynamic filter network for feature upsampling. By comparing Figure 2 in this paper and Figure 2 in the CARAFE paper [Wang et al. ICCV 2019], it seems to me that the two methods are * the same*. The only difference is the application of the layout-to-image task. The paper in the related work section suggests, “the settings of these tasks are significantly different from ours, making their methods cannot be used directly.” I respectfully disagree with this statement because both methods take in a feature tensor and produce an upsampled feature tensor. I believe that the application would be straightforward without any modification. Given the similarity to prior work, it would be great for the authors to (1) describe in detail the differences (if any) and (2) compare with prior work that also uses spatially varying upsampling kernel.
Minor: - CARAFE: Content-Aware ReAssembly of Features. The paper is in ICCV 2019, not CVPR 2019
In sum, I think the idea of the spatially adaptive upsampling kernel is technically sound. I also like the extensive evaluation in this paper. However, I have concerns about the high degree of similarity with the prior method and the lack of comparison with CARAFE.
** After discussions **
I have read other reviewers' comments. Many of the reviewers share similar concerns regarding the technical novelty of this work. I don't find sufficient ground to recommend acceptance of this paper.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['1', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['0', '1', '0']}",,
"1. How to get a small degree of bias from a clear community structure needs more explanations. Theorem 1 and 2 prove that GCL conforms to a clearer community structure via intra-community concentration and inter-community scatter, but its relationship with degree bias is not intuitive enough.",NIPS_2022_655,NIPS_2022,"1. How to get a small degree of bias from a clear community structure needs more explanations. Theorem 1 and 2 prove that GCL conforms to a clearer community structure via intra-community concentration and inter-community scatter, but its relationship with degree bias is not intuitive enough. 2. There is some confusion in the theoretical analysis. Why is the supremum in Definition 1 \gamma(\frac{B}{\hat{d}_{\min}^k})^{\frac{1}{2}}? Based on this definition, how to prove that the proposed GRADE reduces this supremum? 3. There is a lack of significance test in Table 1. Despite the weaknesses mentioned above, I believe that this paper is worth publishing. They consider an important degree-bias problem in the graph domain, given that node degrees of real-world graphs often follow a long-tailed power-law distribution. And they show an exciting finding that GCL is more stable w.r.t. the degree bias, and give a preliminary explanation for the underlying mechanism. Although the improvement does not seem significant in Table 1, they may inspire more future research on this promising solution.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?",NIPS_2021_1222,NIPS_2021,"Claims: 1.a) I think the paper falls short of the high-level contributions claimed in the last sentence of the abstract. As the authors note in the background section, there are a number of published works that demonstrate the tradeoffs between clean accuracy, training with noise perturbations, and adversarial robustness. Many of these, especially Dapello et al., note the relevance with respect to stochasticity in the brain. I do not see how their additional analysis sheds new light on the mechanisms of robust perception or provides a better understanding of the role stochasticity plays in biological computation. To be clear - I think the paper is certainly worthy of publication and makes notable contributions. Just not all of the ones claimed in that sentence.
1.b) The authors note on lines 241-243 that “the two geometric properties show a similar dependence for the auditory (Figure 4A) and visual (Figure 4B) networks when varying the eps-sized perturbations used to construct the class manifolds.” I do not see this from the plots. I would agree that there is a shared general upward trend, but I do not agree that 4A and 4B show “similar dependence” between the variables measured. If nothing else, the authors should be more precise when describing the similarities.
Clarifications: 2.a) The authors say on lines 80-82 that the center correlation was not insightful for discriminating model defenses, but then use that metric in figure 4 A&B. I’m wondering why they found it useful here and not elsewhere? Or what they meant by the statement on lines 80-82.
2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?
2.c) The authors report mean capacity and width in figure 2. I think this is the mean across examples as well as across seeds. Is the STD also computed across examples and seeds? The figure caption says it is only computed across seeds. Is there a lot of variability across examples?
2.d) I am unsure why there would be a gap between the orange and blue/green lines at the minimum strength perturbation for the avgpool subplot in figure 2.c. At the minimum strength perturbation, by definition, the vertical axis should have a value of 1, right? And indeed in earlier layers at this same perturbation strength the capacities are equal. So why does the ResNet50 lose so much capacity for the same perturbation size from conv1 to avgpool? It would also be helpful if the authors commented on the switch in ordering for ATResNet and the stochastic networks between the middle and right subplots.
General curiosities (low priority): 3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.
3.b) There appears to be a pretty big difference in the overall trends of the networks when computing the class manifolds vs exemplar manifolds. Specifically, I think the claims made on lines 191-192 are much better supported by Figure 1 than Figure 2. I would be interested to hear what the authors think in general (i.e. at a high/discussion level) about how we should interpret the class vs exemplar manifold experiments.
Nitpick, typos (lowest priority): 4.a) The authors note on line 208 that “Unlike VOneNets, the architecture maintains the conv-relu-maxpool before the first residual block, on the grounds that the cochleagram models the ear rather than the primary auditory cortex.” I do not understand this justification. Any network transforming input signals (auditory or visual) would have to model an entire sensory pathway, from raw input signal to classification. I understand that VOneNets ignore all of the visual processing that occurs before V1. I do not see how this justifies adding the extra layer to the auditory network.
4.b) It is not clear why the authors chose a line plot in figure 4c. Is the trend as one increases depth actually linear? From the plot it appears as though the capacity was only measured at the ‘waveform’ and ‘avgpool’ depths; were there intermediate points measured as well? It would be helpful if they clarified this, or used a scatter/bar plot if there were indeed only two points measured per network type.
4.c) I am curious why there was a switch to reporting SEM instead of STD for figures 5 & 6.
4.c) I found typos on lines 104, 169, and the fig 5 caption (“10 image and”).","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- I found the notation / the explicit split between ""static"" and temporal features into two variables confusing, at least initially. In my view this requires more information than is provided in the paper (what is S and Xt).",NIPS_2019_494,NIPS_2019,"of the approach, it may be interesting to do that. Clarity: The paper is well written but clarity could be improved in several cases: - I found the notation / the explicit split between ""static"" and temporal features into two variables confusing, at least initially. In my view this requires more information than is provided in the paper (what is S and Xt). - even with the pseudocode given in the supplementary material I don't get the feeling the paper is written to be reproduced. It is written to provide an intuitive understanding of the work, but to actually reproduce it, more details are required that are neither provided in the paper nor in the supplementary material. This includes, for example, details about the RNN implementation (like number of units etc), and many other technical details. - the paper is presented well, e.g., quality of graphs is good (though labels on the graphs in Fig 3 could be slightly bigger) Significance: - from just the paper: the results would be more interesting (and significant) if there was a way to reproduce the work more easily. At present I cannot see this work easily taken up by many other researchers mainly due to lack of detail in the description. The work is interesting, and I like the idea, but with a relatively high-level description of it in the paper it would need a little more than the peudocode in the materials to convince me using it (but see next). - In the supplementary material it is stated the source code will be made available, and in combination with paper and information in the supplementary material, the level of detail may be just right (but it's hard to say without seeing the code). Given the promising results, I can imagine this approach being useful at least for more research in a similar direction.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', 'X', 'X', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"1. The motivation for the choice of $\theta = \frac{\pi}{2}(1-h)$ from theorem 3, is not very straightforward and clear. The paper states that this choice is empirical, but there is very little given in terms of motivation for this exact form.",4A5D1nsdtj,ICLR_2024,"1. The motivation for the choice of $\theta = \frac{\pi}{2}(1-h)$ from theorem 3, is not very straightforward and clear. The paper states that this choice is empirical, but there is very little given in terms of motivation for this exact form.
2. For this method, the knowledge of the homophily ratio seems to be important. In many practical scenarios, this may not be possible to be estimated accurately and even approximations could be difficult. No ablation study is presented showing the sensitivity of this model to the accurate knowledge of the homophily ratio.
3. The HetFilter seems to degrade rapidly past h=0.3 whereas OrtFilter is lot more graceful to the varying homophily ratio. It is unclear whether one would consider the presented fluctuations as inferior to the presented UniBasis. For UniBasis, in the region of h >= 0.3, the choice of tau should become extremely important (as is evident from Figure 4, where lower tau values can reduce performance on Cora by about 20 percentage points).","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '2', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', 'X', 'X', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- This paper deals with many graph notions and it is a bit hard to get into it but the writing is generally good though more details could sometimes be provided (definition of the resistance distance, more explanations on Alg. 1 with brief sentences defining A_t, Y_t,...).",NIPS_2019_629,NIPS_2019,"- To my opinion, the setting and the algorithm lack a bit of originality and might seem as incremental combinations of methods of graph labelings prediction and online learning in a switching environment. Yet, the algorithm for graph labelings is efficient, new and seem different from the existing ones. - Lower bounds and optimality of the results are not discussed. In the conclusion section, it is asked whether the loglog(T) can be removed. Does this mean that up to this term the bounds are tight? I would like more discussions on this. More comparison with existing upper-bounds and lower-bound without switches could be made for instance. In addition, this could be interesting to plot the upper-bound on the experiments, to see how tight is the analysis. Other comments: - Only bounds in expectation are provided. Would it be possible to get high-probability bounds? For instance by using ensemble methods as performed in the experiments. Some measure about the robustness could be added to the experiments (such as error bars or standard deviation) in addition to the mean error. - When reading the introduction, I thought that the labels were adversarially chosen by an adaptive adversary. It seems that the analysis is only valid when all labels are chosen in advance by an oblivious adversary. Am I right? This should maybe be clarified. - This paper deals with many graph notions and it is a bit hard to get into it but the writing is generally good though more details could sometimes be provided (definition of the resistance distance, more explanations on Alg. 1 with brief sentences defining A_t, Y_t,...). - How was alpha tuned in the experiments (as 1/(t+1) or optimally)? - Some possible extensions could be discussed (are they straightforward?): directed or weighted graph, regression problem (e.g, to predict the number of bikes in your experiment)... Typo: l 268: the sum should start at 1","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '5', '1', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new.,NIPS_2018_476,NIPS_2018,"Weakness] 1) Originality is limited because the main idea of variable splitting is not new and the algorithm is also not new. 2) Theoretical proofs of existing algorithm might be regarded as some incremental contributions. 3) Experiments are somewhat weak: 3-1) I was wondering why Authors conducted experiments with lambda=1. According to Corollary 1 and 2 lambda should be sufficiently large, however it is completely ignored for experimental setting. Otherwise the proposed algorithm has no difference from [10]. 3-2) In Figure 3, different evaluations are shown in different dataset. It might be regarded as subjectively selected demonstrations. 3-3) I think clustering accuracy is not very significant because there are many other sophisticated algorithms, and initializations are still very important for nice performance. It show just the proposed algorithm is OK for some applications. [Minor points] -- comma should be deleted at line num. 112: ""... + \lambda(u-v), = 0 ..."". -- ""close-form"" --> ""closed-form"" at line num.195-196. --- after feedback --- I understand that the contribution of this paper is a theoretical justification of the existing algorithms proposed in [10]. In that case, the experimental validation with respect to the sensitivity of ""lambda"" is more important rather than the clustering accuracy. So Fig. 1 in feedback file is nice to add paper if possible. I think dropping symmetry is helpful, however it is not new idea that is already being used. So, it will not change anything in practice to use it. Furthermore, in recent years, almost all application researchers are using some application specific extensions of NMF such as sparse NMF, deep NMF, semi-NMF, and graph-regularized NMF, rather than the standard NMF. Thus, this paper is theoretically interesting as some basic research, but weak from an application perspective. Finally, I changed my evaluation upper one as: ""Marginally below the acceptance threshold. I tend to vote for rejecting this submission, but accepting it would not be that bad.""","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['X', '1', '2', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?",NIPS_2016_313,NIPS_2016,"Weakness: 1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim. 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc. 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods. 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work. 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '3', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '2', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The AAAI15 paper titled ""Spectral Clustering Using Multilinear SVD: Analysis, Approximations and Applications"" by Ghoshdastidar and Dukkipati seems to be a related work missed by the authors. This AAAI15 paper deals with hypergraph data with tensors as well so it should be discussed and compared against to provide a better understanding of the state-of-the-art.",NIPS_2016_287,NIPS_2016,"weakness, however, is the experiment on real data where no comparison against any other method is provided. Please see the details comments below.1. While [5] is a closely related work, it is not cited or discussed at all in Section 1. I think proper credit should be given to [5] in Sec. 1 since the spacey random walk was proposed there. The difference between the random walk model in this paper and that in [5] should also be clearly stated to clarify the contributions. 2. The AAAI15 paper titled ""Spectral Clustering Using Multilinear SVD: Analysis, Approximations and Applications"" by Ghoshdastidar and Dukkipati seems to be a related work missed by the authors. This AAAI15 paper deals with hypergraph data with tensors as well so it should be discussed and compared against to provide a better understanding of the state-of-the-art. 3. This work combines ideas from [4], [5], and [14] so it is very important to clearly state the relationships and differences with these earlier works. 4. End of Sec. 2., there are two important parameters/thresholds to set. One is the minimum cluster size and the other is the conductance threshold. However, the experimental section (Sec. 3) did not mention or discuss how these parameters are set and how sensitive the performance is with respect to these parameters. 5. Sec. 3.2 and Sec. 3.3: The real data experiments study only the proposed method and there is no comparison against any existing method on real data. Furthermore, there is only some qualitative analysis/discussion on the real data results. Adding some quantitative studies will be more helpful to the readers and researchers in this area. 6. Possible Typo? Line 131: ""wants to transition"".","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"• Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).",ICLR_2021_1944,ICLR_2021,"I have several concerns regarding this paper.
• Novelty. The authors propose to use Ricci flow to compute the distance between nodes so that to sample edges with respect to that distance. Using Ricci flow for distance computation is a well-studied area (as indicated in related work). The only novel part is that each layer gets a new graph; however, this choice is not motivated (why not to train all layers of GNN on different graphs instead?) and has problems (see next).
• Approach. Computing optimal transport distance is generally an expensive procedure. While authors indicated that it takes seconds to compute it on 36 cores machine, it’s not clear how scalable this method is. I would like to see whether it scales on normal machines with a couple of cores. Moreover, how do you compute exactly optimal transport, because the Sinkhorn method gives you a doubly stochastic matrix (how do you go from it to optimal transport?).
• Algorithm. This is the most obscure part of the paper. First, it’s not indicated how many layers do you use in experiments. This is a major part of your algorithm because you claim that if an edge appears in several layers it means that it’s not adversarial (or that it does not harm your algorithm). In most of the baselines, there are at most 2-3 layers. There are theoretical limitations why GNN with many layers may not work in practice (see, the literature on “GNN oversmoothing”). Considering that you didn’t provide the code (can you provide an anonymized version of the code?) and that your baselines (GCN, GAT, etc.) have similar (or the same) performance as in the original papers (where the number of layers is 2-3), I deduce that your model Ricci-GNN also has this number of layers. With that said, I doubt that it’s possible to make any conclusive results about whether an edge is adversarial or not with 2-3 graphs. Moreover, I would expect to see an experiment on how your approach varies depending on the number of layers. This is a crucial part of your algorithm and not seeing discussion of it in the paper, raises concerns about the validity of experiments.
• Design choices. Another potential problem of your algorithm is that the sampled graphs can become dense. There are hyperparameters \sigma and \beta that control the probabilities and also you limit the sampling only for 2-hop neighborhoods (“To keep graph sparsity, we only sample edges between pairs that are within k hops of each other in G (we always take k = 2 in the experiments).” This is arbitrary and the effect of it on the performance is not clear. How did you select parameters \sigma and \beta? Why k=2? How do you ensure that the sampled graphs are similar to the original one? Does it matter that sampled graphs should have similar statistics to the original graph? I guess, this crucially affects the performance of your algorithm, so I would like to see more experiments on this.
• Datasets. Since this paper is mostly experimental, I would like to see a comparison of this model on more datasets (5-7 in total). Verifying on realistic but small datasets such as Cora and Citeseer limits our intuition about performance. For example, Cora is a single graph of 2.7K nodes. As indicated in [1], “Although small datasets are useful as sanity checks for new ideas, they can become a liability in the long run as new GNN models will be designed to overfit the small test sets instead of searching for more generalizable architectures.” There are many sources of real graphs, you can consider OGB [2] or [3].
• Weak baselines. Another major concern of the validity of the experiments is the choice of the baselines. Neither of GNN baselines (GCN, GAT, etc.) was designed for the defense of adversarial attacks, so choosing them for comparison is not fair. A comparison with previous works (indicated in “Adversarial attack on graphs.” in related work section) is necessary. Moreover, an experiment where you randomly sample edges (instead of using Ricci distance) is desirable to compare the performance against random sampling.
• Ablation. Since you use GCN, why the performance of Ricci-GCN is so different from GCN when there 0 perturbations? For Citeseer the absolute difference is 2% which is quite high for the same models. Also, an experiment with different choices of GNN is desirable.
• Training. Since experiments play important role in this paper, it’s important to give a fair setup for the models in comparison. You write “For each training procedure, we run 100 epochs and use the model trained at 100-th epoch.”. This can disadvantageous for many models. A better way would be to run each model setup until convergence on the training set, selecting the epoch using the validation set. Otherwise, your baselines could suffer from either underfitting or overfitting.
[1] https://arxiv.org/pdf/2003.00982.pdf [2] https://ogb.stanford.edu/ [3] https://paperswithcode.com/task/node-classification ==========
After reading the authors comments.
I applaud the authors for greatly improving their paper via the revision. Now the number of layers is specified and the explanation of having many sampled graphs during training is added, which was missing in the original text and was preventing a full understanding of the reasons why the proposed approach works. Overall, I am leaning toward increasing the score.
I still have several concerns about the practicality of Ricci-GNN. In simple words, the proposed approach uses some metric S (Ricci flow) that dictates how to sample graphs for training. The motivation for using Ricci flow is “that Ricci flow is a global process that tries to uncover the underlying metric space supported by the graph topology and thus embraces redundancy”. This claim cites previous papers, which in turn do not discuss what exactly is meant by “a global process that tries to uncover the underlying metric space”. Spectral embeddings also can be considered as a global metric, so some analysis on what properties of Ricci flow makes it more robust to attacks would be appreciated. Also including random sampling in comparison would confirm that the effect is coming not from the fact that you use more graphs during the training, but from how you sample those graphs. In addition, as the paper is empirical and relies on the properties of Ricci flow which was discussed in previous works and was not addressed in the context of adversarial attacks, having more datasets (especially larger ones) in the experiments would improve the paper.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '3', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '2', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. Â Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \lambda changes the performance, but it would be nice to see how \eta and \tau in equation (7) affect performance. Minor comments:",NIPS_2019_1131,NIPS_2019,"1. There is no discussion on the choice of ""proximity"" and the nature of the task. On the proposed tasks, proximity on the fingertip Cartesian positions is strongly correlated with proximity in the solution space. However, this relationship doesn't hold for certain tasks. For example, in a complicated maze, two nearby positions in the Euclidean metric can be very far in the actual path. For robotic tasks with various obstacles and collisions, similar results apply. The paper would be better if it analyzes what tasks have reasonable proximity metrics, and demostrate failure on those that don't. 2. Â Some ablation study is missing, which could cause confusion and extra experimentation for practitioners. For example, the \sigma in the RBF kernel seems to play a crucial role, but no analysis is given on it. Figure 4 analyzes how changing \lambda changes the performance, but it would be nice to see how \eta and \tau in equation (7) affect performance. Minor comments: 1. The diversity term, defined as the facility location function, is undirected and history-invariant. Thus it shouldn't be called ""curiosity"", since curiosity only works on novel experiences. Please use a different name. 2. The curves in Figure 3 (a) are suspiciously cut at Epoch = 50, after which the baseline methods seem to catch up and perhaps surpass CHER. Perhaps this should be explained.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.,5UW6Mivj9M,EMNLP_2023,"1) The paper was extremely hard to follow. I read it multiple times and still had trouble following the exact experimental procedures and evaluations that the authors conducted.
2) Relatedly, it was hard to discern what was novel in the paper and what had already been tried by others.
3) Since the improvement in numbers is not large (in most cases, just a couple of points), it is hard to tell if this improvement is statistically significant and if it translates to qualitative improvements in performance.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '2', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', 'X', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '1', '2', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,
"2. In section 2, ""Finally, INRs operate on a per-data-instance basis, meaning that one time-series instance is required to train an INR"". This claim is true but I don't think it is an advantage. A model that can only handle a single time series data is almost useless.",FpElWzxzu4,ICLR_2024,"1. In the intro section, the claim that Transformers reply on regularly sampled time-series data is wrong. For example, [1] shows that the Transformer model handles irregularly-sampled time series well for imputation.
2. In section 2, ""Finally, INRs operate on a per-data-instance basis, meaning that one time-series instance is required to train an INR"". This claim is true but I don't think it is an advantage. A model that can only handle a single time series data is almost useless.
3. In section 3.1, why $c_i^t$ is a vector rather than a scalar? It just denotes a temporal coordinate of a time and should be a scalar.
4. Why the frequency of the Fourier feature is uniformly sampled?
5. ""Vectors Bm ∈ R1×DψF , δm ∈ R1×DψF denote the phase shift and the bias respectively"". I think what the author wants to claim is that B represents bias while δm denotes phase shift.
6. In Figure 5, it is unclear what the vertical axis represents.
[1] NRTSI: Non-Recurrent Time Series Imputation, ICASSP 2023.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).",NIPS_2017_110,NIPS_2017,"of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail. This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices. In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times. Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful. Lastly, the details of the experiment are lacking. In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.
Specific comments:
- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
- l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
- Throughout, the authors use the term constrains and should change to constraints.
- l124: What is meant by the (*)?
- l134: Do the authors mean m=2?
- l148: known, instead of know
- l156: please define \gamma_0^{***}
- Figure 1: Please specify the meaning of the colors in the caption as well as the text.
- l280: ""Then we made it explicit"" instead of ""Then we have explicit it""","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1']}",,
"- Limited Experiments - Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.",zpayaLaUhL,EMNLP_2023,"- Limited Experiments
- Most of the experiments (excluding Section 4.1.1) are limited to RoBERTa-base only, and it is unclear if the results can be generalized to other models adopting learnable APEs. It is important to investigate whether the results can be generalized to differences in model size, objective function, and architecture (i.e., encoder, encoder-decoder, or decoder). In particular, it is worthwhile to include more analysis and discussion for GPT-2. For example, I would like to see the results of Figure 2 for GPT-2.
- The input for the analysis is limited to only 100 or 200 samples from wikitext-2. It would be desirable to experiment with a larger number of samples or with datasets from various domains.
- Findings are interesting, but no statement of what the contribution is and how practical impact on the community or practical use. (Question A).
- Results contradicting those reported in existing studies (Clark+'19) are observed but not discussed (Question B).
- I do not really agree with the argument in Section 5 that word embedding contributes to relative position-dependent attention patterns. The target head is in layer 8, and the changes caused by large deviations from the input, such as only position embedding, are quite large at layer 8. It is likely that the behavior is not such that it can be discussed to explain the behavior under normal conditions. Word embeddings may be the only prerequisites for the model to work properly rather than playing an important role in certain attention patterns.
- Introduction says to analyze ""why attention depends on relative position,"" but I cannot find content that adequately answers this question.
- There is no connection or discussion of relative position embedding, which is typically employed in recent Transformer models in place of learnable APE (Question C).","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The parameters in Table 1, the model and the experiments seem to be only good for image data and ViT. Did the authors try to apply the same principles to other areas research areas such as NLP or simpler models in the image domain (CNNs)? I understand the latter might be due to the focus about state of the art performance, but it would show that the method can generalize to different architectures and tasks, not just transformers in vision.",ICLR_2023_2934,ICLR_2023,"- Fig. 1 leaves me with some doubts. It would seem that the private task is solved by using only a head operating on the learned layer for the green task (devanagari). This is at least what I would expect for the claims of the method to still uphold, because if the private task head can alter the weights of the Transformer layer 1 then information from the private task is flowing into the network. I would appreaciate if the authors could clarify this.
- Overall a lot of choices seem to lead towards the necessity for large compute power. The choice of modifying hyperparameters only by a one-hop neighbor is quite restrictive and it implies that we have to evolve/search for quite a while before stumbling on the correct hyperparams. The layer cloning and mutation probability hyperparameter is set at random by the evolutionary process, implying that the level of overall randomicity is very high and therefore large training times are needed to get stable results or be able to reproduce the claimed results (considering the authors use DNN architectures). The authors mention that ""the score can be defined to optimize a mixture of factors depending on application requirements"". It would have been nice to see what the tradeoff between training time and model size vs optimal multi-task performance is, especially considering these high levels of randomicity present in the proposed approach. (and also for others to be able to reproduce somewhat similar results on lesser compute).
- The parameters in Table 1, the model and the experiments seem to be only good for image data and ViT. Did the authors try to apply the same principles to other areas research areas such as NLP or simpler models in the image domain (CNNs)? I understand the latter might be due to the focus about state of the art performance, but it would show that the method can generalize to different architectures and tasks, not just transformers in vision.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '2', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '2', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '1', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"5. **(Performance of TTA methods)** This is an interesting observation, that using non-standard benchmarks breaks a lot of popular TTA methods. If the authors can evaluate TTA on more conditions of natural distribution shift, like WILDS [9], it could really strengthen the paper.",X4ATu1huMJ,ICLR_2024,"**Overall comment**
The paper discusses evaluating TTA methods across multiple settings, and how to choose the correct method during test-time. I would argue most of the methods/model selection strategies that are discussed in the paper are not novel and/or existed before, and the paper does not have a lot of algorithmic innovation.
While this discussion unifies various prior methods and can be a valuable guideline for practitioners to choose the appropriate TTA method, there needs to be more experiments to make it a compelling paper (i.e., add MEMO [1] as a method of comparison, add WILDS-Camelyon 17 [9], WILDS-FMoW [9], ImageNet-A [7], CIFAR-10.1 [8] as dataset benchmarks). But I do feel the problem setup is very important, and adding more experiments and a bit of rewriting can make the paper much stronger.
**Abstract and Introduction**
1. The paper mentions model restarting to avoid error propagation. There has been important work in TTA, where the model adapts its parameters to only one test example at a time, and reverts back to the initial (pre-trained) weights after it has made the prediction, doing the process all over for the next test example. This is also an important setting to consider, where only one test example is available, and one cannot rely on batches of data from a stream. For example, see MEMO [1].
2. (nitpicking, not important to include in the paper) “under extremely long scenarios all existing TTA method results in degraded performance”, while this is true, the paper does not mention some recent works that helps alleviate this. E.g., MEMO [1] in the one test example at a time scenario, or MEMO + surgical FT [2] where MEMO is used in the online setting, but parameter-efficient updating helps with feature distortion/performance degradation. So the claim is outdated.
3. It would be good to cite relevant papers such as [4] as prior works that look into model selection strategies (but not for TTA setting) to motivate the problem statement.
**Section 3.2, model selection strategies in TTA**
1. While accuracy-on-the-line [3] shows correlation between source (ID) and target (OOD) accuracies, some work [4] also say source accuracy is unreliable in the face of large domain gaps. I think table 3 shows the same result. Better to cite [4] and add their observation.
2. Why not look at agreement-on-the-line [5]? This is known to be a good way of assessing performance on the target domain without having labels. For example, A-Line [6] seems to have good performance on TTA tasks. This should also be considered as a model selection method.
**Section 4.1, datasets**
1. Missing some key datasets such as ImageNet-A [7], CIFAR-10.1 [8]. It is important to consider ImageNet-A (to show TTA’s performance on adversarial examples) and CIFAR-10.1, to show TTA’s performance on CIFAR-10 examples where the shift is natural, i.e., not corruptions. Prior work such as MEMO [1] has used some of these datasets.
**Section 4.3, experimental setup**
1. The architecture suite that is used is limited in size. Only ResNext-29 and ResNet-50 are used. Since the paper’s goal is to say something rigorous about model selection strategies, it is important to try more architectures to have a comprehensive result. At least some vision-transformer architecture is required to make the results strong. I would suggest trying RVT-small [12] or ViT-B/32 [13].
Why do the authors use SGD as an optimizer for all tasks? It is previously shown that [14] SGD often performs worse for more modern architectures. The original TENT [15] paper also claims they use SGD for ImageNet and for everything else they use Adam [16].
**Section 5, results**
1. (Table 1) It might be easier if the texts mention that each row represent one method, and each column represents one model selection strategy. When the authors say “green” represents the best number, they mean “within a row”.
2. (Different methods’ ranking under different selection strategies) The results here are not clear and hard to read. How many times does one method outperform the other, when considering all different surrogate based metrics across all datasets? If the goal is to show consistency of AdaContrast as mentioned in the introduction, a better way of presenting this might be making something similar to table 1 of [17].
3. What does the **Median** column in Table 2 and 3 represent? There is no explanation given for this in paper.
4. I assume the 4 surrogate strategies are: S-Acc, Cross-Acc, Ent and Con. If so, then the statement **“While EATA is significantly the best under the oracle selection strategy (49.99 on average) it is outperformed for example by Tent (5th method using oracle selection) when using 3 out of 4 surrogate-based metrics”** is clearly False according to the last section of Table 2: Tent > EATA on Cross-Acc and Con, but EATA > Tent when using S-Acc and Ent.
5. **(Performance of TTA methods)** This is an interesting observation, that using non-standard benchmarks breaks a lot of popular TTA methods. If the authors can evaluate TTA on more conditions of natural distribution shift, like WILDS [9], it could really strengthen the paper.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The required condition on the learning rate (scaling with the number of samples) is not scalable. I never see a step size grows with the sample size in practice, which will lead to unreasonably large learning rate when learning on large-scale dataset. I understand the authors need a way to precisely characterize the benefit of large learning rates, but this condition is not realistic itself.",MY8SBpUece,ICLR_2024,"Weakness:
1. Based on my understanding, the core advantage of the proposed analysis is from the Hermite expansion of the activation layer, which can characterize higher-order nonlinearity and explain more non-linear behaviors than the orthogonal decomposition used in Ba et al. 2022. Please clarify this.
2. The required condition on the learning rate (scaling with the number of samples) is not scalable. I never see a step size grows with the sample size in practice, which will lead to unreasonably large learning rate when learning on large-scale dataset. I understand the authors need a way to precisely characterize the benefit of large learning rates, but this condition is not realistic itself.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"1. Although these tensor networks can be used to represent PMF of discrete variables. How these results are useful to machine learning algorithms or analyze the algorithm is not clear. Hence, the significance of this paper is poor.",NIPS_2019_134,NIPS_2019,"Weakness: 1. Although these tensor networks can be used to represent PMF of discrete variables. How these results are useful to machine learning algorithms or analyze the algorithm is not clear. Hence, the significance of this paper is poor. 2. The two experiments are all based on very small dataset either generated or realistic data. The evaluation is performed on KL-divergence or NLL, which only show how good the model can fit the data, rather than generalization performance. How these results are useful for machine learning? In addition, MPS, BM, LPS are quite similar in the structure. There are many well known tensor models, CP, Tucker, Hirachical Tucker are not compared. There are also more complicated models like MERA, PEPS. I have read the authors' rebuttal, they addressed some of questions well. But the generalization is not considered, thus it becomes a standard non-negative tensor factorization problem on the PMF data. Hence, I will remain the original score.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '1', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '2', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.",ICLR_2021_1181,ICLR_2021,"1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '4', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- It's not clear how to sample from the DPP if the eigenfunctions e_n's are inaccessible (Eq (10) line 130). This seems to be the same problem with sampling from the leverage score in [3], so I'm not sure how sampling from the DPP is easier than sampling from the leverage score.",NIPS_2019_1158,NIPS_2019,"1. The proposed method only gets convergence rate in expectation (i.e. only variance bound), not with high probability. Though Chebyshev's inequality gives bound in probability from the variance bound, this is still weaker than that of Bach [3]. 2. The method description lacks necessary details and intuition: - It's not clear how to get/estimate the mean element mu_g for different kernel spaces. - It's not clear how to sample from the DPP if the eigenfunctions e_n's are inaccessible (Eq (10) line 130). This seems to be the same problem with sampling from the leverage score in [3], so I'm not sure how sampling from the DPP is easier than sampling from the leverage score. - There is no intuition why DPP with that particular repulsion kernel is better than other sampling schemes. 3. The empirical results are not presented clearly: - In Figure 1: what is ""quadrature error""? Is it the sup of error over all possible integrand f in the RKHS, or for a specific f? If it's the sup over all f, how does one get that quantity for other methods such as Bayesian quadrature (which doesn't have theoretical guarantee). If it's for a specific f, which function is it, and why is the error on that specific f representative of other functions? Other comment: - Eq (18), definition of principal angle: seems to be missing absolute value on the right hand side, as it could be negative. Minor: - Reference for Kernel herding is missing [?] - Line 205: Getting of the product -> Getting rid of the product - Please ensure correct capitalization in the references (e.g., [1] tsp -> TSP, [39] rkhss -> RKHSs) [3] F. Bach. On the equivalence between kernel quadrature rules and random feature expansions. The Journal of Machine Learning Research, 18(1):714â751, 2017. ===== Update after rebuttal: My questions have been adequately addressed. The main comparison in the paper seems to be the results of F. Bach [3]. Compared to [3], I do think the theoretical contribution (better convergence rate) is significant. However, as the other reviews pointed out, the theoretical comparison with Bayesian quadrature is lacking. The authors have agreed to address this. Therefore, I'm increasing my score.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '1', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', 'X', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '1']}",,
"- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.",ICLR_2021_1682,ICLR_2021,"+ The value of episodic training is increasingly being questioned, and the submission approaches the topic from a new and interesting perspective.
+ The connection between nearest-centroid few-shot learning approaches and NCA has not been made in the literature to my knowledge and has potential applications beyond the scope of this paper.
+ The paper is well-written, easy to follow, and well-connected to the existing literature.
- The extent to which the observations presented generalize to few-shot learners beyond Prototypical Networks is not evaluated, which may limit the scope of the submission’s contributions in terms of understanding the properties of episodic training.
- The Matching Networks / NCA connection makes more sense in my opinion than the Prototypical Networks / NCA connection.
- A single set of hyperparameters was used across learners for a given benchmark, which can bias the conclusions drawn from the experiments. Recommendation
I’m leaning towards acceptance. I have some issues with the submission that are detailed below, but overall the paper presents an interesting take on a topic that’s currently very relevant to the few-shot learning community, and I feel that the value it brings to the conversation is sufficient to overcome the concerns I have.
Detailed justification
The biggest concern I have with the submission is methodological. One one hand, the authors went beyond the usual practice of reporting accuracies on a single run and instead trained each method with five different random initializations, and this is a practice that I’m happy to see in a few-shot classification paper. On the other hand, the choice to share a single set of hyperparameters across learners for a given benchmark leaves a blind spot in the evaluation. What if Prototypical Networks are more sensitive to the choice of optimizer, learning rate schedule, and weight decay coefficient than NCA? Is it possible that the set of hyperparameters chosen for the experiments happens to work poorly for Prototypical Networks? Would we observe the same trends if we tuned hyperparameters independently for each experimental setting? In its current form the submission shows that Prototypical Networks are sensitive to the hyperparameters used to sample episodes while keeping other hyperparameters fixed, but showing the same trend while doing a reasonable effort at tuning other hyperparameters would make for a more convincing argument. This is why I take the claim made in Section 4.2 that ""NCA performs better than all PN configurations, no matter the batch size"" with a grain of salt, for instance.
I also feel that the submission misses out on an opportunity to support a more general statement about episodic training via observations on approaches such as Matching Networks, MAML, etc. I really like the way Figure 1 explains visually how Prototypical Networks miss out on useful relationships between examples in a batch and is therefore data-inefficient. To me, this is one of the submission’s most important contributions: the suggestion that a leave-one-out strategy could allow episodic approaches to achieve the same kind of data efficiency as non-episodic approaches, alleviating the need for a supervised pre-training / episodic fine-tuning strategy. To be clear, I don’t think the missed opportunity would be a reason to reject the paper, but I think that showing empirically that the leave-one-out strategy applies beyond Prototypical Networks would make me lean more strongly towards acceptance.
The connection drawn between Prototypical Networks and NCA feels forced at times. In the introduction the paper claims to ""show that, without episodic learning, Prototypical Networks correspond to the classic Neighbourhood Component Analysis"", but Section 3.3 lists the creation of prototypes as a key difference between the two which is not resolved by training non-episodically. From my perspective, NCA would be more akin to the non-episodic counterpart to Matching Networks without Full Contextual Embeddings – albeit with a Euclidean metric rather than a cosine similarity metric – since both perform comparisons on example pairs.
This relationship with Matching Networks could be exploited to improve clarity. For instance, row 6 of Figure 4 can be interpreted as a Matching Networks implementation with a Euclidean distance metric. With this in mind, could the difference in performance between ""1-NN with class centroids"" and k-NN / Soft Assignment noted in Section 4.1 – as well as the drop in performance observed in Figure 4’s row 6 – be explained by the fact that a (soft) nearest-neighbour approach is more sensitive to outliers?
Finally, I have some issues with how results are reported in Tables 1 and 2. Firstly, we don’t know how competing approaches would perform if we applied the paper’s proposed multi-layer concatenation trick, and the idea itself feels more like a way to give NCA’s performance a small boost and bring it into SOTA-like territory. Comparing NCA without multi-layer against other approaches is therefore more interesting to me. Secondly, 95% confidence intervals are provided, but the absence of identification of the best-performing approach(es) in each setting makes it hard to draw high-level conclusions at a glance. I would suggest bolding the best accuracy in each column along with all other entries for which a 95% confidence interval test on the difference between the means is inconclusive in determining that the difference is significant. Questions
In Equation 2, why is the sum normalized by the total number of examples in the episode rather than the number of query examples?
Can the authors comment on the extent to which Figure 2 supports the hypothesis that NCA is better for training because it learns from a larger number of positives and negatives? Assuming this is true, we should see that Prototypical Networks configurations that increase the number of positives and negatives should perform better for a given batch size. Does Figure 2 support this assertion?
Can the authors elaborate on the ""no S/Q"" ablation (Figure 4, row 7)? What is the point of reference when computing distances for support and query examples? Is the loss computed in the same way for support and query examples? The text in Section 4.3 makes it appear like the loss for query examples is the NCA loss, but the loss for support examples is the prototypical loss. Wouldn’t it be conceptually cleaner to compute leave-one-out prototypes, i.e. leave each example out of the computation of its own class’ prototype (resulting in slightly different prototypes for examples of the same class)? In my mind, this would be the best way to remove the support/query partition while maintaining prototype computation, thereby showing that the partition is detrimental to Prototypical Networks training.
Additional feedback
This is somewhat inconsequential, but across all implementations of episodic training that I have examined I haven’t encountered an implementation that uses a flag to differentiate between support and query examples. Instead, the implementations I have examined explicitly represent support and query examples as separate tensors. I was therefore surprised to read that ""in most implementations [...] each image is characterised by a flag indicating whether it corresponds to the support or the query set [...]""; can the authors point to the implementations they have in mind when making that assertion?
I would be careful with the assertion that ""during evaluation the triplet {w, n, m} [...] must stay unchanged across methods"". While this is true for the benchmarks considered in this submission, benchmarks like Meta-Dataset evaluate on variable-ways and variable-shots episodes.
I’m not too concerned with the computational efficiency of NCA. The pairwise Euclidean distances can be computed efficiently using the inner- and outer-product of the batch of embeddings with itself.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '2', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '2', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '4', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The contribution of different modalities of different instances may be different, e.g., we have modalities A and B, some instances with good performance of modality A which belongs to the strong modality, whereas some instances with good performance modality B which belongs to the strong modality. Equation 3 directly removes the modal subset of all instances. How to deal with the problem mentioned above.",NIPS_2021_2152,NIPS_2021,"Weakness: 1. This manuscript is more like an experimental discovery paper, and the proposed method is similar to the traditional removal method, i.e., traverse all the modal feature subsets and calculate the perceptual score, removing the last subset. The reviewer believes that the contribution of the manuscript still has room for improvement. 2. The contribution of different modalities of different instances may be different, e.g., we have modalities A and B, some instances with good performance of modality A which belongs to the strong modality, whereas some instances with good performance modality B which belongs to the strong modality. Equation 3 directly removes the modal subset of all instances. How to deal with the problem mentioned above.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '3', '1', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '1', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '1', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
- The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p.,NIPS_2018_232,NIPS_2018,"- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '4']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', 'X', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,
"- I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful.",NIPS_2018_639,NIPS_2018,"Weakness: - I am quite not convinced by the experimental results of this paper. The paper sets to solve POMDP problem with non-convex value function. To motivate the case for their solution the examples of POMDP problem with non-convex value functions used are: (a) surveillance in museums with thresholded rewards; (b) privacy preserving data collection. So then the first question is when the case we are trying to solve are above two, why is there not a single experiment on such a setting, not even a simulated one? This basically makes the experiments section not quite useful. - How does the reader know that the reward definitions of rho for this tasks necessitates a non-convex reward function. Surveillance and data collection has been studied in POMDP context by many papers. Fortunately/unfortunately, many of these papers show that the increase in the reward due to a rho based PWLC reward in comparison to a corresponding PWLC state-based reward (R(s,a)) is not that big. (Papers from Mykel Kochenderfer, Matthijs Spaan, Shimon Whiteson are some I can remember from top of my head.) The related work section while missing from the paper, if existed, should cover papers from these groups, some on exactly the same topic (surveillance and data collection). - This basically means that we have devised a new method for solving non-convex value function POMDPs, but do we really need to do all that work? The current version of the paper does not answer this question to me. Also, follow up question would be exactly what situation do I want to use the methodology proposed by this paper vs the existing methods. In terms of critisim of significance, the above points can be summarized as why should I care about this method when I do not see the results on problem the method is supposedly designed for.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '2', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '1', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: “single target domain with \textbf{sparse} labels”, “…target distribution p_T(x, y) with label observation…” is mentioned, but the notation for target domain \tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).",YRJDZYGmAZ,ICLR_2024,"Many significant problems are found in the current format of the paper that prevents the understanding of the concept. The problem includes but is not limited to confusing writing, inconsistency of notations, expressions of novelty, experimental presentation etc. It is highly recommended that the authors to re-write the paper, re-organize the content and better polish the text for the reader to better understand.
The major problems:
* Novelty is limited:
- The proposed method as in Sec. 3.2 is very similar to DAPL but extended to multi-source scenarios. The only difference is to introduce an additional [DOM]. Note, that DAPL has not been peer-reviewed.
- The motivation for domain-aware mixup is confusing. I cannot be convinced and do not understand in the current writing, how it can enforce to learn domain-specific knowledge. The corresponding literature regarding mixup in the feature space is also not referenced and discussed (e.g. [1]).
- The description for deriving the domain-aware mixup is confusing. I assume the authors are trying to develop a method so that the learned prompt shares the knowledge between source and target domains (depending on Eq. 9)?
* Writing:
- In the first sentence of Abstract: “large vision-language models … strong performance in MFDA”. There is no such reference applying large VL models in MFDA. In fact, MFDA is a rarely studied problem.
- The description of the problem setting (MFDA) should be clearly explained at the beginning (abstract or introduction) so that the reader can refer better to the limitations of prior works.
- Paragraphs 1 & 2 in the introduction: the connection is missing, and ‘prompt learning’ suddenly jumps in, making the concept broken.
- Fig. 1 is not referred to in the paper.
- Related work: after describing the related prior works of each field, it's suggested to write a couple of sentences to distinguish between them to show the novelty of the proposed method.
- The description of the MFDA setting is very confusing in the first paragraph of the Method Section: “single target domain with \textbf{sparse} labels”, “…target distribution p_T(x, y) with label observation…” is mentioned, but the notation for target domain \tau is unlabeled. In the original MFDA paper (Yue et al., 2021a), the target data is unlabeled. What about the unlabeled data in source domains? Are they used during training (as in (Yue et al., 2021a))? It is very confusing that the problem setting description defers significantly as in (Yue et al., 2021a).
- There is significant text overlapping with DAPL in the preliminary sections of both papers (only with some rewording..). It should be strictly prohibited.
- What is [DOM] in Eq. 4? I assume it is a domain ID? And I assume [DOM] is the non-learnable component near the description of Eq. 4?
- Notation: what is subscript d in Eq. 4 and superscript d in Eq. 5? They are not explained in the text. I assume they are the domain IDs?
- What does it mean by ‘d*k categories’ as in the sentence after Eq. 5?
- Eq. 6 is very confusing. For the outer summation on d \in {s, u}, what is the purpose of computing the similarity between the target domain prompt and source image features? How does the learning on unlabeled target data is realized?
- What is inter-source domain mixup? In the current format of writing, I don’t understand why maintaining it will harm the representation learning on the target domain. The motivation is weak.
- In the second paragraph on page 6, the notation of target domain data y_t is different from Section 3.
- In Fig. 3, letters v and f are used to represent the features of “painting” and “real”. But v is used to represent text prompts as in Eq. 3
- The feature mix-up formulation in Fig. 3 is different than Eq. 8. One uses \gamma and another one uses \lambda? and the weighting is different?
- It is really confusing that the letter “t” is used to refer to text and target domain.
- What are D^s and D^u in Eq. 10? They are never defined. I assume they are source and target domains, which is inconsistent with what is described in the problem setting. The problem setting is borrowed from (Yue et al., 2021a). But Eq. 10 is copied from DAPL paper. Please keep everything consistent throughout the paper. Also, Eq. 9 requires source data as well, why only D^u is passed to L_u as in Eq. 10?
- The notations for loss functions in Eq. 7, 9, and 10 should be consistent.
- Table 5 in the last sentence of Page 8 should be Figure 5.
- The experimental setting/comparison is very confusing. What is “single best”, which can be both setting and method as in Table 1&2? What is source combined? Which rows in Tables 1&2 refer to the MFDA? How come the “Large model” in Table 1&2 can be the setting, it should be the model architecture.
- For Figure 6&7, they are hard to see the differences. It is suggested to use a table to report the numbers.
[1] Adversarial Domain Adaptation with Domain Mixup. AAAI 2020.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '3', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.",ICLR_2021_1213,ICLR_2021,"weakness of the paper. Then, I present my additional comments which are related to specific expressions in the main text, proof steps in the appendix etc. I would appreciate it very much if authors could address my questions/concerns under “Additional Comments” as well, since they affect my assessment and understanding of the paper; consequently my score for the paper. Summary:
• The paper focuses on convergence of two newly-proposed versions of AdaGrad, namely AdaGrad-window and AdaGrad-truncation, for finite sum setting where each component is smooth and possibly nonconvex.
• The authors prove convergence rate with respect to number of epochs T, where in each epoch one full pass over the data is performed with respect to well-known “random shuffling” sampling strategy.
• Specifically, AdaGrad-window is shown to achieve O ~ ( T − 1 / 2 )
rate of convergence, whereas AdaGrad-truncation attains ( T − 1 / 2 )
convergence, under component-wise smoothness and bounded gradients assumptions. Additionally, authors introduce a new condition/assumption called consistency ratio which is an essential element of their analysis.
• The paper explains the proposed modification to AdaGrad and provide their intuition for such adjustments. Then, the main results are presented followed by a proof sketch, which demonstrates the main steps of the theoretical approach.
• In order to evaluate the practical performance of the modified adaptive methods in a comparative fashion, two set of experiments were provided: training logistic regression model on MNIST dataset and Resnet-18 model on CIFAR-10 dataset. In these experiments; SGD, SGD with random shuffling, AdaGrad and AdaGrad-window were compared. Additionally, authors plot the behavior of their proposed condition “consistency ratio” over epochs. Strengths:
• I think epoch-wise analysis, especially for finite sum settings, could help provide insights into behaviors of optimization algorithms. For instance, it may enable to further investigate effect of batch size or different sampling strategies with respect to progress of the algorithms after every full pass of data. This may also help with comparative analysis of deterministic and stochastic methods.
• I have checked the proof of Theorem 1 in details and had a less detailed look at Theorems 2 and 3. I appreciate some of the technically rigorous sections of the analysis as the authors bring together analytical tools from different resources and re-prove certain results with respect to their adjustments.
• Performance comparison in the paper is rather simple but the authors try to provide a perspective of their consistency condition through numerical evidence. It gives some rough idea about how to interpret this condition.
• Main text is written in a clear; authors highlight their modification to AdaGrad and also highlight what their new “consistency condition” is. Proposed contributions of the paper are stated clearly although I do not totally agree with certain claims. One of the main theorems has a proof sketch which gives an overall idea about authors’ approach to proving the results. Weaknesses:
• Although numerically the paper provides an insight into the consistency condition, it is not verifiable ahead of time. One needs to run a simulation to get some idea about this condition, although it still wouldn’t verify the correctness. Since authors did not provide any theoretical motivation for their condition, I am not fully convinced out this assumption. For instance, authors could argue about a specific problem setting in which this condition holds.
• Theorem 3 (Adagrad-truncation) sets the stepsize depends on knowledge of r
. I couldn’t figure out how it is possible to compute the value r
ahead of time. Therefore, I do not think this selection is practically applicable. Although I appreciate the theoretical rigor that goes into proving Theorem 3, I believe the concerns about computing r
weakens the importance of this result. If I am missing out some important point, I would like to kindly ask the authors to clarify it for me.
• The related work which is listed in Table 1, within the group “Adaptive Gradient Methods” prove \emph{iteration-wise} convergence rates for variants of Adam and AdaGrad, which I would call the usual practice. This paper argues about \emph{epoch-wise} convergence. The authors claim improvement over those prior papers although the convergence rate quantifications are not based on the same grounds. All of those methods consider the more general expectation minimization setting. I would suggest the authors to make this distinction clear and highlight iteration complexities of such methods while comparing previous results with theirs. In my opinion, total complexity comparison is more important that rate comparison for the setting that this paper considers.
• As a follow up to the previous comment, the related work could have highlighted related results in finite sum setting. Total complexity comparisons with respect to finite sum setting is also important. There exists results for finite-sum nonconvex optimization with variance reduction, e.g., Stochastic Variance Reduction for Nonconvex Optimization, 2016, Reddi et. al. I believe it is important to comparatively evaluate the results of this paper with that of such prior work.
• Numerically, authors only compare against AdaGrad and SGD. I would say this paper is a rather theory paper, but it claims rate improvements, for which I previously stated my doubts. Therefore, I would expect comparisons against other methods as well, which is of interest to ICLR community in my opinion.
• This is a minor comment that should be easy to address. For ICLR, supplementary material is not mandatory to check, however, this is a rather theoretical paper and the correctness/clarity of proofs is important. I would say authors could have explained some of the steps of their proof in a more open way. There are some crucial expressions which were obtained without enough explanations. Please refer to my additional comments in the following part.
Additional Comments:
• I haven’t seen the definition that x t , m + 1 = x t + 1 , 1
in the main text. It appears in the supplements. Could you please highlight this in the main text as it is important for indexing in the analysis?
• Second bullet point of your contributions claim that “[consistency] condition is easy to verify”. I do not agree with this as I cannot see how someone could guarantee/compute the value r
ahead of time or even after observing any sequence of gradients. Could you please clearly define what verification means in this context?
• In Assumption A3, I understand that G t e i = g t , i and G t e = ∑ i = 1 m g t , i
. I believe the existing notation makes it complicated for the reader to understand the implications of this condition.
• In the paragraph right above Section 4.2, authors state that presence of second moments, V t , i
enables adaptive methods to have improved rates of SGD through Lemma 3. Could the authors please explain this in details?
• In Corollary 1, authors state that “the computational complexity is nearly O ( m 5 / 2 n d 2 ϵ − 2 ) ~
”. A similar statement exists in Corollary 2. Could you please explain what “nearly” means in this context?
• In Lemma 8 in the supplements, a a T and b b T
in the main expression of the lemma are rank-1 matrices. This lemma has been used in the proof of Lemma 4. As far as I understood, Lemma 8 is used in such a way that a a T or b b T
correspond to something like g t , j 2 – g t − 1 , j 2
. I am not sure if this construction fits into Lemma 8 because, for instance, the expression g t , j 2 – g t − 1 , j 2
is difference of two rank-1 matrices, which could have rank \leq 2. Hence, there may not exist some vector a
such that a a T = g t , j 2 – g t − 1 , j 2
, hence Lemma 8 may not be applied. If I am mistaken in my judgment I am 100% open for a discussion with the authors.
• In the supplements, in section “A.1.7 PROOF OF MAIN THEOREM 1”, in the expression following the first line, I didn’t understand how you obtained the last upper bound to ∇ f ( x t , i )
. Could you please explain how this is obtained? Score:
I would like to vote for rejecting the paper. I praise the analytically rigorous proofs for the main theorems and the use of a range of tools for proving the key lemmas. Epoch-wise analysis for stochastic methods could provide insight into behavior of algorithms, especially with respect to real-life experimental setting. However, I have some concerns:
I am not convinced about the importance of consistency ratio and that it is a verifiable condition.
Related work in Table 1 has iteration-wise convergence in the general expectation-minimization setting whereas this paper considers finite sum structure with epoch-wise convergence rates. The comparison with related work is not sufficient/convincing in this perspective.
(Minor) I would suggest the authors to have a more comprehensive experimental study with comparisons against multiple adaptive/stochastic optimizers. More experimental insight might be better for demonstrating consistency ratio.
Overall, due to the reasons and concerns stated in my review, I vote for rejecting this paper. I am open for further discussions with the authors regarding my comments and their future clarifications.
======================================= Post-Discussions =======================================
I would like to thank the authors for their clarifications. After exchanging several responses with the authors and regarding other reviews, I decide to keep my score.
Although the authors come up with a more meaningful assumption, i.e., SGC, compared to their initial condition, I am not fully convinced about the contributions with respect to prior work: SGC assumption is a major factor in the improved rates and it is a very restrictive assumption to make in practice.
Although this paper proposes theoretical contributions regarding adaptive gradient methods, the experiments could have been a bit more detailed. I am not sure whether the experimental setup fully displays improvements of the proposed variants of AdaGrad.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '5', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1']}",,
"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].",8fLgt7PQza,ICLR_2025,"1. By reviewing your code and the details in the article, I can see that your workload is immense, however, the contribution of this article is incremental. My understanding is that it is essentially a combination of GraphRAG and GraphCare [1]. Furthermore, many key baselines were not cited. Since the authors mentioned that this paper focuses on RAG for EHR, some essential RAG algorithms should have been introduced, such as MedRetriever [2], and commonly used GraphRAG algorithms like KGRAG [3].
2. In the experiment or appendix section, I did not clearly see the formulas for Sensitivity and Specificity, nor were there any corresponding references, which is quite confusing to me. Moreover, using Accuracy as a metric in cases of highly imbalanced labels is unreasonable. For instance, in the MIMIC-III Mortality Prediction task, the positive rate is 5.42%. If I predict that all patients will survive, I can still achieve an accuracy of 94.58%. Previous works, such as GraphCare [1], have adopted AUROC and AUPRC as evaluation metrics.
3. The article is overly long and filled with detailed content, making it easy for readers to miss important points.
- [1] GraphCare: Enhancing Healthcare Predictions with Personalized Knowledge Graphs. ICLR 2024
- [2] MedRetriever: Target-driven interpretable health risk prediction via retrieving unstructured medical text. CIKM 2021
- [3] Biomedical knowledge graph-enhanced prompt generation for large language models. Arxiv 2023","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance ""I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार"" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification?",ARR_2022_356_review,ARR_2022,"1. As mentioned in Paper Summary, clear distinction between the 3 classes of Extreme Speech are needed. While the authors included definitions, I still find it difficult to differentiate derogatory extreme speech from exclusionary extreme speech. For instance, in the sample data file you provided, why is the instance ""I support it 100/% #मुस्लिमो_का_संपूर्ण_बहिष्कार"" exclusionary extreme speech but derogatory extreme speech? It seems that annotators took into account of local regulation over speech (line 438-441) - which regulation exactly?. Since the local regulation plays a role in annotations, does it reflect on zero-shot cross-country classification? - maybe the poor performance is due to annotation variance not model capability (line 546-548 v.s. line 553-558).
I understand that this task is complicated and believe that adding some examples will help. If possible, providing a screenshot or text description of annotation guideline will be great.
2. The dataset contains only extreme speech - it seems that the authors filtered out somehow the neutral text (or ones that don't require moderation according to their definitions). Why did you decide to discard them? Who set the criteria?
1. In Table 14, the α and overall accuracy are especially low for India. Is there any explanation?
2. In appendix, there are many tables with very little description or even none. If a table is added, it would be better to include relevant discussions. Otherwise, readers can only guess what it meant.
3. One interesting analysis would be comparing annotations between passages using only local language and the ones using local language and English (code-switching).","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '2', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The authors need to show a graph showing the plot of T vs number of images, and Expectation(T) over the imagenet test set. It is important to understand whether the performance improvement stems solely from the network design to exploit spatial redundancies, or whether the redudancies stem from the nature of ImageNet, ie., large fraction of images can be done with Glance and hence any algorithm with lower resolution will have an unfair advantage. Note, algorithms skipping layers or channels do not enjoy this luxury.",NIPS_2020_204,NIPS_2020,"1.The authors have done a good job with placing their work appropriately. One point of weakness is insufficient comparison to approaches that aim to reduce spatial redudancy, or make the networks more efficient specifically the ones skipping layers/channels. Comparison to OctConv and SkipNet even for a single datapoint with say the same backbone architecture will be valuable to the readers. 2. The authors need to show a graph showing the plot of T vs number of images, and Expectation(T) over the imagenet test set. It is important to understand whether the performance improvement stems solely from the network design to exploit spatial redundancies, or whether the redudancies stem from the nature of ImageNet, ie., large fraction of images can be done with Glance and hence any algorithm with lower resolution will have an unfair advantage. Note, algorithms skipping layers or channels do not enjoy this luxury. 3. The authors should add results from [57] and discuss the comparison. Recent alternatives to MSDNets should be compared and discussed. 4. Efficient backbone architectures and approaches tailoring the computation by controlling convolutional operator have the added advantage that they can be generally applied to semantic (object recognition) and dense pixel-wise tasks. Extension of this approach, unlike other approaches exploiting spatial redundancy to alternate vision tasks is not straightforward. The authors should discuss the implications of this approach to other vision tasks.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand. Fig.",ICLR_2022_1842,ICLR_2022,"weakness, right?
Sec. 4.2: just for clarity, is each object's bounding box (for ray intersection) computation axis-aligned with the object coordinate system or the world/scene coordinate system?
Is there anything that constrains (in a soft or hard manner) the outgoing fractions to sum up/integrate to 1 or at most 1 for a given incoming light direction?
Fig. 10: What exactly is N in this figure? N is used in the main text to refer to the number of objects and to the number of point samples along a ray, neither of which seems like the right parameter here.
Minor suggestions for improvements:
Fig. 7: I currently cannot see much in this figure, a comparison to a white/grey environment map would make it easier to tell that there is an effect.
I'm not a fan of the equation two lines after Eq. 4. I understand what it's trying to say but I believe this needs to be changed to be mathematically correct, unless that makes a bunch of other equations messy. Also, why is it L_l instead of just L? That notation should be introduced beforehand.
Fig. 8: Switching out columns 2 and 3 would make the difficult comparison between No Indirect and Full Model easier.
There's a typo at the end of page 2: from from","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '5', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['X', '2', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', '5', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4 - For sequential ensembling, it is important to study the effect of noise accumulation in the context of homomorphic encryption. This limitations prevents the use of even single deep neural networks on homomorphically encrypted data.",ICLR_2023_2057,ICLR_2023,"1 - The main idea of using ensemble of neural networks is trivial and very common in machine learning literature. The paper doesn't provide any specific adaptation to the homomorphic encryption domain. 2 - The discussion on the homomorphic encryption schemes is completely missing. What type of HE do you use? 3 - How do you preform majority voting in the encrypted domain? Most of HE schemes do not support argmax operation. 4 - For sequential ensembling, it is important to study the effect of noise accumulation in the context of homomorphic encryption. This limitations prevents the use of even single deep neural networks on homomorphically encrypted data.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '1', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '1', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '1', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- When trained and evaluated with the same time-step, Figure 5 shows similar performance between the baseline model and the time-aware model. This makes the effectiveness of the proposed methods questionable when the goal is just to achieve good performance. Maybe under some scenarios where the training time-step and evaluation time-step are different, the proposed method might make more sense.",8lwWBSa1pJ,ICLR_2025,"- The motivation referring to the Nyquist-Shannon sampling theorem does not seem to be consistent with the observation of numerical experiments. From the sampling theorem, as long as the sampling frequency is high enough, there won't be any information loss. But the numerical experiments suggest that the non time-aware model performs poorly even when the sampling frequency is the highest. This inconsistency makes the connection to the sampling theorem questionable.
- During the training phase, the sampling step is randomly selected from a log-uniform distribution. It is suggested that this distribution helps stabilize the training process, but no theoretical nor numerical analysis is provided. Some ablation studies using different sampling distribution might provide some insights to the choice.
- In the numerical evaluation, performance is compared across different observation rate. Given any time-step, the time-aware model can provide the appropriate prediction by conditioning on the time-step. But for the model trained with a fixed time-step, the information of the observation time-step does not seem to be adjusted. For example, for a model trained with $\Delta t=1$ms, when evaluating at $\Delta t=2$ms, instead of applying the model once, one might want to apply the model twice to have a more accurate prediction given the knowledge of the doubled time-step. Without doing some kind of adjustments for the baseline models make the fairness of comparisons questionable.
- When trained and evaluated with the same time-step, Figure 5 shows similar performance between the baseline model and the time-aware model. This makes the effectiveness of the proposed methods questionable when the goal is just to achieve good performance. Maybe under some scenarios where the training time-step and evaluation time-step are different, the proposed method might make more sense.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '2', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '1']}",,
"- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.",NIPS_2021_37,NIPS_2021,", * Typos/Comments)
Overall, I like and value the research topic and motivation of this paper and lean positive. However, some details are not clear enough. I would update my rating depending on the authors' feedback. The details are as follows.
+ Interesting and important research problem. This paper focuses on how to obtain disentangle representations for feature-level augmentation. This topic is interesting and important, and will attract many interests of the NeurIPS community.
+ Good quality of writing and organization. Overall, the writing quality is good and the paper is well organized. It is comfortable to read this paper, although some details are not clear.
+ Comprehensive experiments. Experiments are conducted on two synthetic datasets Colored MNIST and Corrupted CIFAR-10) and two real-world datasets (BAR and Biased FFHQ).
- Relative difficulty score and generalized cross-entropy (GCE) loss. It is not clear how the relative difficulty score W ( x )
in Eq. (1) is used in the pipeline. W(x) is not mentioned again in both the overall objective functions Eq. (2) or Algorithm 1. Since readers may not be familiar with the generalized cross-entropy (GCE) loss, it is encouraged to briefly introduce the formulation and key points of the GCE loss to make this paper more self-contained.
- How bias-conflicting samples and bias-aligned samples are selected. This weakness follows the first one. It seems that the ""bias-conflicting"" is determined based on the relative difficulty score, but the details are missed. Also, the ablation study on how the ""bias-conflicting"" is determined, e.g., setting the threshold for the relative difficulty score, is encouraged to be considered and included.
- Disentanglement. It is not clear how disentanglement is guaranteed. Although ""Broader Impacts and Limitations"" stated that ""Obtaining fully disentangled latent vectors ... a limitation"", it is still important to highlight how the disentanglement is realized and guaranteed without certain bias types.
- Inference stage. It is not clear how the inference is conducted during testing. Which encoders/decoders are preserved during the test stage?
- Figure 1 is not clear. First, it seems that the two y towards L CE
are the outputs of C i
, but they are illustrated like labels rather than predictions. Second, the illustration of the re-weighting module is not clear. Does it represent Eq. (4)?
- Table 4 reported a much lower performance of ""swapping"" on BAR compared to the other three datasets. Is there any explanation for this, like the difference of datasets?
- Sensitivity to hyperparameters. The proposed framework consists of three important hyperparameters, ( λ dis , λ s w a p b , λ swap )
. It is not clear whether the framework is sensitive to these hyperparameters and how these hyperparameters are determined.
* (Suggestion) Illustration of backpropagation. As introduced in Line 167-168, the loss from C i
is not backpropagated to E b
. It would be clearer if this can be added in Figure 1.
* Line 280. Is ""the first row and column ... respectively"" a typo? It is a little confusing for me to understand this.
* Typos in Algorithm 1. Are λ dis and λ s w a p b
missed in L dis and L swap ?
* Typo in Line 209. Corrputed -> Corrupted.
============================= After rebuttal ===================================
After reading the authors' response to my questions and concerns, I would like to vote for acceptance.
The major strengths of this paper are:
The research problem, unbiased classification via learning debiased representation, is interesting and would attract the NeurIPS audience's attention.
The proposed method is simple but effective. The method is built on top of LfF [12] and further considers (1) intrinsic and bias feature disentanglement and (2) data augmentation by swapping the bias features among training samples.
The paper is clearly written and well organized.
These strengths and contributions are also pointed out by other colleague reviewers.
My main concerns were:
Unclear technical details of the GCE loss and the relative difficulty score. This concern was also shared with Reviewer 8Ai1 and iKKw. The authors' response clearly introduced the details and addressed my concern well.
Sensitivity to hyper-parameters. The authors' response provided adequate results to show the sensitivity to hyper-parameters. Other details of implementation and analysis of experimental results. The authors' responses clearly answered my questions.
Considering both strengths and the weakness, I am happy to accept this paper.
The authors have adequately addressed the limitations and potential negative societal impact of their work.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '3', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', 'X', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"1) First of all, to compare the obtained complexity for the proposed method with the previous result in a strongly-convex concave case, it should be used standard regularization trick.",kklwv4c4dI,ICLR_2024,"Table 1 presents the previous and current results strangely:
1) First of all, to compare the obtained complexity for the proposed method with the previous result in a strongly-convex concave case, it should be used standard regularization trick.
2) From my point of you, when complexity contains several terms, each of them should be added.
About Table 2, The authors claim that ""The sequential version of FeDualEx leads to the stochastic dual extrapolation for CO and yields, to our knowledge, the first convergence rate for the stochastic optimization of composite SPP in non-Euclidean settings ."" It is not true, there is a wide field related to operator splitting in deterministic and stochastic cases. Look at this paper please https://epubs.siam.org/doi/epdf/10.1137/20M1381678.
Also, compared to the previous works, the authors use bounded stochastic gradient assumption and homogeneity of data. In many federated learning papers, those assumptions are avoided. Despite that the authors write ""Assumption e is a standard assumption"", it would be better to provide analysis without it to have more generality.
In Theorem 1, and Theorem 2, the final result contains mistakes in complexity, because some of them were done in the proof.
The first mistake is made in theorem 3 and repeats in the main theorem. Please look at the last inequality on page 40:
To make $3\eta^2\beta^2 -1 \leq 0$, the stepsize should be chosen in the following way: $\eta \leq \frac{1}{\sqrt{3}\beta}$. This will change the complexity of the methods. The same was done in the proof of Theorems 1, and 2. Please see Lemma 3, 17.
The second mistake is made in the proof of Lemma 13, in the last two inequalities, where should be $\dots\sqrt{2V^l_z(\cdot)} \leq \dots \sqrt{B}$. This thing also will change the final complexity.
The appendix is hard to read in terms of the order of Lemmas. I think it would be better if the numeration of Lemmas had a strict order (for example, after Lemma 5 lemma 6 follows.)
Other things dealing with weaknesses, please, see in questions.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '5', '4', '3']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '3', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '4', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- It would be good to include in the left graph in fig 3 the learning curve for a model without any mean teacher or pi regularization for comparison, to see if mean teacher accelerates learning or slows it down.",NIPS_2017_114,NIPS_2017,"- More evaluation would have been welcome, especially on CIFAR-10 in the full label and lower label scenarios.
- The CIFAR-10 results are a little disappointing with respect to temporal ensembles (although the results are comparable and the proposed approach has other advantages)
- An evaluation on the more challenging STL-10 dataset would have been welcome. Comments
- The SVNH evaluation suggests that the model is better than pi an temporal ensembling especially in the low-label scenario. With this in mind, it would have been nice to see if you can confirm this on CIFAR-10 too (i.e. show results on CIFAR-10 with less labels)
- I would would have like to have seen what the CIFAR-10 performance looks like with all labels included.
- It would be good to include in the left graph in fig 3 the learning curve for a model without any mean teacher or pi regularization for comparison, to see if mean teacher accelerates learning or slows it down.
- I'd be interested to see if the exponential moving average of the weights provides any benefit on it's own, without the additional consistency cost.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- The proposed objective Eq. 2 in line 128, requires the optimisation over both the parameters of the transformation \phi and the shared model \theta_S. The effect on the number of parameters vs. prior work eg. AlignFlow (Grover et. al. 2019) has not been discussed clearly.",NIPS_2020_1772,NIPS_2020,"- Central parts of the paper are unclear eg. in line 80 \log P_M (X; \theta) should be the negative cross entropy. - The proposed objective Eq. 2 in line 128, requires the optimisation over both the parameters of the transformation \phi and the shared model \theta_S. The effect on the number of parameters vs. prior work eg. AlignFlow (Grover et. al. 2019) has not been discussed clearly. - The paper is sparse in quantitative results and does not compare to important prior work based on GANs [1]. The only quantitative results are on adaptation from USPS to MNIST in line 268. However, prior work [1] achieves 96.5% accuracy in comparison to the 55% accuracy achieved by the proposed method. - The empirical evaluation is restricted to small datasets eg. moons, MNIST and USPS. It would be desirable to evaluate the proposed approach on the more complex Facades/Maps/Cityscapes using the MSE metric to facilitate comparison with AlignFlow and [1]. - The shared model (\theta_s) is trained on two datasets simultaneously. It is unclear how the inductive bias from each of the datasets influence the shared space. [1] CyCADA: Cycle-Consistent Adversarial Domain Adaptation, ICML 2018.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '2', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '4', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.",NIPS_2021_291,NIPS_2021,"The writing is clear and the motivation is clarified clearly. Besides, the theoretical grounding and experimental evaluation are not sufficient to show their originality and significance. Here are some of the suggestions: 1) I would like to see ablation studies for the proposed training method, the traditional backpropagation framework refers as the baseline. 2) How to deal with the different types of inputs (e.g., bio-medical signals or speech)? It would be valuable to discuss it and present your solutions in this paper. The citation seems a bit disordered.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '3', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '2', '4', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '2', 'X', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '3', '3', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. If Eq. 4 stands, does that mean the u^l in Eq.3 tends to be 1? -- The improvement of the designed solutions in Table 5, is not significant on some datasets. For example, on OfficeHome, the CSAC achieves 64.35, and the proposed solution achieves 64.71, which is a marginal improvement.",ICLR_2023_2298,ICLR_2023,"Weakness:
-- The work is heavily dependent on FedBN. The main difference is that author of this work designed an adaptive interpolation parameter estimation method. This jeopardizes the novelty and technical contribution of the whole work. -- I am a little conservative about Eq. 4. If Eq. 4 stands, does that mean the u^l in Eq.3 tends to be 1? -- The improvement of the designed solutions in Table 5, is not significant on some datasets. For example, on OfficeHome, the CSAC achieves 64.35, and the proposed solution achieves 64.71, which is a marginal improvement.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '2', '1', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', 'X', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '3', '1', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work ""Targeting Ultimate Accuracy: Face Recognition via Deep Embedding"", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.",NIPS_2016_208,NIPS_2016,"1. The novelty is a little weak. It is not clear what's the significant difference and advantage compared to NCA [6] and ""Small codes and large image databases for recognition"", A. Torralba et al., 2008, which used NCA in deep learning. 2. In the experiment of face recognition, some state-of-the art references are missing, such as Baidu' work ""Targeting Ultimate Accuracy: Face Recognition via Deep Embedding"", http://vis-www.cs.umass.edu/lfw/results.html#baidu. In that work, the triplet loss is also used and it reported the result trained on the dataset containing 9K identities and 450K images, which is similar with Webface. The VRF can achieve 98.65% on LFW which is better than the result in Table 3 in this paper.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '4', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"2. The question answering requires the template mapping to transform the question into a masked statement, which might cause the poor generalization to questions that are not 'Wh-types'/transformable.",9RugvdmIBa,EMNLP_2023,"1. Such strategy requires extra parallel data, which might not exist in many datasets/tasks especially during the pre-training stage. The authors did not consider such cases to propose some cheap ways to acquire such parallel data. Also, utilizing the parallel data for training increase the size of context window, which require larger context window for models and might be expensive.
2. The question answering requires the template mapping to transform the question into a masked statement, which might cause the poor generalization to questions that are not 'Wh-types'/transformable.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '1', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '3', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '1']}",,
"2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively.",NIPS_2022_728,NIPS_2022,"Weakness 1. The setup of capturing strategy is complicated and is not easy for applications in real life. To initialize the canonical space, the first stage is to capture the static state using a moving camera. Then to model motions, the second stage is to capture dynamic states using a few (4) fixed cameras. Such a 2-stage capturing is not straightforward. 2. To utilize a volumetric representation in the deformation field is not a novel idea. In the real-time dynamic reconstruction task, VolumeDeform [1] has proposed volumetric grids to encode both the geometry and motion, respectively. 3. The quantitative experiments (Tab. 2 and Tab. 3) show that the fidelity of rendered results highly depends on the 2-stage training strategy. In a general capturing case, other methods can obtain more accurate rendered images. Oppositely, Tab. 2 shows that it is not easy to fuse the designed 2-stage training strategy into current mainstream frameworks, such as D-NeRF, Nerfies and HyperNeRF. It verifies that the 2-stages training strategy is not a general design for dynamic NeRF.
[1] Innmann, Matthias, Michael Zollhöfer, Matthias Nießner, Christian Theobalt, and Marc Stamminger. ""Volumedeform: Real-time volumetric non-rigid reconstruction."" In European conference on computer vision (ECCV), pp. 362-379. Springer, Cham, 2016.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '2', '3', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '4', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '1']}",,
"3. The ICL-HAR, while improving consistency and verifiability, has greatly impedes the accuracy scores (dropping from 70.4 to 55.6 on TRIP). This should be discussed or at least acknowledged in the main text in more detail.",Yz4VKLeZMG,EMNLP_2023,"1. Generalizability: both fine-tuning and in-context learning strategies seem to be tailored for shifting model attention to a smaller chunk of key information, which is confirmed by the attention weight analysis. This makes me to worry to what extent this method could be generalized to other datasets where the information is not presented in contrastive pairs, and where the conflicting information is not restricted to one or two sentences but widely spread in the entire passage. For example, the task of identifying conflicting sentences might have over-simplified the reasoning task by taking the short-cut to ignore lots information, which just happens to be trivial in these specific datasets.
2. The developed strategies, while interesting, might just be marginally relevant to the cognitive process of heuristic / analytical dual passes of human reasoning. The heuristic reasoning process is more related to the information being utilized and the amount of attention paid to more fine-grained details. For instance, in online language comprehension, comprehenders might ignore fine-grained syntactic structured and rely on the semantic meaning of the words and their prior knowledge to interpret ""the hearty meal was devouring..."" as ""the hearty meal was devoured..."" (Kim and Osterhout, 2005). However, the heuristic process is less concerned with gratuity of final decision, as implied by the HAR model. The HAR framework breaks the reasoning tasks into multiple sub-tasks, where the gratuity of the decision gradually becomes finer-grained. This might be better characteristics as step-by-step chain of reasoning rather than heuristic decision-making.
3. The ICL-HAR, while improving consistency and verifiability, has greatly impedes the accuracy scores (dropping from 70.4 to 55.6 on TRIP). This should be discussed or at least acknowledged in the main text in more detail.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '3', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '5', '5', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', 'X', 'X']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '4', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'boda'], 'labels': ['1', '1', '1']}",,
"4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately.",NIPS_2018_707,NIPS_2018,"weakness of the paper is the lack of experimental comparison with the state of the art. The paper spends whole page explaining reasons why the presented approach might perform better under some circumstances, but there is no hard evidence at all. What is the reason not to perform an empirical comparison to the joint belief state approach and show the real impact of the claimed advantages and disadvantages? Since this is the main point of the paper, it should be clear when the new modification is useful. 3) Furthermore, there is an incorrect statement about the performance of the state of the art method. The paper claims that ""The evidence suggests that in the domain we tested on, using multi-valued states leads to better performance."" because the alternative approach ""was never shown to defeat prior top AIs"". This is simply incorrect. Lack of an experiment is not evidence for superiority of the method that performed the experiment without any comparison. 4) The rock-paper-scissors example is clearly inspired by an example that appeared in many previous work. Please, cite the source appropriately. 5) As explained in 1), the presented method is quite heuristic. The algorithm does not actually play the blueprint strategy, only few values are used in the leaf states, which cannot cover the whole variety of the best response values. In order to assess whether the presented approach might be applicable also for other games, it would be very useful to evaluate it on some substantially different domains, besides poker. Clarity: The paper is well written and organized, and it is reasonably easy to understand. The impact of the key differences between the theoretic inspiration and the practical implementation should be explained more clearly. Originality: The presented method is a novel modification of continual resolving. The paper clearly explains the main distinction form the existing method. Significance: The presented method seems to substantially reduce the computational requirements of creating a strong poker bot. If this proofs to be the case also for some other imperfect information games, it would be a very significant advancement in creating algorithms for playing these games. Detailed comments: 190: I guess the index should be 1 339: I would not say MCCFR is currently the preferred solution method, since CFR+ does not work well with sampling 349: There is no evidence the presented method would work better in stratego. It would depend on the specific representation and how well would the NN generalize over the types of heuristics. Reaction to rebuttal: 1) The formulation of the formal statement should be clearer. Still, while you are using the BR values from the blueprint strategy in the computation, I do not see how the theory can give you any real bounds the way you use the algorithm. One way to get more realistic bounds would be to analyze the function approximation version and use error estimates from cross-valiadation. 2) I do not believe head-to-head evaluation makes too much sense because of well known intransitivity effects. However, since the key difference between your algorithm and DeepStack is the form of the used leaf evaluation function, it would certainly not take man-years to replace the evaluation function with the joint belief in your framework. It would be very interesting to see comparison of exploitability and other trade-offs on smaller games, where we can still compute it. 4) I meant the use of the example for save resolving. 5) There is no need for strong agents for some particular games to make rigorous evaluation of equilibrium solving algorithms. You can compute exploitability in sufficiently large games to evaluate how close your approach is to the equilibrium. Furthermore, there are many domain independent algorithms for approaximating equilibriua in these games you can compare to. Especially the small number of best response values necessary for the presented approach is something that would be very interesting to evaluate in other games. Line 339: I just meant that I consider CFR+ to be ""the preferred domain-independent method of solving imperfect-information games"", but it is not really important, it was a detailed comment.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '2', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '1', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '4', '2', '4']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
3. The innovations of network architecture design and constraint embedding are rather limited. The authors discussed that the performance is limited by the performance of the oracle expert.,NIPS_2022_69,NIPS_2022,"1. This work uses an antiquated GNN model and method, it seriously impacts the performance of this framework. The baseline algorithms/methods are also antiquated. 2. The experimental results did not show that this work model obviously outperforms other variant comparison algorithms/models. 3. The innovations of network architecture design and constraint embedding are rather limited.
The authors discussed that the performance is limited by the performance of the oracle expert.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['3', '2', '3', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '2', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '2']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '0', '0']}",,
"- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.",NIPS_2017_502,NIPS_2017,"Weakness]
- This paper is poorly written.
- The motivation, the contribution, and the intuition of this work can barely be understood based on the introduction.
- Sharing the style of citations and bullet items is confusing.
- Representing a translation vector with the notation $t^{i} = [x^{i}, y^{i}, z^{i} ] ^{T}$ is usually more preferred.
- The experimental results are not convincing.
- The descriptions of baseline models are unclear.
- Comparing the performance of the methods which directly optimize the mesh parameters, rotation, translation, and focal length according to the metric provided (projection error) doesn't make sense since the objective is in a different domain of the measurement of the performance.
- Comparing the performance of the model only pre-trained on synthetic data is unfair; instead, demonstrating that the proposed three projection errors are important is more preferred. In other words, providing the performance of the models pre-trained on synthetic data but fine-tuned on real-world datasets with different losses is necessary.
- The reason of claiming that it is a supervised learning framework is unclear. In my opinion, the supervision signals are still labeled. [Reproducibility]
The proposed framework is very simple and well explained with sufficient description of network parameters and optimization details. I believe it's trivial to reproduce the results. [Overall]
In term of the proposed framework, this paper only shows the improvement gained of fine-tuning the model based on the proposed losses defined by the reprojection errors of key points, optical flow, and foreground-background segmentation.
Taking into account that this work does show that fine-tuning the model pre-trained on synthetic datasets on real-world video clips improves the performance especially, it's still a convicting article.
In sum, as far as I am concerned this work makes a contribution but is insufficient.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '5', '5']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '3', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '5', '2', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['4', '4', '3', '3']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['0', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"- L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).",ARR_2022_161_review,ARR_2022,"The amount of background provided can be reduced, and consists of quite a few detailed descriptions of topics and experiments that are not directly related to the experiments of the paper (e.g. the Priming paragraph at L210, Novel verbs paragraph at L224). The space that is currently occupied by this extensive background could be used more efficiently, and cutting it down a bit opens up space for additional experiments. The ‘Jabberwocky constructions’ experiment is quite prone to several potential confounds that need to be explored in more detail in order to ensure that the current set of results truly hints at ‘the neural reality of argument structure constructions’. The fact that the contextualised embeddings of verbs in the same syntactic configuration is highly similar isn’t that surprising in itself (as is noted by the authors as well). The authors decided to drop the ‘priming’ component of the original paper in order to adapt the experiment to LMs, but there are other options that can be explored to align the setup more closely to the original (see section below for some ideas).
### Comments / Questions - Could the results of the Sentence Sorting be driven by the fact that sentence embeddings are obtained by averaging over word embeddings? It seems that this procedure would be quite prone to simply cluster based on features stemming from individual tokens, instead of a more general abstract signal. I could imagine that in a BERT-like architecture the representation at the [CLS] position might serve as a sentence representation as well.
- Alternatively, would it be possible to set up the sentence sorting experiment in such a way that the lexical overlap in between sentences is limited? This is common in structural priming experiments as well, and models are known to rely heavily on lexical heuristics. ,
- Did you consider different measures of similarity in the Jabberwocky experiment? Euclidean distance might not be the most perfect measure for expressing similarity, and I would suggest looking into alternatives as well, like cosine similarity. - A bit pedantic, but Jabberwocky words are non-existing nonce words, whereas the setup that the authors arrived at is only semantically nonsensical, yet still made up of existing words (a la ‘Colorless green ideas’). Referring to them as Jabberwocky (L.458) would give the impression of actually using nonce words.
- How many ASCs have been argued to exist (within English)? Is there a reason why the 4 constructions used in Case Study 1 (_transitive, ditransitive, caused-motion, resultative_; L.165), are slightly different from Case Study 2 (_ditransitive, resultative, caused-motion, removal_; Table 2)?
--- ### Suggestions: - L.490: “we take the embedding of the first subword token as the verb embedding.” It is also quite common in cases like that to average over the subword representations, which is done e.g. by [Hewitt and Manning (2019, footnote 4)](https://aclanthology.org/N19-1419.pdf).
- I suggest not phrasing the Jabberwocky experiment as “probing” (L.444 ‘method to probe LMs’, L.465 ‘probing strategy’, etc.), given the connotation this term has with training small classifiers on top of a model’s hidden state representations.
- Could the ‘priming’ aspect of Johnson and Goldberg (2013) in the Jabberwocky experiment perhaps be emulated more closely by framing it as a “Targeted Syntactic Evaluation” task (akin to Marvin & Linzen (2018), a.o.). In the context of priming, a similar setup has recently been utilised by [Sinclair et al. (2021)](https://arxiv.org/pdf/2109.14989.pdf). One could compare the probability of _P(gave | She traded her the epicenter. He)_ to that of _P(gave | He cut it seasonal. She)_, and likewise for the other 2 constructions. This way you wouldn’t run into the confounding issues that stem from using the contextualisation of ‘gave’. - An additional experiment that might be interesting to explore is by probing for construction type across layers at the position of the verb. In the ‘Jabberwocky’ setup one would expect that at the word embedding level construction information can’t be present yet, but as it is contextualised more and more in each layer the ASC information is likely to increase gradually. Would also be interesting than to see how the curve of a jabberwocky verb compares to that of a sensical/prototypical verb (like _gave_): there is probably _some_ degree of argument structure already encoded in the word embedding there (as a lexicalist would argue), so I would expect probing performance for such verbs to be much higher at lower levels already. - Adding to the previous point: probing in itself would not even be necessary to gain insight into the layerwise contextualisation; some of the current experiments could be conducted in such a fashion as well.
--- ### Typos/Style: Very well written paper, no remarks here.","{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '4', '2']}",,,3,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '4', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['5', '5', '5', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['2', '3', '4', '5']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '0', '0', '0']}",,,"{'annotators': ['tpuEIMI7', 'nZ3mMZhw', 'aAISOUiD', 'boda'], 'labels': ['1', '1', '1', '1']}",,
"4. The model AUC can assess the model discriminant ability, i.e., the probability of a positive case is bigger than that of a negative case, but may be hard to show its consistency between predicted score and actual risk. However, this consistency may be more crucial to the clinical scoring system (differentiated with classification task). Therefore, the related studies are encouraged to conduct calibration curves to show the agreement. It would be better to prove the feasibility of the generated scoring system? The difference between the traditional method and our method can also be discussed in this paper.",NIPS_2022_1637,NIPS_2022,"1. The examples of scoring systems in the Introduction seem out of date, there are many newer and recognized clinical scoring systems. It also should briefly introduce the traditional framework of the scoring system and its difference in methodology and performance with the proposed method. 2. As shown in figure 3, the performance improvement of proposed methods seems not so significant, the biggest improvement in the bank dataset was ~0.02. Additionally, using some tables to directly show the key improvements may be more intuitive and detailed. 3. Although extensive experiments and discussion on performance, in my opinion, its most significant improvement would be efficiency, and there are few discussions or ablation experiments on efficiency. 4. The model AUC can assess the model discriminant ability, i.e., the probability of a positive case is bigger than that of a negative case, but may be hard to show its consistency between predicted score and actual risk. However, this consistency may be more crucial to the clinical scoring system (differentiated with classification task). Therefore, the related studies are encouraged to conduct calibration curves to show the agreement. It would be better to prove the feasibility of the generated scoring system? The difference between the traditional method and our method can also be discussed in this paper.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '1', '1']}",,
"2) From Figure 4, the range of ID and OOD seems not to be changed much by sparsification. Similarly, Lemma 2 requires approximately identical mean as the assumption. These conditions are crucial for DICE, but is not well discussed, eg., how to ensure DICE meet these conditions.",ICLR_2022_3058,ICLR_2022,". At the end of section 2, the authors tried to explain noisy signals are harmful for the OOD detection. It's obvious that with more independent units the variance of the output is higher. But this affects both ID and OOD data. The explanation is not clear.
. The analysis in section 6 is kind of superficial. 1) Lemma 2: the conclusion is under the assumption that the mean is approximately the same. However, as DICE is not designed to guarantee this assumption, the conclusion in Lemma 2 may not apply to DICE. 2) mean of output: the scoring function used for OOD detection is max_cf_c(x). The difference of mean is not directly related to the detection scoring, so the associated observation may not be used to explain why the algorithm works.
. Overall, it is not well explained why the proposed algorithm would work for some OOD detection. 1) From the observation, although DICE can reduce the variance of both ID and OOD data, the effect on OOD seems more significant. This may due to the large difference between ID and OOD. Therefore, it would be interesting to exam the performance of DICE by varying the likeness between OOD and ID. 2) From Figure 4, the range of ID and OOD seems not to be changed much by sparsification. Similarly, Lemma 2 requires approximately identical mean as the assumption. These conditions are crucial for DICE, but is not well discussed, eg., how to ensure DICE meet these conditions.
. In the experiment, the OOD samples generally are significantly different from ID samples (thus less challenging). As pointed out in the above comment, it would be interesting to compare the performance of DICE by varying the OODness of test samples. For example, the ID data is 8 from MNIST, OOD datasets can be 1) 3 from MNIST; 2) 1 from MNIST; 3) FMNIST; and 4) CIFAR-10.
. The comparison between DICE and generative-based model (Table 3) is unfair as DICE is supervised while the benchmarks are unsupervised. It's not surprising that DICE is better. The authors should add comments on that.
. It is claimed in the experimental part that the in-distribution classification accuracy can be maintained under DICE. Only the result on CIFAR-10 is shown. Please provide more results to support the conclusion if possible.
. Instead of using directed sparsification, one possible solution may be just using a simpler network. Of course this would change the original network architecture. But as one part of the ablation study, it would be interesting to know whether a simpler network would be more beneficial for the OOD detection.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct? I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into \paragraphs — it's currently a huge wall of text.",ICLR_2022_3205,ICLR_2022,"This method trades one intractible problem for another: it requires the learning of cross-values v e ′ ( x t ; e )
for all pairs of possible environments e , e ′
. It is not clear that this will be an improvement when scaling up.
At a few points the paper introduces approximations, but the gap to the true value and the implications of these approximations are not made completely clear to me. The authors should be more precise about the tradeoffs and costs of the methods they propose, both in terms of accuracy and computational cost.
On page 6, it claims that estimating v c
according to samples will lead to Thompson sampling-like behavior, which might lead to better exploration. This seems a bit facetious given that this paper attempts to find a Bayes-optimal policy and explicitly points out the weaknesses of Thompson sampling in an earlier section.
Not scaled to larger domains, but this is understandable.
Questions and minor comments
Is the belief state conditioning the policy also supposed to change with time τ
? As written it looks like the optimal Bayes-adaptive policy conditions on one sampled belief about the environment and then plays without updating that belief.
It is not intuitive to me how it is possible to estimate v f
, despite the Bellman equation written in Eq. 12. It would seem that this update would have to integrate over all possible environments in order to be meaningful, assuming that the true environment is not known at update time. Is that correct?
I guess this was probably for space reasons, but the bolded sections in page 6 should really be broken out into \paragraphs — it's currently a huge wall of text.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"• Besides, the experiments seem not too strong and fair to me. I don't understand why all the baselines use the position kernels, why don't we use the default settings of these baselines in the literature? Besides, it seems like some baselines related to BO with discrete & categorial variables are missing. The paper also needs to compare its proposed approach with these baselines. I think the paper does not mention much about the limitations or the societal impacts of their proposed approach.",NIPS_2022_601,NIPS_2022,"Although I like the general idea of using DPP, I found there are various issues with the current version of the paper. Please see my detailed comments as follows.
• The paper specifically targets the permutation problems, but I don't see how this permutation property is incorporated into the design of the proposed acquisition function (except the fact that batch BO is used so that we can evaluate multiple data points parallel and thus can avoid the issue of large search space for permutation problems).
• Even though the paper provides various theoretical analysis, but these analyses seem not to be rigorous, and might not really help to answer the analytical properties of the proposed approach.
o The Acquisition Weighted Kernel L^{AW} defined in Line 122 seems to not be a real (valid) kernel? As it depends on any acquisition function a(x), so it seems impossible to me that it is a valid kernel for all cases. Besides, this seems to be like a component of the proposed acquisition function, rather than to be called a ""kernel"".
o The regret analysis in Theorem 3.6 depends on the maximum information gain \gamma_T, but this \gamma_T value is not properly bounded in Theorem 3.9. Theorem 3.9 only shows that \gamma_T is smaller than a function of \lambda_{max} but there is no guarantee that \lambda_{max} is bounded when T goes to infinity. This is a key analysis in any BO analysis. In the literature, only several kernels have been shown that their \lambda_{max} is bounded when T goes to infinity.
o Again, same problem with Theorem 3.12, is there any guarantee that the \lambda_{max} of the position kernel is upper bounded?
• The performance of the proposed approach on the permutation optimization problems are not that good though (Section 5.1). LAW-EI performs pretty bad, much worse than other baselines in various cases, while LAW-EST is on par with other baselines and only performs well in one problem. Besides, I don't understand why LAW-UCB is not added as one of the baselines. The justification regarding the size of search space does not seem reasonable to me.
• Besides, the experiments seem not too strong and fair to me. I don't understand why all the baselines use the position kernels, why don't we use the default settings of these baselines in the literature? Besides, it seems like some baselines related to BO with discrete & categorial variables are missing. The paper also needs to compare its proposed approach with these baselines.
I think the paper does not mention much about the limitations or the societal impacts of their proposed approach.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1) there is a drop of correlation after a short period of training, which goes up with more training iterations;",NIPS_2022_1770,NIPS_2022,"Weakness: There are still several concerns with the finding that the perplexity is highly correlated with the number of decoder parameters.
According to Figure 4, the correlation decreases as top-10% architectures are chosen instead of top-100%, which indicates that the training-free proxy is less accurate for parameter-heavy decoders.
The range of sampled architectures should also affect the correlation. For instance, once the sampled architectures are of similar sizes, it could be more challenging to differentiate their perplexity and thus the correlation can be lower.
Detailed Comments:
Some questions regarding Figure 4: 1) there is a drop of correlation after a short period of training, which goes up with more training iterations; 2) the title ""Top-x%"" should be further explained;
Though the proposed approach yields the Pareto frontier of perplexity, latency and memory, is there any systematic way to choose a single architecture given the target perplexity?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
10. Looks like all sparsity patterns do almost equally well. No insight provided as to what is happening here. Is this something unique to the sparsity detection problem or is this true for GNN in general? Section 4.3: presentation bits --> representation bits,ICLR_2023_4878,ICLR_2023,"1. The proposed sparsity technique seems to be limited in scope. While it has been shown to work well for a particular misinformation detector, there is no guarantee that it will work well for other networks. 2. Though the experimental results are encouraging, it is not clear why the simple pre-fixed should work well. No explanation is provided. 3. The dataset used for evaluation is not a widely used dataset. Basically only previous work has used it and this work is an extension of the previous work. 4. There is no novelty in the methodological aspects of the work.
Questions for the authors: 1. Why do misinformation detection models need to be deployed on smartphones? Can you give a real-world use case? 2. How does the proposed sparsity pattern compare with masks that are inferred from a pretrained model or learned during training? 3. Wu et al use event level detection results for document: ""these two tasks are not mutually independent. Intuitively, document-level detection can benefit from the results of event-level detection, because the presence of a large number of false events indicates that the document is more likely to be fake. Therefore, we feed the results produced by a well-trained event-level detector into each layer of the document-level detector."" Their ablation study (Table 4) shows that event level detection is crucial for getting best document level results (86.76 vs 84.57 F1). This seems to go against your claim that document level detection model needs to be separated from event level detection model. 4. Results in Table 1 (doc classifier exit #4) doesn't match with those of Wu et al. Why is sparse model giving better results than unpruned? 5. HSF, GROVER and CDMD are fake news detection algorithms whereas MP and LTH are sparse n/w methods. Which fake news detection algorithm is used in conjunction with MP and LTH? (Table 4) 6. Doc classifier exit #1 is nearly as good as exit # 2. And there's not a whole lot of difference b/w exit #1 and exit #4. Is this because document level event detection is a easy problem or something to do with the dataset? 7. Why no event level results for 90% sparsity in Table 4? Do the event level results degrade more drastically than even level as the sparsity is increased? 8. Why no results for a random sparsity pattern? That would be a good baseline for relative assessment of sparsity patterns. 9. In Tables 4,5 and 6, why is SMD 90% sparsity better than SMD 50%? 10. Looks like all sparsity patterns do almost equally well. No insight provided as to what is happening here. Is this something unique to the sparsity detection problem or is this true for GNN in general?
Section 4.3: presentation bits --> representation bits","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2)The derivation from Eqn. 3 to Eqn. 4 misses the temperature τ , τ should be shown in a rigorous way or this paper mention it.",ICLR_2023_650,ICLR_2023,"1.One severe problem of this paper is that it misses several important related work/baselines to compare[1,2,3,4], either in discussion [1,2,3,4]or experiments[1,2]. This paper addresses to design a normalization layer that can be plugged in the network for avoiding the dimensional collapse of representation (in intermediate layer). This idea has been done by the batch whitening methods [1,2,3] (e.g, Decorrelated Batch Normalization (DBN), IterNorm, etal.). Batch whitening, which is a general extent of BN that further decorrelating the axes, can ensure the covariance matrix of the normalized output as Identity (IterNorm can obtain an approximate one). These normalization modules can surely satisfy the requirements these paper aims to do. I noted that this paper cites the work of Hua et al, 2021, which uses Decorrelated Batch Normalization for Self-supervised learning (with further revision using shuffling). This paper should note the exist of Decorrelated Batch Normalization. Indeed, the first work to using whitening for self-supervised learning is [4], where it shows how the main motivations of whitening benefits self-supervised learning.
2.I have concerns on the connections and analyses, which is not rigorous for me. Firstly, this paper removes the A D − 1
in Eqn.6, and claims that “In fact, the operation corresponds to the stop-gradient technique, which is widely used in contrastive learning methods (He et al., 2020; Grill et al., 2020). By throwing away some terms in the gradient, stop-gradient makes the training process asymmetric and thus avoids representation collapse with less computational overhead. It verifies the feasibility of our discarding operation”. I do not understand how to stop gradients used in SSL can be connected to the removement of A D − 1
, I expect this paper can provide the demonstration or further clarification.
Secondly, It is not clear why layerNorm is necessary. Besides, how the layer normalization can be replace with an additional factor (1+s) to rescale H shown in claims “For the convenience of analysis, we replace the layer normalization with an additional factor 1 + s to rescale H”. I think the assumption is too strong.
In summary, the connections between the proposed contraNorm and uniformity loss requires: 1) removing A D − 1
and 2) add layer normalization, furthermore the propositions for support the connection require the assumption “layer normalization can be replace with an additional factor (1+s) to rescale H”. I personally feel that the connection and analysis are somewhat farfetched.
Other minors:
1)Figure 1 is too similar to the Figure 1 of Hua et al, 2021, I feel it is like a copy at my first glance, even though I noted some slightly differences when I carefully compare Figure 1 of this paper to Figure 1 of Hua et al, 2021.
2)The derivation from Eqn. 3 to Eqn. 4 misses the temperature τ , τ
should be shown in a rigorous way or this paper mention it.
3)In page 6. the reference of Eq.(24)? References:
[1] Decorrelated Batch Normalization, CVPR 2018
[2] Iterative Normalization: Beyond Standardization towards Efficient Whitening, CVPR 2019
[3] Whitening and Coloring transform for GANs. ICLR, 2019
[4]Whitening for Self-Supervised Representation Learning, ICML 2021","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Line 156. It'd be useful to the reader to add a citation on differential privacy, e.g. one of the standard works like [2].",3vXpZpOn29,ICLR_2025,"It is unclear that linear datamodels extend to other kinds of tasks, e.g. language modeling or regression problems. I believe this to be a major weakness of the paper. While linear datamodels lead to simple algorithms in this paper, the previous work [1] does not have a good argument for why linear datamodels work [1; Section 7.2]---in fact Figure 6 of [1] display imperfect matching using linear datamodels. It'd be useful to mention this limitation in this manuscript as well, and discuss the limitation's impact to machine learning.
# Suggestions:
1. Line 156. It'd be useful to the reader to add a citation on differential privacy, e.g. one of the standard works like [2].
2. Line 176. $\hat{f}$ should have output range in $\mathbb{R}^k$ since the range of $f_x$ is in $\mathbb{R}^k$.
3. Line 182. ""show"" -> ""empirically show"".
4. Definition 3. Write safe, $S_F$, and input $x$ explicitly in KLoM, otherwise KLoM$(\mathcal{U})$ looks like KLoM of the unlearning function across _all_ safe functions and inputs. I'm curious why the authors wrote KLoM$(\mathcal{U})$.
5. Add a Limitations section.
[1] Ilyas, A., Park, S. M., Engstrom, L., Leclerc, G., & Madry, A. (2022). Datamodels: Predicting predictions from training data. arXiv preprint arXiv:2202.00622.
[2] Dwork, C., & Roth, A. (2014). The algorithmic foundations of differential privacy. Foundations and Trends® in Theoretical Computer Science, 9(3–4), 211-407.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Claim (first para of Section 3.2) that ""this methodology requires significant additional assumptions"" seems too extreme to me. The only additional assumption is that the test set be drawn from the same distribution as the query set, which is natural for many machine learning settings where the train, validation, test sets are typically assumed to be from the same iid distribution. (If this assumption is in fact too hard to satisfy, then Theorem 4 isn't very useful too.) - Inequality on line 310 has wrong sign; compare inequality line 227 --- base \alpha < 1.",NIPS_2018_43,NIPS_2018,"- Theoretical analyses are not particularly difficult, even if they do provide some insights. That is, the analyses are what I would expect any competent grad student to be able to come up with within the context of a homework assignment. I would consider the contributions there to be worthy of a posted note / arXiv article. - Section 4 is interesting, but does not provide any actionable advice to the practitioner, unlike Theorem 4. The conclusion I took was that the learned function f needs to achieve a compression rate of \zeta / m with a false positive rate F_p and false negative rate F_n. To know if my deep neural network (for example) can do that, I would have to actually train a fixed size network and then empirically measure its errors. But if I have to do that, the current theory on standard Bloom filters would provide me with an estimate of the equivalent Bloom filter that achieves the same error false positive as the learned Bloom filter. - To reiterate the above point, the analysis of Section 4 doesn't change how I would build, evaluate, and decide on whether to use learned Bloom filters. - The analytical approach of Section 4 gets confusing by starting with a fixed f with known \zeta, F_p, F_n, and then drawing the conclusion for an a priori fixed F_p, F_n (lines 231-233) before fixing the learned function f (lines 235-237). In practice, one typically fixes the function class (e.g. parameterized neural networks with the same architecture) *first* and measures F_p, F_n after. For such settings where \zeta and b are fixed a priori, one would be advised to minimize the learned Bloom filter's overall false positive (F_p + (1-F_p)\alpha^{b/F_n}) in the function class. An interesting analysis would then be to say whether this is feasible, and how it compares to the log loss function. Experiments can then conducted to back this up. This could constitute actionable advice to practitioners. Similarly for the sandwiched learned Bloom filter. - Claim (first para of Section 3.2) that ""this methodology requires significant additional assumptions"" seems too extreme to me. The only additional assumption is that the test set be drawn from the same distribution as the query set, which is natural for many machine learning settings where the train, validation, test sets are typically assumed to be from the same iid distribution. (If this assumption is in fact too hard to satisfy, then Theorem 4 isn't very useful too.) - Inequality on line 310 has wrong sign; compare inequality line 227 --- base \alpha < 1. - No empirical validation. I would have like to see some experiments where the bounds are validated.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating.",ICLR_2021_1504,ICLR_2021,"W1) The authors should compare their approach (methodologically as well as experimentally) to other concept-based explanations for high-dimensional data such as (Kim et al., 2018), (Ghorbani et al., 2019) and (Goyal et al., 2019). The related work claims that (Kim et al., 2018) requires large sets of annotated data. I disagree. (Kim et al., 2018) only requires a few images describing the concept you want to measure the importance of. This is significantly less than the number of annotations required in the image-to-image translation experiment in the paper where the complete dataset needs to be annotated. In addition, (Kim et al., 2018) allows the flexibility to consider any given semantic concept for explanation while the proposed approach is limited either to semantic concepts captured by frequency information, or to semantic concepts automatically discovered by representation learning, or to concepts annotated in the complete dataset. (Ghorbani et al., 2019) also overcomes the issue of needing annotations by discovering useful concepts from the data itself. What advantages does the proposed approach offer over these existing methods?
W2) Faithfulness of the explanations with the pretrained classifier. The methods of disentangled representation and image-to-image translation require training another network to learn a lower-dimensional representation. This runs the risk of encoding some biases of its own. If we find some concerns with the explanations, we cannot infer if the concerns are with the trained classifier or the newly trained network, potentially making the explanations useless.
W3) In the 2-module approach proposed in the paper, the second module can theoretically be any explainability approach for low-dimensional data. What is the reason that the authors decide to use Shapely instead of other works such as (Breiman, 2001) or (Ribeiro et al., 2016)?
W4) Among the three ways of transforming the high-dimensional data to low-dimensional latent space, what criteria should be used by a user to decide which method to use? Or, in other words, what are the advantages and disadvantages of each of these methods which might make them more or less suitable for certain tasks/datasets/applications?
W5) The paper uses the phrase “human-interpretable explainability”. What other type of explainability could be possible if it’s not human-interpretable? I think the paper might benefit with more precise definitions of these terms in the paper.
References mentioned above which are not present in the main paper:
(Ghorbani et al., 2019) Amirata Ghorbani, James Wexler, James Zou, Been Kim. Towards Automatic Concept-based Explanations. NeurIPS 2019.
(Goyal et al., 2019) Yash Goyal, Amir Feder, Uri Shalit, Been Kim. Explaining Classifiers with Causal Concept Effect (CaCE). ArXiv 2019.
—————————————————————————————————————————————————————————————— ——————————————————————————————————————————————————————————————
Update after rebuttal: I thank the authors for their responses to all my questions. However, I believe that these answers need to be justified experimentally in order for the paper’s contributions to be significant for acceptance. In particular, I still have two major concerns. 1) the faithfulness of the proposed approach. I think that the authors’ answer that their method is less at risk to biases than other methods needs to be demonstrated with at least a simple experiment. 2) Shapely values over other methods. I think the authors need to back up their argument for using Shapely value explanations over other methods by comparing experimentally with other methods such as CaCE or even raw gradients. In addition, I think the paper would benefit a lot by including a significant discussion on the advantages and disadvantages of each of the three ways of transforming the high-dimensional data to low-dimensional latent space which might make them more or less suitable for certain tasks/datasets/applications. Because of these concerns, I am keeping my original rating.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Although the related work is comprehensive, Section 6 could benefit from comparing the perspective taken in the present manuscript to the contributions of prior efforts.",NIPS_2017_337,NIPS_2017,"of the manuscript stem from the restrictive---but acceptable---assumptions made throughout the analysis in order to make it tractable. The most important one is that the analysis considers the impact of data poisoning on the training loss in lieu of the test loss. This simplification is clearly acknowledged in the writing at line 102 and defended in Appendix B. Another related assumption is made at line 121: the parameter space is assumed to be an l2-ball of radius rho.
The paper is well written. Here are some minor comments:
- The appendices are well connected to the main body, this is very much appreciated.
- Figure 2 and 3 are hard to read on paper when printed in black-and-white.
- There is a typo on line 237.
- Although the related work is comprehensive, Section 6 could benefit from comparing the perspective taken in the present manuscript to the contributions of prior efforts.
- The use of the terminology ""certificate"" in some contexts (for instance at line 267) might be misinterpreted, due to its strong meaning in complexity theory.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '4']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- I would assume that the performance is closely related to the number of scenarios used for training, and therefore, it is interesting to examine the performance with different numbers of scenarios (which is fixed as 200 in the paper).",ICLR_2022_497,ICLR_2022,"I have the following questions to which I wish the author could respond in the rebuttal. If I missed something in the paper, I would appreciate it if the authors could point them out.
Main concerns: - In my understanding, the best scenarios are those generated from the true distribution P (over the scenarios), and therefore, the CVAE essentially attempts to approximate the true distribution P. In such a sense, if the true distribution P is independent of the context (which is the case in the experiments in this paper), I do not see the rationale for having the scenarios conditioned on the context, which in theory does not provide any statistical evidence. Therefore, the rationale behind CVAE-SIP is not clear to me. If the goal is not to approximate P but to solve the optimization problem, then having the objective values involved as a predicting goal is reasonable; in this case, having the context involved is justified because they can have an impact on the optimization results. Thus, CVAE-SIPA to me is a valid method. - While reducing the scenarios from 200 to 10 is promising, the quality of optimization has decreased a little bit. On the other hand, in Figure 2, using K-medoids with K=20 can perfectly recover the original value, which suggests that K-medoids is a decent solution and complex learning methods are not necessary for the considered settings. In addition, I am also wondering the performance under the setting that the 200 scenarios (or random scenarios of a certain number from the true distributions) are directly used as the input of CPLEX. In addition, to justify the performance, it is necessary to provide information about robustness as well as to identify the case where simple methods are not satisfactory (such as larger graphs).
Minor concerns: - Given the structure of the proposed CVAE, the generation process takes the input of z and c where z
is derived from w
. This suggests that the proposed method requires us to know a collection of scenarios from the true distribution. If this is the case, it would be better to have a clear problem statement in Sec 3. Based on such understanding, I am wondering about the process of generating scenarios used for getting K representatives - it would be great if codes like Alg 1 was provided. - I would assume that the performance is closely related to the number of scenarios used for training, and therefore, it is interesting to examine the performance with different numbers of scenarios (which is fixed as 200 in the paper). - The structure of the encoder is not clear to me. The notation q_{\phi} is used to denote two different functions q(z w,D) and q ( c , D )
. Does that mean they are the same network? - It would be better to experimentally justify the choice of the dimension of c and z. - It looks to me that the proposed methods are designed for graph-based problems, while two-stage integer programming does not have to be graph problems in general. If this is the case, it would be better to clearly indicate the scope of the considered problem. Before reaching Sec 4.2, I was thinking that the paper could address general settings. - The paper introduces CVAE-SIP and CVAE-SIPA in Sec 5 -- after discussing the training methods, so I am wondering if they follow the same training scheme. In particular, it is not clear to me by saying “append objective values to the representations” at the beginning of Sec 5. - The approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table. It would be better to provide a mathematical characterization.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. generating a quality label does not necessarily mean that the model has the ability to predict it. I am wondering if there is some disturbances are made to the sentence in the training data, will the proposed model generate the correct quality label (showing the quality goes down)?",tsbdcgaCtk,ICLR_2024,"1. generating a quality label does not necessarily mean that the model has the ability to predict it. I am wondering if there is some disturbances are made to the sentence in the training data, will the proposed model generate the correct quality label (showing the quality goes down)?
2. according to fig.1 , the prediction of quality labels is not good at all. The model seems not to be able to discriminate candidates with different qualities.
3. using QE label as the generation labels seems to be an interesting idea. Will you please give some examples of the same source sentence translated with different QE labels? It would be nice to see the effect demonstrated.
4. I am not quite sure how is the quality difference between two translations with 1 point difference in MetricX or Comet score. It will be better to give some examples to show how the translation quality is improved indeed.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', 'X', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation.",NIPS_2022_2373,NIPS_2022,"weakness in He et al., and proposes a more invisible watermarking algorithm, making their method more appealing to the community. 2. Instead of using a heuristic search, the authors elegantly cast the watermark search issue into an optimization problem and provide rigorous proof. 3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation. 4. This work theoretically proves that CATER is resilient to statistical reverse-engineering, which is also verified by their experiments. In addition, they show that CATER can defend against ONION, an effective approach for backdoor removal.
Weakness: 1. The authors assume that all training data are from the API response, but what if the adversary only uses the part of the API response? 2. Figure 5 is hard to comprehend. I would like to see more details about the two baselines presented in Figure 5.
The authors only study CATER for the English-centric datasets. However, as we know, the widespread text generation APIs are for translation, which supports multiple languages. Probably, the authors could extend CATER to other languages in the future.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (""strategic predictions"", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player).",NIPS_2017_143,NIPS_2017,"For me the main issue with this paper is that the relevance of the *specific* problem that they study -- maximizing the ""best response"" payoff (l127) on test data -- remains unclear. I don't see a substantial motivation in terms of a link to settings (real or theoretical) that are relevant:
- In which real scenarios is the objective given by the adverserial prediction accuracy they propose, in contrast to classical prediction accuracy?
- In l32-45 they pretend to give a real example but for me this is too vague. I do see that in some scenarios the loss/objective they consider (high accuracy on majority) kind of makes sense. But I imagine that such losses already have been studied, without necessarily referring to ""strategic"" settings. In particular, how is this related to robust statistics, Huber loss, precision, recall, etc.?
- In l50 they claim that ""pershaps even in most [...] practical scenarios"" predicting accurate on the majority is most important. I contradict: in many areas with safety issues such as robotics and self-driving cars (generally: control), the models are allowed to have small errors, but by no means may have large errors (imagine a self-driving car to significantly overestimate the distance to the next car in 1% of the situations).
Related to this, in my view they fall short of what they claim as their contribution in the introduction and in l79-87:
- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (""strategic predictions"", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player).
- In particular, in the experiments, it doesn't come as a complete surprise that the opponent can be outperformed w.r.t. the multi-agent payoff proposed by the authors, because the opponent simply doesn't aim at maximizing it (e.g. in the experiments he maximizes classical SE and AE).
- Related to this, in the experiments it would be interesting to see the comparison of the classical squared/absolute error on the test set as well (since this is what LSE claims to optimize).
- I agree that ""prediction is not done in isolation"", but I don't see the ""main"" contribution of showing that the ""task of prediction may have strategic aspects"" yet. REMARKS:
What's ""true"" payoff in Table 1? I would have expected to see the test set payoff in that column. Or is it the population (complete sample) empirical payoff?
Have you looked into the work by Vapnik about teaching a learner with side information? This looks a bit similar as having your discrapency p alongside x,y.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
2. Appendix A.2 does not illustrate the state space representation of the environment clearly.,ICLR_2021_863,ICLR_2021,"Weakness 1. The presentation of the paper should be improved. Right now all the model details are placed in the appendix. This can cause confusion for readers reading the main text. 2. The necessity of using techniques includes Distributional RL and Deep Sets should be explained more thoroughly. From this paper, the illustration of Distributional RL lacks clarity. 3. The details of state representation are not explained clear. For an end-to-end method like DRL, it is crucial for state representation for training a good agent, as for network architecture. 4. The experiments are not comprehensive for validating that this algorithm works well in a wide range of scenarios. The efficiency, especially the time efficiency of the proposed algorithm, is not shown. Moreover, other DRL benchmarks, e.g., TD3 and DQN, should also be compared with. 5. There are typos and grammar errors.
Detailed Comments 1. Section 3.1, first paragraph, quotation mark error for ""importance"". 2. Appendix A.2 does not illustrate the state space representation of the environment clearly. 3. The authors should state clearly as to why the complete state history is enough to reduce POMDP for the no-CSI case. 4. Section 3.2.1: The first expression for J ( θ )
is incorrect, which should be Q ( s t 0 , π θ ( s t 0 ) )
. 5. The paper did not explain Figure 2 clearly. In particular, what does the curve with the label ""Expected"" in Fig. 2(a) stand for? Not to mention there are multiple misleading curves in Fig. 2(b)&(c). The benefit of introducing distributional RL is not clearly explained. 6. In Table 1, only 4 classes of users are considered in the experiment sections, which might not be in accordance with practical situations, where there can be more classes of users in the real system and more user numbers. 7. In the experiment sections, the paper only showed the Satisfaction Probability of the proposed method is larger than conventional methods. The algorithm complexity, especially the time complexity of the proposed method in an ultra multi-user scenario, is not shown. 8. There is a large literature on wireless scheduling with latency guarantees from the networking community, e.g., Sigcomm, INFOCOM, Sigmetrics. Representative results there should also be discussed and compared with.
====== post rebuttal: My concern regarding the experiments remains. I will keep my score unchanged.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The authors approach is only applicable for problems that are small or medium scale. Truly large problems will overwhelm current LP-solvers.,NIPS_2018_430,NIPS_2018,"- The authors approach is only applicable for problems that are small or medium scale. Truly large problems will overwhelm current LP-solvers. - The authors only applied their method on peculiar types of machine learning applications that were already used for testing boolean classifier generation. It is unclear whether the method could lead to progress in the direction of cleaner machine learning methods for standard machine learning tasks (e.g. MNIST). Questions: - How where the time limits in the inner and outer problem chosen? Did larger timeouts lead to better solutions? - It would be helpful to have an algorithmic writeup of the solution of the pricing problem. - SVM gave often good results on the datasets. Did you use a standard SVM that produced a linear classifier or a Kernel method? If the former is true, this would mean that the machine learning tasks where rather easy and it would be necessary to see results on more complicated problems where no good linear separator exists. Conclusion: I very much like the paper and strongly recommend its publication. The authors propose a theoretically well grounded approach to supervised classifier learning. While the number of problems that one can attack with the method is not so large, the theoretical (problem formulation) and practical (Dantzig-Wolfe solver) contribution can possibly serve as a starting point for further progress in this area of machine learning.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions: [A. Khaled and P. Richt´arik]. Better theory for sgd in the nonconvex world. TMLR 2023. [R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.",nE1l0vpQDP,ICLR_2025,"- Given the existing literature on the implicit bias of optimization methods, the primary concern is the significance of the results presented. For instance, the classic result by [Z. Ji and M. Telgarsky] demonstrates a convergence rate $\log\log n/\log n$ of GD to the L2-margin solution, which is faster than the rate shown in this submission. Moreover, [C. Zhang, D. Zou, and Y. Cao] have shown much faster rates for Adam converging to the L-infinity margin solution. This submission also lacks citations to these papers and other relevant works:
[Z. Ji and M. Telgarsky] The implicit bias of gradient descent on nonseparable data, COLT 2019.
[C. Zhang, D. Zou, and Y. Cao] The Implicit Bias of Adam on Separable Data. 2024.
[S. Xie and Z. Li] Implicit Bias of AdamW: l_\infty-Norm Constrained Optimization. ICML 2024
[M. Nacson, N. Srebro, and D. Soudry] Stochastic gradient descent on separable data: Exact convergence with a fixed learning rate. AISTATS 2019.
- Since AdaGrad-Norm has the same implicit bias as GD, the advantages of using AdaGrad-Norm over GD are unclear.
- The bounded noise assumption, while common, is somewhat restrictive in stochastic optimization literature. There have been several efforts to extend these noise conditions:
[A. Khaled and P. Richt´arik]. Better theory for sgd in the nonconvex world. TMLR 2023.
[R. Gower, O. Sebbouh, and N. Loizou] Sgd for structured nonconvex functions: Learning rates, minibatching and interpolation. AISTATS 2021.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
- The overall motivation of using characteristic function regularization is not clear.,BSGQHpGI1Q,ICLR_2025,"- The overall motivation of using characteristic function regularization is not clear.
- The abstract states “improves performance … by preserving essential distributional properties…” -> How does the preservation of such properties aid in generalization?
- The abstract states that the method is meant to be used in conjunction with existing regularization methods. Were the results presented results utilizing multiple forms of regularization (such as $L_2 + \psi_2$) or were the only singular forms of regularization?
- In the conclusion, the author state the follwoing: “integrating these techniques can offer a probability theory based perspective on model architecture construction which allows assembling relevant regularization mechanisms.” —> I do not see how this can be done after reading the work. can you give a concrete example of how the results presented in this work may give any insight into model architecture construction?
## Overall
While I found the work interesting and captivating to read, after finishing the manuscript I am left wondering what possible benefit the regularization provides over existing methods. The results are somewhat ambiguous and I find they do not demonstrate why or when a clear benefit can be achieved by applying the given regularization method. If the authors could provide some insight as to when and why the method would be successful, I think it would go a long way in demonstrating the real-world usefulness of characteristic function regularization. Even if this could be demonstrated in a synthetic toy setting, it could provide interesting insights.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The paper appears to be limited to a combination of existing techniques: adaptation to an unknown level of corruption (Lykouris et al., 2018); varying variances treated with a weighted version of OFUL (Zhou et al., 2021); variable decision sets (standard in contextual linear bandits). The fact that these results can be combined together is not surprising, and thus the contribution could be considered incremental.",NIPS_2021_2367,NIPS_2021,"1. The paper appears to be limited to a combination of existing techniques: adaptation to an unknown level of corruption (Lykouris et al., 2018); varying variances treated with a weighted version of OFUL (Zhou et al., 2021); variable decision sets (standard in contextual linear bandits). The fact that these results can be combined together is not surprising, and thus the contribution could be considered incremental. 2. The regret bounds seem sub-optimal in the level of corruption (they are of order C^2 when existing bounds seem to be of order C). The authors should discuss more this sub-optimality, is it due to the unknown corruption level? Why should we incur it?
Other comments:
The authors state that if C is known and the variances are fixed then one could directly apply OFUL with a modified variance (to add the corruption). This yields several questions: a. The regret bound would then be of order O((R+C)d\sqrt{T}), right? It would be informative to write it, so that we can compare it with the bound of Thm. 5.1. b. It is then claimed that one of the problems is the varying variances. But this problem was already solved by Weighed OFUL (Thm. 4.2 of Zhou et al 2021) for OFUL without corruption. Isn't it possible to apply the same reasoning with this algorithm? c. For the adaptation to the unknown value of C, I am wondering whether it is not possible to just apply the Corral algorithm (see Cor. 6 of [1]) to (Weighed)-OFUL with an exponential grid of possible values for C (from 1 to T). Wouldn't this imply a bound of order C \sqrt{T} log T?
The assumption that the variance is revealed by the adversary (l. 135) is not clear and should be better motivated. We understand that it was already done by Kirschner and Krause (2018) and Zhou et al (2021) but examples of practical applications would be enjoyable to make the paper more self-contained. Similarly, the assumption of varying decision sets is standard in linear bandits but a few lines to recall why this allows dealing with contexts could be helpful for a reader new to the area.
About the experiments: a.The results seem significantly different when C = 0 and 300. What is the intermediate regime? For what level of corruption does Multi-level OFUL outperform algorithms that do not consider corruption? b. I regret that the algorithm is only compared to baselines that are not designed to deal with corruption and suffer linear regrets. It would be interesting to compare Multi-level OFUL with algorithms for linear bandits with corruptions. This could be done by considering fixed variance and fixed decision sets for instance to apply existing algorithms, so that we can see the actual cost of having a more general algorithm. The algorithm could also be compared with the version of OFUL which knows C and takes into account the corruption.
Minor remarks:
How a \min in (6.3) is obtained using lemma 6.5 is not clear and should be clarified. Same for the \min in (6.8) using lemma 6.6?
How substituting (6.5) and (6.6) into (6.3) gives (6.7) should also be more detailed.
[1] Agarwal et al. Corralling a Band of Bandit Algorithms, 2017.
The authors did not mention the limitations and potential negative societal impact of their work.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"- Multiscale modeling:- The aggregation operation after ""Integration"" needs further clarification. Please provide more details in the main paper, and if you refer to other architectures, acknowledge their structure properly.",8HG2QrtXXB,ICLR_2024,"- Source of Improvement and Ablation Study:
- Given the presence of various complex architectural choices, it's difficult to determine whether the Helmholtz decomposition is the primary source of the observed performance improvement. Notably, the absence of the multi-head mechanism leads to a performance drop (0.1261 -> 0.1344) for the 64x64 Navier-Stokes, which is somewhat comparable to the performance decrease resulting from the ablation of the Helmholtz decomposition (0.1261 -> 0.1412). These results raise questions about the model's overall performance gain compared to the baseline models when the multi-head trick is absent. Additionally, the ablation studies need to be explained more comprehensively with sufficient details, as the current presentation makes it difficult to understand the methodology and outcomes.
- The paper claims that Vortex (Deng et al., 2023) cannot be tested on other datasets, which seems unusual, as they are the same type of task and data that are disconnected from the choice of dynamics modeling itself. It should be further clarified why Vortex cannot be applied to other datasets.
- Interpretability Claim:
- The paper's claim about interpretability is not well-explained. If the interpretability claim is based on the model's prediction of an explicit term of velocity, it needs further comparison and a more comprehensive explanation. Does the Helmholtz decomposition significantly improve interpretability compared to baseline models, such as Vortex (Deng et al., 2023)?
- In Figure 4, it appears that the model predicts incoherent velocity fields around the circle boundary, even with non-zero velocity outside the boundary, while baseline models do not exhibit such artifacts. This weakens the interpretability claim.
- Multiscale modeling:
- The aggregation operation after ""Integration"" needs further clarification. Please provide more details in the main paper, and if you refer to other architectures, acknowledge their structure properly.
- Regarding some missing experimental results with cited baselines, it's crucial to include and report all baseline results to ensure transparency, even if the outcomes are considered inferior.
- Minor issues:
- Ensure proper citation format for baseline models (Authors, Year).
- Make sure that symbols are well-defined with clear reference to their definitions. For example, in Equation (4), the undefined operator $\mathbb{I}_{\vec r\in\mathbb{S}}$ needs clarification. If it's an indicator function, use standard notation with a proper explanation. ""Embed(•)"" should be indicated more explicitly.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2: Based on the paper's description, I think it will be hard to replicate the result. It would be great if the authors can release the code after the acceptance of the paper.",NIPS_2018_768,NIPS_2018,"Weakness] 1: I like the paper's idea and result. However, this paper really REQUIRE the ablation study to justify the effectiveness of different compositions. For example: - In eq2, what is the number of m, and how m affect the results? - In eq3, what is the dimension of w_n? what if the use the Euclidean coordinate instead of Polar coordinate? - In eq3, how the number of Gaussian kernels changes the experiment results. 2: Based on the paper's description, I think it will be hard to replicate the result. It would be great if the authors can release the code after the acceptance of the paper. 3: There are several typos in the paper, need better proof reading.","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
"1. The paper contains severe writing issues such as grammatical errors, abuses of mathematical symbols, unclear sentences, etc.",ICLR_2022_1675,ICLR_2022,"Weakness] 1. The paper contains severe writing issues such as grammatical errors, abuses of mathematical symbols, unclear sentences, etc. 2. The paper needs more literature survey, especially about the existing defense methods using the manifold assumption. 3. The paper does not have enough (either theoretical or experimental) progress to get accepted, compared to previous methods using the manifold assumption.
[Comments] 1. First of all, use some spell/grammar checker (or ask someone else to proofread) to fix basic grammatical errors. 2. Section 3 is very unclear in general. First, I cannot understand the reason why the Section is needed at all. Manifold-based defense against adversarial example is not a new approach and reviewers know well about the manifold assumption in the adversarial machine learning setting. Section 3 does not introduce anything new more than those reviewers’ understanding, and the reasonings are too crude to be called an “analysis”. Second, the writing is not cohesive enough. Each paragraph is saying some topic, however, the connections between the paragraphs are not very clear, making Section 3 more confusing. Even in a single paragraph, the logical reasonings between sentences are sometimes not provided at all. Third, some of those contents are added for no reason. For example, Figure 1 exists for no reason whereas the figure is not referred at all in the paper. The propositions mentioned in Section 3.3 are vaguely written and not used at all. By having these unnecessary parts, the writing looks to be verbose and overstating. 3. In Section 5, the defense method should be written with more formality. Based on the description given in the paper “dx = p(max)-p(secondmax)”, it is very unclear what each term means. Each probability (the authors did not even say that they are probabilities) must correspond to the output from a softmax layer, but which model provides such a softmax layer output, the target classifier, or is there another classifier prepared for it? How are the described transformations used to get the divergence value? What does the detector do with the divergence? (All of these details should be described in Section 5.) Section 6.2 mentions some thresholding strategies, how did the detector work in Section 6.1, though? When thresholding is used, what is the threshold value used and what is the rationale of the choice of the threshold value? There are so many missing details to understand the method. 4. Section 7 looks to be a conclusion for experiments. This should be moved to Section 6 and Section 7 should be an overall conclusion of the paper. 5. The suggested method is neither creative nor novel, compared to the existing methods utilizing the distance from manifolds. As pointed out, the defense based on the manifold assumption is not a new approach. [1][3][4][6](These papers are only a few representative examples. There are many other papers on this type of defense.) Moreover, the idea of using probability divergence is already proposed by previous work [1] and an effective attack for such detection already exists. [2] (Of course, this paper proposes another probability divergence, but there is no support that this method could be significantly better than the previous work.) 6. The experiment should be done more extensively. It looks like that some transformations were brought from the Raff et al. paper [5] which tested the defense against the adversary as strong as possible. Specifically, Raff et al. considered potential improvements of existing attacks to attack their work then tested the defense performance against the improved attack. However, the paper only uses vanilla implementation in the Cleverhans library (or by the original authors). The authors should have shown that the proposed method is robust against a stronger adversary because adversaries who are aware of the method will not use a simple version of the attack. (At least, those adversaries will try using the attack suggested by Raff et al.) [References]
[1] (Meng & Chen) MagNet: a Two-Pronged Defense Against Adversarial Examples
[2] (Carlini & Wagner) MagNet and “Efficient Defenses Against Adversarial Attacks” are Not Robust to Adversarial Examples
[3] (Samangouei et al.) Defense-GAN: Protecting Classifiers Against Adversarial Attacks Using Generative Models
[4] (Jiang et al.) To Trust or Not to Trust a Classifier
[5] (Raff et al.) Barrage of Random Transforms for Adversarially Robust Defense
[6] (Dubey et al.) Defense Against Adversarial Images using Web-Scale Nearest-Neighbor Search","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"3) mentioned above would become even more important. If the figures do not show results for untrained networks then please run the corresponding experiments and add them to the figures and Table 1. Clarify: Random data (Fig 3c). Was the network trained on random data, or do the dotted lines show networks trained on unaltered data, evaluated with random data? Clarify: Random data (Fig 3). Was the non-random data normalized or not (i.e. is the additional “unit-ball” noise small or large compared to the data). Ideally show some examples of the random data in the appendix.",ICLR_2021_1716,ICLR_2021,"Results are on MNIST only. Historically it’s often been the case that strong results on MNIST would not carry over to more complex data. Additionally, at least some core parts of the analysis does not require training networks (but could even be performed e.g. with pre-trained classifiers on ImageNet) - there is thus no severe computational bottleneck, which is often the case when going beyond MNIST.
The “Average stochastic activation diameter” is a quite crude measure and results must thus be taken with a (large) grain of salt. It would be good to perform some control experiments and sanity checks to make sure that the measure behaves as expected, particularly in high-dimensional spaces.
The current paper reports the hashing effect and starts relating it to what’s known in the literature, and has some experiments that try to understand the underlying causes for the hashing effect. However, while some factors are found to have an influence on the strength of the effect, some control experiments are still missing (training on random labels, results on untrained networks, and an analysis of how the results change when starting to leave out more and more of the early layers).
Correctness Overall the methodology, results, and conclusions seem mostly fine (I’m currently not very convinced by the “stochastic activation diameter” and would not read too much into the corresponding results). Additionally some claims are not entirely supported (in fullest generality), based on the results shown, see comments for more on this.
Clarity The main idea is well presented and related literature is nicely cited. However, some of the writing is quite redundant (some parts of the intro appear as literal copies later in the text). Most importantly the writing in some parts of the manuscript seems quite rushed with quite a few typos and some sentences/passages that could be rephrased for more fluent reading.
Improvements (that would make me raise my score) / major issues (that need to be addressed)
Experiments on more complex datasets.
One question that is currently unresolved is: is the hashing effect mostly attributed to early layer activations? Ultimately, a high-accuracy classifier will “lump together” all datapoints of a certain class when looking at the network output only. The question is whether this really happens at the very last layer or already earlier in the network. Similarly, when considering the input to the network (the raw data) the hashing effect holds since each data-point is unique. It is conceivable that the first layer activations only marginally transform the data in which case it would be somewhat trivially expected to see the hashing effect (when considering all activations simultaneously). However that might not explain e.g. the K-NN results. I think it would be very insightful to compute the redundancy ratio layer-wise and/or when leaving out more and more of the early layer activations (i.e. more and more rows of the activation pattern matrix). Additionally it would be great to see how this evolves over time, i.e. is the hashing effect initially mostly localized in early layers and does it gradually shape deeper activations over training? This would also shed some light on the very important issue of how a network that maps each (test-) data-point to a unique pattern generalize well?
Another unresolved question is whether it’s mostly the structure of the input-data or the labels driving the organization of the hashed space? The random data experiments answers this partially. Additionally it would be interesting to see what happens when (i) training with random data, (ii) training with random labels - is the hashing effect still there, does the K-NN classification still work?
Clarify: Does Fig 3c and 4a show results for untrained networks? I.e. is the redundancy ratio near 0 for training, test and random data in an untrained network? I would not be entirely surprised by that (a “reservoir effect”) but if that’s the case that should be commented/discussed in the paper, and improvement 3) mentioned above would become even more important. If the figures do not show results for untrained networks then please run the corresponding experiments and add them to the figures and Table 1.
Clarify: Random data (Fig 3c). Was the network trained on random data, or do the dotted lines show networks trained on unaltered data, evaluated with random data?
Clarify: Random data (Fig 3). Was the non-random data normalized or not (i.e. is the additional “unit-ball” noise small or large compared to the data). Ideally show some examples of the random data in the appendix.
P3: “It is worth noting that the volume of boundaries between linear regions is zero” - is this still true for non-ReLU nonlinearities (e.g. sigmoids)? If not what are the consequences (can you still easily make the claims on P1: “This linear region partition can be extended to the neural networks containing smooth activations”)? Otherwise please rephrase the claims to refer to ReLU networks only.
I disagree that model capacity is well measured by layer width. Please use the term ‘model-size’ instead of ‘model-capacity’ throughout the text. Model capacity is a more complex concept that is influenced by regularizers and other architectural properties (also note that the term capacity has e.g. a well-defined meaning in information theory, and when applied to neural networks it does not simply correspond to layer-width).
Sec 5.4: I disagree that regularization “has very little impact” (as mentioned in the abstract and intro). Looking at the redundancy ratio for weight decay (unfortunately only shown in the appendix) one can clearly see a significant and systematic impact of the regularizer towards higher redundancy ratios (as theoretically expected) for some networks (I guess the impact is stronger for larger networks, unfortunately Fig 8 in the appendix does not allow to precisely answer which networks are which).
Minor comments A) Formally define what “well-trained” means. The term is used quite often and it is unclear whether it simply means converged, or whether it refers to the trained classifier having to have a certain performance.
B) There is quite an extensive body of literature (mainly 90s and early 2000s) on “reservoir effects” in randomly initialized, untrained networks (e.g. echo state networks and liquid state machines, however the latter use recurrent random nets). Perhaps it’s worth checking that literature for similar results.
C) Remark 1: is really only the training distribution meant, i.e. without the test data, or is it the unaltered data generating distribution (i.e. without unit-ball noise)?
D) Is the red histogram in Fig 3a and 3b the same (i.e. does Fig 3b use the network trained with 500 epochs)?
E) P2 - Sufficiently-expressive regime: “This regime involves almost all common scenarios in the current practice of deep learning”. This is a bit of a strong claim which is not fully supported by the experiments - please tone it down a bit. It is for instance unclear whether the effect holds for non-classification tasks, and variational methods with strong entropy-based regularizers, or Dropout, ...
F) P2- The Rosenblatt 1961 citation is not entirely accurate, MLP today typically only loosely refers to the original Perceptron (stacked into multiple-layers), most notably the latter is not trained via gradient backpropagation. I think it’s fine to use the term MLP without citation, or point out that MLP refers to a multi-layer feedforward network (trained via backprop).
G) First paragraph in Sec. 4 is very redundant with the first two bullet points on P2 (parts of the text are literally copied). This is not a good writing style.
H) P4 - first bullet point: “Generally, a larger redundancy ratio corresponds a worse encoding property.”. This is a quite hand-wavy statement - “worse” with respect to what? One could argue that for instance for good generalization high redundancy could be good.
I) Fig 3: “10 epochs (red) and 500 epochs (blue),” does not match the figure legend where red and blue are swapped.
J) Fig 3: Panel b says “Rondom” data.
K) Should the x-axis in Fig 3c be 10^x where x is what’s currently shown on the axis? (Similar to how 4a is labelled?)
L) Some typos P2: It is worths noting P2: By contrast, our the partition in activation hash phase chart characerizes goodnessof-hash. P3: For the brevity P3: activation statue","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- Search models comparison 5.1: what does 100 steps here mean? Is it 100 sampled strategies?,NIPS_2018_947,NIPS_2018,"weakness of the paper, in its current version, is the experimental results. This is not to say that the proposed method is not promising - it definitely is. However, I have some questions that I hope the authors can address. - Time limit of 10 seconds: I am quite intrigued as to the particular choice of time limit, which seems really small. In comparison, when I look at the SMT Competition of 2017, specifically the QF_NIA division (http://smtcomp.sourceforge.net/2017/results-QF_NIA.shtml?v=1500632282), I find that all 5 solvers listed require 300-700 seconds. The same can be said about QF_BF and QF_NRA (links to results here http://smtcomp.sourceforge.net/2017/results-toc.shtml). While the learned model definitely improves over Z3 under the time limit of 10 seconds, the discrepancy with the competition results on similar formula types is intriguing. Can you please clarify? I should note that while researching this point, I found that the SMT Competition of 2018 will have a ""10 Second wonder"" category (http://smtcomp.sourceforge.net/2018/rules18.pdf). - Pruning via equivalence classes: I could not understand what is the partial ""current cost"" you mention here. Thanks for clarifying. - Figure 3: please annotate the axes!! - Bilinear model: is the label y_i in {-1,+1}? - Dataset statistics: please provide statistics for each of the datasets: number of formulas, sizes of the formulas, etc. - Search models comparison 5.1: what does 100 steps here mean? Is it 100 sampled strategies? - Missing references: the references below are relevant to your topic, especially [a]. Please discuss connections with [a], which uses supervised learning in QBF solving, where QBF generalizes SMT, in my understanding. [a] Samulowitz, Horst, and Roland Memisevic. ""Learning to solve QBF."" AAAI. Vol. 7. 2007. [b] Khalil, Elias Boutros, et al. ""Learning to Branch in Mixed Integer Programming."" AAAI. 2016. Minor typos: - Line 283: looses -> loses","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* The use of energy models for image generation is much more unexplored compared to GANs and VAEs and so exploring it further is great. However, note that the motivation and goals of the model -- to achieve compositional generation through logical combination of concepts learned through data subsets, is similar to a prior VAE paper. See further details in the related work review part.",NIPS_2020_556,NIPS_2020,"* The visual quality/fidelity of the generated images is quite low. Making sure that the visual fidelity on common metrics such as FID matches or is at least close enough to GAN models will be useful to validate that the approach supports high fidelity (as otherwise it may be the case that it achieves compositionality at the expense of lower potential for fine details or high fidelity, as is the case in e.g. VAEs). Given that there have been many works that explore combinations of properties for CelebA images with GANs, showing that the proposed approach can compete with them is especially important. * It is unclear to me if MCMC is efficient in terms of training and convergence. Showing learning plots as well compared to other types of generative models will be useful. * The use of energy models for image generation is much more unexplored compared to GANs and VAEs and so exploring it further is great. However, note that the motivation and goals of the model -- to achieve compositional generation through logical combination of concepts learned through data subsets, is similar to a prior VAE paper. See further details in the related work review part. * Given the visual samples in the paper, it looks as if it might be the case that the model has limited variability in generated images: the face images in figure 3 show that both in the second and 4th rows the model tends to generate images that feature unspecified but correlated properties, such as the blonde hair or the very similar bottom three faces. That’s also the case in figure 5 rows 2-4. Consequently, it gives the sense that the model or sampling may not allow for large variation in the generated images, but rather tend to take typical likely examples, as happened in the earlier GAN models. A quantitative comparison of the variance in the images compared to other types of generative models will be useful to either refute or validate this.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1) While the authors' methods allow for learning a state-action-dependent weighting of the shaping rewards, it seemed to me possible that in all of the experiments presented, learning a *uniform* state-action-independent weighting would have sufficed. Moreover, since learning a state-action-independent weighting is much simpler (i.e. it is a single scalar), it may even outperform the authors' methods for the current experiments. Based on this, I would like to suggest the following: 1a) Could the authors provide visualizations of the state-action variation of their learnt weightings? They plot the average weight in some cases (Fig 1 and",NIPS_2020_1335,NIPS_2020,"Given how strong the first four sections (five pages) of the paper were, I was relatively disappointed in the experiments, which were somewhat light. Specifically: 1) While the authors' methods allow for learning a state-action-dependent weighting of the shaping rewards, it seemed to me possible that in all of the experiments presented, learning a *uniform* state-action-independent weighting would have sufficed. Moreover, since learning a state-action-independent weighting is much simpler (i.e. it is a single scalar), it may even outperform the authors' methods for the current experiments. Based on this, I would like to suggest the following: 1a) Could the authors provide visualizations of the state-action variation of their learnt weightings? They plot the average weight in some cases (Fig 1 and 3), but given Cartpole has such a small state-action space, it should be possible to visualize the variation. The specific question here is: do the weights vary much at all in these cases? 1b) Could the authors include a baseline of learnt state-action-*independent* weights? In other words, this model has a single parameter, replacing z_phi(s,a) with a single scalar z. This should be pretty easy to implement. The authors could take any (or all) of their existing gradient approximators and simply average them across all (s,a) in a batch to get the gradient w.r.t. z. 1c) Could the authors include an additional experiment that specifically benefits from learning state-action-*dependent* (so non-uniform) weights? Here is a simple example for Cartpole: the shaping reward f(s,a) is helpful for half the state space and unhelpful for the other half. The ""halves"" could be whether the pole orientation is in the left or right half. The helpful reward could be that from Section 5.1 while the unhelpful reward could be that from the first adaptability test in Section 5.3. 2) To me, the true power of the author's approach is not in learning to ignore bad rewards (just turn them off!) but to intelligently incorporate sort-of-useful-but-not-perfect rewards. This way a researcher can quickly hand design an ok shaping reward but then let the authors' method transform it into a good one. Thus, I was surprised the experiments focussed primarily on ignoring obviously bad rewards and upweighting obviously good rewards. In particular, the MuJoCo experiments would be more compelling if they included more than just a single unhelpful shaping reward. I think the authors could really demonstrate the usefulness of their method there by doing the following: hand design a roughly ok shaping reward for each task. For example, the torso velocity or head height off the ground for Humanoid-v2. Then apply the authors' method and show that it outperforms naive use of this shaping reward. 3) Although the authors discussed learning a shaping reward *from scratch* in the related work section, I was surprised that they did not included this as a baseline. One would like to see that their method, when provided with a decent shaping reward to start, can learn faster by leveraging this hand-crafted knowledge. Fortunately, it seems to me again very easy to implement a baseline like this within the author's framework: simply set f(s,a)=1 and use the authors' methods (perhaps also initializing z_phi(s,a)=0).","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
"- The improvement over previous methods is small, about 0.2%-1%. Also the results in Table 1 and Fig.5 don't report the mean and standard deviation, and whether the difference is statistically significant is hard to know. I will suggest to repeat the experiments and conduct statistical significance analysis on the numbers. Thus, due to the limited novelty and marginal improvement, I suggest to reject the paper.",NIPS_2018_985,NIPS_2018,"Weakness: - One drawback is that the idea of dropping a spatial region in training is not new. Cutout [22] and [a] have been explored this direction. The difference towards previous dropout variants is marginal. [a] CVPR'17. A-Fast-RCNN: Hard Positive Generation via Adversary for Object Detection. - The improvement over previous methods is small, about 0.2%-1%. Also the results in Table 1 and Fig.5 don't report the mean and standard deviation, and whether the difference is statistically significant is hard to know. I will suggest to repeat the experiments and conduct statistical significance analysis on the numbers. Thus, due to the limited novelty and marginal improvement, I suggest to reject the paper.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"1. evaluation on a single domain The method is evaluated only on the tasks from Meta World, a robotic manipulation domain. Hence, it is difficult to judge whether the results will generalize to other domains. I strongly recommend running experiments on a different benchmark such as Atari which is commonly used in the literature. This would also verify whether the method works with discrete action spaces and high-dimensional observations.",ICLR_2023_624,ICLR_2023,"1. evaluation on a single domain
The method is evaluated only on the tasks from Meta World, a robotic manipulation domain. Hence, it is difficult to judge whether the results will generalize to other domains. I strongly recommend running experiments on a different benchmark such as Atari which is commonly used in the literature. This would also verify whether the method works with discrete action spaces and high-dimensional observations.
2. evaluation on a setting created by the authors, no well-established external benchmark
The authors seem to create their own train and test splits in Meta World. This seems strange since Meta World recommends a particular train and test split (e.g. MT10 or MT50) in order to ensure fair comparison across different papers. I strongly suggest running experiments on a pre-established setting so that your results can easily be compared with prior work (without having to re-implement or re-run them). You don't need to get SOTA results, just show how it compares with reasonable baselines like the ones you already include. Otherwise, there is a big question mark around why you created your own ""benchmark"" when a very similar one exists already and whether this was somehow carefully designed to make your approach look better.
3. limited number of baselines
While you do have some transformer-based baselines I believe the method could greatly benefit from additional ones like BC, transformer-BC, and other offline RL methods like CQL or IQL. Such comparisons could help shed more light into whether the transformer architecture is crucial, the hypernetwork initialization, the adaptation layers, or the training objective.
4. more analysis is needed
It isn't clear how the methods compare with the given expert demonstrations on the new tasks. Do they learn to imitate the policy or do they learn a better policy than the given demonstration? I suggest comparing with the performance of the demonstration or policy from which the demonstration was collected.
If the environment is deterministic and the agent gets to see expert demonstrations, isn't the problem of learning to imitate it quite easy? What happens if there is more stochasticity in the environments or the given demonstration isn't optimal?
When finetuning transformers, it is often the case that they forget the tasks they were trained on. It would be valuable to show the performance of your different methods on the tasks they were trained on after being finetuned on the downstream tasks. Are some of them better than the others at preserving previously learned skills?
5. missing some important details
The paper seems to be missing some important details regarding the experimental setup. For example, it wasn't clear to me how the learning from observations setting works. At some point you mention that you condition on the expert observations while collecting online data. Does this assume the ability to reset the environment in any state / observation? If so, this is a big assumption that should be more clearly emphasized and discussed. how exactly are you using the expert observations in combination with online learning?
There are also some missing details regarding the expertise of the demonstrations at test time. Are these demonstrations coming from an an expert or how good are they? Minor
sometimes you refer to generalization to new tasks. however, you finetune your models, so i believe a better term would be transfer or adaptation to new tasks.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
". First of all, the experimental results are quite interesting, especially that the algorithm outperforms DQN on Atari. The results on the synthetic experiment are also interesting. I have three main concerns about the paper.",NIPS_2016_450,NIPS_2016,". First of all, the experimental results are quite interesting, especially that the algorithm outperforms DQN on Atari. The results on the synthetic experiment are also interesting. I have three main concerns about the paper. 1. There is significant difficulty in reconstructing what is precisely going on. For example, in Figure 1, what exactly is a head? How many layers would it have? What is the ""Frame""? I wish the paper would spend a lot more space explaining how exactly bootstrapped DQN operates (Appendix B cleared up a lot of my queries and I suggest this be moved into the main body). 2. The general approach involves partitioning (with some duplication) the samples between the heads with the idea that some heads will be optimistic and encouraging exploration. I think that's an interesting idea, but the setting where it is used is complicated. It would be useful if this was reduced to (say) a bandit setting without the neural network. The resulting algorithm will partition the data for each arm into K (possibly overlapping) sub-samples and use the empirical estimate from each partition at random in each step. This seems like it could be interesting, but I am worried that the partitioning will mean that a lot of data is essentially discarded when it comes to eliminating arms. Any thoughts on how much data efficiency is lost in simple settings? Can you prove regret guarantees in this setting? 3. The paper does an OK job at describing the experimental setup, but still it is complicated with a lot of engineering going on in the background. This presents two issues. First, it would take months to re-produce these experiments (besides the hardware requirements). Second, with such complicated algorithms it's hard to know what exactly is leading to the improvement. For this reason I find this kind of paper a little unscientific, but maybe this is how things have to be. I wonder, do the authors plan to release their code? Overall I think this is an interesting idea, but the authors have not convinced me that this is a principled approach. The experimental results do look promising, however, and I'm sure there would be interest in this paper at NIPS. I wish the paper was more concrete, and also that code/data/network initialisation can be released. For me it is borderline. Minor comments: * L156-166: I can barely understand this paragraph, although I think I know what you want to say. First of all, there /are/ bandit algorithms that plan to explore. Notably the Gittins strategy, which treats the evolution of the posterior for each arm as a Markov chain. Besides this, the figure is hard to understand. ""Dashed lines indicate that the agent can plan ahead..."" is too vague to be understood concretely. * L176: What is $x$? * L37: Might want to mention that these algorithms follow the sampled policy for awhile. * L81: Please give more details. The state-space is finite? Continuous? What about the actions? In what space does theta lie? I can guess the answers to all these questions, but why not be precise? * Can you say something about the computation required to implement the experiments? How long did the experiments take and on what kind of hardware? * Just before Appendix D.2. ""For training we used an epsilon-greedy ..."" What does this mean exactly? You have epsilon-greedy exploration on top of the proposed strategy?","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
"- Although the method is presented nicely and the experiments are rather good and complete, a bit of analysis on what the model does, which can be extremely interesting, is missing (check the feedback/suggestions).",NIPS_2020_1817,NIPS_2020,"There are a few points that are not clear from the paper, which I list below: - As far as I understood in the clustered attention (not the improved one), the value of the i-th query becomes the value of the centroid of the cluster that the query belongs to. So after one round of applying the clustered attention, we have a set C distinct values in N nodes. I wonder what is the implication of this for the next round of the clustered attention, because there is no way to have two nodes that were in the same cluster in the previous round to be in different clusters in the next round (as their values will be the same after round 1) and the only change in the clustering that makes sense is merging clusters (which is not the case as apparently the number of clusters stays the same). Isn’t this too restrictive? What if the initial clustering is not good, then the model has no chance to recover? If the number of clusters stays the same, does the clustering in the layer after layer 1 does anything different than the clustering in the layer 1 (if not they're removable)? - It’s a bit unclear if LSH-X is the Reformer, or a simpler version of the reformer (LSH Transformer). The authors mentioned that the Reformer can’t be used in a setup with heterogeneous queries and keys. First of all, I think it shouldn't be that hard to modify Reformer to support this case. Besides, authors don’t have any task in that setup to see how well the clustered attention does when the clustered queries are not the projections of the inputs that the keys are projected from. - The experiments that are done in the setup that the model has to deal with long sequences is limited to a single modality. Would be nice to have the model evaluated on large inputs in vision/text/algorithmic tasks as well. - Although the method is presented nicely and the experiments are rather good and complete, a bit of analysis on what the model does, which can be extremely interesting, is missing (check the feedback/suggestions). - The authors only consider vanilla transformer and (I think an incomplete version of) Reformer, while there are obvious baselines, e.g. Longformer, sparse transformer, or even Local attention (check the feedback/suggestions).","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date?",ARR_2022_209_review,ARR_2022,"1. The task setup is not described clearly. For example, which notes in the EHR (only the current admission or all previous admissions) do you use as input and how far away are the outcomes from the last note date?
2. There isn't one clear aggregation strategy that gives consistent performance gains across all tasks. So it is hard for someone to implement this approach in practice.
1. Experimental setup details: Can you explain how you pick which notes from the patient's EHR do you use an input and how far away are the outcomes from the last note date? Also, how do you select the patient population for the experiments? Do you use all patients and their admissions for prediction? Is the test set temporally split or split according to different patients?
2. Is precision more important or recall? You seem to consider precision more important in order to not raise false alarms. But isn't recall also important since you would otherwise miss out on reporting at-risk patients?
3. You cannot refer to appendix figures in the main paper (line 497). You should either move the whole analysis to appendix or move up the figures.
4. How do you think your approach would compare to/work in line with other inputs such as structured information? AUCROC seems pretty high in other models in literature.
5. Consider explaining the tasks and performance metrics when you call them out in the abstract in a little more detail. It's a little confusing now since you mention mortality prediction and say precision@topK, which isn't a regular binary classification metric.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The proposed algorithm DMLCBO is based on double momentum technique. In previous works, e.g., SUSTAIN[1] and MRBO[2], double momentum technique improves the convergence rate to $\mathcal{\widetilde O}(\epsilon^{-3})$ while proposed algorithm only achieves the $\mathcal{\widetilde O}(\epsilon^{-4})$. The authors are encouraged to discuss the reason why DMLCBO does not achieve it and the theoretical technique difference between DMLCBO and above mentioned works.",K98byXpOpU,ICLR_2024,"1. The proposed algorithm DMLCBO is based on double momentum technique. In previous works, e.g., SUSTAIN[1] and MRBO[2], double momentum technique improves the convergence rate to $\mathcal{\widetilde O}(\epsilon^{-3})$ while proposed algorithm only achieves the $\mathcal{\widetilde O}(\epsilon^{-4})$. The authors are encouraged to discuss the reason why DMLCBO does not achieve it and the theoretical technique difference between DMLCBO and above mentioned works.
2. In the experimental part, the author only shows the results of DMLCBO in early time, it will be more informative to provide results in the later steps.
3. In Table 3, DMLCBO exhibits higher variance compared with other baselines in MNIST datasets, the authors are encouraged to discuss more experimental details about it and explain the behind reason.
[1] A Near-Optimal Algorithm for Stochastic Bilevel Optimization via Double-Momentum
[2] Provably Faster Algorithms for Bilevel Optimization","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The approach section is missing in the main paper. The reviewer did go through the “parallelization descriptions” in the supplementary material but the supplementary should be used more like additional information and not as an extension to the paper as it is. Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In International Conference on Learning Representations, 2021. Update: Please see my comment below. I have increased the score from 3 to 5.",NIPS_2021_386,NIPS_2021,"1. It is unclear if this proposed method will lead to any improvement for hyper-parameter search or NAS kind of works for large scale datasets since even going from CIFAR-10 to CIFAR-100, the model's performance reduced below prior art (if #samples are beyond 1). Hence, it is unlikely that this will help tasks like NAS with ImageNet dataset. 2. There is no actual new algorithmic or research contribution in this paper. The paper uses the methods of [Nguyen et al., 2021] directly. The only contribution seems to be running large-scale experiments of the same methods. However, compared to [Nguyen et al., 2021], it seems that there are some qualitative differences in the obtained images as well (lines 173-175). The authors do not clearly explain what these differences are, or why there are any differences at all (since the approach is identical). The only thing reviewer could understand is that this is due to ZCA preprocessing which does not sound like a major contribution. 3. The approach section is missing in the main paper. The reviewer did go through the “parallelization descriptions” in the supplementary material but the supplementary should be used more like additional information and not as an extension to the paper as it is.
Timothy Nguyen, Zhourong Chen, and Jaehoon Lee. Dataset meta-learning from kernel ridge-regression. In International Conference on Learning Representations, 2021.
Update: Please see my comment below. I have increased the score from 3 to 5.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* The statement in the introduction regarding the biological plausibility of backpropagation may be too weak (""While the backpropagation ..., its biological plausibility remains a subject of debate.""). It is widely accepted that backpropagation is biologically implausible.",oEuTWBfVoe,ICLR_2024,"I think the paper has several weaknesses. Please see the following list and the questions sections.
* The statement in the introduction regarding the biological plausibility of backpropagation may be too weak (""While the backpropagation ..., its biological plausibility remains a subject of debate.""). It is widely accepted that backpropagation is biologically implausible.
* Regarding the following sentence in Section 3 ""We further define a readout ... ,i.e., $\mathbf{m}^t = f(y^t).$"", did you mean to write $\mathbf{m}^t = f(\mathbf{y}^t)$ ($\mathbf{y}^t$ with boldsymbol)?
* On page 4, ""setting $\theta_{110} = 1$ and $\theta_{012} = -1$,"" the second term should be $\theta_{021}$ rather than $\theta_{012}$.
* The initialization of polynomial coefficient parameters is not clear, and it seems they are initialized close to zero according to Figure 2. It would be valuable to explain how they were initialized.
* The paper models synaptic plasticity rules only for feedforward connections. It would be interesting to explore the impact of lateral connections (by adding additional terms in Equation 6). Have you experimented with such a setup?
* In page 7, the authors state that ""In the case of the MLP, we tested various architectures and highlight results for a 3-10-1 neuron topology."" What are the results for other various architectures? Putting them into the paper would also be valuable (as ablation studies).
* The hyperparameters for the experiments are missing? What is the learning rate, what is the optimizer, etc.?
* I do not see that much difference between the experiment presented in Section 4 and the experiment in (Confavreux et al., 2020) (Section 3.1) except the choice of optimization method. In your experimental setup, you also do not model the global reward. Therefore, I think it makes it more similar to the experiment in (Confavreux et al., 2020).
* Comparison to previous work is missing.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The modulator is heuristically designed. It is hard to justify if there is a scalability issue that might need tedious hyperparameter tuning for diverse training data.,ztT70ubhsc,ICLR_2025,"- The professional sketches (Multi-Gen-20M) considered in this work are in binarised versions of HED edges, which is very different from what a real artist would draw (no artist or professional sketcher would produce lines like those in Figure 1). This makes the basic assumptions/conditions of the paper not very rigorous, somewhat deviating from the ambitious objectives, i.e., dealing with pro-sketch and any other complexity levels with a unified model.
- The modulator is heuristically designed. It is hard to justify if there is a scalability issue that might need tedious hyperparameter tuning for diverse training data.
- The effectiveness and applicability of the knob mechanism is questionable.
- From Figure 6, the effect does not seem very pronounced: in the volcano example, the volcano corresponding to the intermediate gamma value appears to match the details of the input sketch better; in Keith's example (the second row from the bottom), the changes in facial details are also not noticeable.
- Besides, the user has to try different knob values until satisfaction (and this may be pretty different for diverse input sketches) since it has no apparent relation to the user's need for the complexity level from the input sketches.
- The impact of fine-grained cues is hard to manage precisely, as they have been injected into the model at early denoising steps, and the effect will last in the following denoising steps.
- The current competitors in experiments are not designed for sketches. It would be great if some sketch-guided image generation works, e.g., [a], could be compared and discussed.
- There is a “second evaluation set” with 100 hand-drawn images created by novice users used for experiments. It would be great to show these sketch images for completion.
[a] Sketch-Guided Text-to-Image Diffusion Models, SIGGRAPH 2023","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Though f_R and f_P can be adapted over time, the experiments performed here did incorporate a great deal of domain knowledge into their structure. A less informed f_R/f_P might require an impractical amount of data to learn.",NIPS_2016_241,NIPS_2016,"/challenges of this approach. For instance... - The paper does not discuss runtime, but I assume that the VIN module adds a *lot* of computational expense. - Though f_R and f_P can be adapted over time, the experiments performed here did incorporate a great deal of domain knowledge into their structure. A less informed f_R/f_P might require an impractical amount of data to learn. - The results are only reported after a bunch of training has occurred, but in RL we are often also interested in how the agent behaves *while* learning. I presume that early in training the model parameters are essentially garbage and the planning component of the network might actually *hurt* more than it helps. This is pure speculation, but I wonder if the CNN is able to perform reasonably well with less data. - I wonder whether more could be said about when this approach is likely to be most effective. The navigation domains all have a similar property where the *dynamics* follow relatively simple, locally comprehensible rules, and the state is only complicated due to the combinatorial number of arrangements of those local dynamics. WebNav is less clear, but then the benefit of this approach is also more modest. In what kinds of problems would this approach be inappropriate to apply? ---Clarity--- I found the paper to be clear and highly readable. I thought it did a good job of motivating the approach and also clearly explaining the work at both a high level and a technical level. I thought the results presented in the main text were sufficient to make the paper's case, and the additional details and results presented in the supplementary materials were a good compliment. This is a small point, but as a reader I personally don't like the supplementary appendix to be an entire long version of the paper; it makes it harder to simply flip to the information I want to look up. I would suggest simply taking the appendices from that document and putting them up on their own. ---Summary of Review--- I think this paper presents a clever, thought-provoking idea that has the potential for practical impact. I think it would be of significant interest to a substantial portion of the NIPS audience and I recommend that it be accepted.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"• In order to apply imitation learning, it is necessary to obtain labeled data by optimally solving various problems. There are no experiments on whether there are any difficulties in obtaining the corresponding data, and how the performance changes depending on the size of the labeled data.",NIPS_2022_532,NIPS_2022,"• It seems that ODA, one of the methods of solving the MOIP problem, has learned the policy to imitate the problem-solving method, but it did not clearly suggest how the presented method improved the performance and computation speed of the solution rather than just using ODA.
• In order to apply imitation learning, it is necessary to obtain labeled data by optimally solving various problems. There are no experiments on whether there are any difficulties in obtaining the corresponding data, and how the performance changes depending on the size of the labeled data.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Since overparameterization can often lead to powerful memorization and good generalization performance, the necessary conditions may have stronger implications if they are connected to generalization bounds. It is not clear in the paper that the constructions of ReLU networks for robust memorization would lead to robust generalization. I know the authors acknowledge this in the conclusion, but I think this is a very serious question.",47hDbAMLbc,ICLR_2024,"- The paper is mainly dedicated to the existence of robust training. No results on optimization or robust generalization are derived. Given that, the scope seems to be quite limited.
- Since overparameterization can often lead to powerful memorization and good generalization performance, the necessary conditions may have stronger implications if they are connected to generalization bounds. It is not clear in the paper that the constructions of ReLU networks for robust memorization would lead to robust generalization. I know the authors acknowledge this in the conclusion, but I think this is a very serious question.
- The main theorems 4.8 and 5.2 only guarantee the existence of optimal robust memorization. These results would be more useful if an optimization or constructive algorithm is given to find the optimal memorization.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"1 While the paper provides valuable insights for contrastive learning in code search tasks, it does not thoroughly explore the implications of their proposed method for other NLP tasks. This somewhat limits the generalizability of the results.",D0gAwtclWk,EMNLP_2023,"1 While the paper provides valuable insights for contrastive learning in code search tasks, it does not thoroughly explore the implications of their proposed method for other NLP tasks. This somewhat limits the generalizability of the results.
2 The paper does not discuss the computational efficiency of the proposed method. As the Soft-InfoNCE method involves additional computations for weight assignment, it would be important to understand the trade-off between improved performance and increased computational cost.
3 While the authors present the results of their experiments, they do not provide an in-depth analysis of these results. More detailed analysis, including a discussion of cases where the proposed method performs exceptionally well or poorly, could have added depth to the paper.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"- The use of the terminology ""certificate"" in some contexts (for instance at line 267) might be misinterpreted, due to its strong meaning in complexity theory.",NIPS_2017_337,NIPS_2017,"of the manuscript stem from the restrictive---but acceptable---assumptions made throughout the analysis in order to make it tractable. The most important one is that the analysis considers the impact of data poisoning on the training loss in lieu of the test loss. This simplification is clearly acknowledged in the writing at line 102 and defended in Appendix B. Another related assumption is made at line 121: the parameter space is assumed to be an l2-ball of radius rho.
The paper is well written. Here are some minor comments:
- The appendices are well connected to the main body, this is very much appreciated.
- Figure 2 and 3 are hard to read on paper when printed in black-and-white.
- There is a typo on line 237.
- Although the related work is comprehensive, Section 6 could benefit from comparing the perspective taken in the present manuscript to the contributions of prior efforts.
- The use of the terminology ""certificate"" in some contexts (for instance at line 267) might be misinterpreted, due to its strong meaning in complexity theory.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods.",NIPS_2020_696,NIPS_2020,"* A big concern for me is that this paper was hard to read. Since it is very applications specific, I am not familiar with a lot of the theory or the inverse problem(s) considered here. As a result, I am unable to appreciate the key aspects of the paper. For example, the introduction directly gets into the details of wave based imaging without sufficient detail or context with more commonly considered inverse problems. This makes it unapproachable for someone not familiar with this exact application. There is quite a bit of detail left in the supplement, but I believe this should be in the main paper for the contribution to be fully appreciated. * A second point about it being so applications specific is that the paper lacks context to existing methods, for e.g. how can FIONets be useful for someone outside of wave-based imaging? * Another issue is that there are no quantitative comparisons in the main paper (but in the supplement), leaving only qualitative comparisons. * There are no comparisons to any other method other than a U-Net (which essentially serves as an ablation of whether or not including the physics based network helps). Considering this is a linear inverse problem, what are other existing solutions to this problem? It is imperative to compare the proposed FIONet to iterative or classical solutions to the problem to place them in context. * Regarding the OOD experiments, this is indeed interesting because the trained network is able to give strong OOD generalization. However, particularly in imaging in the recent few years several papers have shown that untrained NNs (like deep image prior Ulyanov et al., CVPR 2018) can be used to solve inverse problems across a very wide class of images. It maybe good to mention this in the paper and place the current method in context and Ideally, also compare with those class of methods. * I am not very sure how to read or interpret figure 7 describing the diffeomorphisms. * A minor comment, there is already a model called “routing networks” (Rosenbaum et al, ICLR 2018) which are different from those described in the paper. In the interest of mitigating confusion for the reader it maybe better to clarify or re-name the model.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The experiments are limited to toy data. There is a range of problems with real data where barycenters can be used and it would be interesting to show performance of the method in those settings too.,NIPS_2020_83,NIPS_2020,"While I think the paper makes a good contribution, there are some limitation at the present stage: - [Remark 3.1] While it has been done in previous works, I think that a deeper understanding of those cases where modelling the pushforward P in (8) as a composition of perturbation in an RKHS does not introduce an error, would increase the quality of the work. Alternatively, trying to undestand the kind of error that this parametrization introduces would be valuable too. - The analysis does not cover explicitly what happens when the input measures \beta_i are absolutely continuous and one has to rely on samples. How does the sampling part impact the bound? - The experiments are limited to toy data. There is a range of problems with real data where barycenters can be used and it would be interesting to show performance of the method in those settings too.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"5: More experiments on deeper networks (e.g., ResNet-50) and other network structures (e.g., MobileNet) are needed to further strengthen the paper. References: [1] MoBiNet: A Mobile Binary Network for Image Classification, in WACV 2020. [2] Dynamic Channel Pruning: Feature Boosting and Suppression, in ICLR2019. [3] Learning Dynamic Routing for Semantic Segmentation, in CVPR2020.",ICLR_2021_738,ICLR_2021,"---:
1: This paper ensembles some existing compression/NAS approaches to improve the performance of BNNs, which is not significant enough.
The dynamic routing strategy (conditional on input) has been widely explored. For example, the proposed dynamic formulation in this paper has been used in several studies [2, 3].
Varying width and depth has been extensively explored in the quantization literature, especially in AutoML based approaches [Shen et al. 2019, Bulat et al. 2020], to design high capacity quantized networks.
The effectiveness of the group convolution in BNNs was initially studied in [1]. Later works also incorporate the group convolution into the search space in NAS+BNNs methods [e.g., Bulat et al. 2020a] to reduce the complexity.
2: In each layer, the paper introduces a full-precision fully-connected layer to decide which expert to use. However, for deeper networks, such as ResNet-101, it will include ~100 full-precision layers, which can be very expensive especially in BNNs. As a result, it deteriorates the benefits and practicability of the dynamic routing mechanism.
3: The actual speedup, memory usage and energy consumption on edge devices (e.g., CPU/GPU/FPGA) or IoT devices must be reported. Even though the full-precision operations only account for a small amount of computations in statistics, it can have a big influence on the efficiency on platforms like FPGA.
4: This paper proposes to learn the binary gates via gradient-based optimization while exploring the network structure via EfficientNet manner. Then the problem comes. This paper can formulate the <width, depth, groups and layer arrangement> as configuration vectors and optimize them using policy gradients and so on, with the binary gates learning unified in a gradient-based framework. So what is the advantage of the ""semi-automated"" method of EfficientNet over the gradient-based optimization? In addition, how about learning a policy agent via RL to predict the gates? I encourage the authors can add comparsions and discussions with these alternatives.
5: More experiments on deeper networks (e.g., ResNet-50) and other network structures (e.g., MobileNet) are needed to further strengthen the paper. References:
[1] MoBiNet: A Mobile Binary Network for Image Classification, in WACV 2020.
[2] Dynamic Channel Pruning: Feature Boosting and Suppression, in ICLR2019.
[3] Learning Dynamic Routing for Semantic Segmentation, in CVPR2020.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- The experimental results are interesting and promising, clearly demonstrate the significance of varying level of supervision on the detection performance - Table 1.",NIPS_2018_87,NIPS_2018,"for a wide range of supervisory signals such as, video level action labels, single temporal point, one GT bounding box, temporal bounds etc. The method is experimentally evaluated on the UCF-101-24 and DALY action detection datasets. Paper Strengths: - The paper is clear and easy to understand. - The problem formulation is interesting and described with enough details. - The experimental results are interesting and promising, clearly demonstrate the significance of varying level of supervision on the detection performance - Table 1. - On DALY dataset, as expected, the detection performance increases with access to more supervision. - The proposed approach outperforms the SOA [46] by a large margin of 18% (video mAP) on DALY dataset at all levels of supervision. - On UCF-101-24, the proposed approach outperforms the SOA [46] when bounding box annotations are available at any level, i.e., Temp.+1 BB, Temp. + 3 BBs, Fully Spervised (cf. Table 1). - The visuals are helpful, support well the paper, and the qualitative experiments (in supplementary material) are interesting and convincing. Paper Weaknesses: I haven't noticed any major weakness in this paper, however would like to mention that - on UCF-101-24, the proposed method has drop in performance as compared to the SOA [46] when supervision level is ""Temporal + spatial points"". This work addresses one of the major problems associated with action detection approaches based on fully supervised learning, i.e., these methods require dense frame level GT bounding box annotations and their labels, which is impractical for large scale video datasets and also highly expensive. The proposed unified action detection framework provides a way to train a ML model with weak supervision at various levels, contributing significantly to address the aforementioned problem. Thus, I vote for a clear accept.","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.,NIPS_2022_477,NIPS_2022,"1.In experiments, the PRODEN method also uses mixup and consistency training techniques for fair comparisons. What about other competitive baselines? I'd like to see how much the strong CC method could benefit from the representation training technique.
2.It is not clear why the proposed sample selection mechanism helps preserve the label distribution.
3.In App. B.2, a relaxed solution of Sinkhorn-Knopp algorithm is proposed. Why the relaxed problem guarantees to converge?Does Solar always run this relaxed version of Sinkhorn-Knopp?
4.How is gamma in the Sinknhorn-Knopp affect the performance?
5.How does the class distribution estimate for PRODEN in Figure 1?
Societal Impacts: The main negative impact is lower annotation costs may decrease the requirement for annotator employment.
Limitations: The experiments need to be further improved.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- The results/analysis albeit being detailed and comprehensive, only two relatively old and small models are evaluated.",NIPS_2020_232,NIPS_2020,"- The results/analysis albeit being detailed and comprehensive, only two relatively old and small models are evaluated. - Some of the comparison with other related works, is not completely apple-to-apples, for instance comparing fixed point representation for training, while comparing against AdderNet and DeepShift which use at least half floating points for training. Its understandable the fixed point representation has benefits, but it would probably have been more relevant to compare against similar such fixed point training, for instance other works such (not limited to) - https://arxiv.org/abs/1802.00930, https://arxiv.org/abs/1802.04680, https://arxiv.org/abs/1909.02384 - While the FPGA implementation and results with that are quite impressive and is very valuable as a prototype for the proposed method. However, for such HW solution it would help if the authors extend to do a wider comparison. For instance, with a dedicated ASIC-based implementation the quantum of benefits (table.2) would be considerably reduced. Since fp multiplication could still be cheaper since most optimizations would require non-trivial changes to the datapath, which would take away for the benefits of the faster computations","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"4. Given the current stochastic problem in Eq.(1), I believe that the prox-linear subproblem can be reformulated using the conjugate function and becomes the same as the subproblem in Algorithm 1. That is to say, we can simply improve prox-linear algorithms for solving stochastic problem in Eq.(1). This makes the motivation of Algorithm 1 unclear.",NIPS_2020_930,NIPS_2020,"1. The title is misleading and the authors might overclaim their contribution. Indeed, the stochastic problem in Eq.(1) is a special instance of nonconvex-concave minimax problems and equivalent to nonconvex compositional optimization problem in Eq.(2). Solving such problem is easier than the general case consider in [23, 34]; see also (Rafique, Arxiv 1810.02060) and (Thekumparampil, NeurIPS'19). In addition, the KKT points and approximate KKT points are also defined based on such special structure. 2. The literature review is not complete. The authors mainly focus on the algorithms for stochastic compositional optimization instead of stochastic nonconvex-concave minimax optimization. 3. The algorithm is not single-loop in general. To be more specific, Algorithm 1 needs to solve Eq.(9) at each loop. This is also a nonsmooth strongly convex problem in general and the solution does not have the closed form. To this end, what is the advantage of Algorithm 1 over prox-linear algorithms in nonsmooth case? 4. Given the current stochastic problem in Eq.(1), I believe that the prox-linear subproblem can be reformulated using the conjugate function and becomes the same as the subproblem in Algorithm 1. That is to say, we can simply improve prox-linear algorithms for solving stochastic problem in Eq.(1). This makes the motivation of Algorithm 1 unclear. 5. The proof techniques heavily depend on the biased hybrid estimators introduced in [29]. The current paper does not convince me that such extension is nontrivial and has sufficient technical novelty.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Although I acknowledge that KD and LS are not identical, I believe KD can be viewed as a special form of LS. This is particularly true when the teacher network is uniformly distributed and the temperature is set at 1, then LS and KD are equivalent.",j9e3WVc49w,EMNLP_2023,"- The claim is grounded in empirical findings and does not provide a solid mathematical foundation.
- Although I acknowledge that KD and LS are not identical, I believe KD can be viewed as a special form of LS. This is particularly true when the teacher network is uniformly distributed and the temperature is set at 1, then LS and KD are equivalent.
- The authors only compared one of the existing works in this area and did not sufficiently address related works.
Here are some related works for LS and KD:
Lee, Dongkyu, Ka Chun Cheung, and Nevin Zhang. ""Adaptive Label Smoothing with Self-Knowledge in Natural Language Generation."" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.
Zhang, Zhilu, and Mert Sabuncu. ""Self-distillation as instance-specific label smoothing."" Advances in Neural Information Processing Systems 33 (2020): 2184-2195.
Li Yuan, Francis EH Tay, Guilin Li, Tao Wang, and Jiashi Feng. ""Revisit knowledge distillation: a teacher-free framework."" arXiv preprint arXiv:1909.11723, 2019.
Yun, Sukmin, et al. ""Regularizing class-wise predictions via self-knowledge distillation."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
35. No.3. 2021. Competing dynamic-pruning methods are kind of out-of-date. More recent works should be included. Only results on small scale datasets are provided. Results on large scale datasets including ImageNet should be included to further verify the effectiveness of the proposed method.,ICLR_2023_1599,ICLR_2023,"of the proposed method are listed as below:
There are two key components of the method, namely, the attention computation and learn-to-rank module. For the first component, it is a common practice to compute importance using SE blocks. Therefore, the novelty of this component is limited.
Some important SOTAs are missing and some of them as below outperform the proposed method: (1) Ding, Xiaohan, et al. ""Resrep: Lossless cnn pruning via decoupling remembering and forgetting."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. (2) Li, Bailin, et al. ""Eagleeye: Fast sub-net evaluation for efficient neural network pruning."" European conference on computer vision. Springer, Cham, 2020. (3) Ruan, Xiaofeng, et al. ""DPFPS: dynamic and progressive filter pruning for compressing convolutional neural networks from scratch."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 3. 2021.
Competing dynamic-pruning methods are kind of out-of-date. More recent works should be included.
Only results on small scale datasets are provided. Results on large scale datasets including ImageNet should be included to further verify the effectiveness of the proposed method.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
7. FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?,NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly. Here are some examples: (1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is ""trivial"" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)? (2) In line 96, I do not understand the sentence ""our lower hierarchical layers zoom in time"" and the sentence following that.",NIPS_2017_567,NIPS_2017,"Weakness:
1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly.
Here are some examples:
(1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is ""trivial"" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)?
(2) In line 96, I do not understand the sentence ""our lower hierarchical layers zoom in time"" and the sentence following that.
2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN.
3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper.
4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Make the captions more descriptive. It's annoying to have to search through the text for your interpretation of the figures, which is usually on a different page 4. Explain the scramble network better...",NIPS_2019_165,NIPS_2019,"of the approach and experiments or list future direction for readers. The writeup is exceptionally clear and well organized-- full marks! I have only minor feedback to improve clarity: 1. Add a few more sentences explaining the experimental setting for continual learning 2. In Fig 3, explain the correspondence between the learning curves and M-PHATE. Why do you want to want me to look at the learning curves? Does worse performing model always result in structural collapse? What is the accuracy number? For the last task? or average? 3. Make the captions more descriptive. It's annoying to have to search through the text for your interpretation of the figures, which is usually on a different page 4. Explain the scramble network better... 5. Fig 1, Are these the same plots, just colored differently? It would be nice to keep all three on the same scale (the left one seems condensed) M-PHATE results in significantly more interpretable visualization of evolution than previous work. It also preserves neighbors better (Question: why do you think t-SNE works better in two conditions? The difference is very small tho). On continual learning tasks, M-PHATE clearly distinguishes poor performing learning algorithms via a collapse. (See the question about this in 5. Improvement). The generalization vignette shows that the heterogeneity in M-PHATE output correlates with performance. I would really like to recommend a strong accept for this paper, but my major concern is that the vignettes focus on one dataset MNIST and one NN architecture MLP, which makes the experiments feel incomplete. The results and observations made by authors would be much more convincing if they could repeat these experiments for more datasets and NN architectures.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder. While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated. Why is this particular dimension of difficulty interesting?",NIPS_2017_434,NIPS_2017,"---
This paper is very clean, so I mainly have nits to pick and suggestions for material that would be interesting to see. In roughly decreasing order of importance:
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not
ablated. How important is the added complexity? Will one IN do?
2. Section 4.2: To what extent should long term rollouts be predictable? After a certain amount of time it seems MSE becomes meaningless because too many small errors have accumulated. This is a subtle point that could mislead readers who see relatively large MSEs in figure 4, so perhaps a discussion should be added in section 4.2.
3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder.
While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated.
Why is this particular dimension of difficulty interesting?
4. line 232: This hypothesis could be specified a bit more clearly. How do noisy rollouts contribute to lower rollout error?
5. Are the learned object state embeddings interpretable in any way before decoding?
6. It may be beneficial to spend more time discussing model limitations and other dimensions of generalization. Some suggestions:
* The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).
* How many different kinds of physical interaction can be in one simulation?
* How sensitive is the visual encoder to shorter/longer sequence lengths? Does the model deal well with different frame rates?
Preliminary Evaluation ---
Clear accept. The only thing which I feel is really missing is the first point in the weaknesses section, but its lack would not merit rejection.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Although the paper argue that proposed method finds the flat minima, the analysis about flatness is missing. The loss used for training base model is the averaged loss for the noise injected models, and the authors provided convergence analysis on this loss. However, minimizing the averaged loss across the noise injected models does not ensure the flatness of the minima. So, to claim that the minima found by minimizing the loss in Eq (3), the analysis on the losses of the noise-injected models after training is required.",NIPS_2021_121,NIPS_2021,"Weakness] 1. Although the paper argue that proposed method finds the flat minima, the analysis about flatness is missing. The loss used for training base model is the averaged loss for the noise injected models, and the authors provided convergence analysis on this loss. However, minimizing the averaged loss across the noise injected models does not ensure the flatness of the minima. So, to claim that the minima found by minimizing the loss in Eq (3), the analysis on the losses of the noise-injected models after training is required. 2. In Eq (4), the class prototypes before and after injecting noise are utilized for prototype fixing regularization. However, this means that F2M have to compute the prototypes of the base class every time the noise is injected: M+1 times for each update. Considering the fact that there are many classes and many samples for the base classes, this prototype fixing is computationally inefficient. If I miss some details about the prototype fixing, please fix my misunderstanding in rebuttal. 3. Analysis on the sampling times M
and noise bound value b
is missing. These values decide the flat area around the flat minima, and the performance would be affected by theses value. However, there is no analysis on M and b
in the main paper nor the appendix. Moreover, the exact value M
used for the experiments is not reported. 4. Comparison with single session incremental few-shot learning is missing. Like [42] in the main paper, there are some meta-learning based single session incremental FSL methods are being studied. Although this paper targets on multi-session incremental FSL with different setting and different dataset split, it would be more informative to compare the proposed F2M with that kind of methods, considering that the idea of finding flat minima seems valuable for the single session incremental few-shot learning task too.
There is a typo in Table 2 – the miniImageNet task is 5-way, but it is written as 10-way.
Post Rebuttal
Reviewer clarified the confusing parts of the paper, and added useful analysis during rebuttal. Therefore, I raise my score to 6.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text.,ICLR_2023_1765,ICLR_2023,"weakness, which are summarized in the following points:
Important limitations of the quasi-convex architecture are not addressed in the main text. The proposed architecture can only represent non-negative functions, which is a significant weakness for regression problems. However, this is completed elided and could be missed by the casual reader.
The submission is not always rigorous and some of the mathematical developments are unclear. For example, see the development of the feasibility algorithm in Eq. 4 and Eq. 5. Firstly, t ∈ R while y , f ( θ ) ∈ R n
, where n
is the size of the training set, so that the operation y − t − f ( θ )
is not well-defined. Moreover, even if y , f ( θ ) ∈ R
, the inequality ψ t ( θ ) ≤ 0 implies l ( θ ) ≤ t 2 / 2
, rather than ( θ ) ≤ t
. Since, in general, the training problem will be defined for y ∈ R n
, the derivations in the text should handle this general case.
The experiments are fairly weak and do not convince me that the proposed models have sufficient representation power to merit use over kernel methods and other easy-to-train models. The main issue here is the experimental evaluation does not contain a single standard benchmark problem nor does it compare against standard baseline methods. For example, I would really have liked to see regression experiments on several UCI datsets with comparisons against kernel regression, two-layer ReLU networks, etc. Although boring, such experiments establish a baseline capacity for the quasi-concave networks; this is necessary to show they are ""reasonable"". The experiments as given have several notable flaws:
Synthetic dataset: This is a cute synthetic problem, but obviously plays to the strength of the quasi-concave models. I would have preferred to see a synthetic problem for which was noisy with non piece-wise linear relationship.
Contour Detection Dataset: It is standard to report the overall test ODS, instead of reporting it on different subgroups. This allows the reader to make a fair overall comparison between the two methods.
Mass-Damper System Datasets: This is a noiseless linear regression problem in disguise, so it's not surprising that quasi-concave networks perform well.
Change-point Detection: Again, I would really have rather seen some basic benchmarks like MNIST before moving on to novel applications like detecting changes in data distribution.
Minor Comments
Introduction: - The correct reference for SGD is the seminal paper by Robbins and Monro [1]. - The correct reference for backpropagation is Rumelhart et al. [2]
- ""Issue 1: Is non-convex deep neural networks always better?"": ""is"" should be ""are"". - ""While some experiments show that certain local optima are equivalent and yield similar learning performance"" -- this should be supported by a reference. - ""However, the derivation of strong duality in the literature requires the planted model assumption"" --- what do you mean by ""planted model assumption""? The only necessary assumption for these works is that the shallow network is sufficiently wide.
Section 4: - ""In fact, suppose there are m weights, constraining all the weights to be non-negative will result in only 1 / 2 m
representation power."" -- A statement like this only makes sense under some definition of ""representation power"". For example, it is not obvious how non-negativity constraints affect the underlying hypothesis class (aside from forcing it to contain only non-negative functions), which is the natural notion of representation power. - Equation 3: There are several important aspects of this model which should be mentioned explicitly in the text. Firstly, it consists of only one neuron; this is obvious from the notation, but should be stated as well. Secondly, it can only model non-negative functions. This is a strong restriction and should be discussed somewhere. - ""Among these operations, we choose the minimization procedure because it is easy to apply and has a simple gradient."" --- the minimization operator may produce a non-smooth function, which does not admit a gradient everywhere. Nor is it guaranteed to have a subgradient since the negative function only quasi-convex, rather than convex. - ""... too many minimization pooling layers will damage the representation power of the neural network"" --- why? Can the authors expand on this observation?
Section 5: - ""... if we restrict the network output to be smaller than the network labels, i.e., f ( θ ) ≤ y
"" --- note that this observation requires y ≥ 0
, which does not appear to be explicitly mentioned. - What method is being used to solve the convex feasibility problem in Eq. (5)? I cannot find this stated anywhere.
Figure 6: - Panel (b): ""conveyers"" -> ""converges"".
Figure 7: - The text inside the figure and the labels are too small to read without zooming. This text should be roughly the same size as the manuscript text. - ""It could explain that the classification accuracy of QCNN (94.2%) outperforms that of deep networks (92.7%)"" --- Is this test accuracy, or training accuracy? I assume this is the test metric on the hold-out set, but the text should state this clearly. References
[1] Robbins, Herbert, and Sutton Monro. ""A stochastic approximation method."" The annals of mathematical statistics (1951): 400-407.
[2] Rumelhart, David E., Geoffrey E. Hinton, and Ronald J. Williams. ""Learning representations by back-propagating errors."" nature 323.6088 (1986): 533-536.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
2.The motivation is not clear at all. The introduction should be carefully revised to make this paper easy to follow.,ICLR_2023_3948,ICLR_2023,"1.This paper lacks novelty and is only a combination of some existing approaches, such as Qu et al. (2020). Moreover, I find that the equations are similar.
2.The motivation is not clear at all. The introduction should be carefully revised to make this paper easy to follow.
3.I find the experimental analysis is vague, and why the model works better is not clear. No case studies and no detailed ablation analysis.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Although it is intuitive that including multiple local prompts helps, for different categories, the features and their positions are not the same.",NIPS_2022_2813,NIPS_2022,"1. The proposed method is a two-stage optimization strategy, which is a bit difficult to balance the two steps optimization. Could it be end-to-end training? 2. Although it is intuitive that including multiple local prompts helps, for different categories, the features and their positions are not the same.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['sBdHWtl1', 'boda'], 'labels': ['1', '1']}",,
2. The alignment of relabeled reward data with human annotator judgments remains insufficiently validated.,iamWnRpMuQ,ICLR_2025,"The results have a few issues which make evaluating the contribution difficult:
1. The paper lacks a comparison with some existing works, particularly methods involve iterative PPO/DPO method that train a reward model simultaneously and reward ensembles [1].
[1] Coste T, Anwar U, Kirk R, et al. Reward model ensembles help mitigate overoptimization.
2. The alignment of relabeled reward data with human annotator judgments remains insufficiently validated.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.",NIPS_2017_104,NIPS_2017,"---
There aren't any major weaknesses, but there are some additional questions that could be answered and the presentation might be improved a bit.
* More details about the hard-coded demonstration policy should be included. Were different versions of the hard-coded policy tried? How human-like is the hard-coded policy (e.g., how a human would demonstrate for Baxter)? Does the model generalize from any working policy? What about a policy which spends most of its time doing irrelevant or intentionally misleading manipulations? Can a demonstration task be input in a higher level language like the one used throughout the paper (e.g., at line 129)?
* How does this setting relate to question answering or visual question answering?
* How does the model perform on the same train data it's seen already? How much does it overfit?
* How hard is it to find intuitive attention examples as in figure 4?
* The model is somewhat complicated and its presentation in section 4 requires careful reading, perhaps with reference to the supplement. If possible, try to improve this presentation. Replacing some of the natural language description with notation and adding breakout diagrams showing the attention mechanisms might help.
* The related works section would be better understood knowing how the model works, so it should be presented later.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
2. This paper only does experiments on a very limited number of molecules and only provides in-distribution testing for these samples. I think the value of this method would be limited if it needs to train for each molecule individually.,Wo66GEFnXd,ICLR_2025,"1. This paper just simply combines neural networks into the physical sciences problems for predicting TDDFT for molecules. Due to the lack of comparison with other learning based methods and insufficient experiment results, I don’t see the novelty and effectiveness of this method from the learning perspective. Maybe this work is more appropriate for some physical science journals.
2. This paper only does experiments on a very limited number of molecules and only provides in-distribution testing for these samples. I think the value of this method would be limited if it needs to train for each molecule individually.
3. There is no comparison for this method with other state-of-art work but I think using neural networks to predict for molecules is a very popular topic.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
1. Symbols are a little bit complicated and takes a lot of time to understand.,NIPS_2018_461,NIPS_2018,"1. Symbols are a little bit complicated and takes a lot of time to understand. 2. The author should probably focus more on the proposed problem and framework, instead of spending much space on the applications. 3. No conclusion section Generally I think this paper is good, but my main concern is the originality. If this paper appears a couple years ago, I would think that using meta-learning to solve problems is a creative idea. However, for now, there are many works using meta-learning to solve a variety of tasks, such as in active learning and reinforcement learning. Hence, this paper seems not very exciting. Nevertheless, deciding the number of clusters and selecting good clustering algorithms are still useful. Quality: 4 of 5 Clarity: 3 of 5 Originality: 2 of 5 Significance: 4 of 5 Typo: Line 240 & 257: Figure 5 should be Figure 3.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- Figure 3: I don't understand the red line: Where does the test data come from? Do you have a ground truth?,NIPS_2019_1350,NIPS_2019,"of the method. CLARITY: The paper is well organized, partially well written and easy to follow, in other parts with quite some potential for improvement, specifically in the experiments section. Suggestions for more clarity below. SIGNIFICANCE: I consider the work significant, because there might be many settings in which integrated data about the same quantity (or related quantities) may come at different cost. There is no earlier method that allows to take several sources of data into account, and even though it is a fairly straightforward extension of multi-task models and inference on aggregated data, it is relevant. MORE DETAILED COMMENTS: --INTRO & RELATED WORK: * Could you state somewhere early in the introduction that by ""task"" you mean ""output""? * Regarding the 3rd paragraph of the introduction and the related work section: They read unnaturally separated. The paragraph in the introduction reads very technical and it would be great if the authors could put more emphasis there in how their work differs from previous work and introduce just the main concepts (e.g. in what way multi-task learning differs from multiple instance learning). Much of the more technical assessment could go into the related work section (or partially be condensed). --SECTION 2.3: Section 2 was straightforward to follow up to 2.3 (SVI). From there on, it would be helpful if a bit more explanation was available (at the expense of parts of the related work section, for example). More concretely: * l.145ff: $N_d$ is not defined. It would be good to state explicitely that there could be a different number of observations per task. * l.145ff: The notation has confused me when first reading, e.g. $\mathbb{y}$ has been used in l.132 for a data vector with one observation per task, and in l.145 for the collection of all observations. I am aware that the setting (multi-task, multiple supports, different number of observations per task) is inherently complex, but it would help to better guide the reader through this by adding some more explanation and changing notation. Also l.155: do you mean the process f as in l.126 or do you refer to the object introduced in l.147? * l.150ff: How are the inducing inputs Z chosen? Is there any effect of the integration on the choice of inducing inputs? l.170: What is z' here? Is that where the inducing inputs go? * l.166ff: It would be very helpful for the reader to be reminded of the dimensions of the matrices involved. * l.174 Could you explicitly state the computational complexity? * Could you comment on the performance of this approximate inference scheme based on inducing inputs and SVI? --EXPERIMENTS: * synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by ""support data"" and what by ""predicted training count data""? Could you write down the model used here explicitly, e.g. add it to the appendix? * Fertility rates: - It is unclear to me how the training data is aggregated and over which inputs, i.e. what you mean by 5x5. - Now that the likelihood is Gaussian, why not go for exact inference? * Sensor network: - l.283/4 You might want to emphasize here that CI give high accuracy but low time resolution results, e.g. ""...a cheaper method for __accurately__ assessing the mass..."" - Again, given a Gaussian likelihood, why do you use inducing inputs? What is the trade-off (computational and quality) between using the full model and SVI? - l.304ff: What do you mean by ""additional training data""? - Figure 3: I don't understand the red line: Where does the test data come from? Do you have a ground truth? - Now the sensors are co-located. Ideally, you would want to have more low-cost sensors that high-cost (high accuracy) sensors in different locations. Do you have a thought on how you would account for spatial distribution of sensors? --REFERENCES: * please make the style of your references consistent, and start with the last name. Typos etc: ------------- * l.25 types of datasets * l.113 should be $f_{d'}(v')$, i.e. $d'$ instead of $d$ * l.282 ""... but are badly bias"" should be ""is(?) badly biased"" (does the verb refer to measurement or the sensor? Maybe rephrase.) * l.292 biased * Figure 3: biased, higher peaks, 500 with unit. * l.285 consisting of? Or just ""...as observations of integrals"" * l.293 these variables","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. I'm curious to what extent the results are due to being able to capture periodicity, rather than compositionality more generally. The comparison model is one that cannot capture periodic relationships, and in all of the experiments except Experiment 1b the relationships that people were learning involved periodicity. Would adding periodicity to the spectral kernel be enough to allow it to capture all of these results at a similar level to the explicitly compositional model?",NIPS_2016_417,NIPS_2016,"1. Most of the human function learning literature has used tasks in which people never visualize data or functions. This is also the case in naturalistic settings where function learning takes place, where we have to form a continuous mapping between variables from experience. All of the tasks that were used in this paper involved presenting people with data in the form of a scatterplot or functional relationship, and asking them to evaluate lines applied to those axes. This task is more akin to data analysis than the traditional function learning task, and much less naturalistic. This distinction matters because performance in the two tasks is likely to be quite different. In the standard function learning task, it is quite hard to get people to learn periodic functions without other cues to periodicity. Many of the effects in this paper seem to be driven by periodic functions, suggesting that they may not hold if traditional tasks were used. I don't think this is a major problem if it is clearly acknowledged and it is made clear that the goal is to evaluate whether data-analysis systems using compositional functions match human intuitions about data analysis. But it is important if the paper is intended to be primarily about function learning in relation to the psychological literature, which has focused on a very different task. 2. I'm curious to what extent the results are due to being able to capture periodicity, rather than compositionality more generally. The comparison model is one that cannot capture periodic relationships, and in all of the experiments except Experiment 1b the relationships that people were learning involved periodicity. Would adding periodicity to the spectral kernel be enough to allow it to capture all of these results at a similar level to the explicitly compositional model? 3. Some of the details of the models are missing. In particular the grammar over kernels is not explained in any detail, making it hard to understand how this approach is applied in practice. Presumably there are also probabilities associated with the grammar that define a hypothesis space of kernels? How is inference performed?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The paper is not very well-written, possibly hurriedly written, so not easy to read. A lot is left desired in presentation and formatting, especially in figures/tables.",NIPS_2018_537,NIPS_2018,"1. The motivation or the need for this technique is unclear. It would have been great to have some intuition why replacing last layer of ResNets by capsule projection layer is necessary and why should it work. 2. The paper is not very well-written, possibly hurriedly written, so not easy to read. A lot is left desired in presentation and formatting, especially in figures/tables. 3. Even though the technique is novel, the contributions of this paper is not very significant. Also, there is not much attempt in contrasting this technique with traditional classification or manifold learning literature. 4. There are a lot of missing entries in the experimental results table and it is not clear why. Questions for authors: Why is the input feature vector from backbone network needed to be decomposed into the capsule subspace component and also its component perpendicular to the subspace? What shortcomings in the current techniques lead to such a design? What purpose is the component perpendicular to the subspace serving? The authors state that this component appears in the gradient and helps in detecting novel characteristics. However, the gradient (Eq 3) does not only contain the perpendicular component but also another term x^T W_l^{+T} - is not this transformation similar to P_l x (the projection to the subspace). How to interpret this term in the gradient? Moreover, should we interpret the projection onto subspace as a dimensionality reduction technique? If so, how does it compare with standard dimensionality reduction techniques or a simple dimension-reducing matrix transformation? What does ""grouping neurons to form capsules"" mean - any reference or explanation would be useful? Any insights into why orthogonal projection is needed will be helpful. Are there any reason why subspace dimension c was chosen to be in smaller ranges apart from computational aspect/independence assumption? Is it possible that a larger c can lead to better separability? Regarding experiments, it will be good to have baselines like densenet, capsule networks (Dynamic routing between capsules, Sabour et al NIPS 2017 - they have also tried out on CIFAR10). Moreover it will be interesting to see if the capsule projection layer is working well only if the backbone network is a ResNet type network or does it help even when backbone is InceptionNet or VGGNet/AlexNet.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
1. The introduction to orthogonality in Part 2 could be more detailed.,oKn2eMAdfc,ICLR_2024,"1. The introduction to orthogonality in Part 2 could be more detailed.
2. No details on how the capsule blocks are connected to each other.
3. The fourth line of Algorithm 1 does not state why the flatten operation is performed.
4.The presentation of the α-enmax function is not clear.
5. Eq. (4) does not specify why BatchNorm is used for scalars (L2-norm of sj).
6. The proposed method was tested on relatively small datasets, so that the effectiveness of the method was not well evaluated.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"4. Given that prior work already theoretically shows that sample-wise multiple descent can occur in linear regression, the main contribution of the paper appears to be the result that optimal regularization can remove double descent even in certain anisotropic settings. If this is not the case, the paper should do a better job of highlighting the novelty of their result in relation to prior results. I am not too familiar with the particular techniques and tools used in the paper and could not verify the claims but they seem correct.",ICLR_2022_1216,ICLR_2022,"of the paper: Overall the paper is reasonably well-written but the writing can improve in certain aspects. Some comments and questions below. 1. It is not apparent to the reader why the authors choose an asymptotic regime to focus on. My understanding is that the primary reason is easier theoretical tractability. It would help the reader to know why the paper focuses on the asymptotic setting. 2. It is unclear in the write-up if sample-wise descent occurs only in the over-parameterized regime or not. Pointing this explicitly in the place where you list your contributions would help. More broadly, it is important to have a discussion around these regimes in the main body and also a discussion around how they are defined in the asymptotic regime would help. 3. The paper is written in a very technical manner with very little proof intuition provided in the main body. It would benefit from having more intuition on the tools used and the reasons the main theorems hold. 4. Given that prior work already theoretically shows that sample-wise multiple descent can occur in linear regression, the main contribution of the paper appears to be the result that optimal regularization can remove double descent even in certain anisotropic settings. If this is not the case, the paper should do a better job of highlighting the novelty of their result in relation to prior results.
I am not too familiar with the particular techniques and tools used in the paper and could not verify the claims but they seem correct.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
1) The proposed methods - contrastive training objective and contrastive search - are two independent methods that have little inner connection on both the intuition and the algorithm.,NIPS_2022_2315,NIPS_2022,Weakness: 1) The proposed methods - contrastive training objective and contrastive search - are two independent methods that have little inner connection on both the intuition and the algorithm. 2) The justification for isotropic representation and contractive search could be more solid.,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"1) The first concern is the goal of the paper. Indeed, DAS earthquake detectors exists (one of them was cited by the autors, PhaseNet-Das, Zhu et al. 2023, there might be others), and no comparison was made, nor a justification on the benefit of your method against theirs. If the claim is to say that this is a foundation model, and the test on this task is only as a proof of concept, it should be clearer, and then show or justify a future useful application.",7ipjMIHVJt,ICLR_2024,"1) The first concern is the goal of the paper. Indeed, DAS earthquake detectors exists (one of them was cited by the autors, PhaseNet-Das, Zhu et al. 2023, there might be others), and no comparison was made, nor a justification on the benefit of your method against theirs. If the claim is to say that this is a foundation model, and the test on this task is only as a proof of concept, it should be clearer, and then show or justify a future useful application.
2) I think the purpose of a foundation model would be its applicability at a larger scale. Yet, is your method generalizable to other DAS sensors? It is not clear whether it is site and sensor-specific or not; if so it means a new self-training needs to be performed again for any new DAS.
3) The whole idea of this method is that earthquakes are unpredictible. It is clever indeed, but I see 2 major limitations: 1) this foundation model is thus harder to use for other tasks (which could be predictable) 2) in a series of aftershocks (which could maybe be seen as more predictable), how does your measure performs?
4) The comparison with other multi-variate time series are somehow misleading. Indeed, in multi-variate time-series, we suppose that the different time series (or sensors) are not ordered and not equally-spaced: DAS is a very particular type of 'multi-variate time-series'. I don't think it is worth presenting all of these methods (maybe only one), and it should be clearly stated in the paper. Yet, a comparison with image 2D foundation models, or by modifying a video framework from a 2D+t to a 1D+t, would be more relevant.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
4)Why the results of Table 6 is not aligned with Table 1 (MCT-pair)? Also what about the ablation studies of MCT without the adaptive metrics.,ICLR_2021_2846,ICLR_2021,"Weakness: There are some concerns authors should further address: 1)The transductive inference stage is essentially an ensemble of a serial of models. Especially, the proposed data perturbation can be considered as a common data augmentation. What if such an ensemble is applied to the existing transductive methods? And whether the flipping already is adopted in the data augmentation before the inputs fed to the network? 2)During meta-training, only the selected single path is used in one transductive step, what about the performance of optimizing all paths simultaneously? Given during inference all paths are utilized. 3)What about the performance of MCT (pair + instance)? 4)Why the results of Table 6 is not aligned with Table 1 (MCT-pair)? Also what about the ablation studies of MCT without the adaptive metrics. 5)Though this is not necessary, I'm curious about the performance of the SOTA method (e.g. LST) combined with the adaptive metric.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '4']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.",RnYd44LR2v,ICLR_2024,"- Similar analyses are already present in prior works, although on a (sometimes much) smaller scale, and then the results are not particularly surprising. For example, the robustness of CIFAR-10 models on distributions shifts (CIFAR-10.1, CINIC-10, CIFAR-10-C, which are also included in this work) was studied on the initial classifiers in RobustBench (see [Croce et al. (2021)](https://arxiv.org/abs/2010.09670)), showing a similar linear correlation with ID robustness. Moreover, [A, B] have also evaluated the robustness of adversarially trained models to unseen attacks.
- A central aspect of evaluating adversarial robustness is the attacks used to measure it. In the paper, this is described with sufficient details only in the appendix. In particular for the non $\ell_p$-threat models I think it would be important to discuss the strength (e.g. number of iterations) of the attacks used, since these are not widely explored in prior works.
[A] https://arxiv.org/abs/1908.08016
[B] https://arxiv.org/abs/2105.12508","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The 10 sub-tasks are rather simplistic for bAbi. They could solve all the sub-tasks with their final model. More discussions are required here.,NIPS_2016_93,NIPS_2016,"- The claims made in the introduction are far from what has been achieved by the tasks and the models. The authors call this task language learning, but evaluate on question answering. I recommend the authors tone-down the intro and not call this language learning. It is rather a feedback driven QA in the form of a dialog. - With a fixed policy, this setting is a subset of reinforcement learning. Can tasks get more complicated (like what explained in the last paragraph of the paper) so that the policy is not fixed. Then, the authors can compare with a reinforcement learning algorithm baseline. - The details of the forward-prediction model is not well explained. In particular, Figure 2(b) does not really show the schematic representation of the forward prediction model; the figure should be redrawn. It was hard to connect the pieces of the text with the figure as well as the equations. - Overall, the writing quality of the paper should be improved; e.g., the authors spend the same space on explaining basic memory networks and then the forward model. The related work has missing pieces on more reinforcement learning tasks in the literature. - The 10 sub-tasks are rather simplistic for bAbi. They could solve all the sub-tasks with their final model. More discussions are required here. - The error analysis on the movie dataset is missing. In order for other researchers to continue on this task, they need to know what are the cases that such model fails.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The restriction to triplets (or a sliding window of length 3) is quite limiting. Is this a fundamental limitation of the approach or is an extension to longer subsequences (without a sliding window) straightforward?,NIPS_2016_321,NIPS_2016,"======================== Positive aspects: + The paper is well written and has a clear and coherent structure. The discussion of related work is comprehensive. + Non-parametric emission distributions add flexibility to the general HMM framework and reduce bias due to wrong modeling assumptions. Progress in this area should have theoretical and practical impact. + The paper builds upon existing spectral methods for parametric HMMs but introduces novel techniques to extend those approaches to the non-parametric case. + The theoretical bounds (section 5) are interesting, even though most of the results are special cases or straightforward extensions of known results. Negative aspects: - The restriction to triplets (or a sliding window of length 3) is quite limiting. Is this a fundamental limitation of the approach or is an extension to longer subsequences (without a sliding window) straightforward? - Since the paper mentions the possibility to use Chebyshev polynomials to achieve a speed-up, it would have been interesting to see a runtime comparison at test time. - The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful. - The experimental evaluation compares the proposed approach to 4 other HMM baselines. Even though NP-SPEC-HMM outperforms those baselines, the experimental evaluation has only toy character (simple length 6 conditional distributions, only one training/test sequence in case of the real datasets). Minor points ============ * l.22: Only few parametric distributions allow for tractable exact inference in an HMM. * l.183: Much faster approximations than Chebyshev polynomials exist for the evaluation of kernel density estimates, especially in low-dimensional spaces (e.g., based on fast multipole methods). * Figure 1: There is probably a âx 10^3â missing in the plot on the bottom right. Questions ========= * Eq. (3): Why the restriction to an isotropic bandwidth and a product kernel? Especially a diagonal bandwidth matrix could have been helpful. Would the approximation with Chebyshev polynomials still work? * The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM? * Is it computationally feasible to use the proposed model in a more realistic application, e.g., action recognition from motion capture sequences? Conclusion ========== The paper is well written and, from a theoretical perspective, interesting to read. However, the experiments are weak and it remains unclear how practical the proposed model would be in real applications. Iâm tending towards accept, but the authors should comment in their rebuttal on the above points.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
2. The hyper-parameters $b$ (bit-width) and $\alpha$ (stability factor) may introduce significant computational overhead in the pursuit of determining the optimal trade-off between model size and accuracy.,of2rhALq8l,ICLR_2024,"1. A significant weakness of this paper is the lack of clarity in explaining the implementation of the core concept, which involves the use of strictly diagonal matrices and the proposed Gradual Mask (GM). Figure 2 suggests that the GM matrix is element-wise multiplied by the matrix A, but the description implies a different interpretation, where it functions as a learning rate for each element in A. This discrepancy needs further clarification to provide a complete understanding of the method.
2. The hyper-parameters $b$ (bit-width) and $\alpha$ (stability factor) may introduce significant computational overhead in the pursuit of determining the optimal trade-off between model size and accuracy.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* L167: What is a ""sqeuence of episodes"" here? Are practice and evaluation the two types of this kind of sequence? Missing related work (seems very related, but does not negate this work's novelty):",NIPS_2019_1180,NIPS_2019,"--- There are two somewhat minor weakness: presentation and some missing related work. The main points in this paper can be understood with a bit of work, but there are lots of minor missing details and points of confusion. I've listed them roughly in order, with the most important first: * What factors varied in order to compute the error bars in figure 2? Were different random initializations used? Were different splits of the dataset used? How many samples do the error bars include? Do they indicate standard deviation or standard error? * L174: How exactly does the reactive baseline work? * L185: What does ""without agent embeddings"" mean precisely? * L201: More details about this metric are needed. I don't know exactly what is plotted on the y axis without reading the paper. Before looking into the details I'm not even sure whether higher or lower is good without looking into the details. (Does higher mean more information or does lower mean more information?) * Section 3: This would be much clearer if an example were used to illustrate the problem from the beginning of the section. * Will code be released? * L162: Since most experiments share perception between speaker and listener it would be much clearer to introduce this as a shared module and then present section 4.3 as a change to that norm. * L118: To what degree is this actually realized? * L84: It's not information content itself that will suffer, right? * L77: This is unnecessary and a bit distracting. * L144: Define M and N here. * L167: What is a ""sqeuence of episodes"" here? Are practice and evaluation the two types of this kind of sequence? Missing related work (seems very related, but does not negate this work's novelty): * Existing work has tried to model human minds, especially in robotics. It looks like [2] and [3] are good examples. The beginning of the related work in [1] has more references along these lines. This literature seems significantly different from what is proposed in this paper because the goals and settings are different. Only the high level motivation appears to be similar. Still, the literature seems significant enough (on brief inspection) to warrent a section in the related work. I'm not very familiar with this literature, so I'm not confident about how it relates to the current paper. [1]: Chandrasekaran, Arjun et al. âIt Takes Two to Tango: Towards Theory of AI's Mind.â CVPR 2017 [2]: Butterfield, Jesse et al. âModeling Aspects of Theory of Mind with Markov Random Fields.â International Journal of Social Robotics 1 (2009): 41-51. [3]: Warnier, Matthieu et al. âWhen the robot puts itself in your shoes. Managing and exploiting human and robot beliefs.â 2012 IEEE RO-MAN: The 21st IEEE International Symposium on Robot and Human Interactive Communication (2012): 948-954. Suggestions --- * L216: It would be interesting to realize this by having the speaker interact with humans since the listeners are analogous to role humans take in the high level motivation. That would be a great addition to this or future work. Final Justification --- Clarity - This work could significantly improve its presentation and add more detail, but it currently is clear enough to get the main idea. Quality - Despite the missing details, the experiments seem to be measuring the right things and support very clear conclusions. Novelty - Lots of work uses reference games with multiple agents, but I'm not aware of attempts to specifically measure and model other agents' minds. Significance - The work is a useful step toward agents with a theory of mind because it presents interesting research directions that didn't exist before. Overall, this is a pretty good paper and should be accepted. Post-rebuttal Updates --- After reading the reviews and the rebuttal this paper seems like a clear accept. After discussion with R3 I think we all roughly agree. The rebuttal addressed all my concerns except the minor one listed below satisfactorily. There is one piece R3 and I touched on which is still missing. I asked about the relation to meta-learning and there was no response. More importantly, R3 asked about a comparison to a practice-stage only reward, which would show the importance of the meta-learning aspect of the reward. This was also not addressed satisfactorily, so it's still hard to understand the role of practice/evaluation stages in this work. This would be nice to have, but rest of the paper provides a valuable contribution without it. Though it's hard to tell how presentation and related work will ultimately be addressed in the final version, the rebuttal goes in the right direction so I'll increase my score as indicated in the Improvements section of my initial review.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- I don't think the study about different subdomain sizes is an ""ablation"" study since they aren't removing a component of the method.",ICLR_2022_1393,ICLR_2022,"I think that:
The comparison to baselines could be improved.
Some of the claims are not carefully backed up.
The explanation of the relationship to the existing literature could be improved.
More details on the above weaknesses:
Comparison to baselines:
""We did not find good benchmarks to compare our unsupervised, iterative inferencing algorithm against"" I think this is a slightly unfair comment. The unsupervised and iterative inferencing aspects are only positives if they have the claimed benefits, as compared to other ML methods (more accurate and better generalization). There is a lot of recent work addressing the same ML task (as mentioned in the related work section.) This paper contains some comparisons to previous work, but as I detail below, there seem to be some holes.
FCNN is by far the strongest competitor for the Laplace example in the appendix. Why is this left off of the baseline comparison table in the main paper? Further, is there any reason that FCNN couldn't have been used for the other examples?
Why is FNO not applied to the Chip cooling (Temperature) example?
A major point in this paper is improved generalization across PDE conditions. However, I think that's hard to check when only looking at the test errors for each method. In other words, is CoAE-MLSim's error lower than UNet's error because the approach fit the training data better, or is it because it generalized better? Further, in some cases, it's not obvious to me if the test errors are impressive, so maybe it is having a hard time generalizing. It would be helpful to see train vs. test errors, and ideally I like to see train vs. val. vs. test.
For the second main example (vortex decay over time), looking at Figures 8 and 33 (four of the fifty test conditions), CoAE-MLSim has much lower error than the baselines in the extrapolation phase but noticeably higher in the interpolation phase. In some cases, it's hard to tell how close the FNO line is to zero - it could be that CoAE-MLSim even has orders of magnitude more error. Since we can see that there's a big difference between interpolation and extrapolation, it would be helpful to see the test error averaged over the 50 test cases but not averaged over the 50 time steps. When averaged over all 50 time steps for the table on page 9, it could be that CoAE-MLSim looks better than FNO just because of the extrapolation regime. In practice, someone might pick FNO over CoAE-MLSim if they aren't interested in extrapolating in time. Do the results in the table for vortex decay back up the claim that CoAE-MLSim is generalizing over initial conditions better than FNO, or is it just better at extrapolation in time?
Backing up claims:
The abstract says that the method is tested for a variety of cases to demonstrate a list of things, including ""scalability."" The list of ""significant contributions"" also includes ""This enables scaling to arbitrary PDE conditions..."" I might have missed/forgotten something, but I think this wasn't tested?
""Hence, the choice of subdomain size depends on the trade-off between speed and accuracy."" This isn't clear to me from the results. It seems like 32^3 is the fastest and most accurate?
I noticed some other claims that I think are speculations, not backed up with reported experiments. If I didn't miss something, this could be fixed by adding words like ""might.""
""Physics constrained optimization at inference time can be used to improve convergence robustness and fidelity with physics.""
""The decoupling allows for better modeling of long range time dynamics and results in improved stability and generalizability.""
""Each solution variable can be trained using a different autoencoder to improve accuracy.""
""Since, the PDE solutions are dependent and unique to PDE conditions, establishing this explicit dependency in the autoencoder improves robustness.""
""Additionally, the CoAE-MLSim apprach solves the PDE solution in the latent space, and hence, the idea of conditioning at the bottleneck layer improves solution predictions near geometry and boundaries, especially when the solution latent vector prediction has minor deviations.""
""It may be observed that the FCNN performs better than both UNet and FNO and this points to an important aspect about representation of PDE conditions and its impact on accuracy."" The representation of the PDE conditions could be why, but it's hard to say without careful ablation studies. There's a lot different about the networks.
Similarly: ""Furthermore, compressed representations of sparse, high-dimensional PDE conditions improves generalizability.""
Relationship to literature:
The citation in this sentence is abrupt and confusing because it sounds like CoAE-MLSim is a method from that paper instead of the new method: ""Figure 4 shows a schematic of the autoencoder setup used in the CoAE-MLSim (Ranade et al., 2021a)."" More broadly, Ranade et al., 2021a, Ranade et al., 2021b, and Maleki, et al., 2021 are all cited and all quite related to this paper. It should be more clear how the authors are building on those papers (what exactly they are citing them for), and which parts of CoAE-MLSim are new. (The Maleki part is clearer in the appendix, but the reader shouldn't have to check the appendix to know what is new in a paper.)
I thought that otherwise the related work section was okay but was largely just summarizing some papers without giving context for how they relate to this paper.
Additional feedback (minor details, could fix in a later version, but no need to discuss in the discussion phase):
- The abstract could be clearer about what the machine learning task is that CoAE-MLSim addresses.
- The text in the figures is often too small.
- ""using pre-trained decoders (g)"" - probably meant g_u?
- Many of the figures would be more clear if they said pre-trained solution encoders & solution decoders, since there are multiple types of autoencoders.
- The notation is inconsistent, especially with nu. For example, the notation in Figures 2 & 3 doesn't seem to match the notation in Alg 1. Then on Page 4 & Figure 4, the notation changes again.
- Why is the error table not ordered 8^3, 16^3, 32^3 like Figure 9? The order makes it harder for the reader to reason about the tradeoff.
- Why is Err(T_max) negative sometimes? Maybe I don't understand the definition, but I would expect to use absolute value?
- I don't think the study about different subdomain sizes is an ""ablation"" study since they aren't removing a component of the method.
- Figure 11: I'm guessing that the y-axis is log error, but this isn't labeled as such. I didn't really understand the the legend or the figure in general until I got to the appendix, since there's little discussion of it in the main paper.
- ""Figure 30 shows comparisons of CoAE-MLSim with Ansys Fluent for 4 unseen objects in addition to the example shown in the main paper."" - probably from previous draft. Now this whole example is in the appendix, unless I missed something.
- My understanding is that each type of autoencoder is trained separately and that there's an ordering that makes sense to do this in, so you can use one trained autoencoder for the next one (i.e. train the PDE condition AEs, then the PDE solution AE, then the flux conservation AE, then the time integration AE). This took me a while to understand though, so maybe this could be mentioned in the body of the paper. (Or perhaps I missed that!)
- It seems that the time integration autoencoder isn't actually an autoencoder if it's outputting the solution at the next time step, not reconstructing the input.
- Either I don't understand Figure 5 or the labels are wrong.
- It's implied in the paper (like in Algorithm 1) that the boundary conditions are encoded like the other PDE conditions. In the Appendix (A.1), it's stated that ""The training portion of the CoAE-MLSim approach proposed in this work corresponds to training of several autoencoders to learn the representations of PDE solutions, conditions, such as geometry, boundary conditions and PDE source terms as well as flux conservation and time integration."" But then later in the appendix (A.1.3), it's stated that boundary conditions could be learned with autoencoders but are actually manually encoded for this paper. That seems misleading.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- line 126: by the black *line* in the input # Further Questions - Would it make sense to include and learn AccNet as part of a larger predictor, e.g., for semantic segmentation, that make use of similar operators?",NIPS_2018_134,NIPS_2018,"- Some parts of the work are harder to follow and it helps to have checked [Cohen and Shashua, 2016] for background information. # Typos and Presentation - The citation of Kraehenbuehl and Koltun: it seems that the first and last name of the first author, i.e. Philipp, are swapped. - The paper seems to be using a different citation style than the rest of the NIPS submission. Is this intended? - line 111: it might make sense to not call g activation function, but rather a binary operator; similar to Cohen and Shashua, 2016. They do introduce the activation-pooling operator though that fulfils the required conditions. - line 111: I believe that the weight w_i is missing in the sum. - line 114: Why not mention that the operator has to be associative and commutative? - eq 6 and related equations: I believe that the operator after w_i should be the multiplication of the underlying vector space and not \cross_g: It is an operator between a scalar and a tensor, and not just between two scalars. - line 126: by the black *line* in the input # Further Questions - Would it make sense to include and learn AccNet as part of a larger predictor, e.g., for semantic segmentation, that make use of similar operators? - Do you plan to publish their implementation of the proposed AccNet? # Conclusion The work shows that the proposed method is expressive enough to approximate high-dimensional filtering operations while being fast. I think the paper makes an interesting contribution and I would like to see this work being published.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
* The new proposed metric is only tested on a single dataset.,rwpv2kCt4X,EMNLP_2023,"The primary concerns include,
* The necessity of evaluating the degree of personalization is not clear to me.
- According to this paper, I only found three previous research that did personalized summarizers. And all of them utilize the current common metrics to measure performance. It seems these metrics are enough for this task.
- Let's assume the new evaluation metric is necessary. When we have the pairs of user profiles (such as user-expected summaries) and generated summaries for each user, why can we not use the average and variance of current metrics (such as Rouge) to show the degree of personalization? The average presents the performance of the summarizer to generate high-quality summaries, and variance can represent the performance of generating summaries close to each user. It is much easier to evaluate based on current metrics than new ones.
* The new proposed metric is only tested on a single dataset.
* There is no human judgment for this new metric. I notice the authors said, in Limitations, they are trying for the human evaluation. I think it is better to accept the next version with human judgment results.
* The metric is high time-cost due to the eight Jenson-Shannon Divergence calculations.
Besides, the details of $rot()$ were missed in Line 184.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN.,NIPS_2019_175,NIPS_2019,"1. Weak novelty. Addressing domain-shift via domain specific moments is not new. It was done among others by Bilen & Vedaldi, 2017,âUniversal representations: The missing link between faces, text, planktons, and cat breedsâ. Although this paper may have made some better design decisions about exactly how to do it. 2. Justification & analysis: A normalisation-layer based algorithm is proposed, but without much theoretical analysis to justify the specific choices. EG: Why is is exactly: that gamma and beta should be domain-agnostic, but alpha should be domain specific. 3. Positioning wrt AutoDial, etc: The paper claims âparameter-freeâ as a strength compared to AutoDIAL, which has a domain-mixing parameter. However, this spin is a bit misleading. It removes one learnable parameter, but instead includes a somewhat complicated heuristic Eq 5-7 governing transferability. Itâs not clear that removing a single parameters (which is learned in AutoDIAL) with a complicated heuristic function (which is hand-crafted here) is a clear win. 4. The evaluation is a good start with comparing several base DA methods with and without the proposed TransferNorm architecture. It would be stronger if the base DA methods were similarly evaluated with/without the architectural competitors such as AutoDial and AdaBN that are direct competitors to TN. 5. English is full of errors throughout. ""Seldom previous works"", etc. ------ Update ----- The authors response did a decent job of responding to the concerns. The paper could be reasonable to accept. I hope the authors can update the paper with the additional information from the response.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Some abbreviations are not defined, e.g., “NE” on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read. [1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. [2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. [3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.",NIPS_2022_489,NIPS_2022,"Concern regarding representativeness of baselines used for evaluation
Practical benefits in terms of communication overhead & training time could be more strongly motivated
Detailed Comments:
Overall, the paper was interesting to read and the problem itself is well motivated. Formulation of the problem as an MPG appears sound and offers a variety of important insights with promising applications. There are, however, some concerns regarding evaluation fairness and practical benefits.
The baselines used for evaluation do not seem to accurately represent the state-of-the-art in CTDE. In particular, there have been a variety of recent works that explore more efficient strategies (e.g., [1-3]) and consistently outperform QMix with relatively low inter-agent communication. Although the proposed work appears effective as a fully-decentralized approach, it is unclear how well it would perform against more competitive CTDE baselines. Comparison against these more recent works would greatly improve the strength of evaluation.
Benefits in terms of reduced communication overhead could also be more strongly motivated. Presumably, communication between agents could be done over purpose-built inter-LB links, thus avoiding QoS degradation due to contention on links between LBs and servers. Even without inter-LB links, the increase in latency demonstrated in Appendix E.2.2 appears relatively low.
Robustness against dynamic changes in network setup are discussed to some degree, but it’s unclear how significant this issue is in a real-world environment. Even in a large-scale setup, the number of LBs/servers is likely to remain fairly constant at the timescales considered in this work (i.e., minutes). Given this, it seems that the paper should at least discuss trade-offs with a longer training time, which could impact the relative benefits of various approaches.
Some confusion in notation: - Algorithm 2, L8 should be t = 1,…,H (for horizon)? - L100, [M] denotes the set of LBs?
Minor notes: - Some abbreviations are not defined, e.g., “NE” on L73 - Superscript notation in Eq 6 is not defined until much later (L166), which hindered understanding in an initial read.
[1] S. Zhang et al, “Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control”, NeurIPS 2019. [2] Z. Ding et al, “Learning Individually Inferred Communication for Multi-Agent Cooperation”, NeurIPS 2020. [3] T. Wang et al, “Learning Nearly Decomposable Value Functions Via Communication Minimization”, ICLR 2020.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3) Sufficient experimental validation is provided to showcase the empirical performance of the prescribed decision rules. Weaknesses: Some of the explanations provided by the authors are a bit unclear to me. Specifically, I have the following questions:",NIPS_2021_442,NIPS_2021,"of the paper:
Strengths: 1) To the best of my knowledge, the problem investigated in the paper is original in the sense that top-m identification has not been studied in the misspecified setting. 2) The paper provides some interesting results:
i) (Section 3.1) Knowing the level of misspecification ε
is a key ingredient, as not knowing the same would yield sample complexity bounds which are no better than the bound obtainable from unstructured ( ε = ∞
) stochastic bandits. ii) A single no-regret learner is used for the sampling strategy instead of assigning a learner for each of the (N choose k) answers, thus exponentially reducing the number of online learners. iii) The proposed decision rules are shown to match the prescribed lower bound asymptotically. 3) Sufficient experimental validation is provided to showcase the empirical performance of the prescribed decision rules.
Weaknesses: Some of the explanations provided by the authors are a bit unclear to me. Specifically, I have the following questions: 1) IMO, a better explanation of investigating top-m identification in this setting is required. Specifically, in this setting, we could readily convert the problem to the general top-m identification by appending the constant 1 to the features (converting them into d + 1
dimensional features) and trying to estimate the misspecifications η
in the higher dimensional space. Why is that disadvantageous?
Can the authors explain how the lower bound in Theorem 1 explicitly captures the effect of the upper bound on misspecification ε
? The relationship could be shown, for instance, by providing an example of a particular bandit environment (say, Gaussian bandits) ala [Kaufmann2016].
Sample complexity: Theorem 2 states the sample complexity in a very abstract way; it provides an equation which needs to be solved in order to get an explicit expression of the sample complexity. In order to make a comparison, the authors then mention that the unstructured confidence interval β t , δ u n s
is approximately log ⁡ ( 1 δ )
in the limit of δ → 0
, which is then used to argue that the sample complexity of MISLID is asymptotically optimal. However, β t , δ u n s
also depends on t
. In fact, my understanding is that as δ
goes to 0
, the stopping time t
goes to infinity, where it is not clear as to what value the overall expression β t , δ u n s
converges. Overall, I feel that the authors need to explicate the sample complexity a bit more. My suggestions are: can the authors find a solution to equation (5) (or at least an upper bound on the solution for different regimes of ε
)? Using such an upper bound, even if the authors could give an explicit expression of the (asymptotic) sample complexity and show how it compares to the lower bound, it would be a great contribution.
Looking at Figure 1A (the second figure from the left, for the case of ε = 2
), it looks like LinGapE outperforms MISLID in terms of average sample complexity. Please correct me if I’m missing something, but if what I understand is correct, then why use MISLID and not LinGapE?
Probable typo: Line 211: Should it be θ
instead of θ t
for the self-normalized concentration?
The authors have explained the limitations of the investigation in Section 6.","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification,NIPS_2017_65,NIPS_2017,"1) the evaluation is weak; the baselines used in the paper are not even designed for fair classification
2) the optimization procedure used to solve the multi-objective optimization problem is not discussed in adequate detail
Detailed comments below:
Methods and Evaluation: The proposed objective is interesting and utilizes ideas from two well studied lines of research, namely, privileged learning and distribution matching to build classifiers that can incorporate multiple notions of fairness. The authors also demonstrate how some of the existing methods for learning fair classifiers are special cases of their framework. It would have been good to discuss the goal of each of the terms in the objective in more detail in Section 3.3. The part that is probably the most weakest in the entire discussion of the approach is the discussion of the optimization procedure. The authors state that there are different ways to optimize the multi-objective optimization problem they formulate without mentioning clearly which is the procedure they employ and why (in Section 3). There seems to be some discussion about the same in experiments section (first paragraph) and I think what was done is that the objective was first converted into unconstrained optimization problem and then an optimal solution from the pareto set was found using BFGS. This discussion is still quite rudimentary and it would be good to explain the pros and cons of this procedure w.r.t. other possible optimization procedures that could have been employed to optimize the objective.
The baselines used to compare the proposed approach and the evaluation in general seems a bit weak to me. Ideally, it would be good to employ baselines that learn fair classifiers based on different notions (E.g., Hardt et. al. and Zafar et. al.) and compare how well the proposed approach performs on each notion of fairness in comparison with the corresponding baseline that is designed to optimize for that notion. Furthermore, I am curious as to why k-fold cross validation was not used in generating the results. Also, was the split between train and test set done randomly? And, why are the proportions of train and test different for different datasets?
Clarity of Presentation:
The presentation is clear in general and the paper is readable. However, there are certain cases where the writing gets a bit choppy. Comments:
1. Lines 145-147 provide the reason behind x*_n being the concatenation of x_n and z_n. This is not very clear.
2. In Section 3.3, it would be good to discuss the goal of including each of the terms in the objective in the text clearly.
3. In Section 4, more details about the choice of train/test splits need to be provided (see above).
While this paper proposes a useful framework that can handle multiple notions of fairness, there is scope for improving it quite a bit in terms of its experimental evaluation and discussion of some of the technical details.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"2) In the first three paragraphs of section 2, the setting needs to be spelled out more clearly. It seems like the authors want to receive credit for doing something in greater generality than what they actually present, and this muddles the exposition.",NIPS_2016_537,NIPS_2016,"weakness of the paper is the lack of clarity in some of the presentation. Here are some examples of what I mean. 1) l 63, refers to a ""joint distribution on D x C"". But C is a collection of classifiers, so this framework where the decision functions are random is unfamiliar. 2) In the first three paragraphs of section 2, the setting needs to be spelled out more clearly. It seems like the authors want to receive credit for doing something in greater generality than what they actually present, and this muddles the exposition. 3) l 123, this is not the definition of ""dominated"" 4) for the third point of definition one, is there some connection to properties of universal kernels? See in particular chapter 4 of Steinwart and Christmann which discusses the ability of universal kernels two separate an arbitrary finite data set with margin arbitrarily close to one. 5) an example and perhaps a figure would be quite helpful in explaining the definition of uniform shattering. 6) in section 2.1 the phrase ""group action"" is used repeatedly, but it is not clear what this means. 7) in the same section, the notation {\cal P} with a subscript is used several times without being defined. 8) l 196-7: this requires more explanation. Why exactly are the two quantities different, and why does this capture the difference in learning settings? ---- I still lean toward acceptance. I think NIPS should have room for a few ""pure theory"" papers.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"2 The experiments are not quite convincing. The authors choose the old baseline like R3D and C3D. To reduce computation complexity, many papers have been proposed in 3D CNN (X3D, SlowFast, etc). Does the proposed method also works on these 3D CNNs? Or compared to these approaches, what is the advantage of the proposed method?",ICLR_2023_4741,ICLR_2023,"Weakness
1 The novelty is limited. The low-rank design is relevant to (Hu et al., 2021). The sparse design is similar to Taylor pruning (Molchanov et al., 2019).
2 The experiments are not quite convincing. The authors choose the old baseline like R3D and C3D. To reduce computation complexity, many papers have been proposed in 3D CNN (X3D, SlowFast, etc). Does the proposed method also works on these 3D CNNs? Or compared to these approaches, what is the advantage of the proposed method?
3 The paper is hard to follow. In fact, I have to read many times to get what it is. I understand it is a theoretical-kind paper. But please further explain the mathmetical formulation clearly to show why and how it works.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
1. It is not very clear how exactly is the attention module attached to the backbone ResNet-20 architecture when performing the search. How many attention modules are used? Where are they placed? After each block? After each stage? It would be good to clarify this.,NIPS_2020_125,NIPS_2020,"1. It is not very clear how exactly is the attention module attached to the backbone ResNet-20 architecture when performing the search. How many attention modules are used? Where are they placed? After each block? After each stage? It would be good to clarify this. 2. Similar to above, it would be good to provide more details of how the attention modules are added to tested architectures. I assume they are added following the SE paper but would be good to clarify. 3. Related to above, how is the complexity of the added module controlled? Is there a tunable channel weight similar to SE? It would be good to clarify this. 4. In Table 3, the additional complexity of the found module is ~5-15% in terms of parameters and flops. It is not clear if this is actually negligible. Would be good to perform comparisons where the complexity matches more closely. 5. In Table 3, it seems that the gains are decreasing for larger models. It would be good to show results with larger and deeper models (ResNet-101 and ResNet-152) to see if the gains transfer. 6. Similar to above, it would be good to show results for different model types (e.g. ResNeXt or MobileNet) to see if the module transfer to different model types. All current experiments use ResNet models. 7. It would be good to discuss and report how the searched module affect the training time, inference time, and memory usage (compared to vanilla baselines and other attention modules). 8. It would be interesting to see the results of searching for the module using a different backbone (e.g. ResNet-56) or a different dataset (e.g. CIFAR-100) and compare both the performance and the resulting module. 9. The current search space for the attention module consists largely of existing attention operations as basic ops. It would be interesting to consider a richer / less specific set of operators.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison? Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison: Guo Lu, et al., ""Content Adaptive and Error Propagation Aware Deep Video Compression."" ECCV 2020.",ICLR_2022_1522,ICLR_2022,"Weakness:
The overall novelty seems limited since the instance-adaptive method is from existing work with no primary changes. Here are some main questions and concerns:
1). How many optimization steps are used to produce the final reported performance in Figure.1 as well as in some other figs and tables?
2). The proposed method looks stronger at high bitrate but close to the baselines at low bitrate. What is the precise bitrate range used for BD-rate comparison?
Besides, a related work about implementing content adaptive algorithm in learned video compression is suggested for discussion or comparison:
Guo Lu, et al., ""Content Adaptive and Error Propagation Aware Deep Video Compression."" ECCV 2020.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- I would recommend the authors to distinguish the all-or-nothing or cutoff phenomenon from usual statistical bounds that the machine learning and NeurIPS community is familiar with.,NIPS_2020_833,NIPS_2020,"I don't think the paper has signficant weaknesses. The model setting is admittedly quite limited, but given the relatively sparse literature on the cutoff phenomenon in learning, I do not think this is a strong complaint. Some suggestions to improve: - I would recommend the authors to distinguish the all-or-nothing or cutoff phenomenon from usual statistical bounds that the machine learning and NeurIPS community is familiar with. - Mention that 'teacher student' setting is another phrasing of 'well-specified' models in statistics, 'realizable setting' in learning theory, and 'proper learning' in computer science. - Regularizing noise: if this is needed for the proof, preferably write it as such. Presumably a limiting argument works for Delta vanishing, e.g. one can always add small noise to the observvations. - It would also be nice to have a rigorous proof for the MMSE portion. Since this is not done in the current paper, but potentially accessible to present techniques, the authors should identify Eq (10) as a 'Claim'.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
2). The improvements on three tasks over the previous works and self-implemented baselines are marginal. Further analysis beyond the main experiments is not sufficient.,NIPS_2022_1516,NIPS_2022,"1). Technically speaking, the contribution of this work is incremental, and its technique deep is shallow. The proposed probabilistic word dropout is not that impressive or novel. To me, it sounds like a probabilistic teacher forcing. The massive notations and formulas appear to be not that necessary for me to understand the idea. 2). The improvements on three tasks over the previous works and self-implemented baselines are marginal. Further analysis beyond the main experiments is not sufficient. 3). The backbone is constrained to be the double LSTM, while popular transformers is not involved. Although this seems to work for different encoders, pre-trained language models are not covered as well. The application of this method to more extensive model structures remains a potential concern.
Limitations are mentioned in Section 6, while it seems to be not sufficient.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
- All the linear convergence rates rely on Theorem 8 which is burried at the end of the appendix and which proof is not clear enough.,NIPS_2017_74,NIPS_2017,"- Theorem 2 which presentation is problematic and does not really provide any convergence guaranty.
- All the linear convergence rates rely on Theorem 8 which is burried at the end of the appendix and which proof is not clear enough.
- Lower bounds on the number of good steps of each algorithm which are not really proved since they rely on an argument of the type ""it works the same as in another close setting"".
The numerical experiments are numerous and convincing, but I think that the authors should provide empirical evidences showing that the computational cost are of the same order of magnitude compared competing methods for the experiments they carried out.
%%%% Details on the main comments
%% Theorem 2
The presention and statement of Theorem 2 (and all the sublinear rates given in the paper) has the following form:
- Given a fixed horizon T
- Consider rho, a bound on the iterates x_0 ... x_T
- Then for all t > 0 the suboptimality is of the order of c / t where c depends on rho.
First, the proof cannot hold for all t > 0 but only for 0 < t <= T. Indeed, in the proof, equation (16) relies on the fact that the rho bound holds for x_t which is only ensured for t < = T.
Second the numerator actually contains rho^2. When T increases, rho could increase as well and the given bound does not even need to approach 0.
This presentation is problematic. One possible way to fix this would be to provide a priori conditions (such as coercivity) which ensure that the sequence of iterates remain in a compact set, allowing to define an upper bound independantly of the horizon T.
In the proof I did not understand the sentence ""The reason being that f is convex, therefore, for t > 0 we have f (x t ) < = f (0).""
%% Lemma 7 and Theorem 8
I could not understand Lemma 7.
The equation is given without any comment and I cannot understand its meaning without further explaination. Is this equation defining K'? Or is it the case that K' can be chosen to satisfy this equation? Does it have any other meaning?
Lemma 7 deals only with g-faces which are polytopes. Is it always the case? What happens if K is not a polytope? Can this be done without loss of generality? Is it just a typo?
Theorem 8:
The presentation is problematic. In Lemma 7, r is not a feasible direction. In Theorem 8, it is the gradient of f at x_t. Theorem 8 says ""using the notation from Lemma 7"". The proof of Theorem 8 says ""if r is a feasible direction"". All this makes the work of the reader very hard.
Notations of Lemma 7 are not properly used:
- What is e? e is not fixed by Lemma 7, it is just a variable defining a maximum. This is a recurent mistake in the proofs.
- What is K? K is supposed to be given in Lemma 7 but not in Theorem 8.
- Polytope?
All this could be more explicit.
""As x is not optimal by convexity we have that < r , e > > 0"". Where is it assumed that $x$ is not optimal? How does this translate in the proposed inequality?
What does the following mean?
""We then project r on the faces of cone(A) containing x until it is a feasible direction""
Do the author project on an intersection of faces or alternatively on each face or something else?
It would be more appropriate to say ""the projection is a feasible direction"" since r is fixed to be the gradient of f. It is very uncomfortable to have the value of r changing within the proof in an ""algorithmic fashion"" and makes it very hard to check accuracy of the arguments.
In any case, I suspect that the resulting r could be 0 in which case the next equation does not make sense. What prevents the resulting r from being null?
In the next sentences, the authors use Lemma 7 which assumes that r is not a feasible direction. This is contradictory with the preceeding paragraph. At this point I was completely confused and lost hope to understand the details of this proof.
What is r' on line 723 and in the preceeding equation?
I understand that there is a kind of recursive process in the proof. Why should the last sentence be true?
%% Further comments
Line 220, max should be argmax
I did not really understand the non-negative matrix facotrization experiment. Since the resulting approximation is of rank 10, does it mean that the authors ran their algorithm for 10 steps only?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Page 2, second paragraph in Related Work, the Walkman algorithm (Mao et al., 2020) is solved by ADMM, with two versions, one is to solve a local optimization problem, the other is to solve a gradient approximation. Therefore, it is not accurate to say that “However, these works are all based on the simple SGD for decentralized optimization.” 3. Section 3, first paragraph, in “It can often have faster convergence and better generalization than the SGD-based Algorithm 1, as will be demonstrated empirically in Section 4.1.” The “it” does not have a clear reference.",yuYMJQIhEU,ICLR_2024,"This paper mostly combines standard algorithms (random walk, Adam without momentum, SAM), although this is not a problem, the theoretically analysis needs to be improved. Meanwhile, the experimental part lacks new insights except for some expected results.
Major comments:
1.	Theorem 3.7 looks like a direct combination of theoretical results obtained by Adam without momentum and SAM. Furthermore, the proof in Appendix does not consider the convergence guarantee that could be achieved by random walk method. That is, the Markov chain is not considered. Note that the last equation in Page 13 is almost the same as the convergence result (Theorem 4.3, Triastcyn et al., 2022)) except it does not have the compression part. The proof also follows exactly as Triastcyn et al. (2022). The perturbed model is not used, means that sharp awareness minimization is not analyzed, which makes me question the soundness of Theorem 3.7.
2.	Since SAM is integrated to prevent potential overfitting, the experiment should present this effect compared with its counterpart that does not have the perturbed model. The lack of this experiment comparison would question the necessity of incorporating SAM in the proposed Localized framework.
3.	The simulation only shows the loss performance of the proposed algorithms and the benchmarks, however, in practical, we would be more interested to see the classification accuracy.
4.	The proposed algorithm is compared with FedAvg, however, for FedAvg, not all agents are communicating all the time, which does not make sense in the setting that FedAvg does not need to consider communication. That means, I suppose that if all agents in FedAvg communicate all the time, the performance of FedAvg might be much better than all the other methods, since there exists a coordinator, although the communication cost would be very high. The figures presented, however, show that Localized SAM is always better than FedAvg in the random sample setting in both performance and communication, which is not a fair comparison.
Minor comments:
1.	In Page 2, first paragraph, Localized SAM is introduced first and then “sharpness-aware minimization (SAM (Foret et al., 2021))” is repeated again. It would be better to revise it.
2.	Page 2, second paragraph in Related Work, the Walkman algorithm (Mao et al., 2020) is solved by ADMM, with two versions, one is to solve a local optimization problem, the other is to solve a gradient approximation. Therefore, it is not accurate to say that “However, these works are all based on the simple SGD for decentralized optimization.”
3.	Section 3, first paragraph, in “It can often have faster convergence and better generalization than the SGD-based Algorithm 1, as will be demonstrated empirically in Section 4.1.” The “it” does not have a clear reference.
4.	In Section 3.1, you introduced $\boldsymbol{u}_k$, which was not defined previously and did not show up after Algorithm 3.
5.	Figure 6 seems to be reused from your previous LoL optimizer work.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The main (and only) theoretical result in the paper provides utility guarantees for the proposed algorithm only when the features and noise are Gaussian. This is a strong requirement on the data, especially given that previous algorithms don’t need this assumption as well. Moreover, the authors should compare the rates achieved by their procedure to existing rates in the literature.",NIPS_2022_2786,NIPS_2022,"1. The main (and only) theoretical result in the paper provides utility guarantees for the proposed algorithm only when the features and noise are Gaussian. This is a strong requirement on the data, especially given that previous algorithms don’t need this assumption as well. Moreover, the authors should compare the rates achieved by their procedure to existing rates in the literature. 2. Experiments: the experimental results in the paper don’t provide a convincing argument for their algorithms. First, all of the experiments are done over synthetic data. Moreover, the authors only consider low-dimensional datasets where d<30 and therefore it is not clear if the same improvements hold for high-dimensional problems. Finally, it is not clear whether the authors used any hyper-parameter tuning for DP-GD (or DP-SGD); this could result in significantly better results for DP-GD. 3. Writing: I encourage the authors to improve the writing in this paper. For example, the introduction could use more work on setting up the problem, stating the main results and comparing to previous work, before moving on to present the algorithm (which is done too soon in the current version). More:
Typo (first sentence): “is a standard”
First paragraph in page 4 has m. What is m? Should that be n?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.",NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"5.In experimental section, authors only compared with two baselines, there’re several works also focus on the same questions, for example [1,2,3], so it’s suggested to add more experimental to show the effectiveness of proposed method.",OBUQNASaWw,ICLR_2025,"1.It’s suggested that authors give a comprehensive survey on adaptive sparse training method. Although authors claim “Previous works have only managed to solve one, or perhaps two of these challenges”, can authors give a comprehensive comparison of existing methods?
2.Considering different clients train different submodels, the server also maintains a full model. So can the sparsity of clients be different to apply for heterogeneous hardware?
3.Can authors further explain why clients should achieves consensus on the clients’ sparse model masks when server always maintain a full model.
4.What’s the definition of the model plasticity?
5.In experimental section, authors only compared with two baselines, there’re several works also focus on the same questions, for example [1,2,3], so it’s suggested to add more experimental to show the effectiveness of proposed method.
6.Considering the model architecture, authors only show the effectiveness on convolutional network, what’s the performance on other architecture, for example Transformer?
[1]Stripelis, Dimitris, et al. ""Federated progressive sparsification (purge, merge, tune)+."" arXiv preprint arXiv:2204.12430 (2022).
[2]Wang, Yangyang, et al. ""Theoretical convergence guaranteed resource-adaptive federated learning with mixed heterogeneity."" Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2023.
[3]Zhou, Hanhan, et al. ""Every parameter matters: Ensuring the convergence of federated learning with dynamic heterogeneous models reduction."" Advances in Neural Information Processing Systems 36 (2024).","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- If I understand correctly, In Tables 1 and 2, the authors report the best results on the **dev set** with the hyper-parameter search and model selection on **dev set**, which is not enough to be convincing. I strongly suggest that the paper should present the **average** results on the **test set** with clearly defined error bars under different random seeds.",ARR_2022_59_review,ARR_2022,"- If I understand correctly, In Tables 1 and 2, the authors report the best results on the **dev set** with the hyper-parameter search and model selection on **dev set**, which is not enough to be convincing. I strongly suggest that the paper should present the **average** results on the **test set** with clearly defined error bars under different random seeds. - Another concern is that the method may not be practical. In fine-tuning, THE-X firstly drops the pooler of the pre-trained model and replaces softmax and GeLU, then conducts standard fine-tuning. For the fine-tuned model, they add LayerNorm approximation and d distill knowledge from original LN layers. Next, they drop the original LN and convert the model into fully HE-supported ops. The pipeline is too complicated and the knowledge distillation may not be easy to control. - Only evaluating the approach on BERTtiny is also not convincing although I understand that there are other existing papers that may do the same thing. For example, a BiLSTM-CRF could yield a 91.03 F1-score and a BERT-base could achieve 92.8. Although computation efficiency and energy-saving are important, it is necessary to comprehensively evaluate the proposed approach.
- The LayerNorm approximation seems to have a non-negligible impact on the performances for several tasks. I think it is an important issue that is worth exploring.
- I am willing to see other reviews of this paper and the response of the authors. - Line #069: it possible -> it is possible?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Insufficient ablation study on \alpha. \alpha is only set to 1e-4, 1e-1, 5e-1 in section 5.4 with a large gap between 1e-4 and 1e-1. The author is recommended to provide more values of \alpha, at least 1e-2 and 1e-3.",ICLR_2023_2396,ICLR_2023,"1. Lack of the explanation about the importance and the necessity to design deep GNN models . In this paper, the author tries to address the issue of over-smoothing and build deeper GNN models. However, there is no explanation about why should we build a deep GNN model. For CNN, it could be built for thousands of layers with significant improvement of the performance. While for GNN, the performance decreases with the increase of the depth (shown in Figure 1). Since the deeper GNN model does not show the significant improvement and consumes more computational resource, the reviewer wonders the explanation of the importance and the necessity to design deep models. 2. The experimental results are not significantly improved compared with GRAND. For example, GRAND++-l on Cora with T=128 in Table 1, on Computers with T=16,32 in Table 2. Since the author claims that GRAND suffers from the over-smoothing issue while DeepGRAND significantly mitigates such issue, how to explain the differences between the theoretical and practical results, why GRAND performs better when T is larger? Besides, in Table 3, DeepGRAND could not achieve the best performance with 1/2 labeled on Citeseer, Pubmed, Computers and CoauthorCS dataset, which could not support the argument that DeepGRAND is more resilient under limited labeled training data. 3. Insufficient ablation study on \alpha. \alpha is only set to 1e-4, 1e-1, 5e-1 in section 5.4 with a large gap between 1e-4 and 1e-1. The author is recommended to provide more values of \alpha, at least 1e-2 and 1e-3. 4. Minor issues. The x label of Figure 2, Depth (T) rather than Time (T).","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"6: How many topics were used? How did you get topic-word parameters for this ""real"" dataset? How big is the AG news dataset? Main paper should at least describe how many documents in train/test, and how many vocabulary words.",ICLR_2022_1872,ICLR_2022,"I list 5 concerns here, with detailed discussion and questions for the authors below
W1: While theorems suggest ""existence"" of a linear transformation that will approximate the posterior, the actual construction procedure for the ""recovered topic posterior"" is unclear
W2: Many steps are difficult to understand / replicate from main paper
W3: Unclear what theorems can say about finite training sets
W4: Justification / intuition for Theorems is limited in the main paper
Responses to W1-W3 are most important for the rebuttal.
W1: Actual procedure for constructing the ""recovered topic posterior"" is unclear
In both synthetic and real experiments, the proposed self-supervised learning (SSL) method is used to produce a ""recovered topic posterior"" p( w x). However, the procedure used here is unclear... how do we estimate p( w x) using the learning function f(x)?
The theorems imply that a linear function exists with limited (or zero) approximation error for any chosen scalar summary of the doc-topic weights w. However, how such a linear function is constructed is unclear. The bottom of page four suggests that when t=1 and A is full rank, that ""one can use the pseudoinverse of A to recover the posterior"", however it seems (1) unclear what the procedure is in general and what its assumptions are, and (2) odd that the prior may not needed at all.
Can the authors clarify how to estimate the recovered topic posterior using the proposed SSL method?
W2: Many other steps are difficult to understand / replicate from main paper
Here's a quick list of questions on experimental steps I am confused about / would have trouble reproducing
For the toy experiments in Sec. 5:
Do you estimate the topic-word parameter A? Or assume the true value is given?
What is the format for document x provided as input to the neural networks that define f(x)? The top paragraph of page 7 makes it seem like you provide an ordered list of words. Wouldn't a bag-of-words count vector be a more robust choice?
How do you set t=1 (predict one word given others) but somehow also use ""the last 6 words are chosen as the prediction target""?
How do you estimate the ""recovered topic posterior"" for each individual model (LDA, CTM, etc)? Is this also using HMC (which is used to infer the ground-truth posterior)?
Why use 2000 documents for the ""pure"" topic model but 500 in test set for other models? Wouldn't more complex models benefit from a larger test set?
For the real experiments in Sec. 6:
How many topics were used?
How did you get topic-word parameters for this ""real"" dataset?
How big is the AG news dataset? Main paper should at least describe how many documents in train/test, and how many vocabulary words.
W3: Unclear what theorems / methods can say about finite training sets
All the theorems seem to hold when considering terms that are expectations over a known distribution over observed-data x and missing-data y. However, in practical data analysis we do not know the true data generating distribution, we only have a finite training set.
I am wondering about this method's potential in practice for modest-size datasets. For the synthetic dataset with V=5000 (a modest vocabulary size), the experiments considered 0.72 million to 6 million documents, which seems quite large.
What practically must be true of the observed dataset for the presented methods to work well?
W4: Justification / intuition for Theorems is limited in the main paper
All 3 theorems in the main paper are presented without much intuition or justification about why they should be true, which I think limits their impact on the reader. (I'll try to wade thru the supplement, but did not have time before the review deadline).
Theorem 3 tries to give intuition for the t=1 case, but I think could be stronger: why should f(x) have an optimal form p ( y = v 1 x )
? Why should ""these probabilities"" have the form A E [ w x ]
? I know space is limited, but helping your reader figure things out a bit more explicitly will increase the impact.
Furthermore, the reader would benefit from understanding how tight the bounds in Theorem 4 are. Can we compute the bound quality for toy data and understand it more practically?
Detailed Feedback on Presentation
No need to reply to these in rebuttal but please do address as you see fit in any revision
Page 3:
""many topic models can be viewed""... should probably say ""the generative process of many topic models can be viewed...""
the definition of A_ij is not quite right. I would not say ""word i \in topic j"", I would say ""word i topic j"". A word is not contained in a topic, Each word has a chance of being generated.
I'd really avoid writing Δ ( K )
and would just use Δ
throughout .... unclear why this needs to be a function of K
but the topic-word parameters (whose size also depends on K
) does not
Should we call the reconstruction objective a ""partial reconstruction"" or ""masked reconstruction""? I'm used to reconstruction in an auto-encoder context, where the usual ""reconstruction"" objective is literally to recover all observed data, not a piece of observed data that we are pretending not to see
In Eq. 1, are you assuming an ordered or unordered representation of the words in x and y?
Page 4:
I would not reuse the variable y in both reconstruction and contrastive contexts. Find another variable. Same with theta.
Page 5:
I would use f ∗
to denote the exact minimizer, not just f
Figure 2 caption should clarify:
what is the takeaway for this figure? Does reader want to see low values? Does this figure suggest the approach is working as expected?
what procedure is used for the ""recovered"" posterior? Your proposed SSL method?
why does Pure have a non-monotonic trend as alpha gets larger?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* The analysis on BRP-NAS is also somewhat barebones: it only compares against 3 basic alternatives and ignores some other NAS (e.g. super-net/one-shot approaches, etc...).",NIPS_2020_878,NIPS_2020,"* The GCN-based predictor and experiments don't have open-sourced code (not mentioned in the main paper or supplement), however the authors do provide detailed descriptions. * Some correctness issues (see next section) * The paper presents 2 important NAS objectives: latency optimization and accuracy optimization. However, the BRP-NAS (section 4) seems out-of-place since the rest of the paper deals with latency prediction. It nearly feels like BRP-NAS could be a separate paper, or Section 3 was used only to suggest using GCN (in this case, why not directly start with accuracy prediction with GCN?). * The analysis on BRP-NAS is also somewhat barebones: it only compares against 3 basic alternatives and ignores some other NAS (e.g. super-net/one-shot approaches, etc...). * Unclear if code will be released, as the GCN implementation may be hard to reproduce without original code (though the author's descriptions are fairly detailed and there is more information in the supplement).","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- I find the zero-shot version and the connection to density estimation a bit distracting to the main point of the paper, which is that one can learn to produce good prototypes that are effective for few-shot learning. However, this is more an aesthetic argument than a technical one.",NIPS_2017_390,NIPS_2017,"- I am curious how the performance varies quantitatively if the training ""shot"" is not the same as ""test"" shot: In realistic applications, knowing the ""shot"" before-hand is a fairly strong and impractical assumption.
- I find the zero-shot version and the connection to density estimation a bit distracting to the main point of the paper, which is that one can learn to produce good prototypes that are effective for few-shot learning. However, this is more an aesthetic argument than a technical one.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.,7GxY4WVBzc,EMNLP_2023,"* The contribution of the vector database to improving QA performance is unclear. More analysis and ablation studies are needed to determine its impact and value for the climate change QA task.
* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.
* The work is focused on a narrow task (climate change QA) in a specific language (Arabic), so its broader impact may be limited.
* The limitations section lacks specific references to errors and issues found through error analysis of the current model. Performing an analysis of the model's errors and limitations would make this section more insightful.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
3. The experiment results can be enriched. it is lack of attacks with different strength. How different thresholds influence the detection performance is also lacking.,kz78RIVL7G,ICLR_2025,"1. Detecting adversarial examples by comparing the original example against its de-noised version is not a new idea. There exist many methods that either use the statistics of model input itself, or statistics of intermediate results when passing though a network. In order to justify that the value of the proposed method, it is critical to show that the proposed method is superior than previous ones either. As the novelty is relative low, the key to justify this work is to show that the proposed method is superior than others, either theoretically or empirically. However, in the paper it is lacking of detailed analysis of drawbacks of pervious works, motivations or intuition of what the additional value that the proposed method can provide, or direct comparison in experiment results. Authors should consider adding more previous methods that falls into the same kind, analyzing their similarity and differences, providing detailed comparison in experiment results, and trying to draw insights that what makes things better. An example of work in this kind is ""Detecting Adversarial Image Examples in Deep Neural Networks with Adaptive Noise Reduction"" but there are more.
2. The presentation is poor. There is lack of motivations and intuition. The whole paper sounds like ""look this is what we did"" but is lack of ""why or what motivates us to do this"". There are a lot of details and figures that can be moved to appendix, while on the other hand there is no diagram for the proposed method. The results are provides without drawing insights.
3. The experiment results can be enriched. it is lack of attacks with different strength. How different thresholds influence the detection performance is also lacking.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '4']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. For evaluation, since the claim of this paper is to reduce exposure bias, training a discriminator on generations from the learned model is needed to confirm if it is the case, in a way similar to Figure 1. Note that it is different from Figure 4, since during training the discriminator is co-adapting with the generator, and it might get stuck at a local optimum.",NIPS_2020_1592,NIPS_2020,"Major concerns: 1. While it is impressive that this work gets slightly better results than MLE, there are more hyper-parameters to tune, including mixture weight, proposal temperature, nucleus cutoff, importance weight clipping, MLE pretraining (according to appendix). I find it disappointing that so many tricks are needed. If you get rid of pretraining/initialization from T5/BART, would this method work? 2. This work requires MLE pretraining, while prior work ""Training Language GANs from Scratch"" does not. 3. For evaluation, since the claim of this paper is to reduce exposure bias, training a discriminator on generations from the learned model is needed to confirm if it is the case, in a way similar to Figure 1. Note that it is different from Figure 4, since during training the discriminator is co-adapting with the generator, and it might get stuck at a local optimum. 4. This work is claiming that it is the first time that language GANs outperform MLE, while prior works like seqGAN or scratchGAN all claim to be better than MLE. Is this argument based on the tradeoff between BLEU and self-BLEU from ""language GANs falling short""? If so, Figure 2 is not making a fair comparison since this work uses T5/BART which is trained on external data, while previous works do not. What if you only use in-domain data? Would this still outperform MLE? Minor concerns: 5. This work only uses answer generation and summarization to evaluate the proposed method. While these are indeed conditional generation tasks, they are close to ""open domain"" generation rather than ""close domain"" generation such as machine translation. I think this work would be more convincing if it is also evaluated in machine translation which exhibits much lower uncertainties per word. 6. The discriminator accuracy of ~70% looks low to me, compared to ""Real or Fake? Learning to Discriminate Machine from Human Generated Text"" which achieves almost 90% accuracy. I wonder if the discriminator was not initialized with a pretrained LM, or is that because the discriminator used is too small? ===post-rebuttal=== The added scratch GAN+pretraining (and coldGAN-pretraining) experiments are fairer, but scratch GAN does not need MLE pretraining while this work does, and we know that MLE pretraining makes a big difference, so I am still not very convinced. My main concern is the existence of so many hyper-parameters/tricks: mixture weight, proposal temperature, nucleus cutoff, importance weight clipping, and MLE pretraining. I think some sensitivity analysis similar to scratch GAN's would be very helpful. In addition, rebuttal Figure 2 is weird: when generating only one word, why would cold GAN already outperform MLE by 10%? To me, this seems to imply that improvement might be due to hyper-parameter tuning.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
2.L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to?,NIPS_2017_631,NIPS_2017,"1.	The main contribution of the paper is CBN. But the experimental results in the paper are not advancing the state-of-art in VQA (on the VQA dataset which has been out for a while and a lot of advancement has been made on this dataset), perhaps because the VQA model used in the paper on top of which CBN is applied is not the best one out there. But in order to claim that CBN should help even the more powerful VQA models, I would like the authors to conduct experiments on more than one VQA model â favorably the ones which are closer to state-of-art (and whose codes are publicly available) such as MCB (Fukui et al., EMNLP16), HieCoAtt (Lu et al., NIPS16). It could be the case that these more powerful VQA models are already so powerful that the proposed early modulating does not help. So, it is good to know if the proposed conditional batch norm can advance the state-of-art in VQA or not.
2.	L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to?
3.	In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.
4.	Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?
5.	Figure 4 visualization: the visualization in figure (a) is from ResNet which is not finetuned at all. So, it is not very surprising to see that there are not clear clusters for answer types. However, the visualization in figure (b) is using ResNet whose batch norm parameters have been finetuned with question information. So, I think a more meaningful comparison of figure (b) would be with the visualization from Ft BN ResNet in figure (a).
6.	The first two bullets about contributions (at the end of the intro) can be combined together.
7.	Other errors/typos:
a.	L14 and 15: repetition of word âimagineâ
b.	L42: missing reference
c.	L56: impact -> impacts
Post-rebuttal comments:
The new results of applying CBN on the MRN model are interesting and convincing that CBN helps fairly developed VQA models as well (the results have not been reported on state-of-art VQA model). So, I would like to recommend acceptance of the paper.
However I still have few comments --
1. It seems that there is still some confusion about test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the performance of the MCB model to be 62.5% on test-standard split. However, 62.5% seems to be the performance of the MCB model on the test-dev split as per table 1 in the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).
2. The reproduced performance reported on MRN model seems close to that reported in the MRN paper when the model is trained using VQA train + val data. I would like the authors to clarify in the final version if they used train + val or just train to train the MRN and MRN + CBN models. And if train + val is being used, the performance can't be compared with 62.5% of MCB because that is when MCB is trained on train only. When MCB is trained on train + val, the performance is around 64% (table 4 in MCB paper).
3. The citation for the MRN model (in the rebuttal) is incorrect. It should be -- @inproceedings{kim2016multimodal,
title={Multimodal residual learning for visual qa},
author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle={Advances in Neural Information Processing Systems}, pages={361--369}, year={2016} }
4. As AR2 and AR3, I would be interested in seeing if the findings from ResNet carry over to other CNN architectures such as VGGNet as well.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1 The Algorithm should be presented and described in detail, which is helpful for understanding the proposed method.",NIPS_2022_528,NIPS_2022,"weakness 1 The Algorithm should be presented and described in detail. 2 The background of Sharpness-Aware Minimization (SAM) shoud be described in detail.
1 The Algorithm should be presented and described in detail, which is helpful for understanding the proposed method. 2 The background of Sharpness-Aware Minimization (SAM) shoud be described in detail.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Since the paper mentions the possibility to use Chebyshev polynomials to achieve a speed-up, it would have been interesting to see a runtime comparison at test time.",NIPS_2016_321,NIPS_2016,"======================== Positive aspects: + The paper is well written and has a clear and coherent structure. The discussion of related work is comprehensive. + Non-parametric emission distributions add flexibility to the general HMM framework and reduce bias due to wrong modeling assumptions. Progress in this area should have theoretical and practical impact. + The paper builds upon existing spectral methods for parametric HMMs but introduces novel techniques to extend those approaches to the non-parametric case. + The theoretical bounds (section 5) are interesting, even though most of the results are special cases or straightforward extensions of known results. Negative aspects: - The restriction to triplets (or a sliding window of length 3) is quite limiting. Is this a fundamental limitation of the approach or is an extension to longer subsequences (without a sliding window) straightforward? - Since the paper mentions the possibility to use Chebyshev polynomials to achieve a speed-up, it would have been interesting to see a runtime comparison at test time. - The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful. - The experimental evaluation compares the proposed approach to 4 other HMM baselines. Even though NP-SPEC-HMM outperforms those baselines, the experimental evaluation has only toy character (simple length 6 conditional distributions, only one training/test sequence in case of the real datasets). Minor points ============ * l.22: Only few parametric distributions allow for tractable exact inference in an HMM. * l.183: Much faster approximations than Chebyshev polynomials exist for the evaluation of kernel density estimates, especially in low-dimensional spaces (e.g., based on fast multipole methods). * Figure 1: There is probably a âx 10^3â missing in the plot on the bottom right. Questions ========= * Eq. (3): Why the restriction to an isotropic bandwidth and a product kernel? Especially a diagonal bandwidth matrix could have been helpful. Would the approximation with Chebyshev polynomials still work? * The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM? * Is it computationally feasible to use the proposed model in a more realistic application, e.g., action recognition from motion capture sequences? Conclusion ========== The paper is well written and, from a theoretical perspective, interesting to read. However, the experiments are weak and it remains unclear how practical the proposed model would be in real applications. Iâm tending towards accept, but the authors should comment in their rebuttal on the above points.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The proposed method seems only works for digit or text images, such as MNIST and SVHN. Can it be used on natural images, such as CIFAR10, which has wider applications in the real world then digit/text.",NIPS_2020_11,NIPS_2020,"1. The proposed method seems only works for digit or text images, such as MNIST and SVHN. Can it be used on natural images, such as CIFAR10, which has wider applications in the real world then digit/text. 2. Are the results obtained on Synbols dataset generalizable to large-scale datasets? For example, if you find algorithm A is better than B on Synbols dataset, will the conclusion hold on large images (e.g., ImageNet scale) in real-world applications? This need to be discussed in the paper.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* Although the style design is clean, the prompts are not well-organized (Table 6, 7). All sentences squeeze together.",tbRPPWDy76,EMNLP_2023,"There might be not enough theoretical discussion and in-depth analyses which help readers understand the prompt design. More motivations and insights are needed.
The engineering part might still need refinement.
* Considering that this work is all about evaluation, there might be a lack of experiments currently. It might be beneficial to conduct more evaluation experiments categorized by language types and dialog content.
* Although the style design is clean, the prompts are not well-organized (Table 6, 7). All sentences squeeze together.
* The Chinese translation of the proposed prompt (Table 7) is bad.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The figures are not clear. For example, in figure 2, it’s confused for the relation of 3 sub-figures. Some modules are not labeled in figure, such as CMAF, L_BT, VoLTA.",ICLR_2023_2622,ICLR_2023,"Weakness: 1. The figures are not clear. For example, in figure 2, it’s confused for the relation of 3 sub-figures. Some modules are not labeled in figure, such as CMAF, L_BT, VoLTA. 2. The experiments results are not significant. 3. Three steps for training are shown in VoLTA, a) switching off CMAF, b) switching on CMAF, c) keep CMAF and random sampling for training. The ablation study on these parts should be conducted. 4. The key point of this paper is GOT, but no ablation on this part. The authors are encouraged to verify which parts works.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.",ARR_2022_314_review,ARR_2022,"1. Although the work is important and detailed, from the novelty perspective, it is an extension of norm-based and rollout aggregation methods to another set of residual connections and norm layer in the encoder block. Not a strong weakness, as the work makes a detailed qualitative and quantitative analysis, roles of each component, which is a novelty in its own right.
2. The impact of the work would be more strengthened with the proposed approach's (local and global) applicability to tasks other than classification like question answering, textual similarity, etc. ( Like in the previous work, Kobayashi et al. (2020))
1. For equations 12 and 13, authors assume equal contribution from the residual connection and multi-head attention. However, in previous work by Kobayashi et al. (2021), it is observed and revealed that residual connections have a huge impact compared to mixing (attention). This assumption seems to be the opposite of the observations made previously. What exactly is the reason for that, for simplicity (like assumptions made by Abnar and Zuidema (2020))?
2. At the beginning of the paper, including the abstract and list of contributions, the claim about the components involved is slightly inconsistent with the rest. For instance, the 8th line in abstract is ""incorporates all components"", line 73 also says the ""whole encoder"", but on further reading, FFN (Feed forward layers) is omitted from the framework. This needs to be framed (rephrased) better in the beginning to provide a clearer picture.
3. While FFNs are omitted because a linear decomposition cannot be obtained (as mentioned in the paper), is there existing work that offers a way around (an approximation, for instance) to compute the contribution? If not, maybe a line or two should be added that there exists no solution for this, and it is an open (hard) problem. It improves the readability and gives a clearer overall picture to the reader.
4. Will the code be made publicly available with an inference script? It's better to state it in the submission, as it helps in making an accurate judgement that the code will be useful for further research.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Some questionable design choices. Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting. How are such factors controlled?",KE5QunlXcr,EMNLP_2023,"- The PLMs used (BERT) are by current standards, quite old, and quite small. As work in scaling PLMs up to sizes orders of magnitude greater, performance on syntactic tasks has shown to improve naturally (along with many other useful emergent forms of knowledge). Some comparison to larger models / application of this method to such models, is necessary to ensure that the method has any practical purpose.
- There are also no baselines from existing work. There are other forms of fine-tuning, such as adapters, which seek to add additional knowledge to PLMs with less chance of catastrophic forgetting. The authors even cite one of these papers. This is also a confusing oversight, because the many appropriate inline citations which contrast various decisions in this work to decisions in existing work demonstrate a great familiarity with the literation, so lacking any comparison to any existing methods is an odd oversight.
- Some questionable design choices. Perplexity is used as a measure of the model retaining semantic information after fine-tuning, and while that does relate to the original task, there are also aspects of domain drift which are possible and separate from catastrophic forgetting. How are such factors controlled?
- Related, there is questionable motivation. Often when talking about catastrophic forgetting, both the original training and the new task are both relevant. This is clear in the context of robotics, where learning a new behavior should not result in hindering the robot from performing existing behaviors. Is this true in this case? PPL is almost always not a valuable end goal, and the entire PLM/LLM paradigm is built around this notion of pre-training in whatever way leads to learning useful linguistic representations, before fine-tuning, aligning, or few-shotting the model towards the task the user actually cares about. If users never care about both tasks in approximately equal measure, than what good is retaining the original model weights which were not pertinent to the target task?
- There's arguably too much going on here. The focus of the paper aims to be about catastrophic forgetting, but secondary to that, is also the problem of matching the right syntactic fine-tuning task to the right problem. This is not entirely known a priori, so all possible pairings are explored, but realistically a good guess can be made (as it would likely be if pursued in a more practical setting). For instance, it is no surprise that the phrase syntax task helps with key phrase identification. The disadvantage of the exhaustive approach is that it has both distracted from the main takeaway points while cutting into the space available for supporting the main hypothesis.
- No inclusion of baselines from existing work / SOTA on performance tables
- Key extraction F1 results are better than standard optimizers, but negligibly so.
- No discussion of GC vs EWC. When a priori would you choose which method? If the paper included only one such method, traditional optimizers would be the best choice in most situations.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1) p_k symbols are used without definition (tough I think I these are the network predictions p(\hat{y}=k I) 2) the relation of the formula presented to the known EMD is not clear. The latter is a problem solved as linear programming or similar, and not a closed form formula",ICLR_2021_2208,ICLR_2021,"+ Nice idea Consistent improvements over cross entropy for hierarchical class structures Improvements w.r.t other competitors (though not consistent) Good ablation study
The improvements are small The novelty is not very significant
More comments:
Figure 1: - It is not clear what distortion is at this stage - It is not clear what perturbed MNist is, and respectively: why is the error of a 3-layer CNN so high (12-16% error are reported)? CNNs with 2-3 layers can solve MNist with accuracy higher than 99.5%? - This figure cannot be presented on page 2 without proper definitions. It should be either presented on page 5, where the experiment is defined, or better explained
Page 4: It is said that s can be computed efficiently and this is shown in the appendix, but the version I have do not have an appendix Page 6: the XE+EMD method is not present in a comprehensible manner. 1) p_k symbols are used without definition (tough I think I these are the network predictions p(\hat{y}=k I) 2) the relation of the formula presented to the known EMD is not clear. The latter is a problem solved as linear programming or similar, and not a closed form formula 3) it is not clear what the role of \mu is and why can be set to 3 irrespective of the scale of metric D page 7: The experiments show small, but consistent improvements of the suggested method over standard cross entropy, and improvements versus most competitors in most cases
I have read the reviews of others and the author's response. My main impression of the work remains as it was: that it is nice idea with small but significant empirical success. However, my acquaintance with the previous literature in this subject is partial compared to the acquaintance of other reviewers, so It may well be possible that they are in a better position than me to see the incremental nature of the proposed work. I therefore reduce the rating a bit, to become closer to the consensus.","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '4']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
3)Since the cluster structure is defined by the identity. How does the number of images impact the model performance? Do more training images make the performance worse or better ? BYOL in the abstract should be explained for its first appearance.,NIPS_2021_953,NIPS_2021,"Although the paper gives detailed theoretical proof, the experiments are somewhat weak. I still have some concerns: 1）The most related works SwaV and Barlow Twins outperform the proposed method in some experimental results, as shown in Table 1,2,5. What are the main advantages of this method compared with SwaV and Barlow Twins? 2) HSIC(Z, Y) can be seen as a distance metric in the kernel space, where the cluster structure is defined by the identity. Although this paper maps identity labels into the kernel space, the information of one-hot label is somewhat limited compared with views embeddings in Barlow Twins. 3)Since the cluster structure is defined by the identity. How does the number of images impact the model performance? Do more training images make the performance worse or better ?
BYOL in the abstract should be explained for its first appearance.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Albeit the observed effects are strong, it remains unclear “why does the method work?” in particular regarding the L_pixel component. Providing stronger arguments or intuitions of why these particular losses are “bound to help” would be welcome.",NIPS_2020_1821,NIPS_2020,"- To my understanding, the experimental section only compares results generated for this paper. This is good because it keeps apples-to-apples comparisons, however it is suspicious since the task is not novel. Some comparison with results from other works (or a justification of why this is not possible/suitable) would be welcome. For example [2, table 3] seems to have directly comparable results, yet these are nowhere mentioned in this paper. - Albeit the observed effects are strong, it remains unclear “why does the method work?” in particular regarding the L_pixel component. Providing stronger arguments or intuitions of why these particular losses are “bound to help” would be welcome.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- The Related Work section is lacking details. The paragraph on long-context language models should provide a more comprehensive overview of existing methods and their limitations, positioning SSMs appropriately. This includes discussing sparse-attention mechanisms [1, 2], segmentation-based approaches [3, 4, 5], memory-enhanced segmentation strategies [6], and recursive methods [7] for handling very long documents.",NJUzUq2OIi,ICLR_2025,"I found the proposed idea, experiments, and analyses conducted by the authors to be valuable, especially in terms of their potential impact on low-resource scenarios. However, for the paper to fully meet the ICLR standards, there are still areas that need additional work and detail. Below, I outline several key points for improvement. I would be pleased to substantially raise my scores if the authors address these suggestions and enhance the paper accordingly.
**General Feedback**
- I noticed that the title of the paper does not match the one listed on OpenReview.
- The main text should indicate when additional detailed discussions are deferred to the Appendix for better reader guidance. **Introduction**
- The Introduction lacks foundational references to support key claims. Both the second and third paragraphs would benefit from citations to strengthen the arguments. For instance, the statement: ""This method eliminates the need for document chunking, *a common limitation in current retrieval systems that often results in loss of context and reduced accuracy*"" needs a supporting citation to substantiate this point.
- The sentence: ""Second, to be competitive with embedding approaches, a retrieval language model needs to be small"" requires further justification. The authors should include in the paper a complexity analysis comparison discussing time and GPU memory consumption to support this assertion.
**Related Work**
- The sentence ""Large Language Models are found to be inefficient processing long-context documents"" should be rewritten for clarity, for example: ""Large Language Models are inefficient when processing long-context documents.""
- The statements ""Transformer models suffer from quadratic computation during training and linear computation during inference"" and ""However, transformer-based models are infeasible to process extremely long documents due to their linear inference time"" are incorrect. Transformers, as presented in ""Attention is All You Need,"" scale quadratically in both training and inference.
- The statement regarding State Space Models (SSMs) having ""linear scaling during training and constant scaling during inference"" is inaccurate. SSMs have linear complexity for both training and inference. The term ""constant scaling"" implies no dependence on sequence length, which is incorrect.
- The Related Work section is lacking details. The paragraph on long-context language models should provide a more comprehensive overview of existing methods and their limitations, positioning SSMs appropriately. This includes discussing sparse-attention mechanisms [1, 2], segmentation-based approaches [3, 4, 5], memory-enhanced segmentation strategies [6], and recursive methods [7] for handling very long documents.
- Similarly, the paragraph on Retrieval-Augmented Generation should specify how prior works addressed different long document tasks. Examples include successful applications of RAG in long-document summarization [8, 9] and query-focused multi-document summarization [10, 11], which are closely aligned with the present work. **Figures**
- Figures 1 and 2 are clear but need aesthetic improvements to meet the conference's standard presentation quality.
**Model Architecture**
- The description ""a subset of tokens are specially designated, and the classification head is applied to these tokens. In the current work, the classification head is applied to the last token of each sentence, giving sentence-level resolution"" is ambiguous. Clarify whether new tokens are added to the sequence or if existing tokens (e.g., periods) are used to represent sentence ends.
**Synthetic Data Generation**
- The ""lost in the middle"" problem when processing long documents [12] is not explicitly discussed. Have the authors considered the position of chunks during synthetic data generation? Ablation studies varying the position and distance between linked chunks would provide valuable insights into Mamba’s effectiveness in addressing this issue.
- More details are needed regarding the data decontamination pipeline, chunk size, and the relative computational cost of the link-based method versus other strategies.
- The authors claim that synthetic data generation is computationally expensive but provide no supporting quantitative evidence. Information such as time estimates and GPU demand would strengthen this argument and assess feasibility.
- There is no detailed evaluation of the synthetic data’s quality. An analysis of correctness and answer factuality would help validate the impact on retrieval performance beyond benchmark metrics. **Training**
- This section is too brief. Consider merging it with Section 3, ""Model Architecture,"" for a more cohesive presentation.
- What was the training time for the 130M model?
**Experimental Method**
- Fix minor formatting issues, such as adding a space after the comma in "",LVeval.""
- Specify in Table 1 which datasets use free-form versus multiple-choice answers, including the number of answers and average answer lengths.
- Consider experimenting with GPT-4 as a retriever.
- Expand on ""The accuracy of freeform answers is judged using GPT-4.""
- Elaborate on the validation of the scoring pipeline, particularly regarding ""0.942 macro F1."" Clarify the data and method used for validation.
- Justify the selection of ""50 sentences"" for Mamba retrievers and explain chunk creation methods for embedding models. Did the chunks consist of 300 fixed-length segments, or was semantic chunking employed [3, 5]? Sentence-level embedding-based retrieval could be explored to align better with the Mamba setting.
- The assertion that ""embedding models were allowed to retrieve more information than Mamba"" implies an unfair comparison, but more context can sometimes degrade performance [12].
- Clarify the use of the sliding window approach for documents longer than 128k tokens, especially given the claim that Mamba could process up to 256K tokens directly. **Results**
- Remove redundancy in Section 7.1.2, such as restating the synthetic data generation strategies.
- Expand the ablation studies to cover different input sequence lengths during training and varying the number of retrieved sentences to explore robustness to configuration changes.
- Highlight that using fewer training examples (500K vs. 1M) achieved comparable accuracy (i.e., 59.4 vs. 60.0, respectively).
- Why not train both the 130M and 1.3B models on a dataset size of 500K examples, but compare using 1M and 400K examples, respectively? **Limitations**
- The high cost of generating synthetic training data is mentioned but lacks quantification. How computationally expensive is it in terms of time or resources? **Appendix**
- Note that all figures in Appendices B and C are the same, suggesting an error that needs correcting.
**Missing References**
[1] Longformer: The Long-Document Transformer. arXiv 2020.
[2] LongT5: Efficient Text-To-Text Transformer for Long Sequences. NAACL 2022.
[3] Semantic Self-Segmentation for Abstractive Summarization of Long Documents in Low-Resource Regimes. AAAI 2022.
[4] Summ^n: A Multi-Stage Summarization Framework for Long Input Dialogues and Documents. ACL 2022.
[5] Align-Then-Abstract Representation Learning for Low-Resource Summarization. Neurocomputing 2023.
[6] Efficient Memory-Enhanced Transformer for Long-Document Summarization in Low-Resource Regimes. Sensors 2023.
[7] Recursively Summarizing Books with Human Feedback. arXiv 2021.
[8] DYLE: Dynamic Latent Extraction for Abstractive Long-Input Summarization. ACL 2022.
[9] Towards a Robust Retrieval-Based Summarization System. arXiv 2024.
[10] Discriminative Marginalized Probabilistic Neural Method for Multi-Document Summarization of Medical Literature. ACL 2022.
[11] Retrieve-and-Rank End-to-End Summarization of Biomedical Studies. SISAP 2023.
[12] Lost in the Middle: How Language Models Use Long Contexts. TACL 2024.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Dependent on the training data - I'm unsure if 44k dialogues is sufficient to capture a wide range of user traits and personalities across different content topics. LLMs are typically trained on trillions of tokens, I do not see how 44k dialogues can capture the combinations of personalities and topics. In theory, this dataset also needs to be massive to cover varied domains.",eCXfUq3RDf,EMNLP_2023,"1. Very limited reproducibility - Unless the authors release their training code, dialogue dataset, as well as model checkpoints, I find it very challenging to reproduce any of the claims in this paper. I encourage the authors to attach their code and datasets via anonymous repositories in the paper submission so that reviewers may verify the claims and try out the model for themselves.
2. Very high model complexity - The proposed paper employs a mathematically and computationally complex approach as compared to the textual input method. Does the proposed method outperform sending textual inputs to a large foundation model such as LLAMA or ChatGPT? The training complexity seems too high for any practical deployment of this model.
3. Dependent on the training data - I'm unsure if 44k dialogues is sufficient to capture a wide range of user traits and personalities across different content topics. LLMs are typically trained on trillions of tokens, I do not see how 44k dialogues can capture the combinations of personalities and topics. In theory, this dataset also needs to be massive to cover varied domains.
4. The paper is hard to read and often unintuitive. The mathematical complexity must be simplified and replaced with more intuitive design and modeling choice explanations so that readers may grasp core ideas faster.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"• I’m not convinced that the binary classification is a justifiable baseline metrics. While I agree with the TAL task is really important here and a good problem to solve, I’m not sure how coarse grained binary classification can assess models understanding of fine-grained error like technique error.",Uj2Wjv0pMY,ICLR_2024,"•	Compared to Assembly 101 (error detection), the paper seems like an inferior / less complicated dataset. Claims like higher ratio of error to normal videos needs to be validated.
•	Compared to datasets, the dataset prides itself on adding different modalities especially depth channel (RGB-D). The paper fails to validate the necessities of such modality. One crucial different between assembly dataset is use of depth values. What role does it play in training baseline models? Does it boost the model’s performance if these weren’t present. In current deep learning area, depth channels should be reasonably be producible via the help of existing models.
•	I’m not convinced that the binary classification is a justifiable baseline metrics. While I agree with the TAL task is really important here and a good problem to solve, I’m not sure how coarse grained binary classification can assess models understanding of fine-grained error like technique error.
•	Timing Error (Duration of an activity) and Temperature based error, does these really need ML based solutions? In sensitive tasks, simple sensor reading can indicate error. I’m not sure testing computer vision models on such tasks is justifiable. These require more heuristics-based methods, working with if-else statement.
•	Procedure Learning: its very vaguely defined, mostly left unexplained and seems like an after thought. I recommend authors devote passage to methods “M1 (Dwibedi et al., 2019)” and “M2 (Bansal, Siddhant et al., 2022)”. In Table 5, value of lambda? Is not mentioned.
•	The authors are dealing with a degree of subjectivity in terms of severity of errors. It would have been greatly beneficial, if the errors could be finely measured. For example if the person uses a tablespoon instead of teaspoon, is still an error? Some errors are more grave than others, is there a weighted scores? Is there a way to measure level of deviation for each type of error or time stamp of occurrence of error. Is one recipe more difficult than the other recipe.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too.",NIPS_2019_653,NIPS_2019,"of the method. Clarity: The paper has been written in a manner that is straightforward to read and follow. Significance: There are two factors which dent the significance of this work. 1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too. 2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The writing should be improved. Some points in the paper is unclear to me.,NIPS_2018_857,NIPS_2018,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo? [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018. ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
1. Authors are suggested to use other metrics to evaluate the Results (e.g. BERTScore).,9Ax0pyaLgh,EMNLP_2023,"1. Authors are suggested to use other metrics to evaluate the Results (e.g. BERTScore).
2. Often it is not sufficient to show automatic evaluation results. The author does not show any human evaluation results and does not even perform a case study and proper error analysis. This does not reflect well on the qualitative aspects of the proposed model.
3. It is difficult to understand the methodology without Figure 1. Parts of section 2 should be written in alignment with Figure 1, and the authors are expected to follow a step-by-step description of the proposed method. (See questions to authors)","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"4. Additionally, there has been a large amount of work on LLM evaluation [2]. While some of the metrics there do not satisfy the proposed desiderata, it would still be good to see how SynTextBench metric compares to the other metrics proposed in the literature. Concretely, from the paper, it is hard to understand under what conditions should one use SynTextBench over other metrics (eg: say MMLU / Big Bench for language generation).",6iM2asNCjK,ICLR_2024,"1. My primary concern is with the limited scope of the paper. The paper primarily considers only evaluating sentence embeddings from LLMs, which while important, is a small part of the overall evaluation landscape of LLMs. Consequently, the title ""Robustness-Accuracy characterization of Large Language Models using synthetic datasets"" is somewhat misleading. Furthermore, the generation methodology for the synthetic tasks using SentiWordNet for polarity detection does seem somewhat restrictive. For sentence embedding evaluation, it does seem to be a good methodology, but it is not clear how well it would generalize to any generative tasks (e.g. question answering, summarization, etc.). Whether this metric can be leveraged for other tasks (especially for a different class of tasks) needs to be demonstrated in my opinion.
2. While the proposed methodology of using a ratio of positive / negative to neutral sentiment words is a good way of defining difficulty, it does seem somewhat restrictive given the contextual nature of languages. Interesting linguistic phenomena such as sarcasm, irony, etc. are not captured by the proposed methodology, which arguably form for a large part of the difficulty in language understanding especially for such large LLMs. While the authors briefly touch upon the issue of negation, negation in natural language is not limited to structured rules, and any methodology testing the robustness of LLMs should provide a way of capturing this, given that LLMs are generally have a poor understanding of negations ([1]).
3. The baseline metrics are still computed on the synthetic dataset. For a generative LLM model training for example, this potentially results in bad sentence embeddings, which subsequently may result in bad task performance. This is especially problematic when done for a single dataset (as is the case for all the baseline metrics). In contrast, the proposed SynTextBench benefits from aggregating across different difficulty levels, and is somewhat more robust to this issue compared to the baseline metrics. A better way for considering the baselines might be to treat them in the same way as SynTextBench is treated (aggregated across different difficulty levels, thresholded for some value of the metric, and then computing the area under the curve).
4. Additionally, there has been a large amount of work on LLM evaluation [2]. While some of the metrics there do not satisfy the proposed desiderata, it would still be good to see how SynTextBench metric compares to the other metrics proposed in the literature. Concretely, from the paper, it is hard to understand under what conditions should one use SynTextBench over other metrics (eg: say MMLU / Big Bench for language generation).","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1)Less Novelty: The algorithm for construction of coresets itself is not novel. Existing coreset frameworks for classical k-means and (k,z) clusterings are extended to the kernelized setting.",ICLR_2022_2425,ICLR_2022,"1)Less Novelty: The algorithm for construction of coresets itself is not novel. Existing coreset frameworks for classical k-means and (k,z) clusterings are extended to the kernelized setting. 2)Clarity: Since the coreset construction algorithm is built up on previous works, a reader without the background in literature on coresets would find it hard to understand why the particular sampling probabilities are chosen and why they give particular guarantees. It would be useful rewrite the algorithm preview and to give at least a bit of intuition on how the importance sampling scores are chosen and how they can give the coreset guarantees Suggestions:
In the experiment section, other than uniform sampling, it would be interesting to use some other classical k-means coreset as baselines for comparison.
Please highlight the technical challenges and contributions clearly when compared to coresets for classical k-means.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '1', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"- After having read the other reviews and the author responses, I decide to maintain my initial rating (6). The contribution of this work is mostly empirical. The stronger results compared to more complex models and the promise to release the code imply that this work deserves to be known, even if fairly incremental.",NIPS_2019_203,NIPS_2019,"* Technical innovation is fairly limited. The bLVNet is a straightforward extension of bLNet (an image model) to video. The TAM involves the use of 1D temporal convolution and depthwise convolution. Both mechanisms that have been widely leveraged before. On the other hand, the paper does not make bold novelty claims and recognizes the contribution as being more empirical than technical. The TAM shares many similarities with Timeception [Hussein et al., CVPR 19], which was not yet published at the time of this submission and thus does not diminish the value of this work. Nevertheless, given the many analogies between these concurrent approaches, it'd be advisable to discuss their relations in future versions (or the camera-ready version) of the paper. * While the memory/efficiency gains are convincingly demonstrated, they are not substantial enough to be a game-changer in the practice of training video understanding models. Due to the overhead of setting up the proposed framework (even though quite simple), adoption by the community may be fairly limited. Final rating: - After having read the other reviews and the author responses, I decide to maintain my initial rating (6). The contribution of this work is mostly empirical. The stronger results compared to more complex models and the promise to release the code imply that this work deserves to be known, even if fairly incremental.","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['X', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['2', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['0', '1']}",,
1: Poor writing and annotations are a little hard to follow.,NIPS_2018_245,NIPS_2018,"Weakness] 1: Poor writing and annotations are a little hard to follow. 2: Although applying GCN on FVQA is interesting, the technical novelty of this paper is limited. 3: The motivation is to solve when the question doesn't focus on the most obvious visual concept when there are synonyms and homographs. However, from the experiment, it's hard to see whether this specific problem is solved or not. Although the number is better than the previous method, it will be great if the authors could product more experiments to show more about the question/motivation raised in the introduction. 4: Following 3, applying MLP after GCN is very common, and I'm not surprised that the performance will drop without MLP. The authors should show more ablation studies on performance when varying the number of facts retrieval, what happened if we different number of layer of GCN?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['X', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- In Table 2, for the proposed method, only 8 of the total 14 evaluation metrics achieve SOTA performances. In addition, under the setting of ""Twitter-2017 $\rightarrow$ Twitter-2015"", why the proposed method achieves best overall F1, while not achieves best F1 in all single types?",WC9yjSosSA,EMNLP_2023,"- The reported experimental results cannot strongly demonstrate the effectiveness of the proposed method.
- In Table 1, for the proposed method, only 6 of the total 14 evaluation metrics achieve SOTA performances.
- In Table 2, for the proposed method, only 8 of the total 14 evaluation metrics achieve SOTA performances. In addition, under the setting of ""Twitter-2017 $\rightarrow$ Twitter-2015"", why the proposed method achieves best overall F1, while not achieves best F1 in all single types?
- In Table 3, for the proposed method, 9 of the total 14 evaluation metrics achieve SOTA performances, which means that when ablating some modules, the performance of the proposed method will improve. Furthermore, The performance improvement that adding a certain module can bring is not obvious.
- In line 284, a transformer layer with self-attention is used to capture the intra-modality relation for the test modality. However, there're a lot of self-attention transformer layers in BERT. Why not using the attention scores in the last self-attention transformer layer?
- In line 322, softmmax -> softmax
- Will the coordination of $b_d$ exceed the scope of the patches?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- Why do you only consider ECG segments with one label assigned to them? I would expect that the associated reports would be significantly easier than including all reports.,ICLR_2022_2531,ICLR_2022,"I have several concerns about the clinical utility of this task as well as the evaluation approach.
- First of all, I think clarification is needed to describe the utility of the task setup. Why is the task framed as generation of the ECG report rather than framing the task as multi-label classification or slot-filling, especially given the known faithfulness issues with text generation? There are some existing approaches for automatic ECG interpretation. How does this work fit into the existing approaches? A portion of the ECG reports from the PTB-XL dataset are actually automatically generated (See Data Acquisition under https://physionet.org/content/ptb-xl/1.0.1/). Do you filter out those notes during evaluation? How does your method compare to those automatically generated reports? - A major claim in the paper is that RTLP generates more clinically accurate reports than MLM, yet the only analysis in the paper related to this is a qualitative analysis of a single report. A more systematic analysis of the quality of generation would be useful to support the claim made in the appendix. Can you ask clinicians to evaluate the utility of the generated reports or evaluate clinical utility by using the generated reports to predict conditions identifiable from the ECG? I think that it’s fine that the RTLP method performs comparable to existing methods, but I am not sure from the current paper what the utility of using RTLP is. - More generally, I think that this paper is trying to do two things at once – present new methods for multilingual pretraining while also developing a method of ECG captioning. If the emphasis is on the former, then I would expect to see evaluation against other multilingual pretraining setups such as the Unicoder (Huang 2019a). If the core contribution is the latter, then clinical utility of the method as well as comparison to baselines for ECG captioning (or similar methods) is especially important. - I’m a bit confused as to why the diversity of the generated reports is emphasized during evaluation. While I agree that the generated reports should be faithful to the associated ECG, diversity may not actually be necessary metric to aim for in a medical context. For instance, if many of the reports are normal, you would want similar reports for each normal ECG (i.e. low diversity). - My understanding is that reports are generated in other languages using Google Translate. While this makes sense to generate multilingual reports for training, it seems a bit strange to then evaluate your model performance on these silver-standard noisy reports. Do you have a held out set of gold standard reports in different languages for evaluation (other than German)?
Other Comments: - Why do you only consider ECG segments with one label assigned to them? I would expect that the associated reports would be significantly easier than including all reports. - You might consider changing the terminology from “cardiac arrythmia” categories to something broader since hypertrophy (one of the categories) is not technically a cardiac arrythmia (although it can be detected via ECG & it does predispose you to them) - I think it’d be helpful to include an example of some of the tokens that are sampled during pretraining using your semantically similar strategy for selecting target tokens. How well does this work in languages that have very different syntactic structures compared to the source language? - Do you pretrain the cardiac signal representation learning model on the entire dataset or just the training set? If the entire set, how well does this generalize to setting where you don’t have the associated labels? - What kind of tokenization is used in the model? Which Spacy tokenizer? - It’d be helpful to reference the appendix when describing the setup in section 3/5 so that the reader knows that more detailed architecture information is there. - I’d be interested to know if other multilingual pretraining setups also struggle with Greek. - It’d be helpful to show the original ECG report with punctuation + make the ECG larger so that they are easier to read - Why do you think RTLP benefits from fine-tuning on multiple languages, but MARGE does not?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions:,NIPS_2016_238,NIPS_2016,"- My biggest concern with this paper is the fact that it motivates âdiversityâ extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity. - The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions: - The first sentence of the abstract needs to be re-written. - Diversity should be toned down. - line 108, the first âfâ should be âgâ in âwe fixed the form of ..â - extra â.â in the middle of a sentence in line 115. One Question: For the baseline MCL with deep learning, how did the author ensure that each of the networks have converged to a reasonable results. Cutting the learners early on might significantly affect the ensemble performance.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
1.Limited Discussion of Scalability Bounds:The paper doesn't thoroughly explore the upper limits of FedDES's scalability；No clear discussion of memory requirements or computational complexity.,pZk9cUu8p6,ICLR_2025,"1.Limited Discussion of Scalability Bounds:The paper doesn't thoroughly explore the upper limits of FedDES's scalability；No clear discussion of memory requirements or computational complexity.
2.Validation Scope:Evaluation focuses mainly on Vision Transformer with CIFAR-10;Could benefit from testing with more diverse models and datasets; Limited exploration of edge cases or failure scenarios
3.Network Modeling:While network delays are considered, there's limited discussion of complex network topologies or dynamic network conditions; The paper could benefit from more detailed analysis of how network conditions affect simulation accuracy","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"- The authors may want to generate instances with more constraints and variables, as few instances in the paper have more than 7 variables. Thus, this raises my concern about LLMs' ability to model problems with large instance sizes.",fsDZwS49uY,ICLR_2025,"- The authors may want to generate instances with more constraints and variables, as few instances in the paper have more than 7 variables. Thus, this raises my concern about LLMs' ability to model problems with large instance sizes.
- Given that a single optimization problem can have multiple valid formulations, it would be beneficial for the authors to verify the accuracy and equivalence of these formulations with ground-truth ones.
- There are questions regarding the solving efficiency of the generated codes. It would be valuable to assess whether the code produced by LLMs can outperform human-designed formulations and codes.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Results presentation can be improved. For example, in Figure 2 and 3, the y-axis is labeled as “performance” which is ambiguous, and the runtime is not represented in those figure. A scatter plot with x/y axes being runtime/performance could help the reader better understand and interpret the results. Best results in tables can also be highlighted. Minor:",RsnWEcuymH,ICLR_2024,"- My main concern is that the performance improvement, though generally better, is not particularly too significant, not to mention that those proxy-based method achieves also pretty good IM results while using only a negligible amount of time compared to BOIM (or other simulation-based method in general)
- Other choices of graph kernel are not considered and experimented with such as random walk or Graphlet kernel? There are probably easy tricks to turn them into valid GP kernels.
- Despite the time reduction introduced by BOIM, proxy-based methods are still substantially cheaper. Would it be possible to use proxy-based methods as heuristics to seed BOIM or other zero-order optimization method (e.g., CMA-ES).
- While the author has shown that GSS has theoretically lower variance, it’d be nice to compare against with random sampling and check empirically how well it performs.
- Results presentation can be improved. For example, in Figure 2 and 3, the y-axis is labeled as “performance” which is ambiguous, and the runtime is not represented in those figure. A scatter plot with x/y axes being runtime/performance could help the reader better understand and interpret the results. Best results in tables can also be highlighted. Minor:
- Typo in Section 2: “Mockus (1998) and has since become…” → “Mockus (1998) has since become…”","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2)“NodeSort differentially sorts nodes depending on the base node.” Does this mean that the base node affects the ordering, affects the key nodes for attention, and further affects the model performance?",ICLR_2023_3063,ICLR_2023,"The novelty and technical contribution are limited.
It is unclear for the deformable graph attention module.
It is unclear why the proposed method has lower computational complexity.
Detailed comments:
What is the motivation to choose personalized pagerank score, bfs, and feature similarity as sorting criteria?
For NodeSort, 1) how to choose the base node, or is every node a base node? 2)“NodeSort differentially sorts nodes depending on the base node.” Does this mean that the base node affects the ordering, affects the key nodes for attention, and further affects the model performance? 3)After getting the sorted node sequence, how to sample the key nodes for each node? And how many key nodes are sampled？is the number of key nodes a hyper-parameter? 4)What are the Value nodes used in Transformer in this paper? 5)How to fuse node representations generated by attention for different ranking criteria.
Intuitively, the design of deformable graph attention is complicated, and the Katz positional encoding involves the exponentiation of adjacency matrix, so Is the computational complexity really reduced? Where can the reduction in complexity be explained from the proposed method compared to baselines? or just from the sparse implementation?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Why is the arrow in Figure 2 from a Gaussian space into the latent space, rather than from the latent space to n^(i)? I thought the main purpose was to influence n^(i)?",ICLR_2022_176,ICLR_2022,"There are two main (and easily fixable) weaknesses.
a) I think the role of the normalizing flow is underexplained. It is stated multiple times that the normalizing flow provides the evidence updates and its purpose is to estimate epistemic uncertainty. The remaining questions for me are 1. From which space to which does the NF map the latent variable z? 2. Why is the arrow in Figure 2 from a Gaussian space into the latent space, rather than from the latent space to n^(i)? I thought the main purpose was to influence n^(i)? 3. Which experiments show that the normalizing flow contributes meaningfully to the epistemic uncertainty (see b))?
b) Figure 1 does a good job of showing the intuition behind NatPNs but it lacks some components and a discussion in the text. The authors choose to show aleatoric (un-)certainty and predictive certainty respectively but don’t show epistemic (un-)certainty. Technically, you could deduce epistemic uncertainty from aleatoric and predictive uncertainty but it would be easier to compare and follow your argument if it was made explicit. Furthermore, I would like to see an explicit discussion of the results. Why, for example, is the difference between aleatoric and predictive uncertainty so low? Is there no or little epistemic uncertainty in this setting? There are two things that would convince me more regarding this problem: a) an additional toy experiment similar to Figure 1 which includes more epistemic uncertainty, e.g. with fewer data points. This could show that the epistemic uncertainty is well-calibrated. b) An argument for why the epistemic uncertainty is (presumably) so low in your setting. a) and b) are not mutually exclusive, doing both would convince me more.
There are a couple of minor improvements: Figure 1 is not referenced in the main text. I find it hard to spot the difference w.r.t the symbols in Figure 1. Maybe just making it less crowded would already improve visibility. In the last paragraph of 3.1, you mention “warm-up” and “fine-tuning”. It would be helpful to explain these concepts briefly in one additional sentence or provide references.
What would raise my score?
I would raise my score by 1 or 2 points if my main weaknesses are well addressed or if evidence is provided that my criticism is the consequence of a misunderstanding.
I would raise my score even further if I’m convinced of the high significance of this work. This will be mostly dependent on the estimate of more expert reviewers but I’m also open to arguments by the authors.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
4. Many abbreviations lack definition and cause confusion. ‘AR’ in Table 5 stands for domain adaptation tasks and algorithms.,ICLR_2023_1946,ICLR_2023,"Weakness: 1. This work raises an essential issue in partial domain adaptation and evaluates many PDA algorithms and model selection strategies. However, it does not present any solution to this problem. 2. The findings of the experiments are a bit trivial. No target label for model selection strategies will hurt the performance, and the random seed would influence the performance. They are commonsense in domain adaptation, even in deep learning. 3. The writing needs to improve. The Tables are referenced but always placed on different pages. For example, Table 2 is referred in page 4 but placed on page 3, making it hard to read. The paper also has many typos, e.g., ‘that’ instead of ‘than’ in section 3. 4. Many abbreviations lack definition and cause confusion. ‘AR’ in Table 5 stands for domain adaptation tasks and algorithms. 5. In section 4.2, heuristic strategies for Hyper-parameter turning is not clearly described. And the author said, “we only consider the model at the end of training”, but should we use the model selection strategies? 6. In section 5.2, part of Model Selection Strategies, the authors give a conclusion that seems to be wrong “only the JUMBOT and SND pair performed reasonably well with respect to the JUMBOT and ORACLE pair on both datasets.” In Table 6, the JUMBOT and SND pair performs worse than the JUMBOT and ORACLE pair by a large margin. For instance, on OFFICE-HOME, the JUMBOT and SND pair reaches 72.29 accuracies, while the JUMBOT and ORACLE pair achieves 77.15 accuracies.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"4. Although using advantage instead of q value is more common in practice, I'm wondering if there is other technical consideration for conducting the analysis with advantage instead of q value.",NIPS_2020_1477,NIPS_2020,"1. I think it is a bit overstated (Line 10 and Line 673) to use the term \epsilon-approximate stationary point of J -- there is still function approximation error as in Theorem 4.5. I think the existence of this function approximation error should be explicitly acknowledged whenever the conclusion about sample complexity is stated. Otherwise readers may have the impression that compatible features (Konda, 2002, Sutton et al 2000) are used to deal with these errors, which are not the case. 2. As shown by Konda (2002) and Sutton et al (2000), compatible features are useful tools to address the function approximation error of the critic. I'm wondering if it's possible to introduce compatible features and TD(1) critic in the finite sample complexity analysis in this paper to eliminate the \epsilon_app term. 3. I feel the analysis in the paper depends heavily on the property of the stationary distribution (e.g., Line 757). I'm wondering if it's possible to conduct a similar analysis for the discounted setting (instead of the average reward setting). Although a discounted problem can be solved by methods for the average reward problem (e.g., discarding each transition w.p. 1 - \gamma, see Konda 2002), solving the discounted problem directly is more common in the RL community. It would be beneficial to have a discussion w.r.t. the discounted objective. 4. Although using advantage instead of q value is more common in practice, I'm wondering if there is other technical consideration for conducting the analysis with advantage instead of q value. 5. The assumption about maximum eigenvalues in Line 215 seems artificial. I can understand this assumption, as well as the projection in Line 8 in Algorithm 1, is mainly used to ensure the boundedness of the critic. However, as in Line 219, R_w indeed depends on \lambda, which we do not know in practice. So it means we cannot implement the exact Algorithm 1 in practice. Instead of using this assumption and the projection, is it possible to use regularization (e.g., ridge) for the critic to ensure it's bounded, as done in asymptotic analysis in Zhang et al (2020)? Also Line 216 is a bit misleading. Only the first half (negative definiteness) is used to ensure solvability. But as far as I know, in policy evaluation setting, we do not need the second half (maximum eigenvalue). 6. Some typos: Line 463 should include \epsilon_app and replace the first + with \leq \epsilon_app (the last term of Line 587) is missing in Line 585 and 586 There shouldn't be (1 - \gamma) in Line 589 In Line 618, there should be no need to introduce the summation from k=0 to t - \tau_t, as the summation from k=\tau_t to t is still used in Line 624. In Line 625, it should be \tau_t instead of \tau In Line 640, I personally think it's not proper to cite [25] (the S & B book) -- that book includes too many. Referring to the definition of w^* should be more easy to follow. In Line 658, it should be ||z_k||^2 In Line 672, \epsilon_app is missing In Line 692, it should be E[....] = 0 In Line 708, there shouldn't be \theta_1, \theta_2, \eta_1, \eta_2 In Line 774, I think expectation is missing in the LHS Konda, V. R. Actor-critic algorithms. PhD thesis, Massachusetts Institute of Technology, 2002. Zhang, S., Liu, B., Yao, H., & Whiteson. Provably Convergent Two-Timescale Off-Policy Actor-Critic with Function Approximation. ICML 2020.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The setting of Unsupervised Online Adaptation is a little bit strange. As described in Sec 3.1, the model requires a training set, including documents, quires and labels. It seems that the adaptation process is NOT ""Unsupervised"" because the training set also requires annatations.",jPrl18r4RA,EMNLP_2023,"1. The setting of Unsupervised Online Adaptation is a little bit strange. As described in Sec 3.1, the model requires a training set, including documents, quires and labels. It seems that the adaptation process is NOT ""Unsupervised"" because the training set also requires annatations.
2. The problem that this paper focuses on may be unrealistic. Figure 8 shows that adapting to test documents leads to performance degradation on unrelated queries. In practice, we expect to update the knowledge of Large Language Models without affecting the performance on general tasks. Besides, existing large scale QA systems, e.g. GPT-4, show strong In-Context Learning abilities. In other words, the model can reason about new documents and answer questions that have never been seen before.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['sBdHWtl1', 'boda'], 'labels': ['1', '1']}",,
"- For effectiveness, the performance comparison in Table 1 is unfair. VINS sets different sample weights W u i in the training process, while most compared baselines like DNS, AOBPR, SA, PRIS set all sample weights as 1.",ICLR_2022_1923,ICLR_2022,"Weakness: 1. The novelty of this paper is limited. First, the analysis of the vertex-level imbalance problem is not new, which is a reformulation of the observations in previous works [Rendle and Freudenthaler, 2014; Ding et al., 2019]. Second, the designed negative sampler uses reject sampling to increase the chance of popular items, which is similar to the proposed one in PRIS [Lian et al., 2020]. 2. The paper overclaims on its ability of debiasing sampling. The “debiased” term in the paper title is confusing. 3. The methodology detail is unclear in Sec. 4.2. The proposed design that improves sampling efficiency seems interesting but the corresponding description is hard to follow given the limited space. 4. Space complexity of the proposed VINS should also be analyzed and compared in empirical studies, given that each (u, i) corresponds to a b u f f e r u i
. 5. Experiment results are not convincing enough to demonstrate the superiority of VINS on effectiveness and efficiency. - For effectiveness, the performance comparison in Table 1 is unfair. VINS sets different sample weights W u i
in the training process, while most compared baselines like DNS, AOBPR, SA, PRIS set all sample weights as 1. - For efficiency, Table 2 should also include the theoretical analysis for contrast.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning,NIPS_2019_1366,NIPS_2019,"Weakness: - Although the method discussed by the paper can be applied in general MDP, the paper is limited in navigation problems. Combining RL and planning has already been discussed in PRM-RL~[1]. It would be interesting whether we can apply such algorithms in more general tasks. - The paper has shown that pure RL algorithm (HER) failed to generalize to distance goals but the paper doesn't discuss why it failed and why planning can solve the problem that HER can't solve. Ideally, if the neural networks are large enough and are trained with enough time, Q-Learning should converge to not so bad policy. It will be better if the authors can discuss the advantages of planning over pure Q-learning. - The time complexity will be too high if the reply buffer is too large. [1] PRM-RL: Long-range Robotic Navigation Tasks by Combining Reinforcement Learning and Sampling-based Planning","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2.It is better for authors to display the performance of accelerating SGMs by involving some other baselines with a different perspective, such as “optimizing the discretization schedule or by modifying the original SGM formulation” [16, 15, 23, 46, 36, 31, 37, 20, 10, 25, 35, 45]",NIPS_2022_1807,NIPS_2022,"Weakness:
1.The authors should provide more descriptions of the wavelet transforms in this paper. It is hard for me to understand the major idea in this paper before learning some necessary knowledge about wavelet whitening, wavelet coefficient, and so on.
2.It is better for authors to display the performance of accelerating SGMs by involving some other baselines with a different perspective, such as “optimizing the discretization schedule or by modifying the original SGM formulation” [16, 15, 23, 46, 36, 31, 37, 20, 10, 25, 35, 45]","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
3. A brief conclusion of the article and a summary of this paper's contributions need to be provided.,ICLR_2021_243,ICLR_2021,"Weakness: 1. As several modifications mentioned in Section 3.4 were used, it would be better to provide some ablation experiments of these tricks to validate the model performance further. 2. The model involves many hyperparameters. Thus, the selection of the hyperparameters in the paper needs further explanation. 3. A brief conclusion of the article and a summary of this paper's contributions need to be provided. 4. Approaches that leveraging noisy label noise label regularization and multi-label co-regularization were not reviewed or compared in this paper.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The synthetic experiment in a non-separable case seems to be a problem. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model?",ICLR_2022_1838,ICLR_2022,"1. When introducing the theoretical results, we should make a detailed comparison with the existing cross-entropy loss results. The current writing method cannot reflect the advantages of square loss. 2. The synthetic experiment in a non-separable case seems to be a problem. Considering the nonlinear expression ability of neural networks, how to explain that the data distribution illustrated in Figure 1 is inseparable from the network model? 3. This paper presents that the loss functions like hinge loss don’t provide reliable information on the prediction confidence. In this regard, there is a lack of references to some relevant literature. [Gao, 2013] has given a detailed analysis of the advantages and disadvantages between the entire margin distribution and the minimum margin. Based on this, [Lyu, 2018] designed a square-type margin distribution loss to improve the generalization ability of DNN.
[Gao, 2013] W. Gao and Z.-H. Zhou. On the doubt about margin explanation of boosting. Artificial Intelligence 203:1-18 2013.
[Lyu, 2018] Shen-Huan Lyu, Lu Wang, and Zhi-Hua Zhou. Improving Generalization of Neural Networks by Leveraging Margin Distribution. http://arxiv.org/abs/1812.10761","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"4: Even though authors compare their framework with an advanced defense APE-GAN, they can further compare the proposed framework with a method that is designed to defend against multiple attacks (maybe the research on defense against multiple attacks is relatively rare). The results would be more meaningful if the authors could present this comparison in their paper. Overall the paper presents an interesting study that would be useful for defending the threat of increasing malicious perturbations.",ICLR_2021_2717,ICLR_2021,"1: The writing could be further improved, e.g., “via being matched to” should be “via matching to” in Abstract.
2: The “Def-adv” needs to be clarified.
3: The accuracies of the target model using different defenses against the FGSM attack are not shown in Figure 1. Hence, it is unclear the difference between the known attacks and the unknown attacks.
4: Even though authors compare their framework with an advanced defense APE-GAN, they can further compare the proposed framework with a method that is designed to defend against multiple attacks (maybe the research on defense against multiple attacks is relatively rare). The results would be more meaningful if the authors could present this comparison in their paper.
Overall the paper presents an interesting study that would be useful for defending the threat of increasing malicious perturbations.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- The results are presented in a convoluted way. In particular, the results disregard the safety violations of the agent in the first 1000 episodes. The reason for presenting the results in this way is unclear.",82VzAtBZGk,ICLR_2025,"The problem formulation is incomplete. The paper does not define the safety properties expected from the RL agent.
- Lack of theoretical results. This paper provides only empirical results to support its claims.
- The results are presented in a convoluted way. In particular, the results disregard the safety violations of the agent in the first 1000 episodes. The reason for presenting the results in this way is unclear.
- The presentation of the DDPG-Lag as a constrained RL algorithm is imprecise, as it uses a fixed weight for the costs, which works as simple reward engineering. In general, with a Lagrangian relaxation, this weight should be adjusted online to ensure the accumulated cost stays below a predefined threshold [1].
- The evaluation in CMDPs is inconsistent. These approaches solve different problems where a predefined accumulated cost is allowed.
- Weak baseline. From the results in Figure 10, it is clear that Tabular Shield does not recognize any unsafe state-action pairs, making it an unsuitable baseline. This is not surprising considering how the state-action space is discretized. Perhaps it is necessary to finetune the discretization of this baseline. Alternatively, it would be more suitable to consider stronger baselines, such as the accumulating safety rules [2] **references**
- [1] Ray, A., Achiam, J., and Amodei, D. (2019). *Benchmarking safe exploration in deep reinforcement learning*. <https://github.com/openai/safety-gym>
- [2] Shperberg, S. S., Liu, B., Allievi, A., and Stone, P. (2022). A rule-based shield: Accumulating safety rules from catastrophic action effects. *CoLLAs*, 231–242. <https://proceedings.mlr.press/v199/shperberg22a.html>","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '2']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.,NIPS_2017_110,NIPS_2017,"of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail. This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices. In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times. Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful. Lastly, the details of the experiment are lacking. In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.
Specific comments:
- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
- l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
- Throughout, the authors use the term constrains and should change to constraints.
- l124: What is meant by the (*)?
- l134: Do the authors mean m=2?
- l148: known, instead of know
- l156: please define \gamma_0^{***}
- Figure 1: Please specify the meaning of the colors in the caption as well as the text.
- l280: ""Then we made it explicit"" instead of ""Then we have explicit it""","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"5)There are some writing errors in the paper, such as ""informative informative"" on page 5 and ""performance"" on page 1, which lacks a title.",ICLR_2023_1980,ICLR_2023,"Motivated by the fact that local learning can limit memory when training the network and the adaptive nature of each individual block, the paper extends local learning to the ResNet-50 to handle large datasets. However, it seems that the results of the paper do not demonstrate the benefits of doing so. The detailed weaknesses are as follows: 1)The method proposed in the paper essentially differs very little from the traditional BP method. The main contribution of the paper is adding the stop gradient operation between blocks, which appears to be less innovative. 2)The local learning strategy is not superior to the BP optimization method. In addition, the model is more sensitive to each block after the model is blocked, especially the first block. More additional corrections are needed to improve the performance and robustness of the model, although still lower than BP's method. 3)Experimental results show that simultaneous blockwise training is better than sequential blockwise training. But the simultaneous blockwise training strategy cannot limit memory. 4)The blockwise training strategy relies on a special network structure like the block structure of the ResNet-50 model. 5)There are some writing errors in the paper, such as ""informative informative"" on page 5 and ""performance"" on page 1, which lacks a title.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- Statement on line 134: Only true for standard sigmoid [1+exp(-x)]^-1, depends on max. slope - Theorem 4.1: Would be useful to elaborate a bit more in the main text why this holds (intuitively, since the RNN unlike the URNN will converge to the nearest FP).",NIPS_2019_1377,NIPS_2019,"- The proof works only under the assumption that the corresponding RNN is contractive, i.e. has no diverging directions in its eigenspace. As the authors point out (line #127), for expansive RNN there will usually be no corresponding URNN. While this is true, I think it still imposes a strong limitation a priori on the classes of problems that could be computed by an URNN. For instance chaotic attractors with at least one diverging eigendirection are ruled out to begin with. I think this needs further discussion. For instance, could URNN/ contractive RNN still *efficiently* solve some of the classical long-term RNN benchmarks, like the multiplication problem? Minor stuff: - Statement on line 134: Only true for standard sigmoid [1+exp(-x)]^-1, depends on max. slope - Theorem 4.1: Would be useful to elaborate a bit more in the main text why this holds (intuitively, since the RNN unlike the URNN will converge to the nearest FP). - line 199: The difference is not fundamental but only for the specific class of smooth (sigmoid) and non-smooth (ReLU) activation functions considered I think? Moreover: Is smoothness the crucial difference at all, or rather the fact that sigmoid is truly contractive while ReLU is just non-expansive? - line 223-245: Are URNN at all practical given the costly requirement to enforce the unitary matrix after each iteration?","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems.",NIPS_2021_2304,NIPS_2021,"There are four limitations: 1. In this experiment, single dataset training and single dataset testing cannot verify the generalizable ability of models, it should conduct experiments on large-scale datasets. 2. The efficiency of such pairwise matching is very low, making it difficult to be used in practical application systems. 3. I hope to see that you can compare your model with ResNet-IBN / ResNet of FastReID, which is practical work in the person Reid task. 4. I think the authors only use the transformer to achieve the local matching, therefore, the contribution is limited.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"* Minor weaknesses The allocation of Figure 1 is too naive. Overall, you could have edited the space of main paper more wisely.",rYhDcQudVI,ICLR_2024,"The methodology appears incremental, building marginally upon JEM's foundation of interpreting classifiers as time-dependent EBMs. The newly introduced self-calibration loss primarily enhances this by applying a standard DSM technique to train the internal score function, thus lacking substantial novelty.
The authors have judiciously selected a range of baseline candidates for semi-supervised learning and reported performance results. However, the focus on datasets like CIFAR-10 and CIFAR-100 limits the assessment of the methodology's generalizability to more diverse or complex data scenarios.
* Minor weaknesses
The allocation of Figure 1 is too naive. Overall, you could have edited the space of main paper more wisely.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '3']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['X', '3', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"* The plan-based method requires manually designing a plan based on the ground truth in advance, which is unrealistic in real-world scenarios. The learned plan methods are not comparable to the methods with pre-defined plans based on Table 2. It indicates that the proposed method may be difficult to generalize to a new dataset without the ground truth summary.",xNn2nq5kiy,ICLR_2024,"* The plan-based method requires manually designing a plan based on the ground truth in advance, which is unrealistic in real-world scenarios. The learned plan methods are not comparable to the methods with pre-defined plans based on Table 2. It indicates that the proposed method may be difficult to generalize to a new dataset without the ground truth summary.
* The novelty of the proposed method is limited. The most effective part is the manually designed plan. Based on that, they should also discuss some plan-based /outline-based prompt studies, such as [1-3].
* Some experimental details are missing. For example,
* The detailed information of the proposed new data sets. For example, the size, average document length, average summary length, average citation number, training/validation/testing split, and so on.
* How to choose the X (sentence number) and Y (words) in plan-based methods?
* What generation configuration is used in LLaMA-2, ChatGPT-3.5, and GPT-4? For example, the greedy decoding or the sampling with temperature.
* How was the human evaluation (In Section 5) conducted? The number of annotators, the inner agreement among annotators, the average compensation, the working hours, and the procedure of annotation should be described in detail.
* How are Avg Rating, Avg Win Rate, and Coverage in Table 8 calculated?
[1] Re3: Generating Longer Stories With Recursive Reprompting and Revision
[2] Plan-and-Solve Prompting: Improving Zero-Shot Chain-of-Thought Reasoning by Large Language Models
[3] Self-planning Code Generation with Large Language Models","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '2', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
- The first sentence of the abstract needs to be re-written.,NIPS_2016_238,NIPS_2016,"- My biggest concern with this paper is the fact that it motivates âdiversityâ extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity. - The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions: - The first sentence of the abstract needs to be re-written. - Diversity should be toned down. - line 108, the first âfâ should be âgâ in âwe fixed the form of ..â - extra â.â in the middle of a sentence in line 115. One Question: For the baseline MCL with deep learning, how did the author ensure that each of the networks have converged to a reasonable results. Cutting the learners early on might significantly affect the ensemble performance.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- It appears that in nearly all experiments, the results are reported for a single held-out test set. Standard practice in most papers on GPs involves using a number of train/test splits or folds which give a more accurate illustration of the methodâs performance. While I imagine that the size of the datasets considered in this work entail that this can take quite a long time to complete, I highly encourage the authors to carry out this exercise;",NIPS_2019_1312,NIPS_2019,"weakness of this paper is its isolation to the plain GP regression setting. Although this is expected given the methodology used to enable tractability, I would have appreciated at least some discussion into whether any of the material presented here can be extended to the classification setting. Of course, one could argue that using Laplace or EP already implicitly takes away from the âexactnessâ of a GP, but I think there is still scope for having an interesting discussion here (possibly akin to that provided in Cutajar et al, 2015). Likewise, any intuition of how/whether this can be extended to more advanced GP set-ups, such as multi-task, convolutional, and recurrent variations (among many others) would also be useful. -- Technical Quality/Evaluation -- The technical contributions and implementation details are easy to follow, and I did not find any faults in that respect. The experimental evaluation is also varied and convincingly shows that exact GP inference widely outperforms standard approximations. Nonetheless I have a few concerns listed below; - It appears that in nearly all experiments, the results are reported for a single held-out test set. Standard practice in most papers on GPs involves using a number of train/test splits or folds which give a more accurate illustration of the methodâs performance. While I imagine that the size of the datasets considered in this work entail that this can take quite a long time to complete, I highly encourage the authors to carry out this exercise; - If I understood correctly, a kernel with a shared length scale is used in all experiments, which does not conform to the ARD kernels one would typically use in a practical setting. While several papers presenting approximate GPs have also made this assumption in the past (e.g. Hensman et al (2013)), more recent work such as the AutoGP by Krauth et al. (2017) emphasise why ARD should more consistently be used, and demonstrate how automatic differentiation frameworks minimise the performance penalty introduced by using such schemes. I believe this has to be addressed more clearly in the paper, and would also give more meaning to the commentary provided in L243-248, which otherwise feels spurious. I consider this to be crucial for painting a more complete picture in the Results section. - For a GP paper, I also find it strange that results for negative log likelihood are not reported here. While these are expected to follow a similar trend to RMSE, I would definitely include such results in a future version of the manuscript since this also has implications on uncertainty calibration. On a related note, I was surprised this paper did not have any supplementary material attached, because further experiments and more results would definitely be welcome. -- Overall recommendation -- This paper does not introduce any major theoretical elements, but the technical contributions featured here, along with the associated practical considerations, are timely in showing how modern GPU architectures can be exploited for carrying out exact GP training and inference to an extent which had not previously been considered. The paper is well written and some of the results should indeed stimulate interesting discussions on whether standard GP approximations are still worthwhile for plain regression tasks. Unfortunately, given how this paperâs worth relies heavily on the quality of the experimental results, there are a few technical issues in the paper which I believe should be addressed in a published version. I also think that several of the discussions featured in the paper can be expanded further - the authors should not refrain from including their own intuition on the broader implications of this work, which I feel is currently missing. -- Post-rebuttal update -- I thank the authors for their rebuttal. I had a positive opinion of the paper in my initial review, and most of my principal concerns were sufficiently addressed in the rebuttal. After reading the other reviews, I do believe that there is a common interest in having more experiments included. Coupled with my other suggested changes to the current experimental set-up, I think there is still some work to be done in this respect. This also coincides with my wish to see more of your own insights included in the paper, which I think will steer and hopefully also encourage more discussion on this interesting dilemma on where to invest computational resources. Nevertheless, I do not expect such additional experiments to majorly alter the primary âstorylineâ of the paper, which is why Iâm not lowering my current evaluation of the paper. Due to the limited novelty of the technical aspects, I am likewise not inclined to raise my score either, but I think this is a good paper nonetheless. Irrespective of whether this paper is ultimately accepted or not, I definitely hope to see an updated version containing an extended experimental evaluation and discussion.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"- The method seems more involved that it needs to be. One would suspect that there is an underlying, simpler, principle that is propulsing the quality gains.",NIPS_2020_335,NIPS_2020,"- The paper reads too much like LTF-V1++, and at some points assumes too much familiarity of the reader to LTF-V1. Since this method is not well known, I wish the paper was a bit more pedagogical/self-contained. - The method seems more involved that it needs to be. One would suspect that there is an underlying, simpler, principle that is propulsing the quality gains.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
6) Adding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution.,ICLR_2022_3330,ICLR_2022,"1) One very serious problem is that this paper is full of grammatical errors. It is too many and many of them can be detected and corrected by grammatical checker. I only list some in here to justify my observations, instead of all because I don’t want to proofread the authors’ paper. Page 1, learned,, Page 2 and Kurakin et al. (2018) starts Page 2, MI-FGSM, which integrate Page 2, several run-on sentences in the section Gradient Optimization Based Attack Page 2, which is divide Page 2 can removes Page 3 is extend Page 4 mollifer Page 5 hyper-parameters .. is Page 8 with SP-MI-FGSM should be SE-MI-FGSM? Page 8 via SP-MI-FGSM should be SE-MI-FGSM? Page 9, overcomes these two feedbacks. I would like to emphasize once again that this list is far from complete. 2) Although this paper has only 6 equations and several of them are copied from previous papers, the authors made a mistake. In Eq. 4, when i=N-1, alpha=l+(N-1) *(r-l), which is not r, different from what is described before (above Eq. 4). 3) The organization of this paper is also problematic. The authors reviewed other input transformation methods in the method section. They should be putted in Section 2. 4) In the comparison, the authors limit their baselines on input transformation methods and only compare with VR, DI and SI. However, they intentionally/unintentionally ignore the state-of-the-art LinBP and some other methods. As far as I know, LinBP is the currently the best transfer method. 5) The authors only evaluate their method on epsilon=16. Although they follow Wang&He 2021, it does not give a complete picture. More different epsilons are expected. 6) Adding a method on the top of other methods to improve transferability is good but cannot be considered a significant contribution. 7) In addition to untargeted attack, target attack, which is more challenging, should be considered. 8) One problem in the experiments is that some images are incorrect classified by the models. However, they authors do not say it clearly. Only said that “mostly classified correctly by the evaluation models”. How do they impact the experimental results? They will naturally provide better numbers to all methods. It means that the success attack accuracy should be lower than the one reported.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
- The hGRU architecture seems pretty ad-hoc and not very well motivated.,NIPS_2018_15,NIPS_2018,"- The hGRU architecture seems pretty ad-hoc and not very well motivated. - The comparison with state-of-the-art deep architectures may not be entirely fair. - Given the actual implementation, the link to biology and the interpretation in terms of excitatory and inhibitory connections seem a bit overstated. Conclusion: Overall, I think this is a really good paper. While some parts could be done a bit more principled and perhaps simpler, I think the paper makes a good contribution as it stands and may inspire a lot of interesting future work. My main concern is the comparison with state-of-the-art deep architectures, where I would like the authors to perform a better control (see below), the results of which may undermine their main claim to some extent. Details: - The comparison with state-of-the-art deep architectures seems a bit unfair. These architectures are designed for dealing with natural images and therefore have an order of magnitude more feature maps per layer, which are probably not necessary for the simple image statistics in the Pathfinder challenge. However, this difference alone increases the number of parameters by two orders of magnitude compared with hGRU or smaller CNNs. I suspect that using the same architectures with smaller number of feature maps per layer would bring the number of parameters much closer to the hGRU model without sacrificing performance on the Pathfinder task. In the author response, I would like to see the numbers for this control at least on the ResNet-152 or one of the image-to-image models. The hGRU architecture seems very ad-hoc. - It is not quite clear to me what is the feature that makes the difference between GRU and hGRU. Is it the two steps, the sharing of the weights W, the additional constants that are introduced everywhere and in each iteration (eta_t). I would have hoped for a more systematic exploration of these features. - Why are the gain and mix where they are? E.g. why is there no gain going from H^(1) to \tilde H^(2)? - I would have expected Eqs. (7) and (10) to be analogous, but instead one uses X and the other one H^(1). Why is that? - Why are both H^(1) and C^(2) multiplied by kappa in Eq. (10)? - Are alpha, mu, beta, kappa, omega constrained to be positive? Otherwise the minus and plus signs in Eqs. (7) and (10) are arbitrary, since some of these parameters could be negative and invert the sign. - The interpretation of excitatory and inhibitory horizontal connections is a bit odd. The same kernel (W) is applied twice (but on different hidden states). Once the result is subtracted and once it's added (but see the question above whether this interpretation even makes sense). Can the authors explain the logic behind this approach? Wouldn't it be much cleaner and make more sense to learn both an excitatory and an inhibitory kernel and enforce positive and negative weights, respectively? - The claim that the non-linear horizontal interactions are necessary does not appear to be supported by the experimental results: the nonlinear lesion performs only marginally worse than the full model. - I do not understand what insights the eigenconnectivity analysis provides. It shows a different model (trained on BSDS500 rather than Pathfinder) for which we have no clue how it performs on the task and the authors do not comment on what's the interpretation of the model trained on Pathfinder not showing these same patterns. Also, it's not clear to me where the authors see the ""association field, with collinear excitation and orthogonal suppression."" For that, we would have to know the preferred orientation of a feature and then look at its incoming horizontal weights. If that is what Fig. 4a shows, it needs to be explained better.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '4', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['X', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
"2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps? [1] https://github.com/watchernyu/REDQ",ICLR_2023_1214,ICLR_2023,"As the authors note, it seems the method still requires a few tweaks to work well empirically. For example, we need to omit the log of the true rewards and scale the KL term in the policy objective to 0.1. While the authors provide a brief intuition on why those modifications are needed, I think the authors should provide more concrete analysis (e.g., empirical results) on what problems the original formula have and how the modifications fix them. Also, it would be better if the authors provide ablation results on those modifications. For example, does the performance drop drastically if the scale of the KL term changes (to 0.05, 0.2, 0.5, ...) ?
The compute comparison vs. REDQ on Figure 3 seems to be misleading. First, less runtime does not necessarily mean less computational cost. Second, if the authors had used the official implementation of REDQ [1], it should be emphasized that this implementation is very inefficient in terms of runtime. In detail, the implementation feeds forward to each Q-network one at a time while this feed-forward process is embarrassingly parallelizable. The runtime of REDQ will drop significantly if we parallelize this process.
The ablation experiments seem to show that the value term for the encoder is not necessary. It would be better to provide an explanation on this result.
Some of the equations seem to have typos. 1) On equation (4), the first product and the second product have exactly the same form. 2) On algorithm 1 Line 8, shouldn't we use s_n instead of s_t? Questions
I am curious of the asymptotic performance of the proposed method. If possible, can the authors provide average return results with more env steps?
[1] https://github.com/watchernyu/REDQ","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.",ICLR_2023_3705,ICLR_2023,"1)The main assumption is borrowed from other works but is actually rarely used in the optimization field. Moreover, the benefits of this assumption is not well investigated. For example, a) why it is more reasonable than the previous one? B) why it can add gradient norm L_1 \nabla f(w_1) in Eqn (3) or why we do not add other term? It should be mentioned that a milder condition does not mean it is better, since it may not reflect the truth. For me, problem B) is especially important in this work, since the authors do not well explain and investigate it.
2)Results in Theorem 1 show that Adam actually does not converge, since this is a constant term O(D_0^{0.5}\delta) in Eqn. (5). This is not intuitive, the authors claim it is because the learning rate may not diminish. But many previous works, e.g. [ref 1], can prove Adam-type algorithms can converge even using a constant learning rate. Of course, they use the standard smooth condition. But (L0,L1)-smoothness condition should not cause this kind of convergence, since for nonconvex problem, in most cases, we only need the learning rate to be small but does not care whether it diminishes to zero.
[ref 1] Dongruo Zhou, Jinghui Chen, et al. On the Convergence of Adaptive Gradient Methods for Nonconvex Optimization
3)It is not clear what are the challenges when the authors analyze Adam under the (L0,L1)-smoothness condition. It seems one can directly apply standard analysis on the (L0,L1)-smoothness condition. So it is better to explain the challenges, especially the difference between this one and Zhang et al.
4)Under the same assumption, the authors use examples to show the advantage of Adam over GD and SGD. This is good. But one issue is that is the example reasonable or does it share similar properties with practical problems, especially for networks. This is important since both SGD and ADAM are widely used in the deep learning field.
5)In the work, when comparing SGD and ADAM, the authors explain the advantage of adam comes from the cases when the local smoothness varies drastically across the domain. It is not very clear for me why Adam could better handle this case. Maybe one intuitive example could help.
6)The most important problem is that this work does not provide new insights, since it is well known that the second order moment could help the convergence of Adam. This work does not provide any insights beyond this point and also does not give any practical solution to further improve.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"34 ""to force the neural network to memorize them"" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact ""critical point"" as such in TopoNet [24]. Minor: I find the method section to be a bit wordy, it could be compressed on the essential definitions. There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l.",NIPS_2022_285,NIPS_2022,"Terminology: Introduction l. 24-26 ""pixels near the peripheral of the object of interest can generally be challenging, but not relevant to topology."" I think this statement is problematic. When considering the inverse e.g. in the case of a surface or vessel, that a foreground pixel changes to background. Such a scenario would immediately lead to a topology mismatch (Betti error 1).
Terminology: ""topologically critical location"" --> I find this terminology to be not optimally chosen. I agree that the warping concept appears to help with identifying pixels which may close loops or fill holes. However, considering the warping I do not see a guarantee that such ""locations"" (as in the exact location) which I understand to refer to individual or groups of pixels are indeed part of the real foreground, nor are these locations unique. A slightly varying warping may propose a set of different pixels.
The identified locations are more likely to be relevant to topological errors. --> this statement should be statistically supported. Compared to what exactly? Does this rely on the point estimate for any pixel? Or given a particularly trained network?
Theorem 1: The presentation of a well known definition from Kong et al. is trivial and could be presented in a different way.
Experimentation, lack of implementation details: In Table 2 and a dedicated section, the authors show an ablation study on the influence of lamda on the results. Lamda is a linear parameter, weighting the contribution of the new loss to the overall loss. Similarly, the studied baseline methods, e.g. TopoNet [24], DMT [25], and clDice [42] have a loss weighting parameter. It would be important to understand how and if the parameters of the baselines were chosen and experimented with. (I understand that the authors cannot train ablation studies for all baselines etc.) However, it is an important information to understand the results in Table 1.
Terminology: l. 34 ""to force the neural network to memorize them"" --> I would tone down this statement, in my understanding, the neural network does not memorize an exact ""critical point"" as such in TopoNet [24]. Minor:
I find the method section to be a bit wordy, it could be compressed on the essential definitions.
There exist several grammatical errors, please double-check these with a focus on plurals and articles. E.g. l. 271 ""This lemma is naturally generalized to 3D case.""
l. 52 ""language of topology"" I find this to be an imprecise definition or formulation. Note:
After rebuttal and discussion I increased the rating to 5.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1) most of person re-ID methods build on the basis of pedestrian detector (two-step method), and there are also end-to-end method that combines detection and re-ID [5];",ICLR_2022_2791,ICLR_2022,"The technical contribution of this paper is limited, which is far from a decent ICLR paper. In particular, All kinds of evaluations, i.e., single-dataset setting (most of existing person re-ID methods), cross-dataset setting [1, 2,3] and live re-id setting [4], have been discussed in previous works. This paper simply makes a systematic discussion.
For cross-dataset setting, this paper only evaluates standard person re-ID methods that train on one dataset and evaluate on another, but fails to evaluate the typical cross-dataset person re-ID methods, e.g., [1, 2, 3].
For live re-ID setting, this paper does not compare with the particular live re-ID baseline [4]
Though some conclusions are drawn from the experiments, the novelty is limited. For example, 1) most of person re-ID methods build on the basis of pedestrian detector (two-step method), and there are also end-to-end method that combines detection and re-ID [5]; 2) It is common that distribution bias exists between datasets. It is hard to find a standard re-ID approach and a training dataset to address the problem unless the dataset is large enough that can cover as much as scenes. 3) cross-dataset methods try to mitigate the generalization problem.
[1] Hu, Yang, Dong Yi, Shengcai Liao, Zhen Lei, and Stan Z. Li. ""Cross dataset person re-identification."" In Asian Conference on Computer Vision, pp. 650-664. Springer, Cham, 2014.
[2] Lv, Jianming, Weihang Chen, Qing Li, and Can Yang. ""Unsupervised cross-dataset person re-identification by transfer learning of spatial-temporal patterns."" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7948-7956. 2018.
[3] Li, Yu-Jhe, Ci-Siang Lin, Yan-Bo Lin, and Yu-Chiang Frank Wang. ""Cross-dataset person re-identification via unsupervised pose disentanglement and adaptation."" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 7919-7929. 2019.
[4] Sumari, Felix O., Luigy Machaca, Jose Huaman, Esteban WG Clua, and Joris Guérin. ""Towards practical implementations of person re-identification from full video frames."" Pattern Recognition Letters 138 (2020): 513-519.
[5] Xiao, Tong, Shuang Li, Bochao Wang, Liang Lin, and Xiaogang Wang. ""Joint detection and identification feature learning for person search."" In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 3415-3424. 2017.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '0']}",,
• Section 3.2 - I suggest to add a first sentence to introduce what this section is about.,ICLR_2021_1740,ICLR_2021,"are in its clarity and the experimental part.
Strong points Novelty: The paper provides a novel approach for estimating the likelihood of p(class image), by developing a new variational approach for modelling the causal direction (s,v->x). Correctness: Although I didn’t verify the details of the proofs, the approach seems technically correct. Note that I was not convinced that s->y (see weakness)
Weak points Experiments and Reproducibility: The experiments show some signal, but are not through enough: • shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 )
, since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution. • Missing details about evaluation split for shifted-MNIST: Did the experiments used a validation set for hyper-param search with shifted-MNIST and ImageCLEF? Was it based on in-distribution data or OOD data? • It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"". • It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST. • What were the ranges used for hyper-param search? What was the search protocol?
Clarity: • The parts describing the method are hard to follow, it will be useful to improve their clarity. • It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them. • What makes the VAE inference mappings (x->s,v) stable to domain shift? E.g. [1] showed that correlated latent properties in VAEs are not robust to such domain shifts. • What makes v distinctive of s? Is it because y only depends on s? • Does the approach uses any information on the labels of the domain?
Correctness: I was not convinced about the causal relation s->y. I.e. that the semantic concept cause the label, independently of the image. I do agree that there is a semantic concept (e.g. s) that cause the image. But then, as explained by [Arjovsky 2019] the labelling process is caused by the image. I.e. s->image->y, and not as argued by the paper. The way I see it, is like a communication channel: y_tx -> s -> image -> y_rx. Could the authors elaborate how the model will change if replacing s->y by y_tx->s ?
Other comments: • I suggest discussing [2,3,4], which learned similar stable mechanisms in images. • I am not sure about the statement that this work is the ""first to identify the semantic factor and leverage causal invariance for OOD prediction"" e.g. see [3,4] • The title may be confusing. OOD usually refers to anomaly-detection, while this paper relates to domain-generalization and domain-adaptation. • It will be useful to clarify that the approach doesn't use any external-semantic-knowledge. • Section 3.2 - I suggest to add a first sentence to introduce what this section is about. • About remark in page 6: (1) what is a deterministic s-v relation? (2) chairs can also appear in a workspace, and it may help to disentangle the desks from workspaces.
[1] Suter et al. 2018, Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness [2] Besserve et al. 2020, Counterfactuals uncover the modular structure of deep generative models [3] Heinze-Deml et al. 2017, Conditional Variance Penalties and Domain Shift Robustness [4] Atzmon et al. 2020, A causal view of compositional zero-shot recognition
EDIT: Post rebuttal
I thank the authors for their reply. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking.
Additionally, I am concerned with one of the author's replies saying All methods achieve accuracy 1 ... on the training distribution, because usually there is a trade-off between accuracy on the observational distribution versus the shifted distribution (discussed by Rothenhäusler, 2018 [Anchor regression]): Achieving perfect accuracy on the observational distribution, usually means relying on the spurious correlations. And under domain-shift scenarios, this would hinder the performance on the shifted-distribution.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
- Line 44: What is meant by the initial rationale selector is perfect? It seems if it were perfect no additional work needs to be done.,NIPS_2021_1604,NIPS_2021,").
Weaknesses - Some parts of the paper are difficult to follow, see also Typos etc below. - Ideally other baselines would also be included, such as the other works discussed in related work [29, 5, 6].
After the Authors' Response My weakness points after been addressed in the authors' response. Consequently I raised my score.
All unclear parts have been answered
The authors' explained why the chosen baseline makes the most sense. It would be great if this is added to the final version of the paper.
Questions - Do you think there is a way to test beforehand whether I(X_1, Y_1) would be lowered more than I(X_2, Y_1) beforehand? - Out of curiosity, did you consider first using Aug and then CF.CDA? Especially for the correlated palate result it could be interesting to see if now CF.CDA can improve. - Did both CDA and MMI have the same lambda_RL (Eq 9) value? From Figure 6 it seems the biggest difference between CDA and MMI is that MMI has more discontinuous phrase/tokens.
Typos, representation etc. - Line 69: Is X_2 defined as all features of X not in X_1? Stating this explicitly would be great. - Line 88: What ideas exactly do you take from [19] and how does your approach differ? - Eq 2: Does this mean Y is a value in [0, 1] for two possible labels? Can this be extended to more labels? This should be clarified. - 262: What are the possible Y values for TripAdvisor’s location aspect? - The definitions and usage of the various variables is sometimes difficult to follow. E.g. What exactly is the definition of X_2? (see also first point above). When does X_M become X_1? Sometimes the augmented data has a superscript, sometimes it does not. In line 131 the meaning of x_1 and x_2 are reverse, which can get confusing - maybe x’_1 and x’_2 would make it easier to follow together with a table that explains the meaning of different variables? - Section 2.3: Before line 116 mentioned the change when adding the counterfactual example, it would be helpful to first state what I(X_2, Y_1) and I(X_1, Y_1) are without it.
Minor points - Line 29: How is desired relationship between input text and target labels defined? - Line 44: What is meant by the initial rationale selector is perfect? It seems if it were perfect no additional work needs to be done. - Line 14, 47: A brief explanation of “multi-aspect” would be helpful - Figure 1: Subscripts s and t should be 1 and 2? - 184: Delete “the”
There is a broader impact section which discusses the limitations and dangers adequately.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1: ""The uncertainty is defined based on the posterior distribution."" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data. p.",ICLR_2021_2047,ICLR_2021,"As noted below, I have concerns around the experimental results. More specifically, I feel that there is a relative lack of discussion around the (somewhat surprising) outperformance of baselines that VPBNN is aiming to approximate, and I feel that the experiments are missing what I see as key VPBNN results that otherwise leave the reader with questions. Additionally, I think the current paper would benefit from including measurements and discussion around the specifics of computational and memory costs of their method. Recommendation
In general, I think this could be a great paper. However, given the above concerns, I'm currently inclined to suggest rejection of the paper in its current state. I would highly recommend that authors push further on the noted areas!
Additional comments
p. 1: ""The uncertainty is defined based on the posterior distribution."" For more clarity it could be helpful to update this to say that the epistemic model uncertainty is represented in the prior distribution, and upon observing data, those beliefs can be updated in the form of a posterior distribution, which yields model uncertainty conditioned on observed data.
p. 2: ""The MC dropout requires a number of repeated feed-forward calculations with randomly sampled weight parameters in order to obtain the predictive distribution."" This should be updated to indicate that in MC dropout, dropout is used (in an otherwise deterministic model) at test time with ""a number of repeated feed-forward calculations"" to effectively sample from the approximate posterior, but not directly via different weight samples (as in a variational BNN). With variational dropout, this ends up having a nice interpretation as a variational Bayes method, though no weight distributions are typically directly used with direct MC dropout.
p. 2: Lakshminarayanan et al. (2017) presented random seed ensembles, not bootstrap ensembles (see p. 4 of their work for more info). They used the full dataset, and trained M ensemble members with different random seeds, rather than resampled data.
p. 4: For variance propagation in a dropout layer with stochastic input, it's not exactly clear from the text how variance from the inputs and dropout is being combined into an output Gaussian. I believe using a Gaussian is an approximation, and while that would be fine, I think it would be informative to indicate that. The same issue comes up with local reparameterization for BNNs with parameter distributions, where they can be reparameterized exactly as output distributions (for, say, mean-field Gaussian weight dists) so long as the inputs are deterministic. Otherwise, the product of, say, two Gaussian RVs is non-Gaussian.
p. 7: Figure 1 is too small.
p. 7: ""Estimation of ρ is possible by observing the outputs of middle layers several times under the approximate predictive distribution. The additional computation cost is still kept quite small compared to MC dropout."" How exactly is ρ
estimated? Is it a one-time cost irregardless of data that can then be used for all predictions from the trained model? Without details, this seems like a key component that can yield arbitrary amounts of uncertainty.
p. 7, 8: For the language modeling experiment, why do you think VPBNN was able to achieve lower perplexity values than MC dropout? The text generally focuses on VPBNN as an approximation to MC dropout, and yet it outperforms it. The text would greatly benefit from more discussion around this point.
p. 8: For the OOD detection experiment, I'm surprised that ρ = 0
was the only VPBNN model used, since Section 5.1 and Figure 1 indicated that it led to overconfident models. Can you include results with other settings of ρ
? Moreover, from Figure 1 we see that (for that model) VPBNN with ρ = 0
qualitatively yielded the same amount of predictive variance as the Taylor approximation. However, in Table 2, we see VPBNN with ρ = 0
outperform MC dropout (with 100 or 2000 samples) and the Taylor approximation. Why do you think this is the case, particularly if the standard deviation was used as the uncertainty signal for the OOD decision. I see that ""This is because the approximation accuracy of the Taylor approximation is not necessarily high as shown in Section B"", but I did not find Section B or Figure 3 to be clear. I think the text would benefit from more discussion here, and from the additional experiments for ρ .
Can you include a discussion and measurements for FLOPS and memory usage for VPBNN? Specifically, given the discussion around efficiency and the implementation that doubles the dimensionality of the intermediates throughout the model, I believe it would be informative to have theoretical and possibly runtime measurements. Minor
p. 1: s/using the dropout/using dropout/
p. 1: s/of the language modeling/of language modeling/
p. 2: s/is the representative of/is representative of/
p. 2: s/In the deep learning/In deep learning/
p. 2: s/This relations/This relation/
p. 5: Need to define s
as the sigmoid function in the LSTM cell equations.","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', 'X', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"1) train a feature extractor ( f in the paper) and a classifier ( g ′ in the paper) using random sampling and any semi-supervised learning method on all data, then",ICLR_2022_1935,ICLR_2022,"Weakness:
A semi-supervised feature learning baseline is missing.
This is my main concern about the paper. The key argument in the paper is that feature learning and classifier learning should 1) be decoupled, 2) use random sampling and class-balanced sampling respectively, 3) train on all labels and only ground-truth labels respectively. The authors, therefore, propose a carefully designed alternate sampling strategy.
However, a more straightforward strategy could be 1) train a feature extractor ( f
in the paper) and a classifier ( g ′
in the paper) using random sampling and any semi-supervised learning method on all data, then 2) freeze the feature extractor ( f
) and train a new classifier ( g
in the paper) using class-balanced sampling on data with ground-truth labels. Compared with the alternate sampling strategy proposed in this paper, the semi-supervised feature learning baseline will take less implementation effort and is easier to combine with any semi-supervised learning methods.
The baseline seems to be missing in the paper. Although the naive baseline may not give the best performance, it should be compared to justify the sophisticatedly designed alternate sampling strategy.
References are not up-to-date.
All references in the paper are in or before 2020. In fact, much research progress has been made since then. For example, some recent works [1, 2] study the class-imbalanced semi-supervised learning as well, a discussion on these methods should be necessary. A recent survey on long-tailed learning [3] could be a useful resource to help update the related works in the paper.
Minor issues: ""The model is the fine-tuned on the combination of ..."" -> ""The model is then fine-tuned on the combination of ...""
[1] Su et al., A Realistic Evaluation of Semi-Supervised Learning for Fine-Grained Classification, CVPR 2021
[2] Wei et al., CReST: A Class-Rebalancing Self-Training Framework for Imbalanced Semi-Supervised Learning, CVPR 2021
[3] Deep Long-Tailed Learning: A Survey, arXiv 2021","{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['X', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1'], 'labels': ['1', '1']}",,
- It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses - Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?,ARR_2022_40_review,ARR_2022,"- Although author state that components can be replaced by other models for flexibility, authors did not try any change or alternative in the paper to proof the robustness of the proposed framework.
- Did authors tried using BlenderBot vs 2.0 with incorporated knowledge? it would be very interesting to see how the dialogs can be improved by using domain ontologies from the SGD dataset. - Although BlenderBot is finetuned on the SGD dataset, it is not clear how using more specific TOD chatbots can provide better results - Lines 159-162: Authors should provide more information about the type/number of personas created, and how the personas are used by the chatbot to generate the given responses. - It is not clear if authors also experimented with the usage of domain ontologies to avoid the generation of placeholders in the evaluated responses - Line 211: How many questions were created for this zero-shot intent classifier and what is the accuracy of this system?
- Line 216: How many paraphrases were created for each question, and what was their quality rate?
- Line 237: How critical was the finetuning process over the SQuad and CommonsenseQA models?
- Line 254-257: How many templates were manually created? - Line 265: How the future utterances are used during evaluation? For the generation part, are the authors generating some sort of sentence embedding representation (similar to SkipThoughs) to learn the generation of the transition sentence? and is it the transition sentence one taken from the list of manual templates? ( In general, this section 2.2.2 is the one I have found less clear) - Merge SGD: Did authors select the TOD dialogue randomly from those containing the same intent/topic? did you tried some dialogue embedding from the ODD part and tried to select a TOD dialogue with a similar dialogue embedding? if not, this could be an idea to improve the quality of the dataset. this could also allow the usage of the lexicalized version of the SGD and avoids the generation of placeholders in the responses - Line 324: how the repeated dialogues are detected? - Line 356: how and how many sentences are finally selected from the 120 generated sentences?
- Lines 402-404: How the additional transitions are generated? using the T5 model? how many times the manual sentences were selected vs the paraphrased ones?
- The paper: Fusing task-oriented and open-domain dialogues in conversational agents is not included in the background section and it is important in the context of similar datasets - Probably the word salesman is misleading since by reading some of the generated dialogues in the appendixes, it is not clear that the salesman agent is in fact selling something. It seems sometimes that they are still doing chitchat but on a particular topic or asking for some action to be done (like one to be done by an intelligent speaker)","{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,4,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['x8G3vam1', 'sBdHWtl1', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019).",NIPS_2020_897,NIPS_2020,"1. Not clear how this method can be applied outside of fully cooperative settings, as the authors claim. The authors should justify this claim theoretically or empirically, or else remove it. 2. Missing some citations to set this in context of other MARL work e.g. recent papers on self-play and population-play with respect to exploration and coordination (such as https://arxiv.org/abs/1806.10071, https://arxiv.org/abs/1812.07019). 3. The analysis is somewhat ""circumstantial"", need more detailed experiments to be a convincing argument in this section. For example the claim in lines 235 - 236 seems to require further evidence to be completely convincing. 4. The link with self-play could be more clearly drawn out. As far as I can tell, the advantage of this over self-play is precisely the different initialization of the separate agents. It is surprising and important that this has such a significant effect, and could potentially spur a meta-learning investigation into optimal initialization for SEAC in future work.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2. It would be better to compare with other self-supervised learning methods that are not based on contrastive learning.,ICLR_2023_1645,ICLR_2023,1. Can this method be used on both SEEG and EEG simultaneously? 2. It would be better to compare with other self-supervised learning methods that are not based on contrastive learning.,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Regarding the abstention process, it appears to be based on a prediction probability threshold, where if the probability is lower than the threshold, the prediction is abstained? How does it different from a decision threshold used by the models? Can authors clarify that?",t8cBsT9mcg,ICLR_2024,"1. The abstract should be expanded to encompass key concepts that effectively summarize the paper's contributions. In the introduction, the authors emphasize the significance of interpretability and the challenges it poses in achieving high accuracy. By including these vital points in the abstract, the paper can provide a more comprehensive overview of its content and contributions.
2. Regarding the abstention process, it appears to be based on a prediction probability threshold, where if the probability is lower than the threshold, the prediction is abstained? How does it different from a decision threshold used by the models? Can authors clarify that?
3. In the results and discussion section, there's limited exploration and commentary on the impact of the solution on system accuracy, as seen in Table 2. Notably, the confirmation budget appears to have a limited effect on datasets like ""noisyconcepts25"" and ""warbler"" compared to others. The paper can delve into the reasons behind this discrepancy.
4. In real-world applications of this solution, questions about the ease of concept approval and handling conflicting user feedback arise. While these aspects may be considered out of scope, addressing them would be beneficial for evaluating the practicality of implementing this approach in real-world scenarios. This is particularly important when considering the potential challenges of user feedback and conflicting inputs in such applications.
Minor things:
Page 4, confirm. we —> replace . with comma
Section 4.2, Table Table 2 —> Table 2
Shouldn’t Table 2 rather be labelled as Figure 2?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works. Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance?",NIPS_2021_1743,NIPS_2021,"1. While the paper claim the importance of language modeling capability of pre-trained models, the authors did not conduct experments on generation tasks that are more likely to require a well-performing language model. Experiments on word similarity and SquAD in section 5.3 cannot really reflect the capability of language modeling. The authors may consider to include tasks like language modeling, machine translation or text sumarization to strenghen this part, as this is one of the main motivations of COCO-LM. 2. Analysis of SCL in section 5.2 regarding few-shot abaility looks not convincing. The paper claims that a more regularized representation space by SCL may result in better generalization ability in few-shot scenarios. However, results in Figure 7(c) and (d) do not meet our expectation such that COCO-LM achieves much more improvements with less labels and the improvements will gradually disappear with more labels. Besides, the authors may check if COCO-LM brings benefits to sentence retrieval tasks with the learned anisotropy text representations. 3. The comparison with Megatron is a little overrated. The performance of Megatron and COCO-LM is close to other approaches, for examples, RoBERTa, ELECTRA, and DeBERTa, which are with similar sizes as COCO-LM. If the author claim that COCO-LM is parameter-efficient, the conclusion is also applicable to the above related works.
Questions for the Authors 1. In experimental setup, why did the authors switch the types of BPE vocabulary, i.e., uncased and cased. Will the change of BPE cause the variance of performance? 2. In Table 2, it looks like COCO-LM especially affects the performance on CoLA and RTE hence the final performance. Can the authors provide some explanation on how the proposed pre-training tasks affect the two different GLEU tasks? 3. In section 5.1, the authors say that the benefits of the stop gradient operation are more on stability. What stability, the training process? If so, are there any learning curves of COCO-LM with and without stop gradient during pre-training to support this claim? 4. In section 5.2, the term “Data Argumentation” seems wrong. Did the authors mean data augmentation?
Typos 1. Check the term “Argumentation” in line 164, 252, and 314. 2. Line 283, “a unbalanced task”, should be “an unbalanced task”. 3. Line 326, “contrast pairs”, should be “contrastive pairs” to be consistent throughout the paper?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"4) The analysis from line 128 to 149 is not convincing enough. From the histogram as shown in Fig 3, the GS-P-50 model has smaller class selectivity score, which means GS-P-50 shares more features and ResNet-50 learns more class specific features. And authors hypothesize that additional context may allow the network to reduce its dependency. What is the reason such an observation can indicate GS-P-50 learns better representation? Reference: [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, CVPR, 2018. [2] W. Luo et al., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS, 2016.",NIPS_2018_865,NIPS_2018,"weakness of this paper are listed: 1) The proposed method is very similar to Squeeze-and-Excitation Networks [1], but there is no comparison to the related work quantitatively. 2) There is only the results on image classification task. However, one of success for deep learning is that it allows people leverage pretrained representation. To show the effectiveness of this approach that learns better representation, more tasks are needed, such as semantic segmentation. Especially, the key idea of this method is on the context propagation, and context information plays an important role in semantic segmentation, and thus it is important to know. 3) GS module is used to propagate the context information over different spatial locations. Is the effective receptive field improved, which can be computed from [2]? It is interesting to know how the effective receptive field changed after applying GS module. 4) The analysis from line 128 to 149 is not convincing enough. From the histogram as shown in Fig 3, the GS-P-50 model has smaller class selectivity score, which means GS-P-50 shares more features and ResNet-50 learns more class specific features. And authors hypothesize that additional context may allow the network to reduce its dependency. What is the reason such an observation can indicate GS-P-50 learns better representation? Reference: [1] J. Hu, L. Shen and G. Sun, Squeeze-and-Excitation Networks, CVPR, 2018. [2] W. Luo et al., Understanding the Effective Receptive Field in Deep Convolutional Neural Networks, NIPS, 2016.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.",kfFmqu3zQm,ICLR_2025,"1. Some conclusions are not convincing. For example, the paper contends that *We believe that continuous learning with unlabeled data accumulates noise, which is detrimental to representation quality.* The results might come from the limited exploration of combination methods. In rehearsal-free continual learning, feature-replay methods have shown great potential, like [R1] in continual learning and [R2] (FRoST) in CCD. A more recent work [R3] also employs feature replay to continually adjust the feature space, which also obtains remarkable performance for continual category discovery.
2. The proposed method is naïve and the novelty is relatively limited. The method includes basic clustering and number estimation. I’m afraid that this method could not provide so many insights to the community.
3. The feature space (i.e., backbone) is only tuned using labeled known classes, does this manner result in overfitting? because the data is purely labeled but the number is limited.
4. The class number estimation algorithm requires a pre-defined threshold $d_{min}$, which is intractable to define in advance and could largely impact the results. Some experiments and ablations should be included.
5. Detailed results of each continual session (at least for one or two datasets) should also be presented to show the performance. References:
[R1]. Prototype Augmentation and Self-Supervision for Incremental Learning. CVPR 2021.
[R2]. Class-incremental Novel Class Discovery. ECCV 2022.
[R3]. Happy: A Debiased Learning Framework for Continual Generalized Category Discovery. NeurIPS 2024. arXiv:2410.0653.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '3', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- **lack of meaningful baselines**: despite mentioning various model criticism techniques in Section 2, the authors limit their comparisons to simple naive baselines. For example, the authors could compare with a chain-of-thought prompting approach.",TY9mstpD02,ICLR_2025,"- **generalizability to other models**: the proposed framework is validated using gpt-4-turbo, a costly language model, which may compromise the applicability of the framework at scale. The paper could be further improved by showing how running the experiments using a cheaper model (e.g., gpt-4o) and/or open source models (e.g., Llama 3.1) would affect the obtained results.
- **generalizability of the results**: the conducted experiments are either too simple (simple synthetic regression setting with 4 variables) or include few data-model pairs (6 in Section 4.2, 36 in Section 4.3, and 10 for the human studies), raising questions about the generalizability of the proposed framework to more complex datasets.
- **lack of meaningful baselines**: despite mentioning various model criticism techniques in Section 2, the authors limit their comparisons to simple naive baselines. For example, the authors could compare with a chain-of-thought prompting approach.
- few insights about the generated and correctness of summary statistics: while the authors provide one example in Section 4.3, the paper could be further improved by adding additional insights and contrasting the proposed discrepancies with commonly discussed discrepancies in the literature (e.g., do these resemble the ones commonly found by humans?)","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Do you pretrain the cardiac signal representation learning model on the entire dataset or just the training set? If the entire set, how well does this generalize to setting where you don’t have the associated labels?",ICLR_2022_2531,ICLR_2022,"I have several concerns about the clinical utility of this task as well as the evaluation approach.
- First of all, I think clarification is needed to describe the utility of the task setup. Why is the task framed as generation of the ECG report rather than framing the task as multi-label classification or slot-filling, especially given the known faithfulness issues with text generation? There are some existing approaches for automatic ECG interpretation. How does this work fit into the existing approaches? A portion of the ECG reports from the PTB-XL dataset are actually automatically generated (See Data Acquisition under https://physionet.org/content/ptb-xl/1.0.1/). Do you filter out those notes during evaluation? How does your method compare to those automatically generated reports? - A major claim in the paper is that RTLP generates more clinically accurate reports than MLM, yet the only analysis in the paper related to this is a qualitative analysis of a single report. A more systematic analysis of the quality of generation would be useful to support the claim made in the appendix. Can you ask clinicians to evaluate the utility of the generated reports or evaluate clinical utility by using the generated reports to predict conditions identifiable from the ECG? I think that it’s fine that the RTLP method performs comparable to existing methods, but I am not sure from the current paper what the utility of using RTLP is. - More generally, I think that this paper is trying to do two things at once – present new methods for multilingual pretraining while also developing a method of ECG captioning. If the emphasis is on the former, then I would expect to see evaluation against other multilingual pretraining setups such as the Unicoder (Huang 2019a). If the core contribution is the latter, then clinical utility of the method as well as comparison to baselines for ECG captioning (or similar methods) is especially important. - I’m a bit confused as to why the diversity of the generated reports is emphasized during evaluation. While I agree that the generated reports should be faithful to the associated ECG, diversity may not actually be necessary metric to aim for in a medical context. For instance, if many of the reports are normal, you would want similar reports for each normal ECG (i.e. low diversity). - My understanding is that reports are generated in other languages using Google Translate. While this makes sense to generate multilingual reports for training, it seems a bit strange to then evaluate your model performance on these silver-standard noisy reports. Do you have a held out set of gold standard reports in different languages for evaluation (other than German)?
Other Comments: - Why do you only consider ECG segments with one label assigned to them? I would expect that the associated reports would be significantly easier than including all reports. - You might consider changing the terminology from “cardiac arrythmia” categories to something broader since hypertrophy (one of the categories) is not technically a cardiac arrythmia (although it can be detected via ECG & it does predispose you to them) - I think it’d be helpful to include an example of some of the tokens that are sampled during pretraining using your semantically similar strategy for selecting target tokens. How well does this work in languages that have very different syntactic structures compared to the source language? - Do you pretrain the cardiac signal representation learning model on the entire dataset or just the training set? If the entire set, how well does this generalize to setting where you don’t have the associated labels? - What kind of tokenization is used in the model? Which Spacy tokenizer? - It’d be helpful to reference the appendix when describing the setup in section 3/5 so that the reader knows that more detailed architecture information is there. - I’d be interested to know if other multilingual pretraining setups also struggle with Greek. - It’d be helpful to show the original ECG report with punctuation + make the ECG larger so that they are easier to read - Why do you think RTLP benefits from fine-tuning on multiple languages, but MARGE does not?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1) Some observations and subsequent design decisions might be hardware and software dependent;,NIPS_2022_2152,NIPS_2022,"The authors clearly addressed some potential limitations of the work: 1) Some observations and subsequent design decisions might be hardware and software dependent; 2) The NAS procedure, specifically the latency-driven slimming procedure is less involved and could be a direction for future exploration.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The structure of this paper is clear and easy to read. Specifically, the motivation of this paper is clear and the structure is well organized; the related work is elaborated in detail; the experimental setup is complete.",ICLR_2021_147,ICLR_2021,"the empirical validation is weak. Therefore, more new models need to be compared. For more details, please refer to “Reasons for reject”
Reasons for accept: 1. The structure of this paper is clear and easy to read. Specifically, the motivation of this paper is clear and the structure is well organized; the related work is elaborated in detail; the experimental setup is complete. 2. Based on the use of replay to solve catastrophic forgetting, the current popular graph structure is introduced to capture the similarities between samples. Combined with the proposed Graph Regularization, this paper provides a new perspective for solving catastrophic forgetting. 3. The experimental results given in the paper can basically show that the proposed method is effective. The ablation study also verified the effectiveness of each component.
Reasons for reject: 1. The lack of comparison of experimental effects after replacing Graph Regularization with other regularization methods mentioned in this paper, or other distance measurement methods, eg., L2.
This paper compares relatively few baselines, especially recent studies. I hope to see the comparison results of some papers in the list below. The latest papers on the three types of methods (regularization, expansion, and rehearsal) for solving catastrophic forgetting are included. Therefore, if it can be compared with some of these models, it will be beneficial to the evaluation of GCL.
[1] Ostapenko O , Puscas M , Klein T , et al. Learning to Remember: A Synaptic Plasticity Driven Framework for Continual Learning. ICML 2019 [2] Y Wu, Y Chen, et al. Large Scale Incremental Learning. CVPR 2019 [3] Liu Y , Liu A A , Su Y , et al. Mnemonics training: Multi-class incremental learning without forgetting. CVPR 2020 [4] Zhang J , Zhang J , Ghosh S , et al. Class-incremental learning via deep model consolidation. 2020 IEEE Winter Conference on Applications of Computer Vision (WACV) [5] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continuous learning of context-dependent processing in neural networks. Nature Machine Intelligence, 2019. [6] Wenpeng Hu, Zhou Lin, et al. Overcoming catastrophic forgetting for continual learning via model adaptation. ICLR 2019 [7] Rao D , Visin F , Rusu A A , et al. Continual Unsupervised Representation Learning. NeurIPS 2019","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['X', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
"1). Is the ground truth sufficiently accurate enough that such a small difference is actually noticeable / measurable or is the difference due to noise or randomness in the training process? b) Similarly, there is little difference between the results reported for the ablation study in Tab.",NIPS_2022_738,NIPS_2022,"W1) The paper states that ""In order to introduce epipolar constraints into attention-based feature matching while maintaining robustness to camera pose and calibration inaccuracies, we develop a Window-based Epipolar Transformer (WET), which matches reference pixels and source windows near the epipolar lines."" It claims that it introduces ""a window-based epipolar Transformer (WET) for enhancing patch-to-patch matching between the reference feature and corresponding windows near epipolar lines in source features"". To me, taking a window around the epipolar line into account seems like an approximation to estimating the uncertainty region around the epipolar lines caused by inaccuracies in calibration and camera pose and then searching within this region (see [Förstner & Wrobel, Photogrammetric Computer Vision, Springer 2016] for a detailed derivation of how to estimate uncertainties). Is it really valid to claim this part of the proposed approach as novel?
W2) I am not sure how significant the results on the DTU dataset are: a) The difference with respect to the best performing methods is less than 0.1 mm (see Tab. 1). Is the ground truth sufficiently accurate enough that such a small difference is actually noticeable / measurable or is the difference due to noise or randomness in the training process? b) Similarly, there is little difference between the results reported for the ablation study in Tab. 4. Does the claim ""It can be seen from the table that our proposed modules improve in both accuracy and completeness"" really hold? Why not use another dataset for the ablation study, e.g., the training set of Tanks & Temples or ETH3D?
W3) I am not sure what is novel about the ""novel geometric consistency loss (Geo Loss)"". Looking at Eq. 10, it seems to simply combine a standard reprojection error in an image with a loss on the depth difference. I don't see how Eq. 10 provides a combination of both losses.
W4) While the paper discusses prior work in Sec. 2, there is mostly no mentioning on how the paper under review is related to these existing works. In my opinion, a related work section should explain the relation of prior work to the proposed approach. This is missing.
W5) There are multiple parts in the paper that are unclear to me: a) What is C in line 106? The term does not seem to be introduced. b) How are the hyperparameters in Sec. 4.1 chosen? Is their choice critical? c) Why not include UniMVSNet in Fig. 5, given that UniMVSNet also claims to generate denser point clouds (as does the paper under review)? d) Why use only N=5 images for DTU and not all available ones? e) Why is Eq. 9 a reprojection error? Eq. 9 measures the depth difference as a scalar and no projection into the image is involved. I don't see how any projection is involved in this loss.
Overall, I think this is a solid paper that presents a well-engineered pipeline that represents the current state-of-the-art on a challenging benchmark. While I raised multiple concerns, most of them should be easy to address. E.g., I don't think that removing the novelty claim from W1 would make the paper weaker. The main exception is the ablation study, where I believe that the DTU dataset is too easy to provide meaningful comparisons (the relatively small differences might be explained by randomness in the training process.
The following minor comments did not affect my recommendation:
References are missing for Pytorch and the Adam optimizer.
Post-rebuttal comments
Thank you for the detailed answers. Here are my comments to the last reply:
Q: Relationship to prior work.
Thank you very much, this addresses my concern.
A: Fig. 5 is not used to claim our method achieves the best performance among all the methods in terms of completeness, it actually indicates that our proposed method could help reconstruct complete results while keeping high accuracy (Tab. 1) compared with our baseline network [7] and the most relevant method [3]. In that context, we not only consider the quality of completeness but also the relevance to our method to perform comparison in Fig. 5.
As I understand lines 228-236 in the paper, in particular ""The quantitative results of DTU evaluation set are summarized in Tab. 1, where Accuracy and Completeness are a pair of official evaluation metrics. Accuracy is the percentage of generated point clouds matched in the ground truth point clouds, while Completeness measures the opposite. Overall is the mean of Accuracy and Completeness. Compared with the other methods, our proposed method shows its capability for generating denser and more complete point clouds on textureless regions, which is visualized in Fig. 5."", the paper seems to claim that the proposed method generates denser point clouds. Maybe this could be clarified?
A: As a) nearly all the learning-based MVS methods (including ours) take the DTU as an important dataset for evaluation, b) the GT of DTU is approximately the most accurate GT we can obtain (compared with other datasets), c) the final results are the average across 22 test scans, we think that fewer errors could indicate better performance. However, your point about the accuracy of DTU GT is enlightening, and we think it's valuable future work.
This still does not address my concern. My question is whether the ground truth is accurate enough that we can be sure that the small differences between the different components really comes from improvements provided by adding components. In this context, stating that ""the GT of DTU is approximately the most accurate GT we can obtain (compared with other datasets)"" does not answer this question as, even though DTU has the most accurate GT, it might not be accurate enough to measure differences at this level of accuracy (0.05 mm difference). If the GT is not accurate enough to differentiate in the 0.05 mm range, then averaging over different test scans will not really help. That ""nearly all the learning-based MVS methods (including ours) take the DTU as an important dataset for evaluation"" does also not address this question. Since the paper claims improvements when using the different components and uses the results to validate the components, I do not think that answering the question whether the ground truth is accurate enough to make these claims in future work is really an option. I think it would be better to run the ablation study on a dataset where improvements can be measured more clearly.
Final rating
I am inclined to keep my original rating (""6: Weak Accept: Technically solid, moderate-to-high impact paper, with no major concerns with respect to evaluation, resources, reproducibility, ethical considerations.""). I still like the good results on the Tanks & Temples dataset and believe that the proposed approach is technically sound. However, I do not find the authors' rebuttals particularly convincing and thus do not want to increase my rating. In particular, I still have concerns about the ablation study as I am not sure whether the ground truth of the DTU dataset is accurate enough that it makes sense to claim improvements if the difference is 0.05 mm or smaller. Since this only impacts the ablation study, it is also not a reason to decrease my rating.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.",39n570rxyO,ICLR_2025,"This paper has weaknesses to address:
* The major weakness of this paper is the extremely limited experiments section. There are many experiments, yet almost no explanation of how they're run or interpretation of the results. Most of the results are written like an advertisement, mostly just stating the method outperforms others. This leaves the reader unclear why the performance gains happen. Ultimately it's not clear when/why the findings would generalize. The result is that some claims appear to be quite overstated. For example, L423-L424 states *""embeddings of domains with shared high-level semantics cluster together, as depicted in Appendix E.1. For example, embeddings of mono and stereo audio group closely, as do those of banking and economics.""* But this is cherry-picked---Temperature is way closer to Mono and Stereo Audio than Banking is to Economics.
* Similarly, many important experimental details are missing or relegated to the Appendix, and the Appendix also includes almost no explanations or interpretations. For example, the PCA experiments in Figures 3, 7, and 8 aren't explained.
* It's unclear how many variables actually overlap between training/testing, which seems to be a key element to make the model outperform others. Yet this isn't analyzed. Showing that others fail by ignoring other variables should be a key element of the experiments.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Since the sampling size 's' depends on the exponential term 2^[1/(e^2K-2)] (in Theorem 3.4), it could be very large if one requires the error tolerance 'e' to be relatively small and the order of tensor 'K' to be high. In that situation, there won't be much benefits to use this algorithm. Question:",NIPS_2017_236,NIPS_2017,"Weakness:
1. The real applications that the proposed method can be applied to seem to be rather restricted. It seems the proposed algorithm can only be used as a fast evaluation of residual error for 'guessing' or 'predetermining' the range of Tucker ranks, not the real ranks.
2. Since the sampling size 's' depends on the exponential term 2^[1/(e^2K-2)] (in Theorem 3.4), it could be very large if one requires the error tolerance 'e' to be relatively small and the order of tensor 'K' to be high. In that situation, there won't be much benefits to use this algorithm. Question:
1. In the Fig.1, why blue curve with large sample size 's=80' achieves the worst error compared with that of red curve with small sample size 's=20'?
Overall, although proposed algorithm is theoretically sound, but appears be limited in applications for practical propose.","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
"- No new evaluation metrics are proposed. Only existing evaluation metrics are linearly combined. In the experimental analysis section, there needed to be an in-depth exploration of the reasons for these experimental results.",UaZe4SwQF2,EMNLP_2023,"-	This paper is a bit difficult to follow. There are some unclear statements, such as motivation.
-	In the introduction, the summarized highlights need to be adequately elaborated, and the relevant research content of this paper needs to be detailed.
-	No new evaluation metrics are proposed. Only existing evaluation metrics are linearly combined. In the experimental analysis section, there needed to be an in-depth exploration of the reasons for these experimental results.
-	A case study should be added.
-	What are the advantages of your method compared to other evaluation metrics? Which needs to be emphasized in the motivation.
-	How do you evaluate the significance of model structure or metrics on the gender bias encoding of the model? Because you only conduct experiments in the FIBER model. Furthermore, you should conduct generalization experiments on the CLIP model or other models.
-	The citation format is chaotic in the paper.
-	There are some grammar mistakes in this paper, which could be found in “Typos, Grammar, Style, and Presentation Improvements”.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '0']}",,
"2). The notation K is abused too: it is used both for a known kernel function (e.g., L166) and the number of layers (e.g., L176).",NIPS_2021_1759,NIPS_2021,"The extension from the EH model is natural. In addition, there has been literature that proves the power of FNN from a theoretical point of view, whereas this paper fails to make a review in this regard. Among other works, Schmidt-Hieber (2020) gave an exact upper bound of the approximation error for FNNs involving the least-square loss. Since the DeepEH optimizes a likelihood-based loss, this paper builds up its asymptotic properties by following assumptions and proofs of Theorems 1 and 2 in Schmidt-Hieber (2020) as well as theories on empirical processes.
Additional Feedback: 1) In the manuscript, P
mostly represents a probability but sometimes for a cumulative distribution function (e.g., Eqs. (3) and (4) and L44, all in Appendix), which leads to confusion. 2). The notation K
is abused too: it is used both for a known kernel function (e.g., L166) and the number of layers (e.g., L176). 3). What is K b
in estimating baseline hazard (L172)?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The weak recovery problem studied here is primarily of theoretical interest, and it is not clear if the AMP algorithm is useful for non-Gaussian problems. So practical impact may be limited.",NIPS_2022_2005,NIPS_2022,"Originality: Main Result 1 relies on known formulas for low-rank matrix factorization. It is not clearly explained what are the major technical challenges, if any, in obtaining this result.
Clarity: The community labels in (3) and the model (4) are such that the E X
does not have sparse columns if k
is small. For this reason, I feel the paper is more about the large k
version of the sparse clustering problem.
Edit 08/19: After discussion with authors, the previous point is resolved.
Clarity: From the main text alone, it is unclear how the information-theoretic threshold is obtained. The formula of the MSE is difficult to interpret, so I can't see if the threshold is a consequence of this. Some further explanation is needed here.
Quality/Clarity: It is difficult to assess the rigour of both main results, especially Main Result 2. This is because the appendix is not organized in a conventional way with a clearly demarcated proof of Main Results 1 and 2. For Main Result 2, I do not see in the Appendix any explanation of how the asymptotic algorithmic MSE is computed. I only see plots rather than arguments. I also do not see any derivation of the Bayes-optimal MSE or reference to known (rigorous) formulas.
Minor issues
Line 39: Equation (2) defines vectors that are (i) standard Gaussian with probability ρ
OR (ii) the zero vector with probability 1 − ρ
. I think what is meant is for there to be a random subset of zero entries, with the rest of the entries being Gaussian.
Main Theorem 1: Please take a careful proofread over this. Here v ∗ , u ∗ , and w
have not been defined in (10). Also Z u
does not appear in (10).
Consider making the boundaries bolder in Figure 1. Also I found the color of λ i t
to be hard on the eyes
Line 70: Typo ""statitiscal""
Line 85: Typo ""analyis""
Line 91: Typo ""Statistics"", change to ""statistics""
Line 111: Change to ""In particular, [10] conjectured and [11] proved...""`
Line 143-144: This comment is hard to understand because (38) is in the Appendix, and then I'm having trouble seeing the connection to (6).
Line 195: Typo ""Invextigate""
Line 261: Replace ""Despite of this fact"" with ""Despite this fact""
Summary of score
My score is due to concerns mostly about the rigor and partially about the novelty of this submission. I also feel there is a lack of clarity in explaining how the main results are obtained.
Update of score 08/19
The authors' rebuttal addressed my concerns about rigor and somewhat about novelty. I agree with other reviewers that the strengths and technical challenges of this paper are not highlighted enough in the main text. I also think further clarity is needed on the level of rigor and the asymptotic regime to which the results apply (which seems to be for k growing large and rho going to 0). I have raised my overall score from 3 to 4 because further serious revision is needed. I have also upgraded the soundness from 1 to 2.
1. I agree with the authors' assessment that there are no apparent potential negative societal impacts.
2. The weak recovery problem studied here is primarily of theoretical interest, and it is not clear if the AMP algorithm is useful for non-Gaussian problems. So practical impact may be limited.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- Unclear whether bringing connections to human cognition makes sense As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"".",NIPS_2017_349,NIPS_2017,"- The paper is not self contained
Understandable given the NIPS format, but the supplementary is necessary to understand large parts of the main paper and allow reproducibility.
I also hereby request the authors to release the source code of their experiments to allow reproduction of their results.
- Use of deep-reinforcement learning is not well motivated
The problem domain seems simple enough that a linear approximation would have likely sufficed? The network is fairly small and isn't ""deep"" either.
- > We argue that such a mechanism is more realistic because it has an effect within the game itself, not just on the scores
This is probably the most unclear part. It's not clear to me why the paper considers one to be more realistic than the other rather than just modeling different incentives? Probably not enough space in the paper but actual comparison of learning dynamics when the opportunity costs are modeled as penalties instead. As economists say: incentives matter. However, if the intention was to explicitly avoid such explicit incentives, as they _would_ affect the model-free reinforcement learning algorithm, then those reasons should be clearly stated.
- Unclear whether bringing connections to human cognition makes sense
As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"".
* Minor comments
** Line 16:
> [18] found them...
Consider using \citeauthor{} ?
** Line 167:
> be the N -th agentâs
should be i-th agent?
** Figure 3:
Clarify what the `fillcolor` implies and how many runs were the results averaged over?
** Figure 4:
Is not self contained and refers to Fig. 6 which is in the supplementary. The figure is understandably large and hard to fit in the main paper, but at least consider clarifying that it's in the supplementary (as you have clarified for other figures from the supplementary mentioned in the main paper).
** Figure 5:
- Consider increasing the axes margins? Markers at 0 and 12 are cut off.
- Increase space between the main caption and sub-caption.
** Line 299:
From Fig 5b, it's not clear that |R|=7 is the maximum. To my eyes, 6 seems higher.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- Wording is overly exaggerated in the conclusion: "" ... our pioneering contributions herald a new era in robotic adaptability ... "". Word choice is a bit flamboyant in multiple places in the writing.",EODzbQ2Gy4,ICLR_2024,"- Wording is overly exaggerated in the conclusion: "" ... our pioneering
contributions herald a new era in robotic adaptability ... "". Word choice is a bit flamboyant in multiple places in the writing.
- This paper seems to only be tackle in-distribution task-transfer where typically transfer is thought of as learning task A can help with a completely different task B.
- Additionally, object shape transfer is mentioned as one of the applications, but only object pose transfer is considered in the experiments.
- Reward function seems to be very hand-engineered. How many data points is required to fit the Q-network with the pretraining dataset? Is this dataset hard to collect?
- Is there any comparison with other works that use differentiable physics for task transfer?
- One of the claimed novelty in this work is the path planning algorithm for sampling new subtasks. Can you include more comparisons against other path planning algorithms in classical literature like RRT, A*, sampling-based methods, etc?
Typo and writing comments:
Figure 1: Sub-Task Accomplishment ...
Section 5.2 MAML: repeated the word ""application""
Confusing last sentence in Section 5.1.4.
Section 5.3, why is this transfer task considered ""innovative""?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs.",Va4t6R8cGG,ICLR_2024,"- This paper does not seem to be the first work of fully end-to-end spatio-temporal localization, while TubeR has proposed to directly detect an action tubelet in a video by simultaneously performing action localization and recognition before. This weakens the novelty of this paper. The authors claim the differences with TubeR but the most significant difference is that the proposed method is much less complex.
- The symbols in this paper are inconsistent, e.g., b.
- The authors need to perform ablation experiments to compare the proposed method with other methods (e.g., TubeR) in terms of the number of learnable parameters and GFLOPs.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2) how the number of graph neural layers affects the overall performance. Overall, the paper is easy to read. The idea of integrating semantic information in few-shot classification is interesting while it has been widely explored in existing works. Given the reported results of the proposed model and lack of analyses in the proposed model, I am inclined to the score ""Ok but not good enough - rejection"". [1] “Cross Attention Network for Few-shot Classification”, Hou et al., NeurIPS ‘19",ICLR_2021_2804,ICLR_2021,"are listed as follows. Strengths:
The paper is easy to read, and the proposed idea is also easy to follow. Figure 1 can help the understanding of the proposed model.
The proposed model does not need the manual labeled relationship between semantic knowledge and target categories, and this may further reduce the supervision for knowledge graph construction. Weaknesses:
Incomplete comparison with existing works: the authors should list a more complete comparison table to compare the results of existing works. Some existing works can perform competitive or even superior results compared with the proposed model under the same feature backbone (ResNet-12).
mini-ImageNet tiered-ImageNet
1-shot 5-shot 1-shot 5-shot
Proposed 63.29+-0.71%. 80.12+-0.22% 66.69+-0.75%. 83.04+-0.61%
CAN[1] 63.85+-0.48%. 79.44+-0.34% 69.89+-0.51%. 84.23+-0.37%
CAN +T[1] 67.19+-0.55%. 80.64+-0.35% 73.21+-0.58%. 84.93+-0.38%
Comparison with unitary modality methods is misleading, as these baseline methods do not use any external information for the training and inference. The proposed method should consider other cross-modal few-shot classification works as baseline methods.
As the proposed model contains several modules, such as knowledge graph construction, graph neural network, and other few linear layers between components, this makes the reader hard to understand which module is crucial. It will be great if the authors could provide an ablation study to verify the effectiveness of each module in the next version of this paper. For example, the authors could provide studies like 1) what is the performance if the semantic information is removed in the knowledge graph, or 2) how the number of graph neural layers affects the overall performance.
Overall, the paper is easy to read. The idea of integrating semantic information in few-shot classification is interesting while it has been widely explored in existing works. Given the reported results of the proposed model and lack of analyses in the proposed model, I am inclined to the score ""Ok but not good enough - rejection"".
[1] “Cross Attention Network for Few-shot Classification”, Hou et al., NeurIPS ‘19","{'annotators': ['qDjzAPQZ'], 'labels': ['2']}",,,5,"{'annotators': ['qDjzAPQZ'], 'labels': ['4']}",,,"{'annotators': ['qDjzAPQZ'], 'labels': ['3']}",,,"{'annotators': ['qDjzAPQZ'], 'labels': ['3']}",,,"{'annotators': ['qDjzAPQZ'], 'labels': ['1']}",,,"{'annotators': ['qDjzAPQZ'], 'labels': ['0']}",,,"{'annotators': ['qDjzAPQZ'], 'labels': ['1']}",,
- The biggest weakness of the paper is that it does not compare to simple feature acquisition baselines like expected utility or some such measure to prove the effectiveness of the proposed approach. Writing style and other issues:,NIPS_2018_125,NIPS_2018,"- Some missing references and somewhat weak baseline comparisons (see below) - Writing style needs some improvement, although, it is overall well written and easy to understand. Technical comments and questions: - The idea of active feature acquisition, especially in the medical domain was studied early on by Ashish Kapoor and Eric Horvitz. See https://www.microsoft.com/en-us/research/wp-content/uploads/2016/12/NIPS2009.pdf There is also a number of missing citations to work on using MDPs for acquiring information from external sources. Kanani et al, WSDM 2012, Narsimhan et al, ""Improving Information Extraction by Acquiring External Evidence with Reinforcement Learning"", and others. - section 3, line 131: ""hyperparameter balancing the relative importances of two terms is absorbed in the predefined cost"". How is this done? The predefined cost could be externally defined, so it's not clear how these two things interact. - section 3.1, line 143"" ""Then the state changes and environment gives a reward"". This is not true of standard MDP formulations. You may not get a reward after each action, but this makes it sound like that. Also, line 154, it's not clear if each action is a single feature or the power set. Maybe make the description more clear. - The biggest weakness of the paper is that it does not compare to simple feature acquisition baselines like expected utility or some such measure to prove the effectiveness of the proposed approach. Writing style and other issues: - Line 207: I didn't find the pseudo code in the supplementary material - The results are somewhat difficult to read. It would be nice to have a more cleaner representation of results in figures 1 and 2. - Line 289: You should still include results of DWSC if it's a reasonable baseline - Line 319: your dollar numbers in the table don't match! - The paper will become more readable by fixing simple style issues like excessive use of ""the"" (I personally still struggle with this problem), or other grammar issues. I'll try and list most of the fixes here. 4: joint 29: only noise 47: It is worth noting that 48: pre-training is unrealistic 50: optimal learning policy 69: we cannot guarantee 70: manners meaning that => manner, that is, 86: work 123: for all data points 145: we construct an MDP (hopefully, it will be proper, so no need to mention that) 154: we assume that 174: learning is a value-based 175: from experience. To handle continuous state space, we use deep-Q learning (remove three the's) 176: has shown 180: instead of basic Q-learning 184: understood as multi-task learning 186: aim to optimize a single 208: We follow the n-step 231: versatility (?), we perform extensive 233: we use Adam optimizer 242: We assume uniform acquisition cost 245: LSTM 289: not only feature acquisition but also classification. 310: datasets 316: examination cost?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"11 is wonderful, how about other bit operations? Fig. 5 a seems strange, please give more explanations. When the input is aer format, how did you deal with DVS input? If you can analyze the energy consumption as reference[15] did, this paper would be more solid.",NIPS_2021_2123,NIPS_2021,"This paper still existed some problems that I hope the authors could illustrate in a clearer way.
The authors argued that they were the first time to directly training deep SNNs with more than 100 layers. I don’t think this is the core contribution in this paper, because of the residual block, the spiking could be deeper. In my opinion, SEW structure is the most important point in this paper, and directly training a 50-layer and 100-layer snn is not a huge breakthrough. Otherwise, if they could give a more detailed analysis about why other methods can’t train a 100-layer snn except section 3.2, it may be more reasonable.
Why the RBA block can be seen as a special case of the SEW block? I mean SEW is another kind of RBA with binary input and output.
Equ. 11 is wonderful, how about other bit operations?
Fig. 5 a seems strange, please give more explanations.
When the input is aer format, how did you deal with DVS input?
If you can analyze the energy consumption as reference[15] did, this paper would be more solid.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed.",ICLR_2022_21,ICLR_2022,"However, some key architectural details can be clarified further for full reproducibility and analysis. Specifically: 1. How are historical observations combined with inputs known over all time given differences in sequence lengths (L vs L+M)? The text mentions separate embedding and addition with positional encoding, but clarifications on how the embeddings are combined and fed into the CSCM are needed. 2. Can each node attend to its own lower-level representation? From equation 2, it seems to be that only neighbouring nodes are attended to, based on the description of N_l^(s). 3. Do the authors have any guidelines on how to select S/A/C (and consequently N) for a given receptive field L?
In addition, while the ablation analysis tests the impact of changing CSCM architectures, it would be good to evaluate the base performance without the PAM to determine the value added by attention. This would also provide a simple comparison vs dilated CNNs which have been used successfully in time series forecasting applications (e.g. WaveNet).
Finally, could I double check which dataset was used for the ablation analysis as well? I seem to be having some difficulty lining the numbers in Tables 4-6 up with Table 3.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. As mentioned in the article itself, the introduction of multi-granularity and multi-scale to enhance model performance is a common approach to convolutional networks, and merely migrating this approach to the field of MLMs is hardly an innovative contribution. Some of the algorithms used in the article from object detection only do some information enhancement on the input side, while many MLMs can already accomplish the object detection task by themselves nowadays.",50RNY6uM2Q,ICLR_2025,"1. As mentioned in the article itself, the introduction of multi-granularity and multi-scale to enhance model performance is a common approach to convolutional networks, and merely migrating this approach to the field of MLMs is hardly an innovative contribution. Some of the algorithms used in the article from object detection only do some information enhancement on the input side, while many MLMs can already accomplish the object detection task by themselves nowadays.
2. The scores achieved on both the MMBench as well as SEEDBench datasets, while respectable, are not compared to some of the more competitive models. I identified MMB as version 1 and SEEDBench as Avg based on the scores of Qwen-VL and MiniCPM-V2, and there are a number of scores on both leaderboards that are higher than the scores of MG-LLaVA work, eg. Honeybee (Cha et al., 2024), AllSeeing-v2 (Wang et al. 2024) based on Vicuna-13b at MMB-test. and then you can also find a lot of similar models with higher scores on the same substrate.
3. In addition to Perception Benchmarks. this problem can also be found in Visual QA and Video QA. such as on the MSRVTT-QA dataset. there are already many models with very high scores in 2024. Some of them also use some methods to improve the model's ability on fine-grained tasks. eg. Flash-VStream (Zhang et al. 2024) Monkey (Li et al. 2023). The article does not seem to compare these new 2024 models.
To summarize, I think the approach proposed in the article is valid, but MG-LLaVA does not do the job of making a difference, either from an innovation perspective or from a performance perspective.
[1] Cha, Junbum, et al. ""Honeybee: Locality-enhanced projector for multimodal llm."" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2024.
[2] Wang, Weiyun, et al. ""The all-seeing project v2: Towards general relation comprehension of the open world."" *arXiv preprint arXiv:2402.19474* (2024).
[3] Zhang, Haoji, et al. ""Flash-VStream: Memory-Based Real-Time Understanding for Long Video Streams."" *arXiv preprint arXiv:2406.08085* (2024).
[4] Li, Zhang, et al. ""Monkey: Image resolution and text label are important things for large multi-modal models."" *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*. 2024.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"5. The sensitivity of hyper-parameters such as $m_1$, $m_2$, $\lambda$ is not discussed. In particular, their values are not specified in the paper.",ICLR_2023_2286,ICLR_2023,"1. The paper is poorly organized. It is hard to quickly get the motivations and main ideas of the proposed methods.
2. The thermal sensor and environment setting for data collection is not described in details. From Figure 2, why is the quality of thermal image significantly higher than RegDB and SYSU-MM01 ? Does the thermal sensor or capturing time cause it?
3. The paper presents a transformer-based network as backbone, what is the benefits over the CNN-based backbones in traditional methods? The reason using such a transformer-based method is not clearly discussed.
4. The proposed multi-task triplet loss is not clearly clarified. It is strongly to re-organize the part and make a proof-reading. In addition, It seems there is mistake in Eq. (1). I suppose a max( x,0) is lost for both terms in $L_{mtri}$.
5. The sensitivity of hyper-parameters such as $m_1$, $m_2$, $\lambda$ is not discussed. In particular, their values are not specified in the paper.
6. There are lots of grammar mistakes, typos and description blurs that makes the paper hard to follow. It is strongly to find some experts to make a proofreading.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. I am concerned whether the similarity-aware positive sample selection will accelerate GNN-based encoder over-smoothing, i.e., similar nodes or graphs will be trained with features that converge excessively and discard their own unique features. In addition, whether selecting positive samples in the same dataset without introducing some perturbation noise would lead to lower generalization performance. The authors experimented with the transfer performance of the model on the graph classification task, though it still did not allay my concerns about the model generalization. I hope there will be more experiments on different downstream tasks and across different domains. Remarks:",NIPS_2021_2338,NIPS_2021,"Weakness: 1. Regarding the adaptive masking part, the authors' work is incremental, and there have been many papers on how to do feature augmentation, such as GraphCL[1], GCA[2]. The authors do not experiment with widely used datasets such as Cora, Citeseer, ArXiv, etc. And they did not compare with better baselines for node classification, such as GRACE[3], GCA[2], MVGRL[4], etc. I think this part of the work is shallow and not enough to constitute a contribution. The authors should focus on the main contribution, i.e., graph-level contrastive learning, and need to improve the node-level augmentation scheme. 2. In the graph classification task, the compared baseline is not sufficient, such as MVGRL[4], gpt-gnn[5] are missing. I hope the authors could add more baselines of graph contrastive learning and test them on some common datasets. 3. I am concerned whether the similarity-aware positive sample selection will accelerate GNN-based encoder over-smoothing, i.e., similar nodes or graphs will be trained with features that converge excessively and discard their own unique features. In addition, whether selecting positive samples in the same dataset without introducing some perturbation noise would lead to lower generalization performance. The authors experimented with the transfer performance of the model on the graph classification task, though it still did not allay my concerns about the model generalization. I hope there will be more experiments on different downstream tasks and across different domains. Remarks: 1. The authors seem to have over-compressed the line spacing and abused vspace. 2. Table 5 is collapsed.
[1] Y. You, T. Chen, Y. Sui, T. Chen, Z. Wang, and Y. Shen, “Graph contrastive learning with augmentations,” Advances in Neural Information Processing Systems, vol. 33, 2020. [2] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang, “Graph contrastive learning with adaptive augmentation,” arXiv preprint arXiv:2010.14945, 2020. [3] Y. Zhu, Y. Xu, F. Yu, Q. Liu, S. Wu, and L. Wang, “Deep graph contrastive representation learning,” arXiv preprint arXiv:2006.04131, 2020. [4] Hassani, Kaveh, and Amir Hosein Khasahmadi. ""Contrastive multi-view representation learning on graphs."" International Conference on Machine Learning. PMLR, 2020. [5] Hu, Ziniu, et al. ""Gpt-gnn: Generative pre-training of graph neural networks."" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
- is fast SMP less expressive than SMP ? I wish to have seen more discussion on the power of different architectures.,NIPS_2020_1185,NIPS_2020,"- The theory (thm 2, cor 1) on the representational power of SMPs is only for simple unlabeled graphs. Is there any similar result for graphs with node and/or edge features ? - The experiments are quite limited. I wish to have seen SMPs in the context of graphs with node and edge features and on standard benchmarks used by GCNs/GIN/GAT models. - how good is SMP at counting cycles ? - is fast SMP less expressive than SMP ? I wish to have seen more discussion on the power of different architectures. - are, in the limit of using sufficiently many layers, all embeddings of node j at each node i becoming equal ? Can this be formally proved ? If this is not true, then isn't it a weakness that each node can learn a potentially different representation of all the nodes in the graph ? - how well is fast SMP perming on the cycle detection task ? ====================== Later edit: I agree with other reviewers' comments on the lack of powerful equivariant baselines and I do believe that the current experimental setup is limited. I am lowering my score as a consequence.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '1', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"6) To obtain robust results, it would have been better to evaluate the methods across different splits of train-val-test, not simply different initialisation seeds.",ICLR_2023_2163,ICLR_2023,"Fully training group labels baselines could include more recent methods such as SGDRO (non-flat version of GDRO proposed in Goel et al.). There is a misleading sentence on page 3 when describing the Waterbirds dataset: “The bird images are then modified with either a water or land background.” There is no “modification” involved, in the dataset the birds are already placed either on water or land background (this is a minor note, but should be revisited). The main weaknesses are all in the “Experiments” section. 1) Although the hyperparameters are fixed, some ablation is still required (for example the regularisation in the second stage for the CivilComments dataset). 2) In Tables 1 and 2 it is not clear what the results are, is it correct to assume they are the mean and std. performances over three different initialization seeds? 3) The results in Tables 1 and 2 are not correct, CROIS is using the validation set for training in the second stage, not only for model selection. For the datasets considered, the validation set has the same distribution as the test set, it does not seem correct to talk about “group shift” anymore. 4) Table 2 should also include the average test accuracy to show the trade-off between average and worst-case performances when varying the size of the validation set. Also, why is the std. not reported here? 5) Table 3 shows promising results, but the best fraction “p” is a hyperparameter that should be tuned using the worst-group performance on the validation set, like the regularization term for GDRO. Its behaviour is not linear (e.g. the more the better) nor consistent across datasets. For example, in both text datasets, only one fraction produces better results than the GDRO baseline. Here, I disagree with the statement “In practice, p is not a parameter to choose (there’s no reason to throw away group labels) but rather is limited by the resources available to obtain group labels”: using p=0.5 is often worse than using p=0.3. 6) To obtain robust results, it would have been better to evaluate the methods across different splits of train-val-test, not simply different initialisation seeds.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
6. The first two bullets about contributions (at the end of the intro) can be combined together.,NIPS_2017_631,NIPS_2017,"1.	The main contribution of the paper is CBN. But the experimental results in the paper are not advancing the state-of-art in VQA (on the VQA dataset which has been out for a while and a lot of advancement has been made on this dataset), perhaps because the VQA model used in the paper on top of which CBN is applied is not the best one out there. But in order to claim that CBN should help even the more powerful VQA models, I would like the authors to conduct experiments on more than one VQA model â favorably the ones which are closer to state-of-art (and whose codes are publicly available) such as MCB (Fukui et al., EMNLP16), HieCoAtt (Lu et al., NIPS16). It could be the case that these more powerful VQA models are already so powerful that the proposed early modulating does not help. So, it is good to know if the proposed conditional batch norm can advance the state-of-art in VQA or not.
2.	L170: it would be good to know how much of performance difference this (using different image sizes and different variations of ResNets) can lead to?
3.	In table 1, the results on the VQA dataset are reported on the test-dev split. However, as mentioned in the guidelines from the VQA dataset authors (http://www.visualqa.org/vqa_v1_challenge.html), numbers should be reported on test-standard split because one can overfit to test-dev split by uploading multiple entries.
4.	Table 2, applying Conditional Batch Norm to layer 2 in addition to layers 3 and 4 deteriorates performance for GuessWhat?! compared to when CBN is applied to layers 4 and 3 only. Could authors please throw some light on this? Why do they think this might be happening?
5.	Figure 4 visualization: the visualization in figure (a) is from ResNet which is not finetuned at all. So, it is not very surprising to see that there are not clear clusters for answer types. However, the visualization in figure (b) is using ResNet whose batch norm parameters have been finetuned with question information. So, I think a more meaningful comparison of figure (b) would be with the visualization from Ft BN ResNet in figure (a).
6.	The first two bullets about contributions (at the end of the intro) can be combined together.
7.	Other errors/typos:
a.	L14 and 15: repetition of word âimagineâ
b.	L42: missing reference
c.	L56: impact -> impacts
Post-rebuttal comments:
The new results of applying CBN on the MRN model are interesting and convincing that CBN helps fairly developed VQA models as well (the results have not been reported on state-of-art VQA model). So, I would like to recommend acceptance of the paper.
However I still have few comments --
1. It seems that there is still some confusion about test-standard and test-dev splits of the VQA dataset. In the rebuttal, the authors report the performance of the MCB model to be 62.5% on test-standard split. However, 62.5% seems to be the performance of the MCB model on the test-dev split as per table 1 in the MCB paper (https://arxiv.org/pdf/1606.01847.pdf).
2. The reproduced performance reported on MRN model seems close to that reported in the MRN paper when the model is trained using VQA train + val data. I would like the authors to clarify in the final version if they used train + val or just train to train the MRN and MRN + CBN models. And if train + val is being used, the performance can't be compared with 62.5% of MCB because that is when MCB is trained on train only. When MCB is trained on train + val, the performance is around 64% (table 4 in MCB paper).
3. The citation for the MRN model (in the rebuttal) is incorrect. It should be -- @inproceedings{kim2016multimodal,
title={Multimodal residual learning for visual qa},
author={Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle={Advances in Neural Information Processing Systems}, pages={361--369}, year={2016} }
4. As AR2 and AR3, I would be interested in seeing if the findings from ResNet carry over to other CNN architectures such as VGGNet as well.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The types of situations/social norms (e.g., physical/psychological safety) are not clear in the main paper.",vg55TCMjbC,EMNLP_2023,"- Although the situations are checked by human annotators, the seed situations are generated by ChatGPT. The coverage of situation types might be limited.
- The types of situations/social norms (e.g., physical/psychological safety) are not clear in the main paper.
- It’s a bit hard to interpret precision on NormLens-MA, where the different labels could be considered as gold.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"3. The paper is locally well-written and the technical presentation flows easily: I can understand the statement of each theorem without having to wade through too much notation, and the authors do a good job of conveying the gist of the proofs. Second, the weaknesses:",NIPS_2016_370,NIPS_2016,", and while the scores above are my best attempt to turn these strengths and weaknesses into numerical judgments, I think it's important to consider the strengths and weaknesses holistically when making a judgment. Below are my impressions. First, the strengths: 1. The idea to perform improper unsupervised learning is an interesting one, which allows one to circumvent certain NP hardness results in the unsupervised learning setting. 2. The results, while mostly based on ""standard"" techniques, are not obvious a priori, and require a fair degree of technical competency (i.e., the techniques are really only ""standard"" to a small group of experts). 3. The paper is locally well-written and the technical presentation flows easily: I can understand the statement of each theorem without having to wade through too much notation, and the authors do a good job of conveying the gist of the proofs. Second, the weaknesses: 1. The biggest weakness is some issues with the framework itself. In particular: 1a. It is not obvious that ""k-bit representation"" is the right notion for unsupervised learning. Presumably the idea is that if one can compress to a small number of bits, one will obtain good generalization performance from a small number of labeled samples. But in reality, this will also depend on the chosen model class used to fit this hypothetical supervised data: perhaps there is one representation which admits a linear model, while another requires a quadratic model or a kernel. It seems more desirable to have a linear model on 10,000 bits than a quadratic model on 1,000 bits. This is an issue that I felt was brushed under the rug in an otherwise clear paper. 1b. It also seems a bit clunky to work with bits (in fact, the paper basically immediately passes from bits to real numbers). 1c. Somewhat related to 1a, it wasn't obvious to me if the representations implicit in the main results would actually lead to good performance if the resulting features were then used in supervised learning. I generally felt that it would be better if the framework was (a) more tied to eventual supervised learning performance, and (b) a bit simpler to work with. 2. I thought that the introduction was a bit grandiose in comparing itself to PAC learning. 3. The main point (that improper unsupervised learning can overcome NP hardness barriers) didn't come through until I had read the paper in detail. When deciding what papers to accept into a conference, there are inevitably cases where one must decide between conservatively accepting only papers that are clearly solid, and taking risks to allow more original but higher-variance papers to reach a wide audience. I generally favor the latter approach, I think this paper is a case in point: it's hard for me to tell whether the ideas in this paper will ultimately lead to a fruitful line of work, or turn out to be flawed in the end. So the variance is high, but the expected value is high as well, and I generally get the sense from reading the paper that the authors know what they are doing. So I think it should be accepted. Some questions for the authors (please answer in rebuttal): -Do the representations implicit in Theorems 3.2 and Theorem 4.1 yield features that would be appropriate for subsequent supervised learning of a linear model (i.e., would linear combinations of the features yield a reasonable model family)? -How easy is it to handle e.g. manifolds defined by cubic constraints with the spectral decoding approach?","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['5', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['X', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
"1. I understand this paper targets a problem which somewhat differs from general segmentation problems. And I do very much appreciate its potential benefit to the neuroscience community. This is indeed a plus for the paper. However, an important question is how much this paper can really improve over the existing solutions. Therefore, to demonstrate that the algorithm is able to correctly find closed contours, and really show stronger robustness against weak boundaries (This is especially important for bottom up methods), the authors do need to refer to more recent trends in the vision community.",NIPS_2016_221,NIPS_2016,"weakness: 1. To my understanding, two aspects which are the keys to the segmentation performance are: (1) The local DNN evaluation of shape descriptors in terms of energy, and (2) The back-end guidance of (super)voxel agglomeration. Although experiment showed gains of the proposed method over GALA, it is yet not clear enough which part is the major contributor of such gain. The original paper of GALA used methods different from this paper (3D-CNN) to generate edge probability maps. Is the edge map extraction framework in this paper + GALA a fair enough baseline? It would be great if such edge map can also be visualized. 2. The proposed method to some extent is not that novel. The idea of generating or evaluating segmentation masks with DNN has been well studied by the vision community in many general image segmentation tasks. In addition, the idea of greedily guiding the agglomeration of (super)voxels by evaluating energies pretty much resembles many bottom-up graph-theoretic merging in early segmentation methods. The authors, however, failed to mention and compare with many related works from the general vision community. Although the problem general image segmentation somewhat differs from the task of this paper, showing certain results on general image segmentation datasets (like the GALA paper) and comparing with state-of-the-art general segmentation methods may give a better view of the performance of the proposed method. 3. Certain parts did not provide enough explanation, or are flooded by too much details and fail to give the big picture clearly enough. For example in Appendix B Definition 2, when explaining the relationship between shape descriptor and connectivity region, some graphical illustrations would have helped the readers understanding the ideas in the paper much easier and better. ----------------------------------------------------------------------------- Additional Comments after Discussion Upon carefully reading the rebuttal and further reviewing some of the related literature, I decide to down-grade scores on novelty and impact. Here are some of the reasons: 1. I understand this paper targets a problem which somewhat differs from general segmentation problems. And I do very much appreciate its potential benefit to the neuroscience community. This is indeed a plus for the paper. However, an important question is how much this paper can really improve over the existing solutions. Therefore, to demonstrate that the algorithm is able to correctly find closed contours, and really show stronger robustness against weak boundaries (This is especially important for bottom up methods), the authors do need to refer to more recent trends in the vision community. 2. I noticed one reference ""Maximin affinity learning of image segmentation, NIPS 2009"" cited in this paper. The paper proposed a very elegant solution to affinity learning. The core ideas proposed in this paper, such as greedy merging, Rand Index like energy function show strong connections to the cited paper. The maximin affinity is basically the weakest edge along a minimum spanning tree (MST), and we know greedy region merging is also based on cutting weakest edges on a MST. The slight difference is the author proposed the energy term at a lot of positions and scales but the previous paper has a single energy term for the global image. In addition cited paper also addressed a similar problem in the experiment. However, the authors not only did not include the citation as an experimental baseline, but also failed to provide detailed discussions on the relation between the two works. 3. The authors argued for greedy strategy, claiming this is better than factorizable energies. ""By sacrificing factorization, we are able to achieve the rich combinatorial modeling provided by the proposed 3d shape descriptors."" I guess what the authors mean is they are putting more emphasis on local predictions. But this statement is not solidly justified by the paper. In addition, although to some extent I could understand this argument (local prediction indeed seems important because the size of cells vs volume are much smaller than segments vs whole image in general segmentation), there are significant advances on making pretty strong local predictions from the general vision community using deep learning, which the authors fail to mention and compare. I think overall the paper addressed an interesting problem and indeed showed solid research works. However it will better if the authors could better address contemporary segmentation literature and further improve the experiments.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. I expect more baselines to be compared and more domains to be tested. As I mentioned, the choices of the weighting and the way of learning density functions are not strongly motivated. In this case, I have to ask for stronger empirical results: baselines with other design choices and more domains.",ICLR_2022_2754,ICLR_2022,"I feel the motivation of the work is confusing. I can understand the authors want to improve CQL somehow further. But it is never made clear:
what existing problems are and why they matter; Is it the lower bound on the exiting CQL is too loose? Why is improving the bound important?
what is the effect you want to achieve? Is it an offline algorithm that can learn a better policy from data generated by a poor behavior policy?
The contribution is incremental, and I doubt the significance. As the paper cited in section 2.2, Kumar et al. (2020) propose to penalize the actions not described by the dataset, which enables a general definition of \mu. Note that the additional weighting scheme can be essentially thought of as a new type of \mu. I don’t see a clear difference between (2) and (3). Both can be considered as the specially designed action sampling distribution.
Why is theorem 1 useful? If I understand correctly, the key thing you want to say is the additional weighting can provide a tighter bound for OOD state-action pairs or those not close to the dataset? But the first step should be figuring out the effects of having a tight/loose bound. Does it hurt optimality/convergence rate/generalization…? Even partially answering this question can better motivate the reweighting approach. I believe the proof of the theorem is a simple modification from the existing CQL work.
The choice of the weighting scheme lacks justification. An intuitive choice is the RBF function. Furthermore, according to theorem 1, the proposed weighting is useful only when the action is OOD, i.e., the weight is 1; when the action is ID, weight should be 0, but your weighting scheme does not give zero?
Section 4.1, the proposed method comes out suddenly. Is there any reason to choose normalizing flows? Of course, normalizing flows is a good method enabling both efficient sampling and density evaluation. In your algorithm, you only need to evaluate the density but not to sample. There should be plenty of other choices. When testing ideas, it is more natural to start with some simple methods.
What is the purpose of section 4.3?
I expect to see how various concrete choices of the weighting scheme can affect the distance (e.g., KL divergence) between the learned policy and the behavior policy.
The experiments. 1. more random seeds should be tested (figure 1) — it is hard to distinguish algorithms from the current learning curves. Readers cannot see a clear message from them. 2. I expect more baselines to be compared and more domains to be tested. As I mentioned, the choices of the weighting and the way of learning density functions are not strongly motivated. In this case, I have to ask for stronger empirical results: baselines with other design choices and more domains. 3. The experiments in Fig 2 are incomplete. Why are there no experiments for half cheetah and walker with expert data? 4. Please provide reproducing details.
The abstract says, “… with a strong theoretical guarantee.” I don’t think there is any strong theory in the paper.
Page 3. Last paragraph. The criticism towards using empirical dataset distribution for \hat{pi}_\beta does not make sense to me. When the state/action is continuous, the empirical estimation should be kernel density estimation, which is a consistent estimator for estimating empirical distributions. The kernel can be chosen as smooth, so the KDE should have support everywhere. Minor:
there is a nontrivial number of grammar issues/typos. Please double-check.
Many sentences are confusing or logically disconnected.
e.g., in the abstract, “A compromise between enhancing …. To alleviate this issue, … ” what issue?
“Improving the learning process.” In what sense? Higher sample efficiency?
“Indeed, the lack of information … ” what information? Why does it provoke overestimation?
I believe saying “based on the amount of information collected” is inaccurate because the paper does not really introduce any information measurement.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
1. Please define the dashed lines in fig. 2A-B and 4B.,NIPS_2016_153,NIPS_2016,"weakness of previous models. Thus I find these results novel and exciting.Modeling studies of neural responses are usually measured on two scales: a. Their contribution to our understanding of the neural physiology, architecture or any other biological aspect. b. Model accuracy, where the aim is to provide a model which is better than the state of the art. To the best of my understanding, this study mostly focuses on the latter, i.e. provide a better than state of the art model. If I am misunderstanding, then it would probably be important to stress the biological insights gained from the study. Yet if indeed modeling accuracy is the focus, it's important to provide a fair comparison to the state of the art, and I see a few caveats in that regard: 1. The authors mention the GLM model of Pillow et al. which is pretty much state of the art, but a central point in that paper was that coupling filters between neurons are very important for the accuracy of the model. These coupling filters are omitted here which makes the comparison slightly unfair. I would strongly suggest comparing to a GLM with coupling filters. Furthermore, I suggest presenting data (like correlation coefficients) from previous studies to make sure the comparison is fair and in line with previous literature. 2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes: 1. Please define the dashed lines in fig. 2A-B and 4B. 2. Why is the training correlation increasing with the amount of training data for the cutout LN model (fig. 4A)? 3. I think figure 6C is a bit awkward, it implies negative rates, which is not the case, I would suggest using a second y-axis or another visualization which is more physically accurate. 4. Please clarify how the model in fig. 7 was trained. Was it on full field flicker stimulus changing contrast with a fixed cycle? If the duration of the cycle changes (shortens, since as the authors mention the model cannot handle longer time scales), will the time scale of adaptation shorten as reported in e.g Smirnakis et al. Nature 1997.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.",ICLR_2022_2196,ICLR_2022,"weakness] Modeling:
The rewards are designed based on a discriminator. As we know, generative adversarial networks are not easy to train since generative networks and discriminative networks are trained alternatively. In the proposed method, the policy network and the discriminator are trained alternatively. I doubt if it is easy to train the model. I would like to see the training curves for rewards value.
The detailed alignment function used in Eq. (1) and Eq. (3) need to be provided.
Experiment: - The results are not satisfying. In the experiment, the generation quality of the proposed method is not good as traditional generative networks in terms of FID. In the image parsing part, the results are far behind the compared methods. - Since the results are not comparable to the existing methods, there seems not too much significance for the proposed methods.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Limited novelty. The paper seems like a straightforward application of existing literature, specifically the DeCorr [1] that focuses on general deep graph neural networks, in a specific application domain. The contribution of this study is mainly the transposition of DeCorr's insights into graph collaborative filtering, with different datasets and backbones. Although modifications like different penalty coefficients for users and items are also proposed, the whole paper still lack enough insights about what are unique challenges of overcorrelation in recommender systems.",53kW6e1uNN,ICLR_2024,"1. Limited novelty. The paper seems like a straightforward application of existing literature, specifically the DeCorr [1] that focuses on general deep graph neural networks, in a specific application domain. The contribution of this study is mainly the transposition of DeCorr's insights into graph collaborative filtering, with different datasets and backbones. Although modifications like different penalty coefficients for users and items are also proposed, the whole paper still lack enough insights about what are unique challenges of overcorrelation in recommender systems.
2. It could be better if one additional figure could be illustrated, i.e., how Corr and SMV metrics evolve with the application of additional network layers—mirroring the Figure 2, but explicitly showcasing the effects of the proposed method—the authors could convincingly validate their auxiliary loss function's efficacy.
3. Presentation issues. The y-axis labels of Figure 2 lack standardization, e.g., 0.26 vs. 0.260 vs. 2600 vs. .2600.
[1] Jin et al. Feature overcorrelation in deep graph neural networks: A new perspective. KDD 2022.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The proposed model contains lots of hyperparameters, and the most important ones are evaluated in ablation studies in the experimental section. It would have been nice to see significance tests for the various configurations in Table 3.",NIPS_2017_575,NIPS_2017,"- While the general architecture of the model is described well and is illustrated by figures, architectural details lack mathematical definition, for example multi-head attention. Why is there a split arrow in Figure 2 right, bottom right? I assume these are the inputs for the attention layer, namely query, keys, and values. Are the same vectors used for keys and values here or different sections of them? A formal definition of this would greatly help readers understand this.
- The proposed model contains lots of hyperparameters, and the most important ones are evaluated in ablation studies in the experimental section. It would have been nice to see significance tests for the various configurations in Table 3.
- The complexity argument claims that self-attention models have a maximum path length of 1 which should help maintaining information flow between distant symbols (i.e. long-range dependencies). It would be good to see this empirically validated by evaluating performance on long sentences specifically.
Minor comments:
- Are you using dropout on the source/target embeddings?
- Line 146: There seems to be dangling ""2""","{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that whether there is information redundancy and interference in the multi-sphere icosahedral discretization process.",ICLR_2021_1465,ICLR_2021,"1. The complexity analysis is insufficient. In the draft, the author only provide the rough overall complexity. A better way is to show the comparison between the proposed method and some other methods, including the number of model parameter and network forwarding time.
2. In the converting of point cloud to concentric spherical signal, the Gaussian radial basis function is adopted to summarize the contribution of points. Is there any other function that can accomplish this job? The reviewer would like to the discussion about this.
3. The Figure 2 is a little ambiguous, where some symbols are not explained clearly. And the reviewer is curious about that whether there is information redundancy and interference in the multi-sphere icosahedral discretization process.
4. There are some typos in the draft. The first is the wrong use of ""intra-sphere"" and ""inter-sphere"". The second is the use of two consecutive ""stacking"" in the Spherical Discretization subsection. Please check the full text carefully.
5. The center choice of the concentric spheres should be discussed both theoretically and experimentally. In the opinion, the center of spheres play a important role in the representation capturing of 3D point clouds in a sphere convolution manner.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- The limitation of the obtained results: The authors assume that the spectrum of a kernel is sub-gaussian. This is OK, as the popular Gaussian kernels are in this class. However, another popular class of kernels such as Matern kernels are not included, since their spectrum only decay polynomially. In this sense, the results of the paper could be restrictive.",NIPS_2017_585,NIPS_2017,"weakness of the paper is in the experiments: there should be more complete comparisons in computation time, and comparisons with QMC-based methods of Yang et al (ICML2014). Without this the advantage of the proposed method remains unclear.
- The limitation of the obtained results:
The authors assume that the spectrum of a kernel is sub-gaussian. This is OK, as the popular Gaussian kernels are in this class. However, another popular class of kernels such as Matern kernels are not included, since their spectrum only decay polynomially. In this sense, the results of the paper could be restrictive.
- Eq. (3):
What is $e_l$?
Corollaries 1, 2 and 3 and Theorem 4:
All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).
- The equation in Line 170:
What is $e_i$?
- Subsampled dense grid:
This approach is what the authors used in Section 5 on experiments. However, it looks that there is no theoretical guarantee for this method. Those having theoretical guarantees seem not to be practically useful.
- Reweighted grid quadrature:
(i) It looks that there is no theoretical guarantee with this method.
(ii) The approach reminds me of Bayesian quadrature, which essentially obtains the weights by minimizing the worst case error in the unit ball of an RKHS. I would like to look at comparison with this approach.
(iii) Would it be possible to derive a time complexity?
(iv) How do you chose the regularization parameter $\lambda$ in the case of the $\ell_1$ approach?
- Experiments in Section 5:
(i) The authors reported the results of computation time very briefly (320 secs vs. 384 seconds for 28800 features in MNIST and ""The quadrature-based features ... are about twice as fast to generate, compared to random Fourier features ..."" in TIMIT). I do not they are not enough: the authors should report the results in the form of Tables, for example, varying the number of features.
(ii) There should be comparison with the QMC-based methods of Yang et al. (ICML2014, JMLR2016). It is not clear what is the advantage of the proposed method over the QMC-based methods.
(iii) There should be explanation on the settings of the MNIST and TIMIT classification tasks: what classifiers did you use, and how did you determine the hyper-parameters of these methods? At least such explantion should be included in the appendix.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"2), From the data in Table 4, it indicates that the unsupervised pretraining is a key factor on the performance gain. However, there is no detailed discussion on the unsupervised pretraining in the main paper, which might be a problem. In fact, compared with ablation study of Table 5, the unsupervised pretraining is much more important than other modules presented in this paper. Therefore, I will suggest on focusing more on the pretraining method in the main paper.",ICLR_2023_4654,ICLR_2023,"Weakness:
1), The proposed approach is straightforward (not a demerit), and is a native extension on how to extend the DETR into few-shot, although there exist some specific mechanism designs in this paper to facilitate such extension. However, similar ideas also can be found in existing papers such as [1], which appeared in 2021 in arXiv and published in 2022. Since [1] is already published, it should be included as a fair baseline to compare with and discuss and [1] should be the most close research reported in few-shot object detection. However, the performance reported in this manuscript seems is not as high as in [1] with a significant margin.
[1] Meta-DETR: Image-Level Few-Shot Object Detection with Inter-Class Correlation Exploitation, T-PAMI, 2022.
2), From the data in Table 4, it indicates that the unsupervised pretraining is a key factor on the performance gain. However, there is no detailed discussion on the unsupervised pretraining in the main paper, which might be a problem. In fact, compared with ablation study of Table 5, the unsupervised pretraining is much more important than other modules presented in this paper. Therefore, I will suggest on focusing more on the pretraining method in the main paper.
3), I also cannot very agree with three “desiderata” claimed by the authors (although this is not a serious issue). In standard few-shot object detection, fine-tuning or re-training is not an evil. Moreover, “without re-training” is a merit to all attention-based few-shot approaches, not a unique merit of this approach. The second point, “an arbitrary number of novel objects” actually is not even an issue for those “re-training” few-shot methods. And the ""re-training"" based method may also have the merit that it no need to require the ""queries"" for the detection on both base and novel classes, and the selection of ""queries"" may also affect the performance of the detection performance.
4), In fact, many few-shot research is also focusing on the performance on both base and novel classes. One another important “desiderata” should be it can eliminate the performance drop as much as possible on base classes when adapting to novel classes. However, in this paper (similar for most ""retraining-free"" FS methods), the base class performance is not focused totally, and no experiment statistics are provided at all, this would be a problem to compare with most few-shot methods. Since the attention-based approach relying on the ""queries"", it's base-class performance may be worse than the ""retraining"" methods?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- How would we choose which ELM to pick (male/female)? Does this require us to know the speaker’s gender beforehand, i.e., at inference time? This seems like a drawback as the accuracy should be calculated after using a gender detection model in the pipeline (at least in the cases where vocal traits match speaker identity).",Ie040B4nFm,EMNLP_2023,"- The proposed system seems to deter the model in terms of their BLEU scores (system degrades in 2 out of the 3 settings). This leads me to think that while the model seems to do well on speaker specific terms/inflections, the overall translations degrade.
- How would we choose which ELM to pick (male/female)? Does this require us to know the speaker’s gender beforehand, i.e., at inference time? This seems like a drawback as the accuracy should be calculated after using a gender detection model in the pipeline (at least in the cases where vocal traits match speaker identity).
- What happens when a single audio file has two speakers (male and female) conversing with each other? Which ELM to pick in that case?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2. The writing is difficult to follow in many places and can be simplified.,ARR_2022_187_review,ARR_2022,"1. Not clear if the contribution of the paper are sufficient for a long *ACL paper. By tightening the writing and removing unnecessary details, I suspect the paper will make a nice short paper, but in its current form, the paper lacks sufficient novelty. 2. The writing is difficult to follow in many places and can be simplified.
1. Line 360-367 are occupying too much space than needed. 2. It was not clear to me that Vikidia is the new dataset that was introduced by the paper until I read the last section :) 3. Too many metrics used for evaluation. While I commend the paper’s thoroughness by using different metrics for evaluation, I believe in this case the multiple metrics create more confusion than clarity in understanding the results. I recommend using the strictest metric (such as RA) because it will clearly highlight the differences in performance. Also consider marking the best results in each column/row using boldface text. 4. I suspect that other evaluation metrics NDCG, SRRR, KTCC are unable to resolve the differences between NPRM and the baselines in some cases. For e.g., Based on the extremely large values (>0.99) for all approaches in Table 4, I doubt the difference between NPRM’s 0.995 and Glove+SVMRank 0.992 for Avg. SRR on NewsEla-EN is statistically significant. 5. I did not understand the utility of presenting results in Table 2 and Table 3. Why not simplify the presentation by selecting the best regression based and classification based approaches for each evaluation dataset and compare them against NPRM in Table 4 itself? 6. From my understanding, RA is the strictest evaluation metric, and NPRM performs worse on RA when compared to the baselines (Table 4) where simpler approaches fare better. 7. I appreciate the paper foreseeing the limitations of the proposed NPRM approach. However, I find the discussion of the first limitation somewhat incomplete and ending abruptly. The last sentence has the tone of “despite the weaknesses, NPRM is useful'' but it does not flesh out why it’s useful. 8. I found ln616-632 excessively detailed for a conclusion paragraph. Maybe simply state that better metrics are needed for ARA evaluation? Such detailed discussion is better suited for Sec 4.4 9. Why was a classification based model not used for the zero shot experiments in Table 5 and Table 6? These results in my opinion are the strongest aspect of the paper, and should be as thorough as the rest of the results. 10. Line 559: “lower performance on Vikidia-Fr compared to Newsela-Es …” – Why? These are different languages after all, so isn’t the performance difference in-comparable?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', 'X', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].,NIPS_2017_217,NIPS_2017,"- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].
- ""Embedding"" is an overloaded word for a scalar value that represents object ID.
- The model of [31] is used in a post-processing stage to refine the detection. Ideally, the proposed model should be end-to-end without any post-processing.
- Keypoint detection results should be included in the experiments section.
- Sometimes the predicted tag value might be in the range of tag values for two or more nearby people, how is it determined to which person the keypoint belongs?
- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck.
Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- It would also be nice to have some intuition of the proof of Theorem 1. Also, the invertible function $f^*$ would depend on the fixed $P^*$. Does certain distributions $P^*$ make it easier to determine $f^*$. In practice, how should you determine which $P^*$ to fix?",ikX6D1oM1c,ICLR_2024,"- I found Sec 5.1 and 5.2 difficult to read and I think clarity can be improved. What confused me initially was that you suggest fixing $P^*(U|x, a)$ but then the $\sup$ in Eq. 5 is also over the distributions $p(u|x, A)$. Reading it further, the sup is only for $A \neq a$ but I think clarifying that you only fix for the treatment $a$ that enters into $Q$ would be useful. Maybe this is obvious, but it will still make it easier to understand what is being optimized over in the $\sup$.
- It would also be nice to have some intuition of the proof of Theorem 1. Also, the invertible function $f^*$ would depend on the fixed $P^*$. Does certain distributions $P^*$ make it easier to determine $f^*$. In practice, how should you determine which $P^*$ to fix?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- I would have expected Eqs. (7) and (10) to be analogous, but instead one uses X and the other one H^(1). Why is that?",NIPS_2018_15,NIPS_2018,"- The hGRU architecture seems pretty ad-hoc and not very well motivated. - The comparison with state-of-the-art deep architectures may not be entirely fair. - Given the actual implementation, the link to biology and the interpretation in terms of excitatory and inhibitory connections seem a bit overstated. Conclusion: Overall, I think this is a really good paper. While some parts could be done a bit more principled and perhaps simpler, I think the paper makes a good contribution as it stands and may inspire a lot of interesting future work. My main concern is the comparison with state-of-the-art deep architectures, where I would like the authors to perform a better control (see below), the results of which may undermine their main claim to some extent. Details: - The comparison with state-of-the-art deep architectures seems a bit unfair. These architectures are designed for dealing with natural images and therefore have an order of magnitude more feature maps per layer, which are probably not necessary for the simple image statistics in the Pathfinder challenge. However, this difference alone increases the number of parameters by two orders of magnitude compared with hGRU or smaller CNNs. I suspect that using the same architectures with smaller number of feature maps per layer would bring the number of parameters much closer to the hGRU model without sacrificing performance on the Pathfinder task. In the author response, I would like to see the numbers for this control at least on the ResNet-152 or one of the image-to-image models. The hGRU architecture seems very ad-hoc. - It is not quite clear to me what is the feature that makes the difference between GRU and hGRU. Is it the two steps, the sharing of the weights W, the additional constants that are introduced everywhere and in each iteration (eta_t). I would have hoped for a more systematic exploration of these features. - Why are the gain and mix where they are? E.g. why is there no gain going from H^(1) to \tilde H^(2)? - I would have expected Eqs. (7) and (10) to be analogous, but instead one uses X and the other one H^(1). Why is that? - Why are both H^(1) and C^(2) multiplied by kappa in Eq. (10)? - Are alpha, mu, beta, kappa, omega constrained to be positive? Otherwise the minus and plus signs in Eqs. (7) and (10) are arbitrary, since some of these parameters could be negative and invert the sign. - The interpretation of excitatory and inhibitory horizontal connections is a bit odd. The same kernel (W) is applied twice (but on different hidden states). Once the result is subtracted and once it's added (but see the question above whether this interpretation even makes sense). Can the authors explain the logic behind this approach? Wouldn't it be much cleaner and make more sense to learn both an excitatory and an inhibitory kernel and enforce positive and negative weights, respectively? - The claim that the non-linear horizontal interactions are necessary does not appear to be supported by the experimental results: the nonlinear lesion performs only marginally worse than the full model. - I do not understand what insights the eigenconnectivity analysis provides. It shows a different model (trained on BSDS500 rather than Pathfinder) for which we have no clue how it performs on the task and the authors do not comment on what's the interpretation of the model trained on Pathfinder not showing these same patterns. Also, it's not clear to me where the authors see the ""association field, with collinear excitation and orthogonal suppression."" For that, we would have to know the preferred orientation of a feature and then look at its incoming horizontal weights. If that is what Fig. 4a shows, it needs to be explained better.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Although the causal matching problem seems interesting and new, it is not well motivated. To the reviewer’s knowledge, interventions on a causal model are tied to inferring the underlying structure (it does not need to be the whole structure of the model). In this regard, it is not clear how exactly matching the means of a causal system is preferable to performing more relaxed cases of soft interventions. The authors are encouraged to further explain how this setting can be beneficial.",NIPS_2021_2024,NIPS_2021,"below). Using the related literature on active interventions would require full identification of the underlying DAG. It is emphasized that matching only the means can be done with significantly smaller number of interventions, and this is the difference from previous works. - Identifiability in terms of Markov equivalence classes (MEC) is well discussed. Graphical characterization of the proposed shift-interventional (shift-I) MEC, and its refinement over the general interventional MEC is given clearly. Assumptions are reasonable within the given setting. - Extending the decomposition of intervention essential graphs to shift interventional essential graphs is sound. Both of the proposed approaches for solving the problem, clique tree and supermodular strategies are reasonable. Use of a lower bound surrogate function to enable supermodularity is clever. - The paper is organized clearly, and the theoretical claims are well supported.
Weaknesses: I have several concerns on the importance of the proposed settings and usefulness of the results. - Although the causal matching problem seems interesting and new, it is not well motivated. To the reviewer’s knowledge, interventions on a causal model are tied to inferring the underlying structure (it does not need to be the whole structure of the model). In this regard, it is not clear how exactly matching the means of a causal system is preferable to performing more relaxed cases of soft interventions. The authors are encouraged to further explain how this setting can be beneficial. - Deterministic shift interventions are useful to test the applicability of the proposed ideas. However, restricting the problem setting to only shift interventions is quite limited and leads to some rather trivial results. For instance, existence and uniqueness results of matching shift-intervention in Lemma 1, and the properties of source nodes in Lemma 2 are immediate observations in a DAG. - Clique tree approximation is just a minor modification of the cited central node algorithm (Greenewald et al., 2019). - Complexity of the submodularity approach subroutine uses SATURATE algorithm (Krause et al., 2008), and is said to scale with N 5
in appendix D.4. It is worth commenting on the feasibility of this approach. For instance, what are the runtimes of the simulations for large models in Section 6? - It is a nice result that the number of proposed interventions is only a logarithmic factor of the lower bound. However, the baselines in the simulations are not very strong to demonstrate the usefulness. Though coloring approach of Shanmugam et al., 2015 is a related active intervention design, the goal of it is broader than finding a matching intervention. For instance, a simple random upstream search, the other baseline, performs much better than coloring due to the simpler objective. That being said, the reviewer understands that the proposed task is new and fair comparisons may not be easy.
Although this paper has several nice properties, the overall contribution, constraints on the problem, and the importance of the results are not adequate for publication at NeurIPS.
Main limitations of the work, which are also stated in the above review, and potential impact of the work, which is not very imminent, are adequately addressed in the discussion section.","{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world.",NIPS_2016_279,NIPS_2016,"Weakness: 1. The main concern with the paper is the applicability of the model to real-world diffusion process. Though the authors define an interesting problem with elegant solutions, however, it will be great if the authors could provide empirical evidence that the proposed model captures the diffusion phenomena in real-world. 2. Though the IIM problem is defined on the Ising network model, all the analysis is based on the mean-field approximation. Therefore, it will be great if the authors can carry out experiments to show how similar is the mean-field approximation compared to the true distribution via methods such as Gibbs sampling. Detailed Comments: 1. Section 3, Paragraph 1, Line 2, if there there exists -> if there exists.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"2. In the ""bidirectional guidance"" part of section 4.3 Ablation Studies, the results shown at the top row of figure 6 seem to be totally different shapes. I understand this can happen for the 2D diffusion model. However the text also says ""... and the 3D diffusion model manifests anomalies in both texture and geometric constructs."". But where are the 3D diffusion results? From my understanding the results from the 3D diffusion model should always look like the same shape and yield consistent multi-view renderings. I did not find these results in figure 6.",V8PhVhb4pp,ICLR_2024,"The main weaknesses of this paper are the lack of enough qualitative results and the ambiguity of explanation.
1. In the ablation study of 4.3, only one particular qualitative example is shown to demonstrate the effectiveness of different components. This is far from being convincing. The authors should have included more than 10 results of different prompts in the appendix for that.
2. In the ""bidirectional guidance"" part of section 4.3 Ablation Studies, the results shown at the top row of figure 6 seem to be totally different shapes. I understand this can happen for the 2D diffusion model. However the text also says ""... and the 3D diffusion model manifests anomalies in both texture and geometric constructs."". But where are the 3D diffusion results? From my understanding the results from the 3D diffusion model should always look like the same shape and yield consistent multi-view renderings. I did not find these results in figure 6.
3. Figure 4 shows the main qualitative results of the proposed feed-forward method. However there is no comparison to previous methods. I think at least the comparison to Shap-E should be included.
4. The results of Zero-1-to-3 shown in figure 5 are weird. Why all the others methods shown are the final 3D results with mesh visualization, but the Zero-1-to-3 only has multi-view generation results? My understanding is to generation the 3D results at the left lower corner of Figure 5, we still need to use the SDS loss. If this is true, then a directly competitor should be using Zero-1-to-3 with SDS loss.
5. More details about the decoupled geometry and texture control in page 8 are needed. What does it mean to fix the 3D prior? Do you mean fixing the initial noise of the 3D diffusion? When fixing the textual prompt if the 2D diffusion, do you also fix the initial noise?","{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
1)The mehtod is only tested on two datasets. Have the authors tried more datasets to get a better idea of the performance.,ICLR_2023_1553,ICLR_2023,of the papers in my opinion are as follows: 1)The mehtod is only tested on two datasets. Have the authors tried more datasets to get a better idea of the performance. 2) The codes for the paper are not released.,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.",NIPS_2017_53,NIPS_2017,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The main contribution of combining attention with other linear mechanisms is not novel, and, as noted in the paper, a lot of alternatives exist.",bIlnpVM4bc,ICLR_2025,"- The main contribution of combining attention with other linear mechanisms is not novel, and, as noted in the paper, a lot of alternatives exist.
- A comprehensive benchmarking against existing alternatives is lacking. Comparisons are only made to their proposed variants and Sliding Window Attention in fair setups. A thorough comparison with other models listed in Appendix A (such as MEGA, adapted to Mamba) would strengthen the findings. Additionally, selected architectures are evaluated on a very small scale and only measured by perplexity. While some models achieve lower perplexity, this alone may not suffice to establish superiority (e.g., in H3 by Dao et al., 2022b, lower perplexity is reported against transformer baselines).
- Results on common benchmarks are somewhat misleading. The paper aims to showcase the architecture’s strengths, yet comparisons are often made against models trained on different data distributions, which weakens the robustness of the conclusions.
- Conclusions on long-context handling remain vague, although this should be a key advantage over transformers. It would be helpful to include dataset statistics (average, median, min, max lengths) to clarify context length relevance.
- The only substantial long-context experiment, the summarization task, should be included in the main paper, with clearer discussion and analysis.
- Section 4, “Analysis,” could benefit from clearer motivation. Some explored dimensions may appear intuitive (e.g., l. 444, where SWA is shown to outperform full attention on larger sequence lengths than those used in training), which might limit the novelty of the findings. Other questions seems a bit unrelated to the paper topics (see Questions).
- Length extrapolation, a key aspect of the paper, is barely motivated or discussed in relation to prior work.
- The paper overall feels somewhat unstructured and difficult to follow. Tables present different baselines inconsistently, and messages regarding architectural advantages are interleaved with comments on training data quality (l. 229). The evaluation setup lacks consistency (performance is sometimes assessed on real benchmarks, other times by perplexity), and the rationale behind baseline choices or research questions is insufficiently explained.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"7. Since the method is applied on each layer, the authors should provide a plot of how different different weights of the model move, for instance plot the relative weight change after unlearning to see which layers are affected the most after unlearning.",pUOesbrlw4,ICLR_2024,"1. The paper is lacking a clear and precise definition of unlearning. Its is important to show the definition of unlearning that you want to achieve through your algorithm.
2. The proposed algorithm is an empirical algorithm without any theoretical guarantees. It is important for unlearning papers to provide unlearning guarantees against an adversary.
3. The approach is very similar to this method (http://proceedings.mlr.press/v130/izzo21a/izzo21a.pdf) applied on each layer, which is not cited.
4. A simple baseline is just applying all the unlearning algorithm mentioned in the paper to the last layer vs the entire model. This comparison is missing.
5. All the unlearning verification are only show wrt accuracy of the model or the confusion matrix, however, the information is usually contained in the weights of the model, hence other metrics like membership attack or re-train time after forgetting show be considered.
6. The authors should also consider applying this method a linear perturbation of the network, as in those settings you will be able to get theoretical guarantees in regards to the proposed method, and also get better results.
7. Since the method is applied on each layer, the authors should provide a plot of how different different weights of the model move, for instance plot the relative weight change after unlearning to see which layers are affected the most after unlearning.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. From the methodology aspect, the novelty of paper appears to be rather limited. The ENCODE part is already proposed in [10] and the incremental contribution lies in the decomposition part which just factorizes the M_v into factor D and slices Phi_v.",NIPS_2017_415,NIPS_2017,"Weakness:
1. From the methodology aspect, the novelty of paper appears to be rather limited. The ENCODE part is already proposed in [10] and the incremental contribution lies in the decomposition part which just factorizes the M_v into factor D and slices Phi_v.
2. For the experiment, I'd like to the effect of optimized connectome in comparison with that of LiFE model, so we can see the performance differences and the effectiveness of the tensor-based LiFE_sd model. This part of experiment is missing.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1.) What is the domain of the inputs? It seems they are lying in the same sphere, not mentioned in the paper.",ICLR_2022_1653,ICLR_2022,"Weakness]: (1) There is a large gap in the proof of Theorem 1. (2) Missing discussion of the line of research using random matrix theory to understand the input-output Jacobian [1], which also consider the operator norm of the input-out Jacobian and draws a very similar conclusion, e.g., the squared operator norm must grow linearly with the number of layers; see eq (17) and follow up discussion in [1].
In what follows, I elaborate (1) and (2) since they are related.
The biggest issue I see in the proof is the equation above (A.1) on page 11. The authors mixed the calculation of finite width networks (on the left of the equation) and infinite width network calculation together (on the right). More precisely, the authors exchange the order of the two limits lim w i d t h → ∞ and
lim sup x α → x
. The exchangeability of the two limits is questionable to me. In the order: lim w i d t h → ∞
lim sup x α →
, we need to handle a product of random matrices (if we compute the Jacobian). This is indeed a core contribution of [1], who uses free probability theory to compute the whole spectrum of the singular values of the Jacobian (assuming certain free independence of the matrices). If we swap the limits (we shouldn't do this without justification) to
lim sup x α → x β lim w i d t h →
, the problem itself is reduced to computing the derivative of the composed correlation map, which is much simpler. I think these two limits are not unchangeable in general. E.g., using the order
lim sup x α → x β lim w i d t h →
, both critical gaussian and orthogonal initialization give the same answer. But using the order lim w i d t h → ∞
lim sup x α →
, gaussian and orthogonal initialization can give different answers, see eq (17) vs (22) in [1].
Several Qs: Q1:
How Theorem 1 leads to the four possible cases after it needs more discussion. In addition, what are the new insights quotient the existing ones from the order-chaotic analysis? It seems: the first case corresponds to the chaotic phase, the second case corresponds to the order phase. The third/fourth cases seem to be a finer analysis of the critical regime.
Q2: Remark1 the critical initialization. Several works have already identified the issue of the polynomial rate convergence of the correlation to 1 for Relu and smooth functions; see Proposition 1 in [2]; sec B.3. in [3].
Q3: I can't find places to explain the legends ""upper bound"", ""largest found"".
Q4: How does Thm1 imply eq (4.1)? Do you assume the operator norm is bounded by O(1)?
[1] Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice, https://arxiv.org/pdf/1711.04735.pdf [2] On the Impact of the Activation Function on Deep Neural Networks Training, https://arxiv.org/abs/1902.06853 [3] Disentangling Trainability and Generalization in Deep Neural Networks, https://arxiv.org/abs/1912.13053
Minors comments: 1.) What is the domain of the inputs? It seems they are lying in the same sphere, not mentioned in the paper.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
3. [A] also proproses a CLN (region proposal generation algorithm). What's about performance comparision with this work.,jfTrsqRrpb,ICLR_2024,"1. This paper generate candidate object regions through unsupervised segmentation methods. However, it cannot be guaranteed that these unsupervised methods can generate object regions that cover all regions. Especially when the number of categories increases, I question the performance of the unsupervised segmentation methods. The author should provide :1) the specific performance of the unsupervised segmentation methods, 2) experimental comparison with existing methods when categories are more, like COCO to LVIS.
2. The author should provide more result metrics with previous methods. For example, LDET also provides AP, AR10. The author should provide related performance comparisons to provide more comprehensive results.
3. [A] also proproses a CLN (region proposal generation algorithm). What's about performance comparision with this work.
4. What's about the details about Refinement module? I feel that this is all about previous methods, no matter the objectness ranking and inference.
[A] Detecting everything in the open world: Towards universal object detection. CVPR 2023","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful.",NIPS_2016_321,NIPS_2016,"======================== Positive aspects: + The paper is well written and has a clear and coherent structure. The discussion of related work is comprehensive. + Non-parametric emission distributions add flexibility to the general HMM framework and reduce bias due to wrong modeling assumptions. Progress in this area should have theoretical and practical impact. + The paper builds upon existing spectral methods for parametric HMMs but introduces novel techniques to extend those approaches to the non-parametric case. + The theoretical bounds (section 5) are interesting, even though most of the results are special cases or straightforward extensions of known results. Negative aspects: - The restriction to triplets (or a sliding window of length 3) is quite limiting. Is this a fundamental limitation of the approach or is an extension to longer subsequences (without a sliding window) straightforward? - Since the paper mentions the possibility to use Chebyshev polynomials to achieve a speed-up, it would have been interesting to see a runtime comparison at test time. - The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful. - The experimental evaluation compares the proposed approach to 4 other HMM baselines. Even though NP-SPEC-HMM outperforms those baselines, the experimental evaluation has only toy character (simple length 6 conditional distributions, only one training/test sequence in case of the real datasets). Minor points ============ * l.22: Only few parametric distributions allow for tractable exact inference in an HMM. * l.183: Much faster approximations than Chebyshev polynomials exist for the evaluation of kernel density estimates, especially in low-dimensional spaces (e.g., based on fast multipole methods). * Figure 1: There is probably a âx 10^3â missing in the plot on the bottom right. Questions ========= * Eq. (3): Why the restriction to an isotropic bandwidth and a product kernel? Especially a diagonal bandwidth matrix could have been helpful. Would the approximation with Chebyshev polynomials still work? * The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM? * Is it computationally feasible to use the proposed model in a more realistic application, e.g., action recognition from motion capture sequences? Conclusion ========== The paper is well written and, from a theoretical perspective, interesting to read. However, the experiments are weak and it remains unclear how practical the proposed model would be in real applications. Iâm tending towards accept, but the authors should comment in their rebuttal on the above points.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Second rule in Lemma 2, i.e., Eq (7) and the definition of minimal conditional dependence seem to be conflicting. Taking Zâ in this definition to be the empty set, we should have that x and y are independent given W, but Eq. (7) says otherwise.",NIPS_2016_499,NIPS_2016,"- The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data. If otherwise, why [10] cannot use these side information? - The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph? - Second rule in Lemma 2, i.e., Eq (7) and the definition of minimal conditional dependence seem to be conflicting. Taking Zâ in this definition to be the empty set, we should have that x and y are independent given W, but Eq. (7) says otherwise.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. While the data presented in figure3 is comprehensive, I noticed that the visual presentation, specifically the subscripts, could be enhanced for better readability and aesthetic appeal.",7EK2hqWmvz,ICLR_2025,"1. The paper does not clearly position itself with respect to existing retrieval-augmented methods that used to accelerate the model’s inference. A more thorough literature review is needed to highlight how RAEE differs from and improves upon prior work.
2. While the data presented in figure3 is comprehensive, I noticed that the visual presentation, specifically the subscripts, could be enhanced for better readability and aesthetic appeal.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The paper mentions that Batch Normalization has the problem of gradient bias because it uses mini-batch to estimate the real gradient distribution. In contrast, Online Normalization can be implemented locally within individual neurons without the dependency on batch size. It sounds like that Online Normalization and Batch Normalization are two different ways to estimate the real gradient distribution. I am confused why Online Normalization is unbiased and Batch Normalization is biased. ** I have read other reviews and the author response. I will stay with my original score. **",NIPS_2019_757,NIPS_2019,"Weakness 1. Online Normalization introduces two additional hype-parameters: forward and backward decay factors. The authors use a logarithmic grid sweep to search the best factors. This operation largely increases the training cost of Online Normalization. Question: 1. The paper mentions that Batch Normalization has the problem of gradient bias because it uses mini-batch to estimate the real gradient distribution. In contrast, Online Normalization can be implemented locally within individual neurons without the dependency on batch size. It sounds like that Online Normalization and Batch Normalization are two different ways to estimate the real gradient distribution. I am confused why Online Normalization is unbiased and Batch Normalization is biased. ** I have read other reviews and the author response. I will stay with my original score. **","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"--- W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.",ICLR_2023_2322,ICLR_2023,"---
W1. The authors have clearly reduced whitespace throughout the paper; equations are crammed together, captions are too close to the figures. This by itself is grounds for rejection since it effectively violates the 9-page paper limit.
W2. An important weakness that is not mentioned anywhere is that the factors A ( k )
in Eq (8) must have dimensions that factorize the dimensions of W
. For example, they must satisfy ∏ k = 1 S a j ( k ) = w j
. So what is hailed as greater flexibility of the proposed model in the caption of Fig 1 is in fact a limitation. For example, if the dimensions of W
are prime numbers, then for each mode of W
, only a single tensor A ( k )
can have a non-singleton dimension in that same mode. This may be fixable with appropriate zero padding, but this has to at least be discussed and highlighted in the paper.
W3. The 2nd point in the list of contributions in Sec 1 claims that the paper provides a means of finding the best approximation in the proposed format. In fact, it is easy to see that this claim is likely to be false: The decomposition corresponds to a difficult non-convex optimization problem, and it is therefore unlikely that a simple algorithm with a finite number of steps could solve it optimally.
W4. SeKron is claimed to generalize various other decompositions. But it is not clear that the proposed algorithm could ever reproduce those decompositions. For example, since there is no SVD-based algorithm for CP decomposition, I strongly suspect that the proposed algorithm (which is SVD-based) cannot recreate the decomposition that, say, an alternating least squares based approach for CP decomposition would achieve.
W5. The paper is unclear and poor notation is used in multiple places. For examples:
Subscripts are sometimes used to denote indices (e.g., Eq (5)), sometimes to denote sequences of tensors (e.g., Eqs (7), (8)), and sometimes used to denote both at the same time (e.g., Thm 3, Eq (35))! This is very confusing.
It is unclear how Eq (7) follows from Eq (5). The confusing indices exacerbate this.
In Thm 1, A ( k )
are tensors, so it's unclear what you mean by "" R i
are ranks of intermediate matrices"".
In Alg 1, you apply SVD to a 3-way tensors. This operation is not defined. If you mean batched SVD, you need to specify that. The W r 1 ⋯ r k − 1 ( k )
tensors in Eq (10) haven't been defined.
The definition of Unfold below Eq (13) is ambiguous. Similarly, you say that Mat reformulates a tensor to a matrix, but list the output space as R d 1 ⋯ d N
, i.e., indicating that the output is a vector.
Below Eq (15) you discuss ""projection"". This is not an appropriate term to use, since these aren't projections; projection is a term with a specific meaning in linear algebra.
In Eq (16), the r k
indices appear on the right-hand side but not on the left-hand side.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"* significance: In terms of significance, I believe that this work would be of interest to a small fraction of researchers within NIPS. In fact, in terms of fit, this looks like more a submission to be found within SODA.",NIPS_2019_95,NIPS_2019,"of the submission. * originality: I enjoyed reading this paper. It introduces a new and interesting twist on the secretary problem, thereby providing a stylized theoretical version capturing the main essence of the task of ranking in many online settings. Some part of the analysis also provides some novel techniques that may be independently useful for other purpose (e.g. the new anti-concentration inequality). The proposed randomized algorithm is natural and somewhat unsurprising, but its analysis building upon connections to linear probing is interesting. * quality: By in-large the paper seems to be technically sound. I have gone through most proofs in detail, and although some would benefit with added clarity (see some examples below), I haven't found any main flaws. * clarity: The paper is in general well written, although there is room for some improvement. I list some examples below. * significance: In terms of significance, I believe that this work would be of interest to a small fraction of researchers within NIPS. In fact, in terms of fit, this looks like more a submission to be found within SODA. * other details/comments: - p.2, line 42: the optimal => an optimal \- p.4, line 155 to 159: this is a bit of a repeat, and somewhat ill-placed. Should appear before line 148 \- p.4, measure of sortedness: perhaps indicate whether there are other interesting measures one could consider (besides Kendall' tau and Spearman's footrule) \- p.5, line 192,1: $R$ should be defined (set of available rank); $r_1$ should be defined as 0 \- p.5, line 192,5: how are ties within the argmin dealt with? \- p.5, line 203: this equation is not numbered, but seems to be referred to as Eq. (3.2) later on; so it should be numbered. Same elsewhere where you have an equation which you want to refer to later on ... \- p.5, proof of Proposition 3.3: in the introduction of the event ${\cal O}_\sigma$ 5 lines above, it was for a permutation $\sigma$ on $t$ elements. Now in the proof, $\sigma$ is a permutation over $t-1$ elements. Which conditional event do we have in the conditional probability. By the way that formula indicates that the relative rank $r_t$ is {\em conditionally} uniformly distributed ... \- p.5, line 227: $t$-the should be $t^{th}$. Also what bound do you refer to at the end of the sentence? \- p.7, line 247: shouldn't we say: is popular at time $t$ w.p. at most $e^{-\Omega(\alpha)}$, that would help in the proof of Lemma 3.5, otherwise I don't see how the last equality in 263 can be true as the integral could be infinite? \- p.8, line 277: should $O(n\sqrt{n})$ be $\Omega(n\sqrt{n})$?","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty.,ICLR_2022_2110,ICLR_2022,"Weakness: 1) Although each part of the proposed method is effective, the overall algorithm is still cumbersome. It has multiple stages. In contrast, many of existing pruning methods do not need fine-tuning. 2) Technical details and formulations are limited. It seems that the main novelty reflected in the scheme or procedure novelty. 3) The experimental results are not convincing. The compared methods are few. Although few authors have attempted to prune EfficientNet, other networks can be compressed in experiments such as ResNet. In addition, the performance gains compared with SOTAs are also marginal, which are also within 1%. 4) The paper is poorly written. There are many typos and some are listed as follows: --In caption of Figure 2, “An subset of a network” should be “A subset of a network”. --In Line157 of Page4, “The output output vector” should be “The output vector”. --In Line283 of Page7, “B0V2 as,” should be “B0V2 as teacher,”. --In Line301 of Page7, “due the inconsistencies” should be “due to the inconsistencies”.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- The concept of local interactions is not as clear as the rest of the paper. Is it local in that it refers to the interactions within a time window, or is it local in that it is within the same modality?",NIPS_2019_1089,NIPS_2019,"- The paper can be seen as incremental improvements on previous work that has used simple tensor products to representation multimodal data. This paper largely follows previous setups but instead proposes to use higher-order tensor products. ****************************Quality**************************** Strengths: - The paper performs good empirical analysis. They have been thorough in comparing with some of the existing state-of-the-art models for multimodal fusion including those from 2018 and 2019. Their model shows consistent improvements across 2 multimodal datasets. - The authors provide a nice study of the effect of polynomial tensor order on prediction performance and show that accuracy increases up to a point. Weaknesses: - There are a few baselines that could also be worth comparing to such as âStrong and Simple Baselines for Multimodal Utterance Embeddings, NAACL 2019â - Since the model has connections to convolutional arithmetic units then ConvACs can also be a baseline for comparison. Given that you mention that âresulting in a correspondence of our HPFN to an even deeper ConACâ, it would be interesting to see a comparison table of depth with respect to performance. What depth is needed to learning âflexible and higher-order local and global intercorrelationsâ? - With respect to Figure 5, why do you think accuracy starts to drop after a certain order of around 4-5? Is it due to overfitting? - Do you think it is possible to dynamically determine the optimal order for fusion? It seems that the order corresponding to the best performance is different for different datasets and metrics, without a clear pattern or explanation. - The model does seem to perform well but there seem to be much more parameters in the model especially as the model consists of more layers. Could you comment on these tradeoffs including time and space complexity? - What are the impacts on the model when multimodal data is imperfect, such as when certain modalities are missing? Since the model builds higher-order interactions, does missing data at the input level lead to compounding effects that further affect the polynomial tensors being constructed, or is the model able to leverage additional modalities to help infer the missing ones? - How can the model be modified to remain useful when there are noisy or missing modalities? - Some more qualitative evaluation would be nice. Where does the improvement in performance come from? What exactly does the model pick up on? Are informative features compounded and highlighted across modalities? Are features being emphasized within a modality (i.e. better unimodal representations), or are better features being learned across modalities? ****************************Clarity**************************** Strengths: - The paper is well written with very informative Figures, especially Figures 1 and 2. - The paper gives a good introduction to tensors for those who are unfamiliar with the literature. Weaknesses: - The concept of local interactions is not as clear as the rest of the paper. Is it local in that it refers to the interactions within a time window, or is it local in that it is within the same modality? - It is unclear whether the improved results in Table 1 with respect to existing methods is due to higher-order interactions or due to more parameters. A column indicating the number of parameters for each model would be useful. - More experimental details such as neural networks and hyperparameters used should be included in the appendix. - Results should be averaged over multiple runs to determine statistical significance. - There are a few typos and stylistic issues: 1. line 2: ""Despite of being compactâ -> âDespite being compactâ 2. line 56: âWe refer multiway arraysâ -> âWe refer to multiway arraysâ 3. line 158: âHPFN to a even deeper ConACâ -> âHPFN to an even deeper ConACâ 4. line 265: ""Effect of the modelling mixed temporal-modality features."" -> I'm not sure what this means, it's not grammatically correct. 5. equations (4) and (5) should use \left( and \right) for parenthesis. 6. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite promising. Weaknesses: - Not really a weakness, but there is a paper at ACL 2019 on ""Learning Representations from Imperfect Time Series Data via Tensor Rank Regularizationâ which uses low-rank tensor representations as a method to regularize against noisy or imperfect multimodal time-series data. Could your method be combined with their regularization methods to ensure more robust multimodal predictions in the presence of noisy or imperfect multimodal data? - The paper in its current form presents a specific model for learning multimodal representations. To make it more significant, the polynomial pooling layer could be added to existing models and experiments showing consistent improvement over different model architectures. To be more concrete, the yellow, red, and green multimodal data in Figure 2a) can be raw time-series inputs, or they can be the outputs of recurrent units, transformer units, etc. Demonstrating that this layer can improve performance on top of different layers would be this work more significant for the research community. ****************************Post Rebuttal**************************** I appreciate the effort the authors have put into the rebuttal. Since I already liked the paper and the results are quite good, I am maintaining my score. I am not willing to give a higher score since the tasks are rather straightforward with well-studied baselines and tensor methods have already been used to some extent in multimodal learning, so this method is an improvement on top of existing ones.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '1', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.",NIPS_2020_1228,NIPS_2020,"- The method section looks not self-contained and lacks descriptions of some key components. In particular: * What is Eq.(9) for? Why ""the SL is the negative logarithm of a polynomial in \theta"" -- where is the ""negative logarithm"" in Eq.(9)? * Eq.(9) is not practically tractable. It looks its practical implementation is discussed in the ""Evaluating the Semantic Loss"" part (L.140) which involves the Weighted Model Count (WMC) and knowledge compilation (KC). However, no details about KC are presented. Considering the importance of the component in the whole proposed approach, I feel it's very necessary to clearly present the details and make the approach self-contained. - The proposed approach essentially treats the structured constraints (a logical rule) as part of the discriminator that supervises the training of the generator. This idea looks not new -- one can simply treat the constraints as an energy function and plug it into energy-based GANs (https://arxiv.org/abs/1609.03126). Modeling structured constraints as a GAN discriminator to train the generative model has also been studied in [15] (which also discussed the relation b/w the structured approach with energy-based GANs). Though the authors derive the formula from a perspective of semantic loss, it's unclear what's the exact difference from the previous work? - The paper claims better results in the Molecule generation experiment (Table.3). However, it looks adding the proposed constrained method actually yields lower validity and diversity.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Algorithm 2 provides the coreset C and the query Q consists of the archetypes z_1, â¦, z_k which are initialised with the FurthestSum procedure. However, it is not quite clear to me how the archetype positions are updated after initialisation. Could the authors please comment on that?",NIPS_2019_651,NIPS_2019,"(large relative error compared to AA on full dataset) are reported. - Clarity: The submission is well written and easy to follow, the concept of coresets is well motivated and explained. While some more implementation details could be provided (source code is intended to be provided with camera-ready version), a re-implementation of the method appears feasible. - Significance: The submission provides a method to perform (approximate) AA on large datasets by making use of coresets and therefore might be potentially useful for a variety of applications. Detailed remarks/questions: 1. Algorithm 2 provides the coreset C and the query Q consists of the archetypes z_1, â¦, z_k which are initialised with the FurthestSum procedure. However, it is not quite clear to me how the archetype positions are updated after initialisation. Could the authors please comment on that? 2. The presented theorems provide guarantees for the objective functions phi on data X and coreset C for a query Q. Table 1 reporting the relative errors suggests that there might be a substantial deviation between coreset and full dataset archetypes. However, the interpretation of archetypes in a particular application is when AA proves particularly useful (as for example in [1] or [2]). Is the archetypal interpretation of identifying (more or less) stable prototypes whose convex combinations describe the data still applicable? 3. Practically, the number of archetypes k is of interest. In the presented framework, is there a way to perform model selection in order to identify an appropriate k? 4. The work in [3] might be worth to mention as a related approach. There, the edacious nature of AA is approached by learning latent representation of the dataset as a convex combination of (learnt) archetypes and can be viewed as a non-linear AA approach. [1] Shoval et al., Evolutionary Trade-Offs, Pareto Optimality, and the Geometry of Phenotype Space, Science 2012. [2] Hart et al., Inferring biological tasks using Pareto analysis of high-dimensional data, Nature Methods 2015. [3] Keller et al., Deep Archetypal Analysis, arxiv preprint 2019. ---------------------------------------------------------------------------------------------------------------------- I appreciate the authorsâ response and the additional experimental results. I consider the plot of the coreset archetypes on a toy experiment insightful and it might be a relevant addition to the appendix. In my opinion, the submission constitutes a relevant contribution to archetypal analysis which makes it more feasible in real-world applications and provides some theoretical guarantees. Therefore, I raise my assessment to accept.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- There are important information about the empirical study missing that should be mentioned in the supplement, such as recording parameters for the MRI, preprocessing steps, was the resting-state recorded under eyes-open or eyes-closed condition? A brief explanation of the harmonization technique would also be appreciated. It would also be helpful to mention the number of regions in the parcellation in the main text.",ICLR_2022_2123,ICLR_2022,"of this submission and make suggestions for improvement:
Strengths - The authors provide a useful extension to existing work on VAEs, which appears to be well-suited for the target application they have in mind. - The authors include both synthetic and empirical data as test cases for their method and compare it to a range of related approaches. - I especially appreciated, that the authors validated their method on the empirical data and also provide an assessment of face validity using established psychological questionnaires (BDI and AQ). - I also appreciated the ethics statement pointing out that the method requires additional validation, before it may enter the clinic. - The paper is to a great extend clearly written.
Weaknesses - In Figure 2 it seems that Manner-1 use of diagnostic information is more important than Manner-2 use of this information, which calls your choice into question to set lambda = 0.5 in equation 3. Are you able to learn this parameter from the data? - Also in Figure 2, when applying your full model to the synthetic data, it appears to me that inverting your model seems to underestimate the within-cluster variance (compared to the ground truth). Could it be that your manner-1 use of information introduces constraints that are too strong, as they do not allow for this variance? - It would strengthen your claims of “superiority” of your approach over others, if you could provide a statistical test that shows that your approach is indeed better at recovering the true relationship compared to others. Please, provide such tests. - There are important information about the empirical study missing that should be mentioned in the supplement, such as recording parameters for the MRI, preprocessing steps, was the resting-state recorded under eyes-open or eyes-closed condition? A brief explanation of the harmonization technique would also be appreciated. It would also be helpful to mention the number of regions in the parcellation in the main text. - The validation scheme using the second study is not clear to me. Were the models trained on dataset A and then directly applied to dataset B or did you simply repeat the training in dataset B. If the latter is the case, I would refer to this as a replication dataset and not a validation dataset (which would require applying the same model on a new dataset, without retraining). - Have you applied multiple testing correction for the FID comparisons across diagnoses. If so which? If not, you should apply it and please, state that clearly in the main manuscript. - It is somewhat surprising that the distance between SCZ and MDD is shorter than between SCZ and ASD as often the latter two are viewed as closely related. It might be helpful to discuss, why that may be the case in more detail. - The third ethics statement is not clear to me. Could you clarify? - The font size in the figures is too small. Please, increase it to improve readability.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking.",NIPS_2019_82,NIPS_2019,"1. One major risk of methods that exploit relationships between action units is that the relationships can be very different accross datasets (e.g. AU6 can occur both in an expression of pain and in happiness, and this co-occurence will be very different in a positive salience dataset such as SEMAINE compared to something like UNBC pain dataset). This difference in correlation can already be seen in Figure 1 with quite different co-occurences of AU1 and AU12. A good way to test the generalization of such work is by performing cross-dataset experiments, which this paper is lacking. 2. The language in the paper is sometimes conversational and not scientific (use of terms like massive), and there are several opinions and claims that are not substantiated (e.g. ""... facial landmarks, which are helpful for the recognition of AUs defined in small regions""), the paper could benefit from copy-editing 3. Why are two instances of the same network (resnet) are used as different views? Would using a different architecture instead be considered a more differing view? Would be great to see a justification for using two resnet networks. 4. Why is the approach limited to two views, it feels like the system should be able to generalize to more views without too much difficulty? Minor comments: - What is PCA style guarantee? - What is v in equation 2? - why are dfferent numbers of unlabeled images using in training BP4D and EmotioNet models? Trivia: massive face images -> large datasets donates -> denotes (x2) adjacent -> adjacency","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- I would have liked more description of the Starcraft environment (potentially in an Appendix?),NIPS_2020_867,NIPS_2020,"- As someone without a linguistics background, it was at times difficult for me to follow some parts of the paper. For example, it’s not clear to me why we care about the speaker payoff and listener payoff (separate from listener accuracy), rather than just a means to obtain higher accuracy --- is it important that the behavior of the speaker at test time stay close to its behavior during training? - I think more emphasis could be placed on the fact that the proposed methods require the speaker to have a model (in fact, in most of the experiments it’s an exact model) of the listener’s conditional probability p(t|m), and vice-versa. - I would have liked more description of the Starcraft environment (potentially in an Appendix?)","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', 'X', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- line 47 - 48 ""over-parametrization invariably overfits the data and results in worse performance"": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1].",NIPS_2019_499,NIPS_2019,"of the method. Are there any caveats to practitioners due to some violation of the assumptions given in Appendix. B or for any other reasons? Clarity: the writing is highly technical and rather dense, which I understand is necessary for some parts. However, I believe the manuscript would be readable to a broader audience if Sections 2 and 3 are augmented with more intuitive explanations of the motivations and their proposed methods. Many details of the derivations could be moved to the appendix and the resultant space could be used to highlight the key machinery which enabled efficient inference and to develop intuitions. Many terms and notations are not defined in text (as raised in ""other comments"" below). Significance: the empirical results support the practical utility of the method. I am not sure, however, if the experiments on synthetic datasets, support the theoretical insights presented in the paper. I believe that the method is quite complex and recommend that the authors release the codes to maximize the impact. Other comments: - line 47 - 48 ""over-parametrization invariably overfits the data and results in worse performance"": over-parameterization seems to be very helpful for supervised learning of deep neural networks in practice ... Also, I have seen a number of theoretical work showing the benefits of over-parametrisation e.g. [1]. - line 71: $\beta$ is never defined. It denotes the set of model parameters, right? - line 149-150 ""the convergence to the asymptotically correct distribution allows ... obtain better point estimates in non-convex optimization."": this is only true if the assumptions in Appendix. B are satisfied, isn't it? How realistic are these assumptions in practice? - line 1: MCMC is never defined: Markov Chain Monte Carlo - line 77: typo ""gxc lobal""=> ""global"" - eq.4: $\mathcal{N}$ and $\mathcal{L}$ are not defined. Normal and Laplace I suppose. You need to define them, please. - Table 2: using the letter `a` to denote the difference in used models is confusing. - too many acronyms are used. References: [1] Allen-Zhu, Zeyuan, Yuanzhi Li, and Zhao Song. ""A convergence theory for deep learning via over-parameterization."" arXiv preprint arXiv:1811.03962 (2018). ---------------------------------------------------------------------- I am grateful that the authors have addressed most of the concerns about the paper, and have updated my score accordingly. I would like to recommend for acceptance provided that the authors reflect the given clarifications in the paper.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work.",NIPS_2016_313,NIPS_2016,"Weakness: 1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim. 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc. 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods. 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work. 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"* The authors combine two existing techniques to get the framework without innovation. The adversarial attack or correction method and the domain adaptation method used by the authors are proposed by prior work. And the adopted domain adaptation method here is a very old and simple method which is proposed eight years ago. Considering there were so many effective domain adaptation methods proposed in the recent few years, why don't you use other domain adaptation methods to further improve the performance?",uSiyu6CLPh,ICLR_2025,"* I suggest that the authors show a more intuitive figure to visualize the framework that includes the images and labels in the original dataset and also the corrected images. This will help the readers to gain more intuition for your method.
* The authors combine two existing techniques to get the framework without innovation. The adversarial attack or correction method and the domain adaptation method used by the authors are proposed by prior work. And the adopted domain adaptation method here is a very old and simple method which is proposed eight years ago. Considering there were so many effective domain adaptation methods proposed in the recent few years, why don't you use other domain adaptation methods to further improve the performance?
* In Section 3.3, the authors align the features of the weak classifier on the original dataset and the synthetic dataset. Considering that the difference between the original and the synthetic datasets is the corrected part, can we omit the correctly classified samples and only minimize the covariance difference for the adversarially corrected sample and the misclassified sample?
* How do you choose the hyper-parameters such as $\lambda,\epsilon$? Does your method work robustly for other choices of hyper-parameters? If not, how do you choose them?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
7. A discussion on the prompt dataset (for the few-shot case) creation together with its source should be discussed.,qb2QRoE4W3,ICLR_2025,"Despite the idea being interesting, I have found some technical issues that weakened the overall soundness. I enumerate them as follows:
1. The assumption that generated URLs are always meaningfully related to the core content of the document from where the premises are to be fetched is not true by and large. It works for Wikipedia because the URLs are well-structured semantically.
2. LLMs generating URLs on Wikidata have a significantly higher probability of being linked with a valid URL because extensive entity linking has already been done. This, however, is not the case for many other web sources.
3. There are several URLs that are not named according to the premise entities. In that case, those sources will never be fetched.
4. How to resolve contradictory entailment from premises belonging to different sources?
5. There can be many sources that are themselves false (particularly for the open Internet and also in cases of unverified Wiki pages). So assuming the premises to be true may lead to incorrect RTE.
6. It is unclear how the prompt templates are designed, i.e., the rationale and methodology that would drive the demonstration example patterns in the few-shot cases.
7. A discussion on the prompt dataset (for the few-shot case) creation together with its source should be discussed.
8. The assumption that RTE (i.e. NLI) being true would imply that the hypothesis (fact/claim) is verified is bit tricky and may not always be true. A false statement can entail a hypothesis as well as its true version. Eg.:
$\textit{Apples come in many colors}$ $\implies$ $\textit{Apples can be blue (claim)}$.
$\textit{John was a follower of Jesus}$ $\implies$ $\textit{John was a Christian (claim)}$.
9. Line 253: What is citation threshold? I could not find the definition.
10. In the comparisons with the baselines and variants of LLM-Cite, what was the justification behind not keeping the model set fixed for all the experiments? I think this should be clear.
11. In sections 4.1 and 4.2, an analysis of why the verification models perform better on model-generated claims as compared to human-generated claims is very important to me. I could not find any adequate analysis for that.
12. The key success of LLM-Cite depends on the NLI model (given that at least one valid URL is generated that points to a valid premise). Hence, a discussion on the accuracy of SOTA NLI models (with citation) and the rationale behind choosing the Check Grounding NLI API and Gemini-1.5-Flash should be included.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- Very difficult to follow the motivation of this paper. And it looks like an incremental engineering paper.,CoEuk8SNI1,EMNLP_2023,"-	Very difficult to follow the motivation of this paper. And it looks like an incremental engineering paper.
-	The abstract looks a little vague. For example, “However, it is difficult to fully model interaction between utterances …” What is 'interaction between utterances' and why is it difficult to model? This information is not evident from the previous context. Additionally, the misalignment between the two views might seem obvious since most ERC models aggregate information using methods like residual connections. So, why the need for alignment? Isn't the goal to leverage the advantages of both features? Or does alignment help in achieving a balance between both features?
-	The author used various techniques to enhance performance, including contrastive learning, external knowledge, and graph networks. However, these methods seem contradictory to the limited experiments conducted. For example, the author proposes a new semi-parametric inferencing paradigm involving memorization to address the recognition problem of tail class samples. However, the term ""semi-parametric"" is not clearly defined, and there is a lack of experimental evidence to support the effectiveness of the proposed method in tackling the tail class samples problem.
-	The Related Work section lacks a review of self-supervised contrastive learning in ERC.
-	The most recent comparative method is still the preprint version available on ArXiv, which lacks convincing evidence.
-	Table 3 needs some significance tests to further verify the assumptions put forward in the paper.
-	In Chapter 5.3, the significant impact of subtle hyperparameter fluctuations on performance raises concerns about the method's robustness. The authors could consider designing an automated hyperparameter search mechanism or decoupling dependencies on these hyperparameters to address this.
-	There is a lack of error analysis.
-	The formatting of the reference list is disorganized and needs to be adjusted.
-	Writing errors are common across the overall paper.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', 'X', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"6. An ablation on the weighting method of the cross-entropy loss would be nice to see. The authors note for example that in Atlantis their method underperforms because ""the game has repetitive background sounds"". This is a scenario I'd expect the weighting might have helped remedy.",NIPS_2020_1253,NIPS_2020,"1. Perhaps the most important limitation I can see is the artificial environments used. In games and especially those old Atari ones, audio events can be repeated exaclty the same and it's quite easy for the network to learn to distinguish new sounds, whereas this might not be the case in more realistic environments, where there's more variance and noise in the audio. 2. L.225: ""Visiting states with already learned audio-visual pairs is necessary for achieving a high score, even though they may not be crucial for exploration"" So that seems like an important limitation, agent won't work well in this sort of environments, which can easily happen in realistic scenarios. 3. L.227 ""The game has repetitive background sounds. Games like SpaceInvaders and BeamRider have background sounds at a fixed time interval, but it is hard to visually associate these sounds"" Same here, repetitive background sounds might often be the case in real applications. 4. L190: It's a bit strange how the authors use vanilla FFT instead of the more common STFT (overlapping segments and a Hann windowing function). Probably a good idea to try this for consistency with literature. Insufficient ablations: 6. An ablation on the weighting method of the cross-entropy loss would be nice to see. The authors note for example that in Atlantis their method underperforms because ""the game has repetitive background sounds"". This is a scenario I'd expect the weighting might have helped remedy. 7. An ablation with adding noise to the audio channel would be interesting. 8. An ablation sampling the negatives from unrelated trajectories would also be interesting 9. Some architectural details are missing and some unclear. For example why is the 2D convnet shown in Fig. 2 fixed to random initialization?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. A critical weakness of the paper is the lack of novelty and incremental nature of work. The paper addresses a particular problem of column operations in designing semantic parsers for Text-to-SQL. They design a new dataset which is a different train/test split of an existing dataset SQUALL. The other synthetic benchmark paper proposed is based on a single question template, ""What was <column> in <year>?"".",ARR_2022_52_review,ARR_2022,"1. A critical weakness of the paper is the lack of novelty and incremental nature of work. The paper addresses a particular problem of column operations in designing semantic parsers for Text-to-SQL. They design a new dataset which is a different train/test split of an existing dataset SQUALL. The other synthetic benchmark paper proposed is based on a single question template, ""What was <column> in <year>?"".
2. The paper assumes strong domain knowledge about the column types and assumes a domain developer first creates a set of templates based on column types. With the help of these column templates, I think many approaches (parsers) can easily solve the problem. For example, parsers utilizing the SQL grammar to generate the output SQL can use these templates to add new rules that can be used while generating the output. Few such works are 1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021.
1. It will good if the authors can learn the templates for schema expansion from source domain data.
2. Compare the proposed approach with methods which uses domain knowledge in the form of grammar. Comparing with below methods will show generality of ideas proposed in the paper in a much better way.
1. A Globally Normalized Neural Model for Semantic Parsing ACl 2021 2. TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation EMNP 2018 3. GraPPa: Grammar-Augmented Pre-Training for Table Semantic Parsing, ICLR 2021.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
- The experiments are only done on one game environment. More experiments are necessary.,NIPS_2018_901,NIPS_2018,Weakness: - The experiments are only done on one game environment. More experiments are necessary. - This method seems not generalizable for other games e.g. FPS game. People can hardly do this on realistic scenes such as driving. Static Assumption too strong.,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', 'X', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples.,NIPS_2020_1451,NIPS_2020,"1. Unlike the works HaoChen and Sra and Nagaraj et.al, this work uses the fact that all component functions f_i are mu strongly convex. 2. The authors need to explain why removing some of the assumptions like bounded variance and bounded gradients is an important contribution via. solid examples. 3. The quantity sigma^{*} being finite also implies that all the gradients are finite via. smoothness property of the functions f_i and gives a natural upper bound.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Although the authors claim they implement ImageNet for the first time, it is very slow and accuracy is very low; ""SHE needs 1 day and 2.5 days to test an ImageNet picture by AlexNet and ResNet-18, respectively"" and accuracy is around 70%",NIPS_2019_900,NIPS_2019,"-no consideration for approximate number schemes in related work. -no support for float numbers. -At many points in the paper, it is not clear if unecrypted model is a model with PAA or a model with ReLU activation. -what is TCN? the abbreviation is explained way too late into the paper -Tables in chapter 5 are overloaded and abbreviations used are not explained properly. -Figure 3a does not highlight that the shift operation is cheap. - Although the authors claim they implement ImageNet for the first time, it is very slow and accuracy is very low; ""SHE needs 1 day and 2.5 days to test an ImageNet picture by AlexNet and ResNet-18, respectively"" and accuracy is around 70%","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Add a few more sentences explaining the experimental setting for continual learning 2. In Fig 3, explain the correspondence between the learning curves and M-PHATE. Why do you want to want me to look at the learning curves? Does worse performing model always result in structural collapse? What is the accuracy number? For the last task? or average?",NIPS_2019_165,NIPS_2019,"of the approach and experiments or list future direction for readers. The writeup is exceptionally clear and well organized-- full marks! I have only minor feedback to improve clarity: 1. Add a few more sentences explaining the experimental setting for continual learning 2. In Fig 3, explain the correspondence between the learning curves and M-PHATE. Why do you want to want me to look at the learning curves? Does worse performing model always result in structural collapse? What is the accuracy number? For the last task? or average? 3. Make the captions more descriptive. It's annoying to have to search through the text for your interpretation of the figures, which is usually on a different page 4. Explain the scramble network better... 5. Fig 1, Are these the same plots, just colored differently? It would be nice to keep all three on the same scale (the left one seems condensed) M-PHATE results in significantly more interpretable visualization of evolution than previous work. It also preserves neighbors better (Question: why do you think t-SNE works better in two conditions? The difference is very small tho). On continual learning tasks, M-PHATE clearly distinguishes poor performing learning algorithms via a collapse. (See the question about this in 5. Improvement). The generalization vignette shows that the heterogeneity in M-PHATE output correlates with performance. I would really like to recommend a strong accept for this paper, but my major concern is that the vignettes focus on one dataset MNIST and one NN architecture MLP, which makes the experiments feel incomplete. The results and observations made by authors would be much more convincing if they could repeat these experiments for more datasets and NN architectures.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,
"3. While consistency training might usually be deployed on unlabeled data, I wonder if it would be beneficial to utilize labeled data for consistency training as well. Specifically, labeled data has exact labels, which might provide effective information for consistency training the model in dealing with the taks of graph anomaly detection. [a] Graph Contrastive Learning Automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang. ICML 2021 [b] Graph Contrastive Learning with Adaptive Augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang. WWW 2021.",elMKXvhhQ9,ICLR_2024,"1. The paper should acknowledge related works that are pertinent to the proposed learnable data augmentation, such as [a] and [b]. It is crucial to cite and discuss the distinctions between these works and the proposed approach, providing readers with a clear understanding of the novel contributions made by this study.
2. The paper predominantly explores the concept of applying learnable data augmentation for graph anomaly detection. While this is valuable, investigating its applicability in broader graph learning tasks, such as node classification with contrastive learning, could significantly expand its scope. For example, how about its benefits to generic graph contrastive learning tasks, compared to existing contrastive techniques?
3. While consistency training might usually be deployed on unlabeled data, I wonder if it would be beneficial to utilize labeled data for consistency training as well. Specifically, labeled data has exact labels, which might provide effective information for consistency training the model in dealing with the taks of graph anomaly detection.
[a] Graph Contrastive Learning Automated. Yuning You, Tianlong Chen, Yang Shen, Zhangyang Wang. ICML 2021
[b] Graph Contrastive Learning with Adaptive Augmentation. Yanqiao Zhu, Yichen Xu, Feng Yu, Qiang Liu, Shu Wu, Liang Wang. WWW 2021.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3) The experimental part needs to be reorganized and further improved. The experimental section has a lot of content, but the experimental content listed in the main text does not highlight the superiority of the method well, so it needs to be reorganized. Based on the characteristics of the article, the experimental suggestions in the main text should include the following:",zkzf0VkiNv,ICLR_2024,"1. Figure 2 shows that, without employing data augmentation and similarity-based regularization, the performance of CR-OSRS is comparable to RS-GM.
2. Could acceleration be achieved by incorporating entropy regularization into the optimization process?
3. It would be beneficial if the authors could provide an analysis of the computational complexity of this method.
4. The author wants to express too much content in the article, resulting in insufficient details and incomplete content in the main text.
5. The experimental part needs to be reorganized and further improved.
Details comments
1) It is recommended to swap the positions of Sections 4.3 and 4.4. According to the diagram, 4.3 is the training section, and 4.4 aims to measure certified space. Both 4.1 and 4.2 belong to the robustness and testing sections. Therefore, putting these parts together feels more reasonable.
2) The author should emphasize ""The article is a general and robust method that can be applied to various GM methods, and we only use NGMv2 as an example."" at the beginning of the article, rather than just showing in the title of Method Figure 1. This can better highlight the characteristics and contribution of the method.
3) The experimental part needs to be reorganized and further improved. The experimental section has a lot of content, but the experimental content listed in the main text does not highlight the superiority of the method well, so it needs to be reorganized. Based on the characteristics of the article, the experimental suggestions in the main text should include the following: 1. Robustness comparison and accuracy analysis with other empirical robustness algorithms for the same type of perturbations, rather than just focusing on the RS method, to clarify the superiority of the method. (You should supplement this part.) 2. Suggest using ablation experiments as the second part to demonstrate the effectiveness of the method. 3. Parameter analysis, elucidating the method's dependence on parameters. 4. Consider its applications on six basic algorithms as an extension part. Afterwards, based on the importance, select the important ones to place in the main text, and show the rest in the appendix.
4) In P16, the proof of claim 2, it should be P(I \in B) not P(I \in A).
5) In Table 2 of appendix, the Summary of main existing literature in learning GM can list the related types of perturbations.
6) In Formula 8, please clarify the meaning of lower p (lower bound of unilateral confidence), and the reason and meaning of setting as 1/2.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The authors propose a new classification network, but I a bit doubt that its classification error is universally as good as the standard softmax network. It is a bit dangerous to build a new model for better detecting out-of-distribution samples, while losing its classification accuracy. Could the authors report the classification accuracy of the proposed classifier on ImageNet data? Some theoretical justifications, if possible, would be great for the issue.",NIPS_2018_681,NIPS_2018,"Weakness: However, I'm not very convinced with experimental results and I a bit doubt that this method would work in general and is useful in any sense. 1. The authors propose a new classification network, but I a bit doubt that its classification error is universally as good as the standard softmax network. It is a bit dangerous to build a new model for better detecting out-of-distribution samples, while losing its classification accuracy. Could the authors report the classification accuracy of the proposed classifier on ImageNet data? Some theoretical justifications, if possible, would be great for the issue. 2. The detection procedure (i.e., measuring the norm of the predicted embedding) is not intuitive and I am not convinced why it is expected to work. Could the authors provide more detailed explanations about it? 3. The baselines to compare are not enough, e.g., compare the proposed method with LID [1] which is one of the state-of-the-art detection methods for detecting adversarial samples. [1] Ma, X., Li, B., Wang, Y., Erfani, S.M., Wijewickrema, S., Houle, M.E., Schoenebeck, G., Song, D. and Bailey, J., Characterizing adversarial subspaces using local intrinsic dimensionality. In ICLR, 2018 4. Similar to Section 4.3, it is better to report AUROC and detection error when the authors evaluate their methods for detecting adversarial samples.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.",NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.
- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.
- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.
- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.",ICLR_2023_2630,ICLR_2023,"- The technical novelty and contributions are a bit limited. The overall idea of using a transformer to process time series data is not new, as also acknowledged by the authors. The masked prediction was also used in prior works e.g. MAE (He et al., 2022). The main contribution, in this case, is the data pre-processing approach that was based on the bins. The continuous value embedding (CVE) was also from a prior work (Tipirneni & Reddy 2022), and also the early fusion instead of late fusion (Tipirneni & Reddy, 2022; Zhang et al., 2022). It would be better to clearly clarify the key novelty compared to previous works, especially the contribution (or performance gain) from the data pre-processing scheme.
- It is unclear if there are masks applied to all the bins, or only to one bin as shown in Fig. 1.
- It is unclear how the static data (age, gender etc.) were encoded to input to the MLP. The time-series data was also not clearly presented.
- It is unclear what is the ""learned [MASK] embedding"" mean in the SSL pre-training stage of the proposed method.
- The proposed ""masked event dropout scheme"" was not clearly presented. Was this dropout applied to the ground truth or the prediction? If it was applied to the prediction or the training input data, will this be considered for the loss function?
- The proposed method was only evaluated on EHR data but claimed to be a method designed for ""time series data"" as in both the title and throughout the paper. Suggest either tone-down the claim or providing justification on more other time series data.
- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.
- Missing reference to the two used EHR datasets at the beginning of Sec. 4.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are âattributeâ. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in âLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings. Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.",NIPS_2017_390,NIPS_2017,"+ Intuitive and appealingly elegant method, that is simple and fast.
+ Authors provide several interpretations which draw connections drawn to other methods and help the reader understand well.
+ Some design choices are well explained , e.g. Euclidean distance outperforms cosine for good reason.
+ Good results
- Some other design decisions (normalisation; number of training classes per episode, etc) less well explained. How much of good results is the proposed method per-se, and how much of it is tuning this stuff?
- Why the zero-shot part specifically works so well should be better explained. Details:
- Recent work also has 54-56% on CUB. (Chanpinyo et al, CVPRâ16, Zhang & Salgrama ECCVâ16)
- This may not necessarily reduce the novelty of this work, but the use of mean of feature vectors from the same class has been proposed for the enhancement over training general (not few-shot specifically) classification model. [A Discriminative Feature Learning Approach for Deep Face Recognition, Wen et al, ECCV 2016]. The âcenterâ in the above paper matches âprototypeâ. Probably this connection should be cited.
- For the results of zero-shot learning on CUB dataset, i.e., Table 3 page 7, the meta-data used here are âattributeâ. This is good for fair comparison. However, from the perspective of getting better performance, better meta-data embeddings options are available. Refer to table 1 in âLearning Deep Representations of Fine-Grained Visual Descriptions, Reed et al, CVPR 2016â. It would be interesting to know the performance of the proposed method when it is equipped with better meta-data embeddings.
Update: Thanks to the authors for their response to the reviews. I think this paper is acceptable for NIPS.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,
"1. **Limited Applicability**: While the paper claims that SGC offers a more flexible, fine-grained tradeoff, PEFT methods typically target compute-constrained scenarios, where such granular control may require extra tuning that reduces practicality. It would be beneficial to include a plot with sparsity on the x-axis and performance on the y-axis to directly compare the flexibility of SGC with LoRA. This visualization could more intuitively demonstrate whether SGC’s fine-grained control offers practical performance benefits at different sparsity levels.",xrtM8r0zdU,ICLR_2025,"1. **Limited Applicability**: While the paper claims that SGC offers a more flexible, fine-grained tradeoff, PEFT methods typically target compute-constrained scenarios, where such granular control may require extra tuning that reduces practicality. It would be beneficial to include a plot with sparsity on the x-axis and performance on the y-axis to directly compare the flexibility of SGC with LoRA. This visualization could more intuitively demonstrate whether SGC’s fine-grained control offers practical performance benefits at different sparsity levels.
2. **Questionable Memory Advantage**: The memory usage for first-order optimization methods largely comes from the model parameters, gradients, activations, and optimizer states. Even with Adam’s two states, optimizer memory costs are typically less than half. SGC, based on Adam, can’t reduce memory below that of simple SGD without momentum, and since it still calculates full gradients, its GPU memory consumption may surpass LoRA, which doesn’t require full gradient computations.
3. **Subpar Performance**: As seen in Table 2, SGC shows no clear performance advantage over methods like LoRA and GaLore, raising questions about its efficacy as a fine-tuning method.
4. **Lack of Related Work Comparison**: The paper omits discussion and comparison with relevant optimizers like Adafactor[1] and CAME[2], which also focus on compressing optimizer states. These omissions reduce the context for understanding SGC’s place among similar methods. Including a comparison on task performance, memory efficiency and convergence speed would better contextualize SGC's advantages and place among similar methods. **References**:
[1] Shazeer, N., & Stern, M. (2018, July). Adafactor: Adaptive learning rates with sublinear memory cost. In *International Conference on Machine Learning* (pp. 4596-4604). PMLR.
[2] Luo, Y., Ren, X., Zheng, Z., Jiang, Z., Jiang, X., & You, Y. (2023). CAME: Confidence-guided Adaptive Memory Efficient Optimization. In *Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)*, pages 4442–4453, Toronto, Canada. Association for Computational Linguistics.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.",NIPS_2020_341,NIPS_2020,"- For theorem 5.1 and 5.2, is there a way to decouple the statement, i.e., separating out the optimization part and the generalization part? It would be clearer if one could give a uniform convergence guarantee first followed by how the optimization output can instantiate such uniform convergence. - In the experiments, is it reasonable for the German and Law school dataset to have shorter training time in Gerrymandering than Independent? Since in Experiment 2, ERM and plug-in have similar performance to Kearns et al. and the main advantage is its computation time, it would be good to have the code published.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"* It will help if the paper describes the possible alternate formulations for Confidence Diversity (CD). I am not asking for additional experiments here. It is difficult to what extra information is CD capturing on top of Predictive Uncertainty in the current form. Or why is entropy not a good measure of ""amount of spreading of teacher predictions over the probability simplex among different (training) samples"" (line 115). Note that line 113 did not clarify it for me.",NIPS_2020_183,NIPS_2020,"* It will help if the paper describes the possible alternate formulations for Confidence Diversity (CD). I am not asking for additional experiments here. It is difficult to what extra information is CD capturing on top of Predictive Uncertainty in the current form. Or why is entropy not a good measure of ""amount of spreading of teacher predictions over the probability simplex among different (training) samples"" (line 115). Note that line 113 did not clarify it for me. * I think the experimental section is somewhat weak. The paper considers three datasets and two models, but all the datasets are vision datasets and are small. Moreover, the baselines are sufficient. I would like to see at least a standard distillation baseline, trained with both soft and hard labels. We don't know the number of generations for the SD baseline as well. I understand the authors point out that parameter tuning is expensive. Still, they should consider ablations (and not parameter tuning) and show that their empirical gains are consistent across different parameters.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- For the human baseline, the human only closely follows a little more than 1 hour of speech recordings rather than the full 15 hours. This makes the human baseline considerably weaker than the model baseline, apart from all the other factors mentioned in Section 4.1 that make the human baseline weaker. In the abstract, the authors mention ""already beating the 34.2% CER and 4.51 BLEU achieved by a human who learned Kalamang from the same resources"" which is a bit misleading given the 1 hour vs.",sjvz40tazX,ICLR_2025,"- One main weakness is in the baselines. There is no comparison to regular finetuned baselines for the ASR and spoken translation (ST) tasks, that make use of the 15 hours of transcribed and translated speech. While the main objective of this work is to promote multimodal in-context learning with text and speech, it would still be useful to see how existing pretrained ASR/ST models fare after finetuning with labeled Kalamang speech.
- For the human baseline, the human only closely follows a little more than 1 hour of speech recordings rather than the full 15 hours. This makes the human baseline considerably weaker than the model baseline, apart from all the other factors mentioned in Section 4.1 that make the human baseline weaker. In the abstract, the authors mention ""already beating the 34.2% CER and 4.51 BLEU achieved by a human who learned Kalamang from the same resources"" which is a bit misleading given the 1 hour vs. 15 hour disparity. The authors should consider softening this claim.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1) The assumption for termination states of instructions are quite strong. In the general case, it is very expensive to label a large number of data manually.",NIPS_2021_1078,NIPS_2021,"weakness of this paper, I am concerned with the following two points: 1) The assumption for termination states of instructions are quite strong. In the general case, it is very expensive to label a large number of data manually. 2) It seems that performance and sample efficiency are sensitive to λ parameters.
(Page 9, lines 310-313) I don't understand how the process of calculating the λ
is done. How is λ
computed from step here?
(Page 8 lines 281-285) The authors explain why ELLA does not increase sample efficiency in a COMBO environment, but I don't quite understand what it means.
[1] Yuri Burda et al, Exploration by Random Network Distillation, ICLR 2019
[2] Deepak Pathak et al, Curiosity-driven Exploration by Self-supervised Prediction, ICML 2017
[3] Roberta Raileanu et al, RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments, ICLR 2020","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
3) Alice draws a random value $R$ (uniformly from the space of possible values of $X_u$,NIPS_2017_235,NIPS_2017,"weakness even if true but worth discussing in detail since it
could guide future work.) [EDIT: I see now that this is *not* the case, see response below]
I also feel that the discussion of Chow-Liu is missing a very
important aspect. Chow-Liu doesn't just correctly recover the
true structure when run on data generated from a tree. Rather,
Chow-Lui finds the *maximum-likelihood* tree for data from an
*arbitrary* distribution. This is a property that almost all
follow-up work does not satisfy (Srebro showed bounds). The
discussion in this paper is all true, but doesn't mention that
""maximum likelihood"" or ""model robustness"" issue at all, which
is hugely important in practice.
For reference: The basic result is that given a single node $u$,
and a hypothesized set of ""separators"" $S$ (neighbors of $u$) then
there will be some set of nodes $I$ with size at most $r-1$ such
that $u$ and $I$ have positive conditional mutual information.
The proof of the central result proceeds by setting up a
""game"", which works as follows:
1) We pick a node $X_u$ to look at.
2) Alice draws two joint samples $X$ and $X'$.
3) Alice draws a random value $R$ (uniformly from the space of
possible values of $X_u$
4) Alice picks a random set of neighbors of $X_u$, call them $X_I$.
5) Alice tells Bob the values of $X_I$
6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
happens if both or neither are true.
Here I first felt like I *must* be missing something, since
this is just establishing that $X_u$ has mutual information
with it's neighbors. (There is no reference to the ""separator""
set S in the main result.) However, it later appears that this
is just a warmup (regular mutual information) and can be
extended to the conditional setting.
Actually, couldn't the conditional setting itself be phrased
as a game, something like
1) We pick a node $X_u$ and a set of hypothesized ""separators""
$X_S$ to look at.
2) Alice draws two joint samples $X$ and $X'$ Both are
conditioned on the some random value for $X_S$.
3) Alice draws a random value $R$ (uniformly from the space of
possible values of $X_u$
4) Alice picks a random set of nodes (not including $X_u$ or $X_S$, call them $X_I$.
5) Alice tells Bob the values of $X_I$
6) Bob get's to wager on if $X_u=R$ or $X'_u=R$. Bob wins his
wager if $X_u=R$ an loses his wager if $X'_u=R$ and nothing
happens if both or neither are true.
I don't think this adds anything to the final result, but is
an intuition for something closer to the final goal.
After all this, the paper discusses an algorithm for greedily
learning an MRF graph (in sort of the obvious way, by
exploiting the above result) There is some analysis of how
often you might go wrong estimating mutual information from
samples, which I appreciate.
Overall, as far as I can see, the result appears to be
true. However, I'm not sure that the purely theoretical result
is sufficiently interesting (at NIPS) to be published with no
experiments. As I mentioned above, Chow-Liu has the major
advantage of finding the maximum likelihood solution, which the current method
does not appear to have. (It would violate hardness results
due to Srebro.) Further, note that the bound given in Theorem
5.1, despite the high order, is only for correctly recovering
the structure of a single node, so there would need to be
another lever of applying the union bound to this result with
lower delta to get a correct full model.
EDIT AFTER REBUTTAL:
Thanks for the rebuttal. I see now that I should understand $r$ not as the maximum clique size, but rather as the maximum order of interactions. (E.g. if one has a fully-connected Ising model, you would have r=2 but the maximum clique size would be n). This answers the question I had about this being a generalization of Bressler's result. (That is, this paper's is a strict generalization.) This does slightly improve my estimation of this paper, though I thought this was a relatively small concern in any case. My more serious concerns are that a pure theoretical result is appropriate for NIPS.","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['X', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
"1) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).",ICLR_2021_1533,ICLR_2021,"1) The nature of the contribution with respect to ECE_sweep is not clearly described in the text. Concretely, this amounts to a way to choose the number of bins using data (i.e., autotuning a hyperparameter in the estimate). While this, of course, leads to a different estimator, this is not something fundamentally different. I would much rather that the paper was upfront about the contribution. (In fact, I was pretty confused about the point the paper was making until I realised this).
2) I don't think the baseline comparisons made in the experiments are appropriate. The proposal is a method to choose the appropriate number of bins in the estimate, and should be compared to other methods to do so instead of to an arbitrary choice of number of bins as is done in section 5.2. Without this comparison, I have no way to judge if this is a good autotuning method or not. Reasonable comparisons could be, e.g., choosing b
by cross validation, or, in equal mass binning, choosing b
so that each bin has a reasonable number of samples for the error y ― k
to not be too large.
3) While the focus of the paper is on bias, it should be noted that by searching over many different bin sizes, the variance of ECE_sweep may be inflated. If this is to such an extent that the gains in bias relative to other autotuning methods are washed out, then this estimator would not be good. To judge this requires at least that the variances for ECE_sweep are reported, but these are never mentioned in the main text.
4) Choice of law in simulation in section 3, which are used to illustrate the dependence of bias on the number of bins, not aligned with the laws/curves in figure 3. Taking the latter as representative of the sort of laws and calibration curves that arise in practice, there are two issues:
4a) The pdfs of f
tend to be a lot more peaked near the end than the one explored in section 3 - this is borne out by the values of α , β
in the fits in Table 1. Beta(1.1,1) is remarkably flat compared to the curves in Fig 3.
4b) There seem to be a few different qualitative properties of the calibration curves - monotone but with a large intercept at 0; those with an inflection point in the middle; and those with the bulk lying below the y = x
line. In particular, all of them tend to have at least some region above the y = x
line. The choice of curve c 2
in section 3 doesn't completely align with any of these cases, but even if we make the case that it aligns with the third type, this leaves two qualitative behaviours unexplored.
In fact, the choice of laws is such that the error of the hard classifier that thresholds f at 1 / 2 is 26 %
. I don't think we're usually interested in the calibration of a predictor as poor as this in practice.
All of this makes me question the relevance of this simulation. Is the dependence of the bias on the number of bins as strong for the estimated laws as it is for these? Seeing the the equivalents of figs 7 and 8 for the laws from section 5 would go a long way in sorting this out.
5) Experiments: As I previously mentioned, I don't think the correct baselines are compared to. Instead of posing the method against other autotuning schemes, just one choice of the number of bins is taken. This already makes it near impossible to judge the efficacy of this method.
Despite this, even the data presented does not make a clear case for ECE_sweep. In Fig. 4 we see that the bias of EW_sweep is even worse than EW. This already means that the sweep estimate doesn't fix the issues of ECE_bin in all contexts. It is the case that EM_sweep has better bias than EM, but again, for samples large enough for the variances to be in control, it seems like these numbers are both converging to the same, so I don't see any distinct advantage when it comes to estimation. (of course, this is moot because this isn't the right comparison anyway)
Also, Fig. 5 is flawed because it compares EW and EM_sweep. It should either compare EM and EM_sweep, or EW and EW_sweep, I don't see why EW and EM_sweep are directly comparable.
Minor issues:
a) Algorithm (1) and the formula for ECE_sweep in section 4 don't compute the same thing. In algorithm (1), you find the largest b
such that the resulting y ― k
is a monotone sequence, and return the ECE_bin for this number of bins. In the formula, you maximise the ECE_bin for all b that yield a monotone y ― k
. From the preceding text, I assumed that the quantity in Algorithm (1) is intended.
b) Why is the L p
norm definition of the ECEs introduced at all? In the paper only p = 2
is used throughout. I feel like the p
just complicates things without adding much - even if you only present the L 2
definition, the fact that a generic p
can be used instead should be obvious to the audience.
c) Design considerations for ECE_sweep - it is worth noting that accuracy is not all that we want in an estimate of calibration error. For instance, one might reasonably want to add this as a regulariser when training a model in order to obtain better calibrated solutions. One issue with ECE_sweep is that it introduces a problem in that how the number of bins in the ECE_sweep estimate changes with a small change in model parameters seems very difficult to handle, which makes this a nondifferentiable loss. Broader issues of this form, and a discussion of how they may be mitigated, could lead to a more well rounded paper. Comments:
a) Exact monotonicity in the ECE_sweep proposal - I find the argument stemming from the monotonicity of the true calibration curve, and the idea to use this to nail down a maximum binning size interesting. However, why should we demand exact monotonicity in the bin heights? Each y ― k
will have noise at the scale of roughly b / n ,
(for equal mass binning with b
bins), and in my opinion, violation of monotonicity at this scale should not be penalised. Also, what if a few y ― k
s decrease but most are increasing (i.e., the sequence has a few falling regions, but the bulk is increasing)? Perhaps instead of dealing with this crudely, the error of a shape constrained estimator may serve as a better proxy.
b) Isn't the procedure for parametrically fitting the pdf of f
, and E [ Y f ( X ) ] ,
and then integrating the bias a completely different estimator for TCE of a model? In fact, if the laws are a good fit, as is claimed in section 5.1, then this plug in estimator might do well simply because the integration is exact. In fact, since the fit is parametric, this can further be automatically differentiated (if, say, f
were a DNN), and thus used to train.
c) It would be interesting to see what number of bins are ultimately adopted in the ECE_sweep computations that are performed.
Overall opinion: The lack of comparison to appropriate baselines makes it near impossible for me to judge the validity of the proposed estimator. I feel like this is a deep methodological flaw when it comes to evaluating the main proposal of the paper. This is a real pity because I quite like some of the ideas in the paper.
Due to the inability to evaluate the main contribution of the paper, i am rating it a strong reject. I'd be completely open to re-rating it if appropriate comparisons are performed, and the case for the method is properly made.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The related work discusses other methods for training NMT models beyond MLE (e.g., RL methods) but none of them is used as a baseline.",bkNx3O0sND,ICLR_2024,"- Some parts of the paper (e.g., the first three paragraphs of the introduction) talk about NLG in general but the paper focuses on machine translation. Reranking methods, in particular, rely on good quality estimation models that exist for MT but may not exist for other tasks. Of course, MBR finetuning can be applied to other NLG tasks (e.g., summarization) but this paper does not touch this problem. I think the authors should either revise the writing and make the scope more clear from the beginning, or perform experiments in other NLG tasks. The contribution is mainly empirical and only validated for MT.
- The study is limited in terms of language coverage. They perform experiments on 2 language pairs only (mid & high resource), both for translation out of English. It’s not clear if the findings hold for lower resource languages and when translating into English. In particular, I’m expecting quality estimation models to be worse for low resource languages, which may end up affecting the quality of the translations produced with their method. Have you tried other languages? If so, what happens in that case?
- The related work discusses other methods for training NMT models beyond MLE (e.g., RL methods) but none of them is used as a baseline.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1) it is unclear if the authors mean a particular efficient proxy or efficient proxies in general, ""is"" suggests that it is a particular proxy, but then there is not proxy called ""Efficient Proxy"", which suggests that it is rather referring to a family of efficient proxies;",ICLR_2023_3956,ICLR_2023,"(W1) Eproxy alone does not actually work that well, it is only Eproxy+DPS that delivers convincing results - however, the necessity of having extra trained architectures (even if only 20) puts this work in a significantly different complexity regime than most of the baselines to which the paper compares - in short, the choice of baselines seems inadequate to the cost of performing EProxy+DPS.
(W2) Related to the above, searching cost on ImageNet for Eproxy+DPS seems to ignore the necessity of training 20 architectures for DPS - including this cost would increase the searching time for Eproxy+DPS SIGNIFICANTLY, make it fall short to many of the baselines presented in Table 8 (e.g., TE-NAS, P-DARTS, likely SNAS and PC-DARTS).
(W3) Comparison to synflow and NASWOT in table 5 seems biased -- why only assume 0 queries for these method where both were originally proposed to be used with either random sampling or evolutionary search? Specifically, results reported for synflow in the original paper are between 51 and 34 queries to reach test accuracy of 94.22, which is significantly better than what is reported for Eproxy+DPS (150 queries). Also, it is unclear what the authors mean by ""we utilize the Eproxy as the fitness function for Regularized Evolution"" and then showing results for varying number of queries, especially 0 queries (this might be also related to the point below).
(W4) Showing that ""Eproxy+DPS can find optimal global architectures within the RE search history"" in Table 4 is incorrect in its context - it is possible that RE significantly helps Eproxy+DPS by prefiltering all architectures based on their accuracy (thus being very costly), showing results of Eproxy+DPS conditioned by running RE is not the same as showing results of just Eproxy+DPS (what the Table's caption suggests, by saying: ""Eproxy+DPS uses substantially lower queries to find the global optimal architectures."")
Minor shortcomings and suggestions:
Figure 1 is not exactly informative as it does not convey any information about quality of each method
Figure 2 also does not seem to add much, I actually had pretty hard time understanding what it tries to say - what is ""W"" at x axis? why does ""arch 1"" finish earlier than ""arch 2""? why do they follow the same curve? Just to be clear, I am quite sure I know what the authors tried to say here, the concept of cheap proxies for NAS is not hard to grasp, but the Figure in its current form is not helpful, in my opinion
while the choice of NAS benchmarks seems adequate to me, it would strengthen the paper if the authors could include experiments on some additional, large search spaces other than benchmarks/DARTS, for example a MobileNet-like search space
zero-cost proxies considered in the paper are not very new, some of the more recent ones that the authors could include to make comparison more convincing are: ZenNAS, KNAS
I would suggest changing the title - in its current form it is unclear really what the authors tried to say, some reasoning: 1) it is unclear if the authors mean a particular efficient proxy or efficient proxies in general, ""is"" suggests that it is a particular proxy, but then there is not proxy called ""Efficient Proxy"", which suggests that it is rather referring to a family of efficient proxies; 2) however, saying that a generic class of efficient proxies is now extensible suggests that the authors propose a generic method to make any efficient proxy extensible, which is not the case. I would recommend changing the title to something like ""Extensible and Efficient Proxy for NAS"", or something similar, it would make it much clear that the authors propose a new proxy that is supposed to be both efficient and extensible.
Actually, another minor comment is that I am not sure if ""extensible"" is the best term here -- in my opinion a better term would be: ""generalizable"", ""transferable"" or ""adaptive"", all of them suggest to me some kind of changes in the training data (i.e., downstream task), which seems to be the main point of Eproxy. At the same time, ""extensible"" suggests more that something new could be easily added to the proposed method in order to make it better. Details:
W1: Specifically, on NDS Eproxy is only comparable to NASWOT, despite performing few-shot training of architectures (i.e., even if the absolute cost is not very high, it's still 10x more costly than NASWOT, without any obvious benefits), furthermore on TransNAS-Bench-Micro Eproxy fails quite significantly on 6 out of 7 tasks. On NAS-Bench-MR Eproxy achieves convincing improvement (i.e., better in at least one metric - spearman-r or top% models - while not compromising another) over the selected baselines in only 2 out of 9 cases, consequently it falls short to synflow on average. On the other hand, as mentioned above, Eproxy+DPS does improve upon both synflow and naswot, most of the time, but it also requires training a number of architecture, which makes it no longer low-cost method - at best, it should be treated as efficient, predictor-based approach. However, the comparison is done almost exclusively with methods that do not assume any training (the only notable exceptions are Table 6, 7, and 8, but they have their own problems, as mentioned in other parts of the review, and the choice of baselines in Table 6 and 7 is not also not very convincing).
W2: reported searching cost is approximately 1.5 GPU hours (0.06 GPU days) but training a single model from the DARTS CNN space on CIFAR-10, using the official code, takes roughly a day on a V100. Even though the authors use models from a different search space (NDS-DARTS benchmark), that in practice come for free, and might also be cheaper to train than standard DARTS, I am not convinced that the presented results are fair in comparing the proposed methods vs. baselines. Specifically, considering that the main focus of the submitted work is to enable low-cost NAS for diverse tasks, it is not unreasonable in my opinion to expect the proposed method to work well in situations when we do not have a priori knowledge of performance of certain architectures, or even if there exists a highly-correlated proxy task/search space (like DARTS vs. NDS-DARTS). Therefore not including the cost of obtaining accuracy for the 20 architectures needed to conduct DPS is likely to be misleading, were the method to be applied in real-world situations, which it tries to address.
W4: to be clear, I am inclined to believe that Eproxy+DPS is likely to produce better results than just using RE, especially if the authors used it to augment RE, like, e.g., Abdelfattah et al. However, formally speaking, this has not been shown in the current paper. Specifically, if we know that RE finds optimal architecture in N steps, then ""finding optimal architecture within RE history"" is actually a problem of finding the correct model within the N models, rathar than within the whole search space -- the a priori knowledge that the optimal models exists in a relatively small subset of the search space provides a very strong prior which can naturally boost a method's performance. In the context of Table 7, RE is shown to find the optimal architecture in ~560 queries, so Eproxy+DPS needing 58 actually explores 10% of all models - if we were to apply the same level of performance to the entire search space (15k models) then it would be 1500 queries. While I would not claim that this is the expected number for Eproxy+DPS, I hope that this clearly shows why the current results are misleading. Even a random search would do significantly better in this setting.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster.",ICLR_2023_1823,ICLR_2023,"Weakness: 1.They stack the methods of Mirzasoleiman et al., 2020 and Group-learning setting, and then use one classical method DBSCAN to cluster. 2. In comparison of gradient space and feature space, the normalization of the data in Figure 2 is not so clear. I think you do not need to do normalization of the data, since the shrinkage of the correctly classified points is profit to outlier identification. 3. It is not clear what variable the derivative is based on. I thought the gradient is a very large dimensional vector (tensor). Since the network is with many layers (e.g. ResNet-50), the dimension of the derivative should be about 50 times. When moving to numerical result, I found the ResNet-50 is pretrained and the derivative is only related to the parameters of logistic regression.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.",NIPS_2021_1222,NIPS_2021,"Claims: 1.a) I think the paper falls short of the high-level contributions claimed in the last sentence of the abstract. As the authors note in the background section, there are a number of published works that demonstrate the tradeoffs between clean accuracy, training with noise perturbations, and adversarial robustness. Many of these, especially Dapello et al., note the relevance with respect to stochasticity in the brain. I do not see how their additional analysis sheds new light on the mechanisms of robust perception or provides a better understanding of the role stochasticity plays in biological computation. To be clear - I think the paper is certainly worthy of publication and makes notable contributions. Just not all of the ones claimed in that sentence.
1.b) The authors note on lines 241-243 that “the two geometric properties show a similar dependence for the auditory (Figure 4A) and visual (Figure 4B) networks when varying the eps-sized perturbations used to construct the class manifolds.” I do not see this from the plots. I would agree that there is a shared general upward trend, but I do not agree that 4A and 4B show “similar dependence” between the variables measured. If nothing else, the authors should be more precise when describing the similarities.
Clarifications: 2.a) The authors say on lines 80-82 that the center correlation was not insightful for discriminating model defenses, but then use that metric in figure 4 A&B. I’m wondering why they found it useful here and not elsewhere? Or what they meant by the statement on lines 80-82.
2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?
2.c) The authors report mean capacity and width in figure 2. I think this is the mean across examples as well as across seeds. Is the STD also computed across examples and seeds? The figure caption says it is only computed across seeds. Is there a lot of variability across examples?
2.d) I am unsure why there would be a gap between the orange and blue/green lines at the minimum strength perturbation for the avgpool subplot in figure 2.c. At the minimum strength perturbation, by definition, the vertical axis should have a value of 1, right? And indeed in earlier layers at this same perturbation strength the capacities are equal. So why does the ResNet50 lose so much capacity for the same perturbation size from conv1 to avgpool? It would also be helpful if the authors commented on the switch in ordering for ATResNet and the stochastic networks between the middle and right subplots.
General curiosities (low priority): 3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.
3.b) There appears to be a pretty big difference in the overall trends of the networks when computing the class manifolds vs exemplar manifolds. Specifically, I think the claims made on lines 191-192 are much better supported by Figure 1 than Figure 2. I would be interested to hear what the authors think in general (i.e. at a high/discussion level) about how we should interpret the class vs exemplar manifold experiments.
Nitpick, typos (lowest priority): 4.a) The authors note on line 208 that “Unlike VOneNets, the architecture maintains the conv-relu-maxpool before the first residual block, on the grounds that the cochleagram models the ear rather than the primary auditory cortex.” I do not understand this justification. Any network transforming input signals (auditory or visual) would have to model an entire sensory pathway, from raw input signal to classification. I understand that VOneNets ignore all of the visual processing that occurs before V1. I do not see how this justifies adding the extra layer to the auditory network.
4.b) It is not clear why the authors chose a line plot in figure 4c. Is the trend as one increases depth actually linear? From the plot it appears as though the capacity was only measured at the ‘waveform’ and ‘avgpool’ depths; were there intermediate points measured as well? It would be helpful if they clarified this, or used a scatter/bar plot if there were indeed only two points measured per network type.
4.c) I am curious why there was a switch to reporting SEM instead of STD for figures 5 & 6.
4.c) I found typos on lines 104, 169, and the fig 5 caption (“10 image and”).","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3.) Line 186 ff. : A usage of the subdominant ultrametric for the cluster-size regularization, would make the algorithms part more consistent with the following considerations in this paper.",NIPS_2019_286,NIPS_2019,"1.) l Line 8,56,70,93,: Usage of the word equivalent. I would suggest a more cautious usage of this word. Especially, if the equivalence is not verified. 2.) A differentiation between the ultrametric d and the ultrametric u would make their different usages clearer. 3.) Line 186 ff. : A usage of the subdominant ultrametric for the cluster-size regularization, would make the algorithms part more consistent with the following considerations in this paper. 4) The paper is not sufficiently clear in some aspects (see below for a list of questions) Overall, I have the impressions the the weaknesses could be fixed until the final paper submission.","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['4', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ'], 'labels': ['1']}",,
"* synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by ""support data"" and what by ""predicted training count data""? Could you write down the model used here explicitly, e.g. add it to the appendix?",NIPS_2019_1350,NIPS_2019,"of the method. CLARITY: The paper is well organized, partially well written and easy to follow, in other parts with quite some potential for improvement, specifically in the experiments section. Suggestions for more clarity below. SIGNIFICANCE: I consider the work significant, because there might be many settings in which integrated data about the same quantity (or related quantities) may come at different cost. There is no earlier method that allows to take several sources of data into account, and even though it is a fairly straightforward extension of multi-task models and inference on aggregated data, it is relevant. MORE DETAILED COMMENTS: --INTRO & RELATED WORK: * Could you state somewhere early in the introduction that by ""task"" you mean ""output""? * Regarding the 3rd paragraph of the introduction and the related work section: They read unnaturally separated. The paragraph in the introduction reads very technical and it would be great if the authors could put more emphasis there in how their work differs from previous work and introduce just the main concepts (e.g. in what way multi-task learning differs from multiple instance learning). Much of the more technical assessment could go into the related work section (or partially be condensed). --SECTION 2.3: Section 2 was straightforward to follow up to 2.3 (SVI). From there on, it would be helpful if a bit more explanation was available (at the expense of parts of the related work section, for example). More concretely: * l.145ff: $N_d$ is not defined. It would be good to state explicitely that there could be a different number of observations per task. * l.145ff: The notation has confused me when first reading, e.g. $\mathbb{y}$ has been used in l.132 for a data vector with one observation per task, and in l.145 for the collection of all observations. I am aware that the setting (multi-task, multiple supports, different number of observations per task) is inherently complex, but it would help to better guide the reader through this by adding some more explanation and changing notation. Also l.155: do you mean the process f as in l.126 or do you refer to the object introduced in l.147? * l.150ff: How are the inducing inputs Z chosen? Is there any effect of the integration on the choice of inducing inputs? l.170: What is z' here? Is that where the inducing inputs go? * l.166ff: It would be very helpful for the reader to be reminded of the dimensions of the matrices involved. * l.174 Could you explicitly state the computational complexity? * Could you comment on the performance of this approximate inference scheme based on inducing inputs and SVI? --EXPERIMENTS: * synthetic data: Could you give an example what kind of data could look like this? In Figure 1, what is meant by ""support data"" and what by ""predicted training count data""? Could you write down the model used here explicitly, e.g. add it to the appendix? * Fertility rates: - It is unclear to me how the training data is aggregated and over which inputs, i.e. what you mean by 5x5. - Now that the likelihood is Gaussian, why not go for exact inference? * Sensor network: - l.283/4 You might want to emphasize here that CI give high accuracy but low time resolution results, e.g. ""...a cheaper method for __accurately__ assessing the mass..."" - Again, given a Gaussian likelihood, why do you use inducing inputs? What is the trade-off (computational and quality) between using the full model and SVI? - l.304ff: What do you mean by ""additional training data""? - Figure 3: I don't understand the red line: Where does the test data come from? Do you have a ground truth? - Now the sensors are co-located. Ideally, you would want to have more low-cost sensors that high-cost (high accuracy) sensors in different locations. Do you have a thought on how you would account for spatial distribution of sensors? --REFERENCES: * please make the style of your references consistent, and start with the last name. Typos etc: ------------- * l.25 types of datasets * l.113 should be $f_{d'}(v')$, i.e. $d'$ instead of $d$ * l.282 ""... but are badly bias"" should be ""is(?) badly biased"" (does the verb refer to measurement or the sensor? Maybe rephrase.) * l.292 biased * Figure 3: biased, higher peaks, 500 with unit. * l.285 consisting of? Or just ""...as observations of integrals"" * l.293 these variables","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods.",ICLR_2023_3912,ICLR_2023,"1. Compared with DyTox, the proposed method obtains a little performance improvement with more parameters and variance. 2. The experiments based on ImageNet-1000 are missed, which is a large dataset and suitable for real-world situations. 3. The evaluation of FGT is only leveraged to evaluate the method performance in the ablation study, which should be used to evaluate the performance of the proposed method and the comparative methods. 4. I want to know why the mode parameters in Table 1 and Figure 5 are different. 5. The article structure of this paper is a mess, whose Appendix should appear in the supplementary material.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
2. The model seems overly simple. This is both a feature and a bug.,NIPS_2020_725,NIPS_2020,"1. I think it is time to retire this dataset in favor of something more realistic that could be more specific about human learning. 2. The model seems overly simple. This is both a feature and a bug. 3. The characterizations are not as systematic as one would like. For example, while all models are characterized in terms of their learning dynamics and final representational structure, only two are displayed in Figure 2 for their stage-like behavior. If this is in the supplementary material, ok, but the paper should be relatively self-contained. 4. The notation is very confusing.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', 'X', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"* The work is focused on a narrow task (climate change QA) in a specific language (Arabic), so its broader impact may be limited.",7GxY4WVBzc,EMNLP_2023,"* The contribution of the vector database to improving QA performance is unclear. More analysis and ablation studies are needed to determine its impact and value for the climate change QA task.
* Details around the filtering process used to create the Arabic climate change QA dataset are lacking. More information on the translation and filtering methodology is needed to assess the dataset quality.
* The work is focused on a narrow task (climate change QA) in a specific language (Arabic), so its broader impact may be limited.
* The limitations section lacks specific references to errors and issues found through error analysis of the current model. Performing an analysis of the model's errors and limitations would make this section more insightful.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', 'X', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- [Originality] The winner-take-all property has been widely used in previous works such as NN-based clustering algorithms [1] and it’s unclear how this paper contributes novelly to the understanding of this behavior with its extremely simplified settings, especially since most of the findings have been reported in previous works (Sec 5).",sJslLVsYNo,ICLR_2024,"- [Originality] The winner-take-all property has been widely used in previous works such as NN-based clustering algorithms [1] and it’s unclear how this paper contributes novelly to the understanding of this behavior with its extremely simplified settings, especially since most of the findings have been reported in previous works (Sec 5).
- [Quality] The quality of the paper is unacceptable due to the following issues:
1) The experimental setup is highly insufficient for a top-tier conference like ICLR, with overly simplified network, datasets and analyses (only scalar plots from single neurons instead of e.g. cluster analysis and/or visualization), leaving the results highly inconclusive.
2) The observed winner-take-most (divide-and-conquer) behavior could be simply due to insufficient training as it contradicts the neural collapse theory [2] which predicts the opposite, the collapse of all intraclass clusters, leaving the main results highly questionable.
3) Claiming that training noises are required for generalization on the synthetic dataset (Sec 3.1) is quite problematic, since sufficient training should generally lead to the max-margin solution (which generalizes) even without training noises [3]. The extremely large learning rate (1.0) could be causing the problem.
- [Significance] Given the critical issues in the paper’s originality and quality as stated above, it’s unfortunately hard to conclude that this work is significant or sufficiently promising.
[1] Clustering: A neural network approach, Neural networks, 2010.\
[2] Prevalence of neural collapse during the terminal phase of deep learning training, PNAS, 2020.\
[3] The Implicit Bias of Gradient Descent on Separable Data, JMLR, 2018.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '3', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
2.) It may be good to briefly mention the negligible computational cost of CHR (which is in the appendix) in the main paper to help motivate the method. A rough example of some run-times in the experiments may also be useful for readers looking to apply the method.,NIPS_2021_304,NIPS_2021,"/Questions:
I only have a few minor points:
1.) For equation (7), does treating | u − l |
as the length require the bins to be equally spaced? I don't think this is stated.
2.) It may be good to briefly mention the negligible computational cost of CHR (which is in the appendix) in the main paper to help motivate the method. A rough example of some run-times in the experiments may also be useful for readers looking to apply the method.
3.) Just a few typographical/communication points:
I found Section 2.2 slightly difficult to read, as the notation gets a little heavy. This may not be necessary, but the authors could consider presenting the nested intervals without randomization (e.g. after Line 119), with the randomization in the Appendix, as it is not needed in Theorem 2. This would give more room for intuitive discussions, related to my next point.
It may be helpful to introduce some intuition on the conformity score in equation (12) and why we need the sets to be nested for readers unfamiliar with previous work, perhaps at the start of Section 2.3.
Line 113: ϵ
is mentioned here before it is defined
Line 188: 'increased' instead of 'increase' ##################################################################### Overall:
This paper is an interesting extension of previous work, and the provided asymptotic justifications of attaining oracle width and conditional coverage is useful. The method is also general and can empirically provide better average widths and conditional coverage than other methods, particularly under skewed data, making it useful in practice. ##################################################################### References:
Romano, Y., Sesia, M., & Candes, E. (2020). Classification with Valid and Adaptive Coverage. Advances in Neural Information Processing Systems, 33, 3581-3591.
Gupta, C., Kuchibhotla, A. K., & Ramdas, A. K. (2019). Nested conformal prediction and quantile out-of-bag ensemble methods. arXiv preprint arXiv:1910.10562.
The authors have described the limitations of their method - in particular their method does not control for upper and lower miscoverage, and they provide alternative recommendations.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Figure 3 presents EEG topography plots for both the input and output during the EEG token quantization process, leading to some ambiguity in interpretation. I would recommend the authors to elucidate this procedure in greater detail. Specifically, it would be insightful to understand whether the spatial arrangement of the EEG sensors played any role in this process.",gp5dPMBzMH,ICLR_2024,"1. Figure 3 presents EEG topography plots for both the input and output during the EEG token quantization process, leading to some ambiguity in interpretation. I would recommend the authors to elucidate this procedure in greater detail. Specifically, it would be insightful to understand whether the spatial arrangement of the EEG sensors played any role in this process.
2. The manuscript introduces BELT-2 as a progression from the prior BELT-1 model. However, the discussion and distinction between the two models are somewhat scanty, especially given their apparent similarities. It would be of immense value if the authors could elaborate on the design improvements made in BELT-2 over BELT-1. A focused discussion highlighting the specific enhancements and their contribution to the performance improvements, as showcased in Table 1 and Table 4, would add depth to the paper.
3. A few inconsistencies are observed in the formatting of the tables, which might be distracting for readers. I'd kindly suggest revisiting and refining the table presentation to ensure a consistent and polished format.
4. In Figure 4 and Section 2.4, there is a mention of utilizing the mediate layer coding as 'EEG prompts'. The concept, as presented, leaves some gaps in understanding, primarily because its introduction and visualization seem absent or not explicitly labeled in the preceding figures and method sections. It would enhance coherence and clarity if the authors could revisit Figures 2 and/or 3 and annotate the specific parts illustrating this mediate layer coding.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- There may be no need to distinguish between the two types of representation distances $d_{SM}$ and $d_{DM}$, as they are calculated in the same way.",GHaoCSlhcK,ICLR_2025,"1. **Limited discussion of related works** on heterogeneous architectures and PCA-based methods. Below are some relevant examples. The authors are encouraged to conduct a more thorough literature review. Without a clear differentiation from existing methods, there is a concern about the novelty of the paper.
- Liu, Yufan, et al. ""Cross-architecture knowledge distillation."" *ACCV 2022*.
- Hofstätter, Sebastian, et al. ""Improving efficient neural ranking models with cross-architecture knowledge distillation."" *arXiv:2010.02666* (2020).
- Ni, Jianyuan, et al. ""Adaptive Cross-Architecture Mutual Knowledge Distillation."" *FG 2024*.
- Chiu, Tai-Yin, and Danna Gurari. ""PCA-based knowledge distillation towards lightweight and content-style balanced photorealistic style transfer models."" *CVPR 2022*.
- Guo, Yi, et al. ""RdimKD: Generic Distillation Paradigm by Dimensionality Reduction."" *arXiv:2312.08700* (2023).
2. **Unclear necessity and effectiveness of using CKA** for modularization. An ablation study comparing CKA-based modularization with simpler approaches, such as dividing networks based on feature map resolution, should be included.
3. **Writing quality could be improved** to enhance rigor and clarity. For example:
- In lines 56-57, the phrase “it begins by training … representative features are transferred first” is unclear. It is not intuitive why training the shallowest module would ensure the most representative features.
- There may be no need to distinguish between the two types of representation distances $d_{SM}$ and $d_{DM}$, as they are calculated in the same way.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- the authors leverage the complexity of checking on the Witness oracle, which is ""polynomial time"" in the tabular case. This feels like not addressing the problem in a direct way.",NIPS_2018_914,NIPS_2018,"of the paper are (i) the presentation of the proposed methodology to overcome that effect and (ii) the limitations of the proposed methods for large-scale problems, which is precisely when function approximation is required the most. While the intuition behind the two proposed algorithms is clear (to keep track of partitions of the parameter space that are consistent in successive applications of the Bellman operator), I think the authors could have formulated their idea in a more clear way, for example, using tools from Constraint Satisfaction Problems (CSPs) literature. I have the following concerns regarding both algorithms: - the authors leverage the complexity of checking on the Witness oracle, which is ""polynomial time"" in the tabular case. This feels like not addressing the problem in a direct way. - the required implicit call to the Witness oracle is confusing. - what happens if the policy class is not realizable? I guess the algorithm converges to an \empty partition, but that is not the optimal policy. minor: line 100 : ""a2 always moves from s1 to s4 deterministically"" is not true line 333 : ""A number of important direction"" -> ""A number of important directions"" line 215 : ""implict"" -> ""implicit"" - It is hard to understand the figure where all methods are compared. I suggest to move the figure to the appendix and keep a figure with less curves. - I suggest to change the name of partition function to partition value. [I am satisfied with the rebuttal and I have increased my score after the discussion]","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '3', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching.",NIPS_2021_1788,NIPS_2021,"- The approach proposed is quite simple and straightforward without much technical innovation. For example, CODAC is a direct combination of CQL and QR-DQN to learn conservative quantiles of the return distribution. - Some parts of the paper need clearer writing (more below)
Comments and questions: - I think in a paragraph from lines 22-30 when discussing distributional RL, the paper lacks relevant literature on using moment matching (instead of quantile regression as most DRL methods) for DRL (Nguyen-Tang et al AAAI’21, “Distributional Reinforcement Learning via Moment Matching”). I think this should be properly discussed when talking about various approaches to DRL that have been developed so far, even though the present paper still uses quantile regression instead of moment matching. - More explanation is needed for Eq (5). For example, what is the meaning of the cost c 0 ( s , a )
? (e.g., to quantify out-of-distribution actions) - The use of s ′ and a ′
when defining $\hat{\pi}{\beta} a t l i n e 107 m i g h t c a u s e c o n f u s i o n a s \mathcal{D} c o n t a i n s
(s,a,r,s’)$. - This paper is about deriving a conservative estimate of the quantiles of the return from offline data where the conservativeness is for penalizing out-of-distribution actions. In the paper, they define OOD actions as those are not drawn from \hat{\pi}{\beta}(.|s) (line 109) but in Assumption 3.1. they assume that \hat{\pi}_{\beta}(a|s) > 0, i.e., there is no OOD actions. Thus, what is the merit of the theoretical result presented in the paper?
The authors have adequately addressed the limitations and social impact of their work.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Missing some baselines: For code completion tasks, I suggest that the authors compare with existing code completion commercial applications, such as Copilot. It can be tested on a smaller subset of RepoEval, and it is essential to compare with these state-of-the-art code completion systems.",q09vTY1Cqh,EMNLP_2023,"1. Missing some baselines: For code completion tasks, I suggest that the authors compare with existing code completion commercial applications, such as Copilot. It can be tested on a smaller subset of RepoEval, and it is essential to compare with these state-of-the-art code completion systems.
2. Time efficiency: For code completion tasks, it is also important to focus on the time efficiency. I recommend the authors could add corresponding experiments to make it clear.
3. Missing some related work: Recently there are some papers on similar topics, such as repo-level benchmark [1], API invocation [2]. The authors could consider discussing them in related work to better tease out research directions.
[1] Liu, Tianyang et al. “RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems.” (2023)
[2] Zhang, Kechi et al. “ToolCoder: Teach Code Generation Models to use API search tools.”(2023)","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- The choice to evaluate on only a subset of the Massive Text Embedding Benchmark (MTEB) raises questions about generalizability; it would be helpful to understand the criteria behind this selection and whether other tasks or datasets might yield different insights.,eFGQ97z5Cd,ICLR_2025,"- While MoEE introduces two methods for combining HS and RW embeddings (concatenation and weighted sum), the concatenation variant appears simplistic and less effective than the weighted sum in terms of similarity calculation. Future work could explore more sophisticated aggregation methods to fully leverage the complementary strengths of HS and RW.
- The claim that RW embeddings are more robust than HS, based solely on prompt variation tests, lacks comprehensive support. Other factors, such as model size (or maybe architectural variations), should be examined to substantiate this claim.
- The choice to evaluate on only a subset of the Massive Text Embedding Benchmark (MTEB) raises questions about generalizability; it would be helpful to understand the criteria behind this selection and whether other tasks or datasets might yield different insights.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).",NIPS_2017_110,NIPS_2017,"weakness of this paper in my opinion (and one that does not seem to be resolved in Schiratti et al., 2015 either), is that it makes no attempt to answer this question, either theoretically, or by comparing the model with a classical longitudinal approach.
If we take the advantage of the manifold approach on faith, then this paper certainly presents a highly useful extension to the method presented in Schiratti et al. (2015). The added flexibility is very welcome, and allows for modelling a wider variety of trajectories. It does seem that only a single breakpoint was tried in the application to renal cancer data; this seems appropriate given this dataset, but it would have been nice to have an application to a case where more than one breakpoint is advantageous (even if it is in the simulated data). Similarly, the authors point out that the model is general and can deal with trajectories in more than one dimensions, but do not demonstrate this on an applied example.
(As a side note, it would be interesting to see this approach applied to drug response data, such as the Sanger Genomics of Drug Sensitivity in Cancer project).
Overall, the paper is well-written, although some parts clearly require a background in working on manifolds. The work presented extends Schiratti et al. (2015) in a useful way, making it applicable to a wider variety of datasets.
Minor comments:
- In the introduction, the second paragraph talks about modelling curves, but it is not immediately obvious what is being modelled (presumably tumour growth).
- The paper has a number of typos, here are some that caught my eyes: p.1 l.36 ""our model amounts to estimate an average trajectory"", p.4 l.142 ""asymptotic constrains"", p.7 l. 245 ""the biggest the sample size"", p.7l.257 ""a Symetric Random Walk"", p.8 l.269 ""the escapement of a patient"".
- Section 2.2., it is stated that n=2, but n is the number of patients; I believe the authors meant m=2.
- p.4, l.154 describes a particular choice of shift and scaling, and the authors state that ""this [choice] is the more appropriate."", but neglect to explain why.
- p.5, l.164, ""must be null"" - should this be ""must be zero""?
- On parameter estimation, the authors are no doubt aware that in classical mixed models, a popular estimation technique is maximum likelihood via REML. While my intuition is that either the existence of breakpoints or the restriction to a manifold makes REML impossible, I was wondering if the authors could comment on this.
- In the simulation study, the authors state that the standard deviation of the noise is 3, but judging from the observations in the plot compared to the true trajectories, this is actually not a very high noise value. It would be good to study the behaviour of the model under higher noise.
- For Figure 2, I think the x axis needs to show the scale of the trajectories, as well as a label for the unit.
- For Figure 3, labels for the y axes are missing.
- It would have been useful to compare the proposed extension with the original approach from Schiratti et al. (2015), even if only on the simulated data.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"• shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 ) , since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution.",ICLR_2021_1740,ICLR_2021,"are in its clarity and the experimental part.
Strong points Novelty: The paper provides a novel approach for estimating the likelihood of p(class image), by developing a new variational approach for modelling the causal direction (s,v->x). Correctness: Although I didn’t verify the details of the proofs, the approach seems technically correct. Note that I was not convinced that s->y (see weakness)
Weak points Experiments and Reproducibility: The experiments show some signal, but are not through enough: • shifted-MNIST: it is not clear why shift=0 is much better than shift~ N ( 0 , σ 2 )
, since both cases incorporate a domain shift • It would be useful to show the performance the model and baselines on test samples from the observational (in) distribution. • Missing details about evaluation split for shifted-MNIST: Did the experiments used a validation set for hyper-param search with shifted-MNIST and ImageCLEF? Was it based on in-distribution data or OOD data? • It would be useful to provide an ablation study, since the approach has a lot of ""moving parts"". • It would be useful to have an experiment on an additional dataset, maybe more controlled than ImageCLEF, but less artificial than shifted-MNIST. • What were the ranges used for hyper-param search? What was the search protocol?
Clarity: • The parts describing the method are hard to follow, it will be useful to improve their clarity. • It will be beneficial to explicitly state which are the learned parametrized distributions, and how inference is applied with them. • What makes the VAE inference mappings (x->s,v) stable to domain shift? E.g. [1] showed that correlated latent properties in VAEs are not robust to such domain shifts. • What makes v distinctive of s? Is it because y only depends on s? • Does the approach uses any information on the labels of the domain?
Correctness: I was not convinced about the causal relation s->y. I.e. that the semantic concept cause the label, independently of the image. I do agree that there is a semantic concept (e.g. s) that cause the image. But then, as explained by [Arjovsky 2019] the labelling process is caused by the image. I.e. s->image->y, and not as argued by the paper. The way I see it, is like a communication channel: y_tx -> s -> image -> y_rx. Could the authors elaborate how the model will change if replacing s->y by y_tx->s ?
Other comments: • I suggest discussing [2,3,4], which learned similar stable mechanisms in images. • I am not sure about the statement that this work is the ""first to identify the semantic factor and leverage causal invariance for OOD prediction"" e.g. see [3,4] • The title may be confusing. OOD usually refers to anomaly-detection, while this paper relates to domain-generalization and domain-adaptation. • It will be useful to clarify that the approach doesn't use any external-semantic-knowledge. • Section 3.2 - I suggest to add a first sentence to introduce what this section is about. • About remark in page 6: (1) what is a deterministic s-v relation? (2) chairs can also appear in a workspace, and it may help to disentangle the desks from workspaces.
[1] Suter et al. 2018, Robustly Disentangled Causal Mechanisms: Validating Deep Representations for Interventional Robustness [2] Besserve et al. 2020, Counterfactuals uncover the modular structure of deep generative models [3] Heinze-Deml et al. 2017, Conditional Variance Penalties and Domain Shift Robustness [4] Atzmon et al. 2020, A causal view of compositional zero-shot recognition
EDIT: Post rebuttal
I thank the authors for their reply. Although the authors answered most of my questions, I decided to keep the score as is, because I share similar concerns with R2 about the presentation, and because experiments are still lacking.
Additionally, I am concerned with one of the author's replies saying All methods achieve accuracy 1 ... on the training distribution, because usually there is a trade-off between accuracy on the observational distribution versus the shifted distribution (discussed by Rothenhäusler, 2018 [Anchor regression]): Achieving perfect accuracy on the observational distribution, usually means relying on the spurious correlations. And under domain-shift scenarios, this would hinder the performance on the shifted-distribution.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,
"2) Lacking detail in experiment description: Description of experimental details would significantly benefit from increased clarity to allow the user to better judge the results, which is very difficult in the manuscript’s current state; See ""Questions"" for further details.",K8Mbkn9c4Q,ICLR_2024,"While I do like the general underlying idea, there are several severe weaknesses present in this work – leading me to lean towards rejection of the manuscript in its current form. The two main areas of concern are briefly listed here, with details explained in the ‘Questions’ part:
### 1) Lacking quality of the “Domain Transformation” part
This is arguably the KEY part of the paper, and needs significant improvement in two points: Underlying intuition/motivation/justification, as well as technical correctness and clarity. There are several fundamental points that are unclear to me and require significant improvement and clarification; This applies to both clarity in terms of writing but, more importantly, to the quality of the approach and justifications/underlying motivations.
Please see the “Questions” part for details.
### 2) Lacking detail in experiment description:
Description of experimental details would significantly benefit from increased clarity to allow the user to better judge the results, which is very difficult in the manuscript’s current state; See ""Questions"" for further details.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1) “expected”. Therefore, this paper should provide more explanation to make it clear. One of the main contributions of this paper is the CBR, so different optimization strategies and the corresponding results should discussion. For example, what will happen by minimizing both of the inter and intra terms in Eq 3 or only minimizing the first term?",ICLR_2022_3204,ICLR_2022,"- It is unclear whether CBR works as expected (i.e., align the distribution of intra-camera and inter-camera distance). Intuitively, there are more than one possible changing directions of the two items in Equ 3. For example, 1) the second term gets larger, the first term gets smaller (as shown in Fig.2), 2) both of them get smaller, 3）the second term stays the same, and the first term gets smaller. However, according to Fig.4 (b) and Fig.5 (b), we can observe that the changes of the distance distribution caused by CBR should be in line with 2) and 3) mentioned above rather than 1) “expected”. Therefore, this paper should provide more explanation to make it clear.
One of the main contributions of this paper is the CBR, so different optimization strategies and the corresponding results should discussion. For example, what will happen by minimizing both of the inter and intra terms in Eq 3 or only minimizing the first term?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper.,NIPS_2021_815,NIPS_2021,"- In my opinion, the paper is a bit hard to follow. Although this is expected when discussing more involved concepts, I think it would be beneficial for the exposition of the manuscript and in order to reach a larger audience, to try to make it more didactic. Some suggestions: - A visualization showing a counting of homomorphisms vs subgraph isomorphism counting. - It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper. - The authors define rooted patterns (in a similar way to the orbit counting in GSN), but do not elaborate on why it is important for the patterns to be rooted, neither how they choose the roots. A brief discussion is expected, or if non-rooted patterns are sufficient, it might be better for the sake of exposition to discuss this case only in the supplementary material. - The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 “Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets”), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes. - Comparison with GSN: The authors mention in section 2 that F-MPNNs are a unifying framework that includes GSNs. In my perspective, given that GSN is a quite similar framework to this work, this is an important claim that should be more formally stated. In particular, as shown by Curticapean et al., 2017, in order to obtain isomorphism counts of a pattern P, one needs not only to compute P-homomorphisms, but also those of the graphs that arise when doing “non-edge contractions” (the spasm of P). Hence a spasm(P)-MPNN would require one extra layer to simulate a P-GSN. I think formally stating this will give the interested reader intuition on the expressive power of GSNs, albeit not an exact characterisation (we can only say that P-GSN is at most as powerful as a spasm(P)-MPNN but we cannot exactly characterise it; is that correct?) - Also, since the concept of homomorphisms is not entirely new in graph ML, a more elaborate comparison with the paper by NT and Maehara, “Graph Homomorphism Convolution”, ICML’20 would be beneficial. This paper can be perceived as the kernel analogue to F-MPNNs. Moreover, in this paper, a universality result is provided, which might turn out to be beneficial for the authors as well.
Additional comments:
I think that something is missing from Proposition 3. In particular, if I understood correctly the proof is based on the fact that we can always construct a counterexample such that F-MPNNs will not be equally strong to 2-WL (which by the way is a stronger claim). However, if the graphs are of bounded size, a counterexample is not guaranteed to exist (this would imply that the reconstruction conjecture is false). Maybe it would help to mention in Proposition 3 that graphs are of unbounded size?
Moreover, there is a detail in the proof of Proposition 3 that I am not sure that it’s that obvious. I understand why the subgraph counts of C m + 1
are unequal between the two compared graphs, but I am not sure why this is also true for homomorphism counts.
Theorem 3: The definition of the core of a graph is unclear to me (e.g., what if P contains cliques of multiple sizes?)
In the appendix, the authors mention they used 16 layers for their dataset. That is an unusually large number of layers for GNNs. Could the authors comment on this choice?
In the same context as above, the experiments on the ZINC benchmark are usually performed with either ~100K or 500K parameters. Although I doubt that changing the number of parameters will lead to a dramatic change in performance, I suggest that the authors repeat their experiments, simply for consistency with the baselines.
The method of Bouritsas et al., arxiv’20 is called “Graph Substructure Networks” (instead of “Structure”). I encourage the authors to correct this.
After rebuttal
The authors have adequately addressed all my concerns. Enhancing MPNNs with structural features is a family of well-performing techniques that have recently gained traction. This paper introduces a unifying framework, in the context of which many open theoretical questions can be answered, hence significantly improving our understanding. Therefore, I will keep my initial recommendation and vote for acceptance. Please see my comment below for my final suggestions which, along with some improvements on the presentation, I hope will increase the impact of the paper.
Limitations: The limitations are clearly stated in section 1, by mainly referring to the fact that the patterns need to be selected by hand. I would also add a discussion on the computational complexity of homomorphism counting.
Negative societal impact: A satisfactory discussion is included in the end of the experimental section.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions).",NIPS_2018_38,NIPS_2018,"Weakness: 1. As I just mentioned, the paper only analyzed, under which cases will the Algorithm 1 converges to permutations as local minima. However, it will be better if the quality of this kind of local minima could be analyzed (e.g. the approximation ratio of these local minima, under certain assumptions). 2. This paper is not very easy to follow. First, many definitions are used before they are defined. For example, on line 59 in Theorem 1, the authors used the definition of ""conditionally positive definite function of order 1"", which is defined on line 126 in Definition 2. Also, the author used the definition ""\epsilon-negative definite"" on line 162, which is defined on line 177 in Definition 3. It will be better if the authors could define those important concepts before using them. Second, the introduction is a little bit too long (more than 2.5 pages) and many parts of that are repeated in Section 2 and 3. It might be better to restructure the first 3 sections for a little bit. Third, it will be good if more captions could be added to the figures in the experiment section so that the readers could understand the results more easily. 3. For the description of Theorem 3, from the proof it seems that we need to use Equation 12 as a condition. It will be better if this information is included in the theorem description to avoid confusions (though I know this is mentioned right before the theorem, it is still better to have it in the theorem description). Also, for the constant c_1, it is better to give it an explicit formula in the theorem. Reference: [1] Burkard, R. E., Cela, E., Pardalos, P. M., and Pitsoulis, L. S. (1998). The quadratic assignment problem. In Handbook of combinatorial optimization, pages 1713â1809. Springer.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The paper is not self contained Understandable given the NIPS format, but the supplementary is necessary to understand large parts of the main paper and allow reproducibility. I also hereby request the authors to release the source code of their experiments to allow reproduction of their results.",NIPS_2017_349,NIPS_2017,"- The paper is not self contained
Understandable given the NIPS format, but the supplementary is necessary to understand large parts of the main paper and allow reproducibility.
I also hereby request the authors to release the source code of their experiments to allow reproduction of their results.
- Use of deep-reinforcement learning is not well motivated
The problem domain seems simple enough that a linear approximation would have likely sufficed? The network is fairly small and isn't ""deep"" either.
- > We argue that such a mechanism is more realistic because it has an effect within the game itself, not just on the scores
This is probably the most unclear part. It's not clear to me why the paper considers one to be more realistic than the other rather than just modeling different incentives? Probably not enough space in the paper but actual comparison of learning dynamics when the opportunity costs are modeled as penalties instead. As economists say: incentives matter. However, if the intention was to explicitly avoid such explicit incentives, as they _would_ affect the model-free reinforcement learning algorithm, then those reasons should be clearly stated.
- Unclear whether bringing connections to human cognition makes sense
As the authors themselves state that the problem is fairly reductionist and does not allow for mechanisms like bargaining and negotiation that humans use, it's unclear what the authors mean by ``Perhaps the interaction between cognitively basic adaptation mechanisms and the structure of the CPR itself has more of an effect on whether self-organization will fail or succeed than previously appreciated.'' It would be fairly surprising if any behavioral economist trying to study this problem would ignore either of these things and needs more citation for comparison against ""previously appreciated"".
* Minor comments
** Line 16:
> [18] found them...
Consider using \citeauthor{} ?
** Line 167:
> be the N -th agentâs
should be i-th agent?
** Figure 3:
Clarify what the `fillcolor` implies and how many runs were the results averaged over?
** Figure 4:
Is not self contained and refers to Fig. 6 which is in the supplementary. The figure is understandably large and hard to fit in the main paper, but at least consider clarifying that it's in the supplementary (as you have clarified for other figures from the supplementary mentioned in the main paper).
** Figure 5:
- Consider increasing the axes margins? Markers at 0 and 12 are cut off.
- Increase space between the main caption and sub-caption.
** Line 299:
From Fig 5b, it's not clear that |R|=7 is the maximum. To my eyes, 6 seems higher.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"• How is the information redundancy built into the Fill, Propagate, Decode algorithms? o In reference to the sentence “ Finally, by comparing the performance of the secret model with or without fusion, we conclude that the robustness of Cans largely comes from the information redundancy implemented in our design of the weight pool”",NIPS_2022_1664,NIPS_2022,"• The paper is missing an integration of the main algorithmic steps (Fill, Propagate, Decode) with the overarching flow diagram in Fig 1 which creates a gap in the presentation.
• The abstract and main text make inconsistent claims about the transmission capacity: o Abstract: “.. covertly transmit over 10000 real-world data samples within a carrier model which has 220× less parameters than the total size of the stolen data,” o Introduction: “… covertly transmit over 10000 real-world data samples within a carrier model which has 100× less parameters than the total size of the stolen data (§4.1),”
• Definitions of metrics and illustrations of qualitative results are insufficiently described and included. o For example, the equation for a earning objective in section 3.3 should be clearly described.
o Page 7: define performance difference and hiding capacity in equations. o Fig 3 is too small for the information to be conveyed (At the 200% digital magnification of Fig 3, I can see some differences in image qualities).
• The choices and constructions of a secret key and noisy vectors are insufficiently described i.e., Are the secret keys similar to the public-private keys used in the current cryptography applications? What are the requirements on creating the noisy vectors?
• How is the information redundancy built into the Fill, Propagate, Decode algorithms? o In reference to the sentence “ Finally, by comparing the performance of the secret model with or without fusion, we conclude that the robustness of Cans largely comes from the information redundancy implemented in our design of the weight pool”","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not ablated. How important is the added complexity? Will one IN do?,NIPS_2017_434,NIPS_2017,"---
This paper is very clean, so I mainly have nits to pick and suggestions for material that would be interesting to see. In roughly decreasing order of importance:
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not
ablated. How important is the added complexity? Will one IN do?
2. Section 4.2: To what extent should long term rollouts be predictable? After a certain amount of time it seems MSE becomes meaningless because too many small errors have accumulated. This is a subtle point that could mislead readers who see relatively large MSEs in figure 4, so perhaps a discussion should be added in section 4.2.
3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder.
While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated.
Why is this particular dimension of difficulty interesting?
4. line 232: This hypothesis could be specified a bit more clearly. How do noisy rollouts contribute to lower rollout error?
5. Are the learned object state embeddings interpretable in any way before decoding?
6. It may be beneficial to spend more time discussing model limitations and other dimensions of generalization. Some suggestions:
* The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).
* How many different kinds of physical interaction can be in one simulation?
* How sensitive is the visual encoder to shorter/longer sequence lengths? Does the model deal well with different frame rates?
Preliminary Evaluation ---
Clear accept. The only thing which I feel is really missing is the first point in the weaknesses section, but its lack would not merit rejection.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- In particular, in the experiments, it doesn't come as a complete surprise that the opponent can be outperformed w.r.t. the multi-agent payoff proposed by the authors, because the opponent simply doesn't aim at maximizing it (e.g. in the experiments he maximizes classical SE and AE).",NIPS_2017_143,NIPS_2017,"For me the main issue with this paper is that the relevance of the *specific* problem that they study -- maximizing the ""best response"" payoff (l127) on test data -- remains unclear. I don't see a substantial motivation in terms of a link to settings (real or theoretical) that are relevant:
- In which real scenarios is the objective given by the adverserial prediction accuracy they propose, in contrast to classical prediction accuracy?
- In l32-45 they pretend to give a real example but for me this is too vague. I do see that in some scenarios the loss/objective they consider (high accuracy on majority) kind of makes sense. But I imagine that such losses already have been studied, without necessarily referring to ""strategic"" settings. In particular, how is this related to robust statistics, Huber loss, precision, recall, etc.?
- In l50 they claim that ""pershaps even in most [...] practical scenarios"" predicting accurate on the majority is most important. I contradict: in many areas with safety issues such as robotics and self-driving cars (generally: control), the models are allowed to have small errors, but by no means may have large errors (imagine a self-driving car to significantly overestimate the distance to the next car in 1% of the situations).
Related to this, in my view they fall short of what they claim as their contribution in the introduction and in l79-87:
- Generally, this seems like only a very first step towards real strategic settings: in light of what they claim (""strategic predictions"", l28), their setting is only partially strategic/game theoretic as the opponent doesn't behave strategically (i.e., take into account the other strategic player).
- In particular, in the experiments, it doesn't come as a complete surprise that the opponent can be outperformed w.r.t. the multi-agent payoff proposed by the authors, because the opponent simply doesn't aim at maximizing it (e.g. in the experiments he maximizes classical SE and AE).
- Related to this, in the experiments it would be interesting to see the comparison of the classical squared/absolute error on the test set as well (since this is what LSE claims to optimize).
- I agree that ""prediction is not done in isolation"", but I don't see the ""main"" contribution of showing that the ""task of prediction may have strategic aspects"" yet. REMARKS:
What's ""true"" payoff in Table 1? I would have expected to see the test set payoff in that column. Or is it the population (complete sample) empirical payoff?
Have you looked into the work by Vapnik about teaching a learner with side information? This looks a bit similar as having your discrapency p alongside x,y.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.",NIPS_2021_998,NIPS_2021,"• Unprofessional writing. - Most starkly, “policies” is misspelled in the title. • At times, information is not given in an easy-to-understand way. - E.G. lines 147 - 152, 284 - 289. • Captions of figures do not help elucidate what is going on in the figure. This problem is mitigated by the quality of the figures, but it still makes it much harder to understand the pipeline of LCP and its components. More emphasis on that pipeline would help with the understanding. • 100 nodes seem like a small maximum test size for TSP problems (though this is an educated guess). Many real-world problems have thousands or tens of thousands of nodes. • Increase in optimality is either not very significant, or not presented to highlight its significance. It would be better to put the improvement into perspective. • Blank spaces in table 1 are unclear. Opportunities:
• It would be good to describe why certain choices were made. For example, why is the REINFORCE algorithm used for training versus something like PPO? I presume it has to do with the attention model paper this one iterates on, but clarification would be good.
• More real-world uses of the algorithm could be included to better understand the societal impact, including details on how LCP could be integrated well.
The paper lacks a high degree of polish and professionalism, but its formatting (e.g. bolded inline subsubsections) and figures are its saving grace. The tables are also well structured, if a bit cluttered --- values are small and bolding is indistinct. This paper does a good job of giving this information and promises open source-code on publication.
Overall, the paper and its presentation have several problems, but the idea seems elegant and useful.
Yes. The authors have adequately addressed the limitations and potential negative societal impact of their work","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. I am confused about the statement in Theorem 5.1, as it might indicate some disadvantage of MMD DRO, as it provides a more conservative upper bound than the variance regularized problem.",NIPS_2019_819,NIPS_2019,"Weakness: Due to the intractbility of the MMD DRO problem, the submission did not find an exact reformulation as much other literature in DRO did for other probability metrics. Instead, the author provides several layers of approximation. The reason why I emphasize the importance of a tight bound, if not an exact reformulation, is that one of the major criticism about (distributionally) robust optimization is that it is sometimes too conservative, and thus a loose upper bound might not be sufficient to mitigate the over-conservativeness and demonstrate the power of distributionally robust optimization. When a new distance is introduced into the DRO framework, a natural question is why it should be used compared with other existing approaches. I hope there will be a more fair comparision in the camera-ready version. =============== 1. The study of MMD DRO is mostly motivated by the poor out-of-sample performance of existing phi-divergence and Wasserstein uncertainty sets. However, I don't believe this is indeed the case. For example, Namkoong and Duchi (2016), and Blanchet, Kang, and Murthy (2016) show the dimension-independent bound 1/\sqrt{n} for a broad class of objective functions in the case of phi-divergence and Wasserstein metric respectively. They didn't require the population distribution to be within the uncertainty set, and in fact, such a requirement is way too conservative and it is exactly what they wanted to avoid. 2. Unlike phi-divergence or Wasserstein uncertainty sets, MMD DRO seems not enjoy a tractable exact equivalent reformulation, which seems to be a severe drawback to me. The upper bound provided in Theorem 3.1 is crude especially because it drops the nonnegative constraint on the distribution, and further approximation is still needed even applied to a simple kernel ridge regression problem. Moreover, it seems restrictive to assume the loss \ell_f belongs to the RKHS as already pointed out by the authors. 3. I am confused about the statement in Theorem 5.1, as it might indicate some disadvantage of MMD DRO, as it provides a more conservative upper bound than the variance regularized problem. 4. Given the intractability of the MMD DRO and several layers of approximation, the numerical experiment in Section 6 is insufficient to demonstrate the usefulness of the new framework. References: Namkoong, H. and Duchi, J.C., 2017. Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980). Blanchet, J., Kang, Y. and Murthy, K., 2016. Robust wasserstein profile inference and applications to machine learning. arXiv preprint arXiv:1610.05627.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn’t provide insight into this but assumed morphologic segmentation will be invariant.",ICLR_2021_2929,ICLR_2021,"Weakness: The major concern is the limited contribution of this work. 1.Using image-to-image translation to unify the representations across-domain is an existing technique in domain adaptation, especially in segmentation tasks [1,2]. 2. The use of morphologic information in this paper is simple as the combination of edge detection and segmentation, which are both employed as tools from existing benchmarks (in this paper the author used DeeplabV3, DexiNed-f, employed as off-the-shelf tools for image pre-processing purpose as mentioned in section 4). 3.There should be more on how to use the morphologic segmentation across-domain, and how morphologic segmentation should be conducted differently for different domains. Or is it exactly the same given any arbitrary domain? These questions are important given the task domain adaptation. This paper didn’t provide insight into this but assumed morphologic segmentation will be invariant. 4. Results compared to other domain adaptation methods (especially generative methods) are missing. There is an obvious lack of evidence that the proposed method is superior.
In brief, the contribution of this paper is limited, the results provided are not sufficient to support the method being effective. A reject.
[1] Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation [2] Image to Image Translation for Domain Adaptation","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1) As mentioned in Sec. 4.2, the mixup technique in LUMP is also adopted for the proposed method in the experiments on SplitCIFAR-100 and SplitTiny-ImageNet, there should be experimental results of excluding such mixup technique from the proposed method in order to demonstrate its pure contribution;",ICLR_2023_4834,ICLR_2023,"There are some concerns regarding the method description and designs: 1) As described in the ASR update strategy, the rehearsal samples for previous tasks are based on the samples with high and low AS scores (the samples with middle AS scores are discarded) while for the current task the rehearsal samples are uniformly sampled from the corresponding data stream sorted by AS scores. Such difference between previous tasks and current task should be experimentally verified (for instance, why can't the current task follow the same principle as the previous tasks to select the rehearsal samples); 2) In line 5 of Algo.1, should it be noted that B^i_{t-}1 is already sorted according to the AS?
There are other concerns regarding the experimental settings and results: 1) As mentioned in Sec. 4.2, the mixup technique in LUMP is also adopted for the proposed method in the experiments on SplitCIFAR-100 and SplitTiny-ImageNet, there should be experimental results of excluding such mixup technique from the proposed method in order to demonstrate its pure contribution; 2) In order to better demonstrate the contribution of using ASR to update the reply buffer and its generalizabilty, there should be experiments of replacing the rehearsal buffer update strategy in the related works (for both supervised continual learning and continual self-supervised learning baselines that also adopt rehearsal buffer) by the proposed ASR update strategy.
The idea behind ""augmentation stability of each sample is positively correlated with its relative position in corresponding category distribution"" is not well proven or verified. Though Fig.1 tries to serve such purpose to show such idea, but it is not enough, can the authors provide more solid discussion or even theoretical proof for such idea if it is possible? Moreover, currently there is a hidden assumption that the distribution of each category is single mode, but how if the category distribution is multi-modal (in which it is very likely to happen in more complicated datasets), will AS still be effective as a proxy to the relative positive in a category distribution?
From my own research experience, for supervised continual learning, different strategies of rehearsal example selection (e.g. random or uniform) do not contribute significant difference to the final performance, can authors provide more discussion on the impact of particularly having both representative and discriminative rehearsal samples to the overall performance?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?,NIPS_2017_53,NIPS_2017,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
* Significance The broad problem addressed here is of the utmost importance. I believe the popularity of (IF) and modularity of using preprocessing to address fairness means the present paper is likely to be used or built upon.,NIPS_2017_382,NIPS_2017,"weakness that there is much tuning and other specifics of the implementation that need to be determined on a case by case basis. It could be improved by giving some discussion of guidelines, principles, or references to other work explaining how tuning can be done, and some acknowledgement that the meaning of fairness may change dramatically depending on that tuning.
* Clarity
The paper is well organized and explained. It could be improved by some acknowledgement that there are a number of other (competing, often contradictory) definitions of fairness, and that the two appearing as constraints in the present work can in fact be contradictory in such a way that the optimization problem may be infeasible for some values of the tuning parameters.
* Originality
The most closely related work of Zemel et al. (2013) is referenced, the present paper explains how it is different, and gives comparisons in simulations. It could be improved by making these comparisons more systematic with respect to the tuning of each method--i.e. compare the best performance of each.
* Significance
The broad problem addressed here is of the utmost importance. I believe the popularity of (IF) and modularity of using preprocessing to address fairness means the present paper is likely to be used or built upon.","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
4. Missing discussion about Set Transformer (https://arxiv.org/abs/1810.00825) and other related works that also uses summary tokens.,qYwdyvvvqQ,ICLR_2024,"1. Figure 1 does not convey the main idea clearly and should be significantly improved.
2. The presentation of the proposed method in 3.2 is confusing and should be significantly improved. For example, it would be better to have a small roadmap on the beginning of 3.2 so that the readers know what each step is doing. Also, it would be better to break up the page-long paragraph to smaller paragraphs, and use each paragraph to explain a small part of computation. Also, explain the intention of each equation and the reasons of design choice.
3. The related work only discussed sparse efficient attentions and Reformer and SMYRF that related to the proposed idea. There are a lot more efficient attentions that have the property of “information flow throughout the entire input sequence” (which was one of the motivation for the proposed idea), such as low rank based attentions (Linformer https://arxiv.org/abs/2006.04768, Nystromformer https://arxiv.org/abs/2102.03902, Performer https://arxiv.org/abs/2009.14794, RFA https://arxiv.org/abs/2103.02143) or multi-resolution based attentions (H-Transformer https://arxiv.org/abs/2107.11906, MRA Attention https://arxiv.org/abs/2207.10284).
4. Missing discussion about Set Transformer (https://arxiv.org/abs/1810.00825) and other related works that also uses summary tokens.
5. In 4-th paragraph of related work, the authors claim some baselines are unstable and the proposed method is stable, but the claim is not supported by any experiments.
6. Experiments are only performed on LRA benchmark, which consists a set of small datasets. The results might be difficult to generalize to larger scale experiments. It would be better to evaluate the methods on larger scale datasets, such as language modeling or at least ImageNet.
7. Given that LRA benchmark is a small scale experiment, it would be better to run the experiment multiple times and calculate the error bars since the results could be very noisy.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1. How Fourier features accelerate NTK convergence in the high-frequency range? Did I overlook something or it's not analyzed? This is an essential theoretical support to the merits of Fourier features.,NIPS_2020_631,NIPS_2020,"1. How Fourier features accelerate NTK convergence in the high-frequency range? Did I overlook something or it's not analyzed? This is an essential theoretical support to the merits of Fourier features. 2. The theory part is limited to the behavior on NTK. I understand analyzing Fourier features on MLPs is highly difficult, but I'm a bit worried there would be a significant gap between NTK and the actual behavior of MLPs (although they are asymptotically equivalent). 3. Examples in Section 5 are limited to 1D functions, which are a bit toyish.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2. I can't find details on how they make the network fit the residual instead of directly learning the input - output mapping.,NIPS_2016_314,NIPS_2016,"I found in the paper includes: 1. The paper mentions that their model can work well for a variety of image noise, but they show results only on images corrupted using Gaussian noise. Is there any particular reason for the same? 2. I can't find details on how they make the network fit the residual instead of directly learning the input - output mapping. - Is it through the use of skip connections? If so, this argument would make more sense if the skip connections exist after every layer (not every 2 layers) 3. It would have been nice if there was an ablation study on what plays the most important factor on the improvement in performance. Whether it is the number of layers or the skip connections, and how does the performance vary when the skip connections are used for every layer. 4. The paper says that almost all existing methods estimate the corruption level at first. There is a high possibility that the same is happening in the initial layers of their Residual net. If so, the only advantage is that theirs is end to end. 5. The authors mention in the Related works section that the use of regularization helps the problem of image- restoration, but they donât use any type of regularization in their proposed model. It would be great if the authors can address these points (mainly 1, 2 and 3) in the rebuttal.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.). [1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf",ICLR_2023_3449,ICLR_2023,"1.The spurious features in Section 3.1 and 3.2 are very similar to backdoor triggers. They both are some artificial patterns that only appear a few times in the training set. For example, Chen et al. (2017) use random noise patterns. Gu et al. (2019) [1] use single-pixel and simple patterns as triggers. It is well-known that a few training examples with such triggers (rare spurious examples in this paper) would have a large impact on the trained model.
2.How neural nets learn natural rare spurious correlations is unknown to the community (to the best of my knowledge). However, most of analysis and ablation studies use the artificial patterns instead of natural spurious correlations. Duplicating the same artificial pattern for multiple times is different from natural spurious features, which are complex and different in every example.
3.What’s the experiment setup in Section 3.3? (data augmentation methods, learning rate, etc.).
[1]: BadNets: Evaluating Backdooring Attacks on Deep Neural Networks. https://messlab.moyix.net/papers/badnets_ieeeaccess19.pdf","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"4. I looked over the original algorithm the authors used (in Section III of “Recognition and Velocity Computation of Large Moving Objects in Images”—RVC paper below—which I would recommend for the authors to cite), and I wonder if an error in the initial calibration steps (steps 1 & 2) occurred that might explain the speed disparities observed between the RSPs and FDs.",p4S5Z6Sah4,ICLR_2024,"Note, the below concerns have resulted in a lower score, which I would be happy to increase pending the authors’ responses.
**A. Wave fields**
The wave-field comparisons, claims, and references seem a bit strained and unnecessary. Presumably, by “wave-field,” the authors simply mean a vector field that supports wave solutions. In any case, since this term is not oft-used in neuroscience or ML that I am aware of, a brief definition should be provided if the term is kept. However, I am unsure that it is necessary or helpful. That the brain supports wavelike activity is well-established, and some evidence for this is appropriately outlined by the authors. Many computational neuroscience models support waves in a way that has been mathematically analyzed (e.g., Wilson-Cowan neural fields equations). The authors’ discretization methodology suggests a similar connection to such analyses. However, appealing to “physical wave fields” to relate waves and memory seems to be overly speculative and unnecessary for the simple system under study in this manuscript. The brain is a dissipative rather than a conservative system, so that many aspects of physical wave fields may well not apply. Moreover, the single reference the authors do make to the concept does not apply either to the brain or to their wave-RNN. Instead, Perrard et al. 2016 describe a specific study that demonstrates that a particle-pilot wave system can still maintain memory in a very specific way that does not at all clearly apply to brains or the authors’ RNN, despite that study studying a dissipative (and chaotic) system. Instead, the readers would benefit much more from gaining an intuition as to why such wavelike activity might benefit learning and recalling sequential inputs. Unfortunately, Fig. 1 does little to help in this vein.
However, the concept certainly is simple enough, and the authors provide a few intuitions in the manuscript that help. I believe the manuscript would improve by removing the discussion of wave fields and instead providing / moving the intuitive explanations (e.g., the “register or ‘tape’” description on p. 20) as to how waves may help with sequential tasks to the same portion of the Introduction.
**B. Fourier analysis**
Overall, I found the wave and Fourier analysis a bit inconsistent and potentially problematic. While I agree that the wRNNs clearly display waves when plotted directly, the mapping and analysis within the spatiotemporal Fourier domain (FD below) does not always match patterns in the regular spatiotemporal plots (RSP below). Moreover, it’s unclear how much substance they add to the analysis results. In more detail:
1. Constant-velocity, 1-D waves don’t need to be transformed to the FD to infer their speeds. The slopes in the RSP correspond to their speeds. For example, in Fig. 2 (top left), there is a wave that begins at unit 250, step ~400, that continues through to unit 0, step ~650, corresponding to a wave speed of ~1.7 units/step, far larger than the diagonal peak shown in the FD below it that would correspond to a speed of ~0.3 units/step, as indicated by the authors.
2. Similar, seemingly speed mismatches can be observed in the Appendix. E.g., in Fig. 9 (2nd column, top), the slopes of the waves are around 0.35-0.42 units/step (close enough to likely be considered the same speed, especially as they converge in time to form a more clustered wave pulse) from what I can tell, whereas the slopes in the FD below it are ~0.3 for the diagonal (perhaps this is close enough to my rough estimate) and ~0.9, well above any observable wave speed. Perhaps there is a much faster wave that is unobservable in the RSP due to the min/max values set for the image intensity in the plot, but in that case the authors should demonstrate this. Given (a) the potential mismatch in the speeds for the waves that can be observed, (b) the mismatch in the speeds discussed above in Fig. 2, and (c) the fact that some waves may be missed in FD (see below), I would worry about assuming this without checking.
3. As alluded to in the point above, iRNN in Fig. 2 appears to have some fast pulse bursts easily observed in the RSP that don’t show in the FD. For example, there is a very fast wave observable in the RSP in units ~175-180, time steps 0-350. Note, the resolution is poor, but zooming in and scrolling to where the wave begins around unit 175, step 0 makes it clear. If one scrolls vertically such that the bottom of the wave at step 0 is just barely unobservable, then one can see the wave rapidly come into view and continue downwards. Similarly some short-lasting, slower pulses in units near 190, steps 0-350 are observable in the RSP. None of these appear in the FD. Note, this would not take away from the claim that wRNNs facilitate wave activity much more than iRNNs do, but rather that some small amounts—likely insufficient amounts for facilitating sequence learning—of wave activity might still arise in iRNNs. If the authors believe these wavelike activities are aberrant, it would be helpful for them to explain why so.
4. I looked over the original algorithm the authors used (in Section III of “Recognition and Velocity Computation of Large Moving Objects in Images”—RVC paper below—which I would recommend for the authors to cite), and I wonder if an error in the initial calibration steps (steps 1 & 2) occurred that might explain the speed disparities observed between the RSPs and FDs.
5. There do seem to be some different wave speeds—e.g., in Fig. 9, there appear to be fast and narrow excitatory waves overlapping with slow and broad inhibitory waves. But given that each channel has its own wave speed parameter $\nu$, it isn’t clear why a single channel would support multiple wave speeds. This should be explored in greater depth, and if obvious examples of sufficiently different speeds of excitatory waves are known (putatively Fig. 9, 2nd column), these should be clearly shown and carefully described and analyzed.
6. Is there cross-talk across the channels? If so, have the authors examined the images of the hidden units (with dimensions __hidden units__ x __channels__) for evidence of cross-channel waves? If so, perhaps this is one reason for multiple wave speeds to exist per channel?
7. Overall, it is unclear overall what FT adds to the detection of 1-D waves. If there are such waves, we should be able to observe them directly in the RSPs. In skimming over the RVC paper, it seems like it would be most useful in determining velocities of 2-D objects and perhaps wave pulses. That suggests that one place the FD analysis might be useful is if there are cross-channel waves as I mention above. If so, the waves should still be observable in the images (and I would encourage such images be shown), but might be more easily characterized following the marginalization decomposition procedure described in the original algorithm in Section III of the RVC paper. Note, the FDs might also facilitate the detection of multiple wave speeds in the network, as potentially shown in Fig. 9. However, in that case it would seem they should only appear in Fig. 9, and if the speeds are otherwise verified.
8. The authors mention they re-sorted the iRNN units to look for otherwise hidden waves. This seems highly problematic. If there are waves, then re-sorting can destroy them, and if there is only random activity then re-sorting can cause them to look like waves.
**C. Mechanisms**
Finally, while the results are overall impressive, and hypotheses made regarding the underlying mechanisms for the performance levels of the network, there is too little analysis of the these mechanisms. While the ablation study is important and helpful, much more could be done to characterize the relationship between wavelike activity and network performance.
**D. Minor**
1. Fig. 2: Both plots on the right have the leftmost y-axis digits obscured
2. Fig. 9, top, plots appear to have their x- and y- labels transposed (or else the lower FD plots and those in Fig. 2 have theirs transposed.
3. Fig. 15 needs axis labels","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3) the artificial networks trained using ASAP (and similar methods) do not necessarily resemble biological networks (other than the weight transport problem, which is of arguable importance) more than other techniques like backprop. Again, I do not hold the authors accountable for this, and this does not affect the review I gave.",NIPS_2021_942,NIPS_2021,"The biggest real-world limitation is that the method does not perform as well as backprop. This is unfortunate, but also understandable. The authors do mention that ASAP has a lower memory footprint, but there are also other methods that can reduce memory footprints of neural networks trained using backprop. Given that this method is worse than backprop, and it is also not easy to implement, I cannot see any practical use for it.
On the theoretical side, although the ideas here are interesting, I take issue with the term ""biologically plausible"" and the appeal to biological networks. Given that cognitive neuroscience has not yet proceeded to a point where we understand how patterns and learning occur in brains, it is extremely premature to try and train networks that match biological neurons on the surface, and claim that we can expect better performance because they are biology inspired. To say that these networks behave more similarly to biological neurons is true only on the surface, and the claim that these networks should therefore be better or superior (in any metric, not just predictive performance) is completely unfounded (and in fact, we can see that the more ""inspiration"" we draw from biological neurons, the worse our artificial networks tend to be). In this particular case, humans can perform image classification nearly perfectly, and better than the best CNNs trained with backprop. And these CNNs trained with backprop do better than any networks trained using other methods (including ASAP). To clarify, I do not blame (or penalize) the authors for appealing to biological networks, since I think this is a bigger issue in the theoretical ML community as a whole, but I do implore them to soften the language and recognize the severe limitations that prevent us from claiming homology between artificial neural networks and biological neural networks (at least in this decade). I encourage the authors to explicitly clarify that: 1) biological neurons are not yet understood, so drawing inspiration from the little we know does not improve our chances at building better artificial networks; 2) the artificial networks trained using ASAP (and similar methods) do not improve our understanding of biological neurons at all; and 3) the artificial networks trained using ASAP (and similar methods) do not necessarily resemble biological networks (other than the weight transport problem, which is of arguable importance) more than other techniques like backprop. Again, I do not hold the authors accountable for this, and this does not affect the review I gave.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '0']}",,
"- Some of the numbers while comparing proposed method vs baselines seem to be pretty close. Wondering, if authors did any statistical significance test?",wcgfB88Slx,EMNLP_2023,"The following are the questions I have and these are not necessarily 'reasons to reject'.
- I was looking for a comparison with the zero-shot chain of thought baseline which authors refer as ZOT (Kojima et al., 2022). The example selection method has a cost. Also, few shot experiments involve extra token usage cost than zero shot.
- Some of the numbers while comparing proposed method vs baselines seem to be pretty close. Wondering, if authors did any statistical significance test?
- A parallel field to explanation selection is prompt/instruction engineering, where we often change the zeroshot instruction. Another alternative is prompt-tuning via gradient descent. Wondering if authors have any thoughts regarding the tradeoff.
- Few shot examples has various types of example biases such as majority bias, recency bias etc. (http://proceedings.mlr.press/v139/zhao21c/zhao21c.pdf, https://aclanthology.org/2023.eacl-main.130/, https://aclanthology.org/2022.acl-long.556.pdf). Wondering if authors have any thought on how the robustness look like with the application of their method?
I am looking forward to hear answers to these questions from the authors.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"* The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM?",NIPS_2016_321,NIPS_2016,"======================== Positive aspects: + The paper is well written and has a clear and coherent structure. The discussion of related work is comprehensive. + Non-parametric emission distributions add flexibility to the general HMM framework and reduce bias due to wrong modeling assumptions. Progress in this area should have theoretical and practical impact. + The paper builds upon existing spectral methods for parametric HMMs but introduces novel techniques to extend those approaches to the non-parametric case. + The theoretical bounds (section 5) are interesting, even though most of the results are special cases or straightforward extensions of known results. Negative aspects: - The restriction to triplets (or a sliding window of length 3) is quite limiting. Is this a fundamental limitation of the approach or is an extension to longer subsequences (without a sliding window) straightforward? - Since the paper mentions the possibility to use Chebyshev polynomials to achieve a speed-up, it would have been interesting to see a runtime comparison at test time. - The presentation is at times too equation-driven and the notation, especially in chapter 3, quite convoluted and hard to follow. An illustrative figure of the key concepts in section 3 would have been helpful. - The experimental evaluation compares the proposed approach to 4 other HMM baselines. Even though NP-SPEC-HMM outperforms those baselines, the experimental evaluation has only toy character (simple length 6 conditional distributions, only one training/test sequence in case of the real datasets). Minor points ============ * l.22: Only few parametric distributions allow for tractable exact inference in an HMM. * l.183: Much faster approximations than Chebyshev polynomials exist for the evaluation of kernel density estimates, especially in low-dimensional spaces (e.g., based on fast multipole methods). * Figure 1: There is probably a âx 10^3â missing in the plot on the bottom right. Questions ========= * Eq. (3): Why the restriction to an isotropic bandwidth and a product kernel? Especially a diagonal bandwidth matrix could have been helpful. Would the approximation with Chebyshev polynomials still work? * The paper focuses on learning HMMs with non-parametric emission distributions, but it does not become clear how those emission distributions affect inference. Which of the common inference tasks in a discrete HMM (filtering, smoothing, marginal observation likelihood) can be computed exactly/approximately with an NP-SPEC-HMM? * Is it computationally feasible to use the proposed model in a more realistic application, e.g., action recognition from motion capture sequences? Conclusion ========== The paper is well written and, from a theoretical perspective, interesting to read. However, the experiments are weak and it remains unclear how practical the proposed model would be in real applications. Iâm tending towards accept, but the authors should comment in their rebuttal on the above points.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The analysis of experimental results is insufficient. For instance, the authors only mention that the scope prompting method shows poor performance on GPT-3.5-turbo, but they do not provide any analysis of the underlying reasons behind this outcome.",vexCLJO7vo,EMNLP_2023,"1. This paper aims to evaluate the performance of current LLMs on different temporal factors and select three types of factors, including cope, order, and counterfactual. What is the rationale behind selecting these three types of factors, and how do they relate to each other?
2. More emphasis should be placed on prompt design. This paper introduces several prompting methods to address issues in MenatQA. Since different prompts may result in varying performance outcomes, it is essential to discuss how to design prompts effectively.
3. The analysis of experimental results is insufficient. For instance, the authors only mention that the scope prompting method shows poor performance on GPT-3.5-turbo, but they do not provide any analysis of the underlying reasons behind this outcome.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. You said only 10 out of 120 datasets are considered as in [7,12]. Why not compare batch and greedy in other 110 datasets?",NIPS_2018_101,NIPS_2018,"Weakness: The ideas of extension seem to be intuitive and not very novel (the authors seem to honestly admit this in the related work section when comparing this work with [3,8,9]). This seems to make the work a little bit incremental. In the experiments, Monte Carlo (batch-ENS) works pretty well consistently, but the authors do not provide intuitions or theoretical guarantees to explain the reasons. Questions: 1. In [12], they also show the results of GpiDAPH3 fingerprint. Why not also run the experiment here? 2. You said only 10 out of 120 datasets are considered as in [7,12]. Why not compare batch and greedy in other 110 datasets? 3. If you change the budget (T) in the drug dataset, does the performance decay curve still fits the conclusion of Theorem 1 well (like Figure 1(a))? 4. In the material science dataset, the pessimistic oracle seems not to work well. Why do your explanations in Section 5.2 not hold in the dataset? Suggestions: Instead of just saying that the drug dataset fits Theorem 1 well, it will be better to characterize the properties of datasets to which you can apply Theorem 1 and your analysis shows that this drug dataset satisfies the properties, which naturally implies Theorem 1 hold and demonstrate the practical value of Theorem 1. Minor suggestions: 1. Equation (1): Using X to denote the candidate of the next batch is confusing because it is usually used to represent the set of all training examples 2. In the drug dataset experiment, I cannot find how large the budget T is set 3. In section 5.2, the comparison of myopic vs. nonmyopic is not necessary. The comparison in drug dataset has been done at [12]. In supplmentary material 4. Table 1 and 2: why not also show results when batch size is 1 as you did in the drug dataset? 5. In the material science dataset experiment, I cannot find how large the budget T is set After rebuttal: Thanks for the explanation. It is nice to see the theorem roughly holds for the batch size part when different budgets are used. However, based on this new figure, the performance does not improve with the rate 1/log(T) as T increases. I suggest authors to replace Figure 1a with the figure in the rebuttal and address the possible reasons (or leave it as future work) of why the rate 1/log(T) is not applied here. There are no major issues found by other reviewers, so I changed my rate from tending to accept to accepting.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
3) I found the motivation in the introduction with the low-rank factorization unnecessary given that the main result is about polytopes. If the result has implications for low-rank matrix factorization I would like to see them explicitly discussed.,NIPS_2020_1584,NIPS_2020,"These are not weaknesses but rather questions. 1) Is there a general relation between the strict complementarity, F*, and the pyramidal width? I understand it in the case of the simplex, I wonder if something can be said in general. 2) It would be useful to discuss some practical applications (for example in sparse recovery) and the implication of the analysis to those. In general, I found the paper would be stronger if better positioned wrt particular practical applications. 3) I found the motivation in the introduction with the low-rank factorization unnecessary given that the main result is about polytopes. If the result has implications for low-rank matrix factorization I would like to see them explicitly discussed.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- can you explicitly say what labels for each dataset in 4.1 are - where are they coming from? from the dataset itself? from what i understand, thatâs easy for the generated datasets (and generated parts of the dataset), but what about cas-peal-r1 and mugshot?",NIPS_2018_544,NIPS_2018,"- the presented results do not give me the confidence to say that this approach is better than any of the other due to a lot of ad-hoc decisions in the paper (e.g. digital identity part of the code vs full code evaluation, the evaluation itself, and the choice of the knn classifier) - the results in table 1 are quite unusual - there is a big gap between the standard autoencoders and the variational methods which makes me ask whether thereâs something particular about the classifier used (knn) that is a better fit for autoencoders. the particularities of the loss or the distribution used when training. why was k-nn used? a logical choice would be a more powerful method like svm or a multilayer perceptron. there is no explanation for this big gap - there is no visual comparison of what some of the baseline methods produce as disentangled representations so itâs impossibly to compare the quality of (dis) entanglement and the semantics behind each factor of variation - the degrees of freedom among features of the code seem binary in this case, therefore it is important which version of vae and beta-vae, as well as infogan are used, but the paper does not provide those details - the method presented can easily be applied on unlabeled data only, and that should have been one point of comparison to the other methods dealing with unlabeled data only - showing whether it works on par with baselines when no labels are used, but that wasnât done. the only trace of that is in the figure 3, but that compares the dual and primary accuracy curves (for supervision of 0.0), but does not compare it with other methods - though parts of the paper are easy to understand, in whole it is difficult to get the necessary details of the model and the training procedure (without looking into the appendix which i admittedly did not do, but then, i want to understand the paper fully (without particular details like hyperparameters) from the main body of the paper). i think the paper would benefit from another writing iteration Questions: - are the features of the code binary? because i didnât find it clear from the paper. if so, then the effects of varying the single code feature is essentially a binary choice, right? did the baseline (beta-)vae and infogan methods use the appropriate distribution in that case? i cannot find this in the paper - is there a concrete reason why you decided to apply the model only in a single pass for labelled data, because you could have applied the dual-swap on labelled data too - how is this method applied at test time - one needs to supply two inputs? which ones? does it work when one supplies the same input for both? - 57 - multi dimension attribute encoding, does this essentially mean that the projection code is a matrix, instead of a vector, and thatâs it? - 60-62 - if the dimensions are not independent, then the disentanglement is not perfect - meaning there might be correlations between specific parts of the representation. did you measure/check for that? - can you explicitly say what labels for each dataset in 4.1 are - where are they coming from? from the dataset itself? from what i understand, thatâs easy for the generated datasets (and generated parts of the dataset), but what about cas-peal-r1 and mugshot? - i find the explanation in 243-245 very unclear. could you please elaborate what this exactly means - why is it 5*3 (and not 5*2, e.g. in the case of beta-vae where thereâs a mean and stdev in the code) - 247 - why was k-nn used, and not some other more elaborate classifier? what is the k, what is the distance metric used? - algorithm 1, ascending the gradient estimate? what is the training algorithm used? isnât this employing a version of gradient descent (minimising loss)? - what is the effect of the balance parameter? it is a seemingly important parameter, but there are no results showing a sweep of that parameter, just a choice between 0.5 and 1 (and why does 0.5 work better)? - did you try beta-vae with significantly higher values of beta (a sweep between 10 and 150 would do)? Other: - the notation (dash, double dash, dot, double dot over inputs is a bit unfortunate because itâs difficult to follow) - algorithm 1 is a bit too big to follow clearly, consider revising, and this is one of the points where itâs difficult to follow the notation clearly - figure 1, primary-stage I assume that f_\phi is as a matter of a fact two f_\phis with shared parameters. please split it otherwise the reader can think that f_\phi accepts 4 inputs (and in the dual-stage it accepts only 2) - figure 3 a and b are very messy / poorly designed - it is impossible to discern different lines in a because thereâs too many of them, plus itâs difficult to compare values among the ones which are visible (both in a and b). log scale might be a better choice. as for the overfitting in a, from the figure, printed version, i just cannot see that overfitting - 66 - is shared - 82-83 Also, require limited weakerâ¦sentence not clear/gramatically correct - 113 - one domain entity - 127 - is shared - 190 - conduct disentangled encodings? strange word choice - why do all citations in parentheses have a blank space between the opening parentheses and the name of the author? - 226 - despite the qualities of hybrid images are not exceptional - sentence not clear/correct - 268 - is that fig 2b or 2a? - please provide an informative caption in figure 2 (what each letter stands for) UPDATE: Iâve read the author rebuttal, as well as all the reviews again, and in light of a good reply, Iâm increasing my score. In short, I think the results look promising on dSprites and that my questions were well answered. I still feel i) the lack of clarity is apparent in the variable length issue as all reviewers pointed to that, and that the experiments cannot give a fair comparison to other methods, given that the said vector is not disentangled itself and could account for a higher accuracy, and iii) that the paper doesnât compare all the algorithms in an unsupervised setting (SR=0.0) where I wouldnât necessarily expect better performance than the other models.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.,NIPS_2017_217,NIPS_2017,"Weakness:
- The paper is rather incremental with respect to [31]. The authors adapt the existing architecture for the multi-person case producing identity/tag heatmaps with the joint heatmaps.
- Some explanations are unclear and rather vague. Especially, the solution for the multi-scale case (end of Section 3) and the pose refinement used in section 4.4 / table 4. This is important as most of the improvement with respect to state of the art methods seems to come from these 2 elements of the pipeline as indicated in Table 4. Comments:
The state-of-the-art performance in multi-person pose estimation is a strong point. However, I find that there is too little novelty in the paper with respect to the stacked hour glasses paper and that explanations are not always clear. What seems to be the key elements to outperform other competing methods, namely the scale-invariance aspect and the pose refinement stage, are not well explained.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '0']}",,
"2) It seems that performance and sample efficiency are sensitive to λ parameters. (Page 9, lines 310-313) I don't understand how the process of calculating the λ is done. How is λ computed from step here? (Page 8 lines 281-285) The authors explain why ELLA does not increase sample efficiency in a COMBO environment, but I don't quite understand what it means. [1] Yuri Burda et al, Exploration by Random Network Distillation, ICLR 2019 [2] Deepak Pathak et al, Curiosity-driven Exploration by Self-supervised Prediction, ICML 2017 [3] Roberta Raileanu et al, RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments, ICLR 2020",NIPS_2021_1078,NIPS_2021,"weakness of this paper, I am concerned with the following two points: 1) The assumption for termination states of instructions are quite strong. In the general case, it is very expensive to label a large number of data manually. 2) It seems that performance and sample efficiency are sensitive to λ parameters.
(Page 9, lines 310-313) I don't understand how the process of calculating the λ
is done. How is λ
computed from step here?
(Page 8 lines 281-285) The authors explain why ELLA does not increase sample efficiency in a COMBO environment, but I don't quite understand what it means.
[1] Yuri Burda et al, Exploration by Random Network Distillation, ICLR 2019
[2] Deepak Pathak et al, Curiosity-driven Exploration by Self-supervised Prediction, ICML 2017
[3] Roberta Raileanu et al, RIDE: Rewarding Impact-Driven Exploration for Procedurally-Generated Environments, ICLR 2020","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- It is briefly mentioned that an alternating direction method is used to solve the min-min problem. Which method?,NIPS_2018_45,NIPS_2018,"---------- The main section of the paper (section 3) seems carelessly written. Some obvious weaknesses: - Algorithm 1 seems more confusing, than clarifying: a) Shouldn't the gradient step be taken in the direction of the gradient of the loss with respect to Theta? b) There is no description of the variables, most importantly X and f. It is better for the reader to define them in the algorithm than later in the text. Otherwise, the algorithm definition seems unnecessary. - Equation (1) is very unclear: a) Is the purpose to define a loss function or the optimization problem? It seems that it is mixing both. b) The optimization variable x is defined to be in R^n. Probably it is meant to be in R^k? c) The constraints notation (s.t. C(A, x)) is rather unusual. - It is briefly mentioned that an alternating direction method is used to solve the min-min problem. Which method? - The constraints in equation (2) are identical to the ones in equation (3). They can be mentioned as such to gain space. - In section 4.1, line 194, K = 10, presumably refers to the number of atoms in the dictionary, namely it should be a small k? The same holds for section 4.4, line 285. - In section 4.1, why is the regularizer coefficient gamma set to zero? Intuitively, structured sparcity should be particularly helpful in finding keypoint correspondences. What is the effect on the solution when gamma is larger than zero? The experimental section of the paper seems well written (with a few exceptions, see above). Nevertheless, the experiments in 4.2 and 4.3 each compare to only one existing work. In general, I can see the idea of the paper has merit, but the carelessness in the formulations in the main part and the lack of comparisons to other works make me hesitant to accept it as is at NIPS.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2.The effectiveness of lower bound double q-learning is doubtful. In MsPacman of Figure2, the algorithm shows slight performance decrease of Clipped DDQN, in some environment such as WizardOfWor, Zaxxon RoadRunner and BattleZone, these algorithms seems converge into same solutions. Besides, the algorithm would cause the overestimate the true maximum value.",ICLR_2021_1916,ICLR_2021,"Weakness: 1.Some experiment is hard to understand. Table1 shows the TD-error and the absolute state-action value which didn’t demonstrate the small approximation error would cause significant estimation error which would cause the sub-optimal fixed points.
2.The effectiveness of lower bound double q-learning is doubtful. In MsPacman of Figure2, the algorithm shows slight performance decrease of Clipped DDQN, in some environment such as WizardOfWor, Zaxxon RoadRunner and BattleZone, these algorithms seems converge into same solutions. Besides, the algorithm would cause the overestimate the true maximum value.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '0']}",,
1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation.,ICLR_2022_3183,ICLR_2022,"Weakness: 1. The novelty is limited. Interpretating the prediction of deep neural networks using linear model is not a new approach for model interpretation. 2. The motivation of the paper is not clear. It seems an experiment report about ~193k models, and obtains the obvious results, such as the middle layers represent the more generalizable features. But it is not about interpretation. 3. The writing is not clear. it's hard to read and follow the work. 4.
Concerns: 1. Why need 101 “source” tasks? What are these? 2. 101 tasks can be done on the retinal image, it is not a unified domain to do the research about model interpretation. 3. The motivation and conclusion should be clearly presented to under the contribution of this paper for model interpretation.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
2. It is unclear for me why the performance of DNN+MMA becomes worse than vanilla DNN when lambda becomes small? See fig.3-4. I would expect it will approach vanilla methods from above but from below.,NIPS_2020_1602,NIPS_2020,There are some questions/concerns however. 1. Haven't you tried to set hyperparameters for the baseline models via cross-validation (i.e. the same method you used for your own model)? Setting it to their default values (even taken from other papers) may have a risk of unfair comparison aganist yours. I do not think this is the case but I would recommend the authors to carry out the corresponding experiments. 2. It is unclear for me why the performance of DNN+MMA becomes worse than vanilla DNN when lambda becomes small? See fig.3-4. I would expect it will approach vanilla methods from above but from below.,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)). Comments:",ARR_2022_121_review,ARR_2022,"1. The writing needs to be improved. Structurally, there should be a ""Related Work"" section which would inform the reader that this is where prior research has been done, as well as what differentiates the current work with earlier work. A clear separation between the ""Introduction"" and ""Related Work"" sections would certainly improve the readability of the paper.
2. The paper does not compare the results with some of the earlier research work from 2020. While the authors have explained their reasons for not doing so in the author response along the lines of ""Those systems are not state-of-the-art"", they have compared the results to a number of earlier systems with worse performances (Eg. Taghipour and Ng (2016)).
Comments: 1. Please keep a separate ""Related Work"" section. Currently ""Introduction"" section of the paper reads as 2-3 paragraphs of introduction, followed by 3 bullet points of related work and again a lot of introduction. I would suggest that you shift those 3 bullet points (""Traditional AES"", ""Deep Neural AES"" and ""Pre-training AES"") to the Related work section.
2. Would the use of feature engineering help in improving the performance? Uto et al. (2020)'s system reaches a QWK of 0.801 by using a set of hand-crafted features. Perhaps using Uto et al. (2020)'s same feature set could also improve the results of this work.
3. While the out of domain experiment is pre-trained on other prompts, it is still fine-tuned during training on the target prompt essays. Typos: 1. In Table #2, Row 10, the reference for R2BERT is Yang et al. (2020), not Yang et al. (2019).
Missing References: 1. Panitan Muangkammuen and Fumiyo Fukumoto. "" Multi-task Learning for Automated Essay Scoring with Sentiment Analysis"". 2020. In Proceedings of the AACL-IJCNLP 2020 Student Research Workshop.
2. Sandeep Mathias, Rudra Murthy, Diptesh Kanojia, Abhijit Mishra, Pushpak Bhattacharyya. 2020. Happy Are Those Who Grade without Seeing: A Multi-Task Learning Approach to Grade Essays Using Gaze Behaviour. In Proceedings of the 2020 AACL-IJCNLP Main Conference.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
3. The authors have reprt significane testing but I think the choice of test might be incorrect. Since the comparision is to be done between two samples generated from same input why not some paired test setting was used like wilcoxon signed ranked test?,wcqBfk4jv6,EMNLP_2023,"1. although the choice of models seems fine at first, but I am not sure as to how much of the citation information is actually being utilized. The maximum input size can only be 1024 and given the size of articles and the abstracts I am not sure how much inforamtion is being used in either. Can you comment on how much information is lost at token-level that is being fed to model.
2. I like the idea of aggregation using various cited articles. The only problem and I might be possibly confused as to how are you ensuring the quality of chosen articles. It could be so that the claims made in the article might also be contradicting to the claims made in cited articles or might not at all be related to the claims being discussed in the articles. Do you have any analysis on correlation between cited articles and the main articles, and whether it affects the quality of generation?
3. The authors have reprt significane testing but I think the choice of test might be incorrect. Since the comparision is to be done between two samples generated from same input why not some paired test setting was used like wilcoxon signed ranked test?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive.",NIPS_2022_1034,NIPS_2022,"Regarding the background: the authors should consider adding a preliminary section to introduce the background knowledge on the nonparametric kernel regression, kernel density estimation, and the generalized Fourier Integral theorem, which could help the readers easily follow the derivation of Section 2 and understand the motivation to use the Fourier Integral theorem as a guide to developing a new self-attention mechanism.
Regarding the experimental evaluation: the issues are three-fold. 1) since the authors provide an analysis of the approximation error between estimators and true functions (Theorem 1 and 2), it is informative to provide an empirical evaluation of these quantities on real data as further verification. 2) The experiments should be more comprehensive and general. For both the language modeling task and image classification task, the model size is limited and the baselines are restrictive. 3) Since the FourierFormer need customized operators for implementation, the authors should also provide the memory/time cost profiling compared to popular Transformer architectures. Based on these issues, the efficiency and effectiveness of the FourierFormer are doubtful.
-------After Rebuttal------- Thank authors for the detailed response. Most of my concerns have been addressed. I have updated my scores to 6.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Line 124-125: For any w, the Hoeffding's bound holds true as long as the samples are drawn independently and so it is always possible to show inequality (2). Stochastic algorithms moreover impose conditioning on the previous iterate further guaranteeing that Hoeffding inequality holds. It will be great if the authors can elaborate on this.",NIPS_2020_68,NIPS_2020,"1. It is unclear how guaranteeing stationary points that have small gradient norms translates to good generalization. The bounds just indicate that these algorithms reach one of the many stationary points for adaptive gradient methods and don't talk about how reaching one of the potentially many population stationary points especially in the non-convex regime can translate to good generalization. A remark on this would be helpful. 2. Line 124-125: For any w, the Hoeffding's bound holds true as long as the samples are drawn independently and so it is always possible to show inequality (2). Stochastic algorithms moreover impose conditioning on the previous iterate further guaranteeing that Hoeffding inequality holds. It will be great if the authors can elaborate on this. 3. The bounds in Theorem 1 have a dependence on d, which the authors have discussed. However, if \mu is small, the bounds are moot. If \mu is large, then the concentration guarantees are not very useful. Based on values in Theorem 2, latter seems to be the case. 4. It seems weird that the bounds in Theorems 2 and 4 do not depend on the initialization w_0 but on w_1. 5. For experiments on Penn-Tree bank, it seems that the algorithms are not stable with respect to train perplexity.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
4: Is it possible to add some optimization based meta-learning approach in the Table-1? like MAML/implicit-MAML?,ICLR_2023_3923,ICLR_2023,"Weakness:
1: The paper focus over the metric learning approach of meta-learning, what about the optimization-based model e.g. MAML [a], Implicit-MAML [b] etc?. How the model will behave over these approaches? Does the same analysis be correct for other meta-learning model?
2: The base model is implemented based on ProtoNet (Deleu et al., 2019) with default parameters given by (Medina et al., 2020). How we can ensure there hyperparameters are optimal?
3: For the domain invariant auxiliary learning, “treat each image as its own class to form the support” how it is a valid labelling? It may be a wrong class assignment, then the result may be worse than using this, but the model works. I cannot understand the proper intuition, could you please provide the detail intuition?
4: Is it possible to add some optimization based meta-learning approach in the Table-1? like MAML/implicit-MAML?
5: What is the size of auxiliary data?
[a] Model-Agnostic Meta-Learning for Fast Adaptation of Deep Networks
[b] Meta-Learning with Implicit Gradients","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '1', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"* The paper lacks an ablation study explaining why they chose the prompt in this specific way, e.g., few-shot examples for CoT might improve performance.",FXObwPWgUc,EMNLP_2023,"* The paper could have provided a clearer use case for why the NMT model is still necessary. When powerful and large general language models are used for post-editing, why should one use a specialized neural machine translation model to get the translations in the first place? Including GPT-4 translations plus GPT-4 post-editing scores would have been insightful.
* The paper lacks an ablation study explaining why they chose the prompt in this specific way, e.g., few-shot examples for CoT might improve performance.
* The reliance on an external model via API, where it's unclear how the underlying model changes, makes it hard to reproduce the results. There is also a risk of data pollution since translations might already be in the training data of GPT-4. The authors only state that the WMT-22 test data is after the cutoff date of the GPT-4 training data, but they do not say anything about the WMT-20 and WMT-21 datasets that they also use.
* The nature of post-edited translation experiments is only partially done: En-Zh and Zh-En for GPT-4 but not for En-De and De-En for GPT-3.5.
* The title is misleading since the authors also evaluate GPT-3.5.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Page 1, ""causal mechanisms"", causality is different from temporal relationship. Please use the terms carefully.",ICLR_2022_1267,ICLR_2022,"Weakness
1. The proposed model's parameterization depends on the number of events and predicates making it difficult to generalize to unseen events or required retraining.
2. The writing needs to be improved to clearly discuss the proposed approach.
3. The experiments baselines are of the authors' own design; it lacks a comparison to the literature baselines using the same dataset. If there is no such baseline, please discuss the criteria in choosing such baselines. Details:
1. Page 1, ""causal mechanisms"", causality is different from temporal relationship. Please use the terms carefully.
2. Page 3, it seems to me that M_T is defined over the probabilities of atomic events. The notation as it is used not making it difficult to make sense of this concept. Please consider providing examples to explain M_T.
3. Page 4, equation (2), it is not usual to feed probabilities to convolution.
a. Please discuss in section 3 how your framework can handle raw inputs, such as video or audio? Do you need an atomic event predictor or human label to use your proposed system? If so, is it possible to extend your framework to directly have video as input instead of event probability distributions? Can you do end2end training from raw inputs, such as video or audio? (although you mentioned Faster R-CNN in the experiment section, it is better to discuss the whole pipeline in the methodology).
b. Have you tried discrete event embeddings to represent the atomic and composite events so as the framework can learn distributional embedding representation of events so as to learn the temporal rules?
4. Page 4, please explain what you want to achieve with M_A = M_C \otimes M_D. It is unusual to multiple length by conv1D output. Also please define \otimes here. I am guessing it is elementwise multiplication from the context.
5. Page 4, ""M_{D:,:,l}=l. This can be thought as a positional encoding. It is not clear to me why this can be taken as positional encoding?
6. Page 6, please detail how do you sample top c predicates. Please define what is s in a = softmax(s). It seems to me the dimension of s with \sum_i (c i) can be quite large making it softmax(s) very costly.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- You write: ""Evidently, replacing any of the procedure steps of XAIFOOLER with a random mechanism dropped its performance"" I'm unsure that ""better than random"" is a strong demonstration of capability.",CblASBV3d4,EMNLP_2023,"- While studying instability of LIME, the work likely confuses that instability with various
other sources of instability involved in the methods:
- Instability in the model being explained.
- Instability of ranking metrics used.
- Instability of the LIME implementation used. This one drops entire words instead of perturbing
embeddings which would be expected to be more stable. Dropping words is a discrete and
impactful process compared to embedding perturbation.
Suggestion: control for other sources of instability. That is, measure and compare model
instability vs. resulting LIME instability; measure and compare metric instability vs. resulting
LIME instability. Lastly consider evaluating the more continuous version of input perturbation
for LIME based on embeddings. While the official implementation does not use embeddings, it
shouldn't be too hard to adjust it given token embedding inputs.
- Sample complexity of the learning LIME uses to produce explanations is not discussed. LIME
attempts to fit a linear model onto a set of inputs of a model which is likely not linear. Even
if the target model was linear, LIME would need as many samples as there are input features to be
able to reconstruct the linear model == explanation. Add to this likelihood of the target not
being linear, the number of samples needed to estimate some stable approximation increases
greatly. None of the sampling rates discussed in the paper is suggestive of even getting close to
the number of samples needed for NLP models.
Suggestion: Investigate and discuss sample complexity for the type of linear models LIME uses as
there may be even tight bounds on how many samples are needed to achieve close to optimal/stable solution.
Suggestion: The limitations discusses the computational effort is a bottleneck in using larger
sample sizes. I thus suggest investigating smaller models. It is not clear that using
""state-of-the-art"" models is necessary to make the points the paper is attempting to make.
- Discussions around focus on changing or not changing the top feature are inconsistent throughout
the work and the ultimate reason behind them is hard to discern. Requiring that the top feature
does not change seems like a strange requirement. Users might not even look at the features below
the top one so attacking them might be irrelevant in terms of confusing user understanding.
""Moreover, its experiment settings are not ideal as it allows perturbations of top-ranked
predictive features, which naturally change the resulting explanations""
Isn't changing the explanations the whole point in testing explanation robustness? You also cite
this point later in the paper:
""Moreover, this requirement also accounts the fact that end-users often consider only the
top k most important and not all of the features""
Use of the ABS metric which focuses on the top-k only also is suggestive of the importance of top
features. If top features are important, isn't the very top is the most important of all? Later:
""... changing the most important features will likely result in a violation to constraint in
Eq. (2)""
Possibly but that is what makes the perturbation/attack problem challenging. The text that
follows does not make sense to me:
""Moreover, this will not provide any meaningful insights to analysis on stability
in that we want to measure how many changes in the perturbed explanation that
correspond to small (and not large) alterations to the document.
I do not follow. The top feature might change without big changes to the document.
Suggestion: Coalesce the discussion regarding the top features into one place and present a
self-consistent argument of where and why they are allowed to be changed or not.
Smaller things:
- The requirement of black-box access should not dismiss comparisons with white-box attacks as baselines.
- You write:
""As a sanity check, we also constrain the final perturbed document to result in at least one
of the top k features decreasing in rank.""
This does not sound like a sanity check but rather a requirement of your method. If it were
sanity check, you'd measure whether at least one of the top k features decreased without imposing
it as a requirement.
- The example of Table 5 seems to actually change the meaning significantly. Why was such a change
allowed given ""think"" (verb) and ""thinking"" (most likely adjective) changed part of speech?
- You write:
""Evidently, replacing any of the procedure steps of XAIFOOLER with a random mechanism dropped
its performance""
I'm unsure that ""better than random"" is a strong demonstration of capability.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '5', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,
"- would like to have seen a discussion of how these results related to the lower bounds on kernel learning using low-rank approximation given in ""On the Complexity of Learning with Kernels"".",NIPS_2018_936,NIPS_2018,"Weakness: - would like to have seen a discussion of how these results related to the lower bounds on kernel learning using low-rank approximation given in ""On the Complexity of Learning with Kernels"". - In Assumption 5, the operator L is undefined. Should that be C?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The novelty of using PCA to reduce interaction count seems incremental and the significance of the paper results is unclear to me. Using PCA to reduce the interaction count seems intuitive, as PCA aims to retain the maximum information in the data with the reduced dimensionality chosen, assuming certain assumptions are met. How well are the assumptions met? [1] Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert Müller, and Pan Kessel. ""Towards robust explanations for deep neural networks."" Pattern Recognition 121 (2022): 108194.",34QscjTwOc,ICLR_2024,"- As far as I see, there is no mention of limitations of this work, let alone a Limitations section. No work is perfect, and every work should include a Limitations section so that, only two reasons given here for concision, (1) readers are quickly aware of cases in which this work applies and in which it doesn't and (2) readers have confidence that the paper is at least somewhat cognizant of (1). I'm unsure whether this is in the Appendix or Supplementary Material.
- Very limited Related Works section. A large section of related works that is relevant is ""sparsity in neural networks,"" and this could be broken down into multiple relevant subsections, such as ""sparsity over training progress"", ""sparsity with respect to {eigenvalues, spectral norms, Hessian properties [1], etc.}""
- Limited rigor in original (at least original as far as I know, such as the categorization of salient features) concepts.
- What quantitative rigor justifies the categorization of a feature into one of the 5 mentioned categories?
- Is there some sort of goodness of fit test or statistical hypothesis test or principled approach for assigning a feature to a category?
- What if the training epochs were extended and the utility trended in a way that changed categorization?
- What was the stopping criteria for training?
- Was any analysis done for the reliability of assigning features to categories?
- Unclear in several aspects. Some include
- Why use only one layer for each of the DNNs? How was this layer selected? How would results changing using a different intermediate layer?
- Why use the threshold values for rank, approximation error for salient feature count, the number of training epochs used, among others?
- Are the results in Figure 5a, 5b, and 5c each for one ""sample"", ""sentence"", and ""image"" in the single DNN model and single dataset listed?
- Do Figures X and Y show results for randomly sampled images? Since it's impossible to confirm whether this was actually the case, are there examples that do not align with these results, or even contradict these results? Is there analysis as to why?
- The novelty of using PCA to reduce interaction count seems incremental and the significance of the paper results is unclear to me. Using PCA to reduce the interaction count seems intuitive, as PCA aims to retain the maximum information in the data with the reduced dimensionality chosen, assuming certain assumptions are met. How well are the assumptions met?
[1] Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert Müller, and Pan Kessel. ""Towards robust explanations for deep neural networks."" Pattern Recognition 121 (2022): 108194.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"4. The few-shot RC models considered in the paper are not state-of-the-art models (e.g. https://aclanthology.org/2022.coling-1.205.pdf, https://ieeexplore.ieee.org/abstract/document/10032649/). How does the performance compare to relation extraction/generation models in few-shot settings.",HtQvhCRTxo,EMNLP_2023,"1. The scope of the paper seems a bit narrow. The dataset focuses only on company relations, but makes strong claims about generalization of RC models. Perhaps that claim needs to be supported with datasets on a few other domains.
2. The dataset is limited to company relations from Wikipedia pages. This may limit the diversity of the dataset. It may also not be representative of real-world data.
3. The dataset contains only a limited number of relations and examples, which may limit its usability in different scenarios.
4. The few-shot RC models considered in the paper are not state-of-the-art models (e.g. https://aclanthology.org/2022.coling-1.205.pdf, https://ieeexplore.ieee.org/abstract/document/10032649/). How does the performance compare to relation extraction/generation models in few-shot settings.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2) the results in section 4 apply only to shallow fully-connected ReLU networks;,ICLR_2023_591,ICLR_2023,"There are multiple axes along which the current paper falls short of applying to realistic settings: 1) the assumption that one is given an oracle adversary, i.e. we have access to the worst-case perturbation (as opposed to a noisy gradient oracle, i.e. just doing PGD); 2) the results in section 4 apply only to shallow fully-connected ReLU networks; 3) the results hold only in a regime very close to initialization and it is assumed one has an early stopping criterion/oracle.
Weaknesses 2) and 3) are not unique to this work, and thus I heavily discount their severity when considering my overall recommendation.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', 'X', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '0']}",,
"1. The organization of this paper could be further improved, such as give more background knowledge of the proposed method and bring the description of the relate literatures forward.",NIPS_2016_43,NIPS_2016,"Weakness: 1. The organization of this paper could be further improved, such as give more background knowledge of the proposed method and bring the description of the relate literatures forward. 2. It will be good to see some failure cases and related discussion.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"4) Some other representative panoptic segmentation models are not compared, like PanopticFPN, Mask2Former, etc.",ICLR_2023_4455,ICLR_2023,"1) Using the center's representation to conduct panoptic segmentation is too similar to PanopticFCN. The core difference would be the island centers for stuff, however, according to Table 6, it does not make significant improvements.
2) Although MaskConver gets significantly better performance than previous works, it is not clear where these improvements come from. It lacks a roadmap-like ablation study from the baseline to MaskConver. For example, in Table 5, the backbones and input sizes are all different among different models, which is not a fair or clear comparison.
3) The novelty of this paper is limited, as it does not propose new modules or training strategies. As it does not provide detailed ablations, it would be susceptible that the improvements mainly come from a highly engineered strong baseline.
4) Some other representative panoptic segmentation models are not compared, like PanopticFPN, Mask2Former, etc.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?",NIPS_2017_351,NIPS_2017,"- As I said above, I found the writing / presentation a bit jumbled at times.
- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).
- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.
- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.
- Figure 3 is never referenced unless I missed it.
Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.
- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?
- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Sensitivity to other hyper-parameters. Minor Comments on language usage: (for example) 1. ""we typically considers"" in the above of (7) 2. ""two permutation"" in the above of Theorem 1 3. ""until converge"" in the above of (14) 4. ...... Please proofread the paper and fix all language problems.",ICLR_2022_2323,ICLR_2022,"Weakness:
1. The literature review is inaccurate, and connections to prior works are not sufficiently discussed. To be more specific, there are three connections, (i) the connection of (1) to prior works on multivariate unlabeled sensing (MUS), (ii) the connection of (1) to prior works in unlabeled sensing (US), and (iii) the connection of the paper to (Yao et al., 2021).
(i) In the paper, the authors discussed this connection (i). However, the experiments shown in Figure 2 do not actually use the MUS algorithm of (Zhang & Li, 2020) to solve (1); instead the algorithm is used to solve the missing entries case. This seems to be an unfair comparison as MUS algorithms are not designed to handle missing entries. Did the authors run matrix completion prior to applying the algorithm of (Zhang & Li, 2020)? Also, the algorithm of (Zhang & Li, 2020) is expected to fail in the case of dense permutation.
(ii) Similar to (i), the methods for unlabeled sensing (US) can also be applied to solve (1), using one column of B_0 at a time. There is an obvious advantage because some of the US methods can handle arbitrary permutations (sparse or dense), and they are immune to initialization. In fact, these methods were used in (Yao et al., 2021) for solving more general versions of (1) where each column of B has undergone arbitrary and usually different permutations; moreover, this can be applied to the d-correspondence problem of the paper. I kindly wish the authors consider incoporating discussions and reviews on those methods.
(iii) Finally, the review on (Yao et al., 2021) is not very accurate. The framework of (Yao et al., 2021), when applied to (1), means that the subspace that contains the columns of A and B is given (when generating synthetic data the authors assume that A and B come from the same subspace). Thus the first subspace-estimation step in the pipeline of (Yao et al., 2021) is automatically done; the subspace is just the column space of A. As a result, the method of (Yao et al., 2021) can handle the situation where the rows of B are densely shuffled, as discussed above in (ii). Also, (Yao et al., 2021) did not consider only ""a single unknown correspondence"". In fact, (Yao et al., 2021) does not utilize the prior knowledge that each column of B is permuted by the same permutation (which is the case of (1)), instead it assumes every column of B is arbitrarily shuffled. Thus it is a more general situation of (1) and of the d-correspondence problem. Finally, (Yao et al., 2021) discusses theoretical aspects of (1) with missing entries, while an algorithm for this is missing until the present work.
2. In several places the claims of the paper are not very rigorous. For example,
(i) Problem (15) can be solved via linear assignment algorithms to global optimality, why do the authors claim that ""it is likely to fall into an undesirable local solution""? Also I did not find a comparison of the proposed approach with linear assignment algorithms.
(ii) Problem (16) seems to be ""strictly convex"", not ""strongly convex"". Its Hessian has positive eigenvalues everywhere but the minimum eigenvalue is not lower bounded by some positive constant. This is my feeling though, as in the situation of logistic regression, please verify this.
(iii) The Sinkhorn algorithm seems to use O(n^2) time per iteration, as in (17) there is a term C(hat{M_B}), which needs O(n^2) time to be computed. Experiments show that the algorithm needs > 1000 iterations to converge. Hence, in the regime where n << 1000 the algorithm might take much more time than O(n^2) (this is the regime considered in the experiments). Also I did not see any report on running times. Thus I feel uncomfortable to see the author claim in Section 5 that ""we propose a highly efficient algorithm"".
3. Even though an error bound is derived in Theorem 1 for the nuclear norm minimization problem, there is no guarantee of success on the alternating minimization proposal. Moreover, the algorithm requires several parameters to tune, and is sensitive to initialization. As a result, the algorithm has very lage variance, as shown in Figure 3 and Table 1. Questions:
1. In (3) the last term r+H(pi_P) and C(pi_P) is very interesting. Could you provide some intuition how it shows up, and in particular give an example?
2. I find Assumption 1 not very intuitive; and it is unclear to me why ""otherwise the influence of the permutation will be less significant"". Is it that the unknown permutation is less harmful if the magnitudes of A and B are close?
3. Solving the nuclear norm minimization program seems to be NP-hard as it involves optimization over permutation matrices and a complicated objective. Is there any hardness result for this problem?
Suggestions: The following experiments might be useful.
1. Sensitivity to permutation sparsity: As shown in the literature of unlabeled sensing, the alternating minimization of (Abid et al., 2017) works well if the data are sparsely permuted. This might also apply to the proposed alternating minimization algorithm here.
2. Sensitivity to initialization: One could present the performance as a function of the distance of initialization M^0 to the ground-truth M^*. That is for varying distance c (say from 0.01:0.01:0.1), randomly sample a matrix M^0 so that M^0 - M^* _F < c as initialization, and report the performance accordingly. One would expect that the mean error and variance increases as the quality of initialization decreases.
3. Sensitivity to other hyper-parameters.
Minor Comments on language usage: (for example)
1. ""we typically considers"" in the above of (7)
2. ""two permutation"" in the above of Theorem 1
3. ""until converge"" in the above of (14)
4. ......
Please proofread the paper and fix all language problems.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2) It is not clear regarding the choice of 20 distribution sets. Can we control the number of distribution sets for each class? What if you select only few number of distribution set?,NIPS_2022_2605,NIPS_2022,"Weakness: 1) In the beginning of the paper, authors often mention that previous works lack the flexibility compared to their work. It is not clear what does it mean and thus makes it harder to understand their explanation. 2) It is not clear regarding the choice of 20 distribution sets. Can we control the number of distribution sets for each class? What if you select only few number of distribution set? 3) The role of Tranfer Matrix T is not discussed or elaborated. 4) It is not clear how to form the target distribution H. How do you formulate H? 5) There is no discussion on how to generate x_H from H and what does x_H constitute of? 6) Despite the significant improvement, it is not clear how this proposed method boost the transferability of the adversarial examples.
As per my understanding, authors briefly addressed the limitations and negative impact in their work.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.",Pb1DhkTVLZ,EMNLP_2023,"1. The assessment criteria for the performance of large language models is limited to accuracy metrics. Such a limited view does not necessarily provide a comprehensive representation of the performance of large language models in real-world applications.
2. The method exhibits dependence on similar examples from the training dataset. This raises potential concerns regarding the distribution consistency between the training and test datasets adopted in the study. An in-depth visualization and analysis of the data distributions might be beneficial to address such concerns.
3. The evaluative framework appears somewhat limited in scope. With considerations restricted to merely three Question-Answering tasks and two language models, there are reservations about the method's broader applicability. Its potential to generalize to other reasoning or generation tasks or more advanced models, such as vicunna or alpaca, remains a subject of inquiry.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"2) The perplexity experiments are carried out on obsolete language models (n-gram HMM, RNN) that are rarely used nowadays. To better align the paper with current NLP trends, I believe the authors should showcase their approach using transformer-based (masked) language models.",HjBDSop3ME,EMNLP_2023,"1) Reducing the vocabulary size is one way of reducing the size of embedding, however, there are other alternatives such as dimensionality reduction (Raunak et al. 2019), quantization (see some works here (Gholami et al. 2021)), bloom embedding (Serra & Karatzoglou 2017), distillation networks (Hinton et al. 2015), etc.
This work should be compared against some of these related baselines to show its true potential as an innovative approach for embedding compactness.
*Raunak, V., Gupta, V., & Metze, F. (2019, August). Effective dimensionality reduction for word embeddings. In Proceedings of the 4th Workshop on Representation Learning for NLP (RepL4NLP-2019) (pp. 235-243).
*Serrà, J., & Karatzoglou, A. (2017, August). Getting deep recommenders fit: Bloom embeddings for sparse binary input/output networks. In Proceedings of the Eleventh ACM Conference on Recommender Systems (pp. 279-287).
*Gholami, A., Kim, S., Dong, Z., Yao, Z., Mahoney, M. W., & Keutzer, K. (2021). A survey of quantization methods for efficient neural network inference. arXiv preprint arXiv:2103.13630.
*Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.
2) The perplexity experiments are carried out on obsolete language models (n-gram HMM, RNN) that are rarely used nowadays. To better align the paper with current NLP trends, I believe the authors should showcase their approach using transformer-based (masked) language models.
3) The reliance of this approach on a secondary step (vowel-retrieval) to make the text human-readable again could limit its applicability. It would be interesting to see how this representation would perform on generation tasks such as translation or summarization. Since the vowel-retrieval process is not loss-less (word-error-rate 9 for consonant-only and ~3 for masked-vowel representations), it may cause a drastic drop in the performance of the models on such tasks.
4) In addition, this extra vowel-retrieval step would add to the required computational steps and may actually increase the computational requirements (as opposed to the paper’s claim on saving on computational resources).","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- The experiments are not sufficient. More empirical experiments or toy experiments (for the simplified self-attention model considered in the theoretical analysis) need to be done to show the validity of the model relaxations and the consistence of the theoretical analysis with empirical results, besides citing the result in Kaplan et al. 2020.",NIPS_2020_1897,NIPS_2020,"- The problem setting of this paper is too simplified, where only a “linearized” self-attention layer with all non-linear activations, layer normalization and softmax operation removed. However, given that the main purpose of the paper is to analyze the functionality of self-attention in terms of integrating inputs, these relaxations are not totally unreasonable. - The experiments are not sufficient. More empirical experiments or toy experiments (for the simplified self-attention model considered in the theoretical analysis) need to be done to show the validity of the model relaxations and the consistence of the theoretical analysis with empirical results, besides citing the result in Kaplan et al. 2020. - Although the paper is well organized, some parts are not well explained, especially for the proof sketch for Theorem 1 and Theorem 2.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"6) The authors discuss the misestimation of mu, but as it is the proportion of missing observations - it is not wholly clear how it can be estimated at all.",NIPS_2016_232,NIPS_2016,"weakness of the suggested method. 5) The literature contains other improper methods for influence estimation, e.g. 'Discriminative Learning of Infection Models' [WSDM 16], which can probably be modified to handle noisy observations. 6) The authors discuss the misestimation of mu, but as it is the proportion of missing observations - it is not wholly clear how it can be estimated at all. 5) The experimental setup borrowed from [2] is only semi-real, as multi-node seed cascades are artificially created by merging single-node seed cascades. This should be mentioned clearly. 7) As noted, the assumption of random missing entries is not very realistic. It would seem worthwhile to run an experiment to see how this assumption effects performance when the data is missing due to more realistic mechanisms.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Sensitivity to initialization: One could present the performance as a function of the distance of initialization M^0 to the ground-truth M^*. That is for varying distance c (say from 0.01:0.01:0.1), randomly sample a matrix M^0 so that M^0 - M^* _F < c as initialization, and report the performance accordingly. One would expect that the mean error and variance increases as the quality of initialization decreases.",ICLR_2022_2323,ICLR_2022,"Weakness:
1. The literature review is inaccurate, and connections to prior works are not sufficiently discussed. To be more specific, there are three connections, (i) the connection of (1) to prior works on multivariate unlabeled sensing (MUS), (ii) the connection of (1) to prior works in unlabeled sensing (US), and (iii) the connection of the paper to (Yao et al., 2021).
(i) In the paper, the authors discussed this connection (i). However, the experiments shown in Figure 2 do not actually use the MUS algorithm of (Zhang & Li, 2020) to solve (1); instead the algorithm is used to solve the missing entries case. This seems to be an unfair comparison as MUS algorithms are not designed to handle missing entries. Did the authors run matrix completion prior to applying the algorithm of (Zhang & Li, 2020)? Also, the algorithm of (Zhang & Li, 2020) is expected to fail in the case of dense permutation.
(ii) Similar to (i), the methods for unlabeled sensing (US) can also be applied to solve (1), using one column of B_0 at a time. There is an obvious advantage because some of the US methods can handle arbitrary permutations (sparse or dense), and they are immune to initialization. In fact, these methods were used in (Yao et al., 2021) for solving more general versions of (1) where each column of B has undergone arbitrary and usually different permutations; moreover, this can be applied to the d-correspondence problem of the paper. I kindly wish the authors consider incoporating discussions and reviews on those methods.
(iii) Finally, the review on (Yao et al., 2021) is not very accurate. The framework of (Yao et al., 2021), when applied to (1), means that the subspace that contains the columns of A and B is given (when generating synthetic data the authors assume that A and B come from the same subspace). Thus the first subspace-estimation step in the pipeline of (Yao et al., 2021) is automatically done; the subspace is just the column space of A. As a result, the method of (Yao et al., 2021) can handle the situation where the rows of B are densely shuffled, as discussed above in (ii). Also, (Yao et al., 2021) did not consider only ""a single unknown correspondence"". In fact, (Yao et al., 2021) does not utilize the prior knowledge that each column of B is permuted by the same permutation (which is the case of (1)), instead it assumes every column of B is arbitrarily shuffled. Thus it is a more general situation of (1) and of the d-correspondence problem. Finally, (Yao et al., 2021) discusses theoretical aspects of (1) with missing entries, while an algorithm for this is missing until the present work.
2. In several places the claims of the paper are not very rigorous. For example,
(i) Problem (15) can be solved via linear assignment algorithms to global optimality, why do the authors claim that ""it is likely to fall into an undesirable local solution""? Also I did not find a comparison of the proposed approach with linear assignment algorithms.
(ii) Problem (16) seems to be ""strictly convex"", not ""strongly convex"". Its Hessian has positive eigenvalues everywhere but the minimum eigenvalue is not lower bounded by some positive constant. This is my feeling though, as in the situation of logistic regression, please verify this.
(iii) The Sinkhorn algorithm seems to use O(n^2) time per iteration, as in (17) there is a term C(hat{M_B}), which needs O(n^2) time to be computed. Experiments show that the algorithm needs > 1000 iterations to converge. Hence, in the regime where n << 1000 the algorithm might take much more time than O(n^2) (this is the regime considered in the experiments). Also I did not see any report on running times. Thus I feel uncomfortable to see the author claim in Section 5 that ""we propose a highly efficient algorithm"".
3. Even though an error bound is derived in Theorem 1 for the nuclear norm minimization problem, there is no guarantee of success on the alternating minimization proposal. Moreover, the algorithm requires several parameters to tune, and is sensitive to initialization. As a result, the algorithm has very lage variance, as shown in Figure 3 and Table 1. Questions:
1. In (3) the last term r+H(pi_P) and C(pi_P) is very interesting. Could you provide some intuition how it shows up, and in particular give an example?
2. I find Assumption 1 not very intuitive; and it is unclear to me why ""otherwise the influence of the permutation will be less significant"". Is it that the unknown permutation is less harmful if the magnitudes of A and B are close?
3. Solving the nuclear norm minimization program seems to be NP-hard as it involves optimization over permutation matrices and a complicated objective. Is there any hardness result for this problem?
Suggestions: The following experiments might be useful.
1. Sensitivity to permutation sparsity: As shown in the literature of unlabeled sensing, the alternating minimization of (Abid et al., 2017) works well if the data are sparsely permuted. This might also apply to the proposed alternating minimization algorithm here.
2. Sensitivity to initialization: One could present the performance as a function of the distance of initialization M^0 to the ground-truth M^*. That is for varying distance c (say from 0.01:0.01:0.1), randomly sample a matrix M^0 so that M^0 - M^* _F < c as initialization, and report the performance accordingly. One would expect that the mean error and variance increases as the quality of initialization decreases.
3. Sensitivity to other hyper-parameters.
Minor Comments on language usage: (for example)
1. ""we typically considers"" in the above of (7)
2. ""two permutation"" in the above of Theorem 1
3. ""until converge"" in the above of (14)
4. ......
Please proofread the paper and fix all language problems.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- (minor) The absolute value operation in the definition of the Frobenius norm in line 77 is not needed because tensor entries are real numbers.,NIPS_2022_2592,NIPS_2022,"- (major) I don’t agree with the limitation (ii) of current TN models: “At least one Nth-order factor is required to physically inherit the complex interactions from an Nth-order tensor”. TT and TR can model complex modes interactions if the ranks are large enough. The fact that there is a lack of direct connections from any pair of nodes is not a limitation because any nodes are fully connected through a TR or TT. However, the price to pay with TT or TR to model complex modes interactions is having bigger core tensor (larger number of parameters). The new proposed topology has also a large price to pay in terms of model size because the core tensor C grows exponentially with the number of dimensions, which makes it intractable in practice. The paper lacks from a comparison of TR/TT and TW for a fixed size of both models (see my criticism to experiments below). - The new proposed model can be used only with a small number of dimensions because of the curse of dimensionality imposed by the core tensor C. - (major) I think the proposed TW model is equivalent to TR by noting that, if the core tensor C is represented by a TR (this can be done always), then by fusing this TR with the cores G_n we can reach to TR representation equivalent to the former TW model. I would have liked to see this analysis in the paper and a discussion justifying TW over TR. - (major) Comparison against other models in the experiments are unclear. The value of the used ranks for all the models are omitted which make not possible a fair comparison. To show the superiority of TW over TT and TR, the authors must compare the tensor completion results for all the models but having the same number of model parameters. The number of model parameters can be computed by adding the number of entries of all core tensors for each model (see my question about experiment settings below). - (minor) The title should include the term “tensor completion” because that is the only application of the new model that is presented in the paper. - (minor) The absolute value operation in the definition of the Frobenius norm in line 77 is not needed because tensor entries are real numbers. - (minor) I don’t agree with the statement in line 163: “Apparently, the O(NIR^3+R^N) scales exponentially”. The exponential grow is not apparent, it is a fact.
I updated my scores after rebuttal. See my comments below
Yes, the authors have stated that the main limitation of their proposed model is its exponentionally grow of model parameters with the number of dimensions.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
- Only bounds in expectation are provided. Would it be possible to get high-probability bounds? For instance by using ensemble methods as performed in the experiments. Some measure about the robustness could be added to the experiments (such as error bars or standard deviation) in addition to the mean error.,NIPS_2019_629,NIPS_2019,"- To my opinion, the setting and the algorithm lack a bit of originality and might seem as incremental combinations of methods of graph labelings prediction and online learning in a switching environment. Yet, the algorithm for graph labelings is efficient, new and seem different from the existing ones. - Lower bounds and optimality of the results are not discussed. In the conclusion section, it is asked whether the loglog(T) can be removed. Does this mean that up to this term the bounds are tight? I would like more discussions on this. More comparison with existing upper-bounds and lower-bound without switches could be made for instance. In addition, this could be interesting to plot the upper-bound on the experiments, to see how tight is the analysis. Other comments: - Only bounds in expectation are provided. Would it be possible to get high-probability bounds? For instance by using ensemble methods as performed in the experiments. Some measure about the robustness could be added to the experiments (such as error bars or standard deviation) in addition to the mean error. - When reading the introduction, I thought that the labels were adversarially chosen by an adaptive adversary. It seems that the analysis is only valid when all labels are chosen in advance by an oblivious adversary. Am I right? This should maybe be clarified. - This paper deals with many graph notions and it is a bit hard to get into it but the writing is generally good though more details could sometimes be provided (definition of the resistance distance, more explanations on Alg. 1 with brief sentences defining A_t, Y_t,...). - How was alpha tuned in the experiments (as 1/(t+1) or optimally)? - Some possible extensions could be discussed (are they straightforward?): directed or weighted graph, regression problem (e.g, to predict the number of bikes in your experiment)... Typo: l 268: the sum should start at 1","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- I am not an expert in the area of pruning. I think this motivation is quite good but the results seem to be less impressive. Moreover, I believe the results should be evaluated from more aspects, e.g., the actual latency on target device, the memory consumption during the inference time and the actual network size.",ICLR_2021_1014,ICLR_2021,"- I am not an expert in the area of pruning. I think this motivation is quite good but the results seem to be less impressive. Moreover, I believe the results should be evaluated from more aspects, e.g., the actual latency on target device, the memory consumption during the inference time and the actual network size. - The performance is only compared with few methods. And the proposed is not consistently better than other methods. For those inferior results, some analysis should be provided since the results violate the motivation.
I am willing to change my rating according to the feedback from authors and the comments from other reviewers.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
- My biggest concern with this paper is the fact that it motivates âdiversityâ extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity.,NIPS_2016_238,NIPS_2016,"- My biggest concern with this paper is the fact that it motivates âdiversityâ extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity. - The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions: - The first sentence of the abstract needs to be re-written. - Diversity should be toned down. - line 108, the first âfâ should be âgâ in âwe fixed the form of ..â - extra â.â in the middle of a sentence in line 115. One Question: For the baseline MCL with deep learning, how did the author ensure that each of the networks have converged to a reasonable results. Cutting the learners early on might significantly affect the ensemble performance.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '3', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', 'X', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"4. some experiments are missing , e.g., contrastive learning and adversarial learning.",z69tlSxAwf,EMNLP_2023,"1. The problem that how catastrophic forgetting exerts a strong influence on novel slots detection keeps unclear.
2. The paper does not study on large language models, which may be the current SOTA models for novel slots detection and their effective usage in dialogue context.
3. The method is little bit complex and hard to follow, e.g., how the method implement the final effect in Figure?
The proposed may be not easy to implement in real-world scenarios.
4. some experiments are missing , e.g., contrastive learning and adversarial learning.
5. The comparing baselines is only few while the proposed method is claimed to be SOTA model.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks:",NIPS_2019_933,NIPS_2019,"+ I liked the simplicity of the solution to divide the problem into star graphs. The domination number introduced seems to be a natural quantity for this problem. +/- To my opinion, the setting seems somewhat contrived combining feedback graphs and switching costs. The application to policy regret with counterfactual however provides a convincing example that the analysis can be useful and inspire future work. +/- The main part of the paper is rather clear and well written. Yet, I found the proofs in the appendices sometimes a bit hard to follow with sequences of unexplained equations. I would suggest to had some details. - There is a gap between the lower bound and the upper-bound (\sqrt(\beta) instead of \beta^{1/3}). In particular, for some graphs, the existing bound with the independence number may be better. This is also true for the results on the adaptive adversary and the counterfactual feedback. Other remarks: - Was the domination number already introduced for feedback graphs without switching costs? If yes, existing results for this problem should be cited. If not, it would be interesting to state what kind of results your analysis would provide without using the mini-batches. - Note that the length of the mini-batches tau_t may be non-integers. This should be clarified to be sure there are no side effects. For instance, what happens if $\tau_t << 1$? I am not sure if the analysis is still valid. - A better (more formal) definition of the independence and the domination numbers should be provided. It took me some time to understand their meaning. - Alg 1 and Thm 3.1: Since only upper-bounds on the pseudo-regret are provided, the exploration parameter gamma seems to be useless, isn't it? The choice gamma=0 seems to be optimal. A remark on high-probability upper-bounds and the role of gamma might be interesting. In particular, do you think your analysis (which is heavily based on expectations) can be extended to high-probability bounds on the regret? - I understand that this does not suit the analysis (which uses the equivalence in expectation btw Alg1 and Alg6) but it seems to be suboptimal (at least in practice) to discard all the feedbacks obtained while playing non-revealing actions. It would be nice to have practical experiments to understand better if we lose something here. It would be also nice to compare it with existing algorithms. Typos: - p2, l86: too many )) - Thm 3.1: A constant 2 in the number of switches is missing. - p13, l457: some notations seem to be undefined (w_t, W_t). - p14, you may add a remark - p15, l458: the number of switches can be upper-bounded by **twice** the number of times the revealing action is played - p16, l514: I did not understand why Thm 3.1 implies the condition of Thm C.5 with alpha=1/2 and not 1. By the way, (rho_t) should be non-decreasing for this condition to hold.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"5. Although FIDs are still being widely used for evaluation, there have been clear flaws associated with them and the simplistic Inception network [C]. Please use DinoV2 Frechet Distances for the comparisons from [C], in addition to the widely used FID metric.",kRjLBXWn1T,ICLR_2025,"1. I find that separating Theorem 3.3 into parts A and B is tangential to the story and overly confusing. In reality, we do not have full control over our correction network and its Lipschitz constant. Therefore, we can never determine the best scheduling. This section seems like its being theoretical for its own sake! It might be clearer to simply present Lemma A.2 of the appendix in its most general form:
$$W_2(p^b_t, p^f_t) \le W_2(p^b_{t_0}, p^f_{t_0}) \cdot e^{L(t-t_0)}$$
and say that improving the Wasserstein distance on the RHS for $t_0$ can effectively bound the Wasserstein distance on the LHS, especially for $t$ that is sufficiently close to $t_0$. I don't think A, B, and examples 3.4 and 3.5 are particularly insightful when it is not directly factored into the decisions made in the experiments. The results in A and B can still be included, but in the appendix.
2. The parallel sampling section seems slightly oversold! To my understanding, while both forward passes can be done in parallel, it cannot be done in one batch because the forward call is on different methods. Could you please provide a time comparison between parallel and serial sampling on one experiment with the hardware that you have?
3. The statement of Lemma 3.6 seems to spill over to the rest of the main text and I generally do not agree with the base assumption that $p_t^f = p^b_{t, \sigma}$ which is the main driver for Lemma 3.6. Please let me know if I am misunderstanding this!
4. I don't find the comparison between this method and Dai and Wipf [B] appropriate! [B] trains a VAE on VAE to fix problems associated with the dimensionality mismatch between the data manifold and the manifold induced by the (first) VAE. That is not a concern in flow-matching and diffusion models as these models are known not to suffer from the manifold mismatch difficulties as much.
5. Although FIDs are still being widely used for evaluation, there have been clear flaws associated with them and the simplistic Inception network [C]. Please use DinoV2 Frechet Distances for the comparisons from [C], in addition to the widely used FID metric.
6. Please also provide evaluations ""matching"" the same NFEs in the corresponding non-corrected models.
### Minor points
1. I personally do not agree with the notation abuse of rewriting the conditional probability flow $p_t(x | z)$ as the marginal probability flow $p_t(x)$; it is highly confusing in my opinion.
2. Rather than introducing the new probability flows $\nu_t$ and $\mu_t$, in theorem 3.3, please consider using the same $p^b_t$ and $p^f_t$ for reduced notation overhead, and then restate the theorem in full formality for the appendix.
3. (nitpick) In Eq. (8), $t$ should be a sub-index of $u$.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1. Novelty seems incremental to me. What are the ways in which this paper differs from https://aclanthology.org/2021.findings-acl.57.pdf? Is it just applying a very similar methodology to new task?,GFgPmhLVhC,EMNLP_2023,"1. Novelty seems incremental to me. What are the ways in which this paper differs from https://aclanthology.org/2021.findings-acl.57.pdf? Is it just applying a very similar methodology to new task?
2. Performance gains seem small. There should be p-test or atleast confidence intervals to check statistical significance.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- In the reported ablation studies in Table 2, for CUB and SOP datasets, the complete loss function performed even worse than those with some terms missing. That does not appear to make sense. Why?",ICLR_2023_3918,ICLR_2023,"- The evaluation results reported in table 1 are based on only three trials for each case. While this is fine, statistically this is not significant, and thus it does not make sense to report the deviations. That is why that in many cases the deviation is 0. Due to this reason, statements such as “our performance is at least two standard deviation better than the next best baseline” do not make sense. - In the reported ablation studies in Table 2, for CUB and SOP datasets, the complete loss function performed even worse than those with some terms missing. That does not appear to make sense. Why?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Finally, and similarly to above, i’d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range “inference” capacity of the model, as opposed to a nearly-supervised reconstruction task. Again, these should be reasonably easy experiments to run. I’d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).",aGH43rjoe4,ICLR_2024,"I do have several queries/concerns however:
- **a. Fixed time horizon**: The use of an MLP to convert the per-timestep embeddings into per-sequence Fourier coefficients means that you can only consider fixed-length sequences. This seems to me to be a real limitation, since often neural/behavioral data – especially naturalistic behavior – is not of a fixed length. This could be remedied by using an RNN or neural process in place of the MLP, so this is not catastrophic as far as I can tell. However, I at least expect to see this noted as a limitation of the method, and, preferably, substitute in an RNN or neural process for the MLP in one of the examples, just to concretely demonstrate that this is not a fundamental limitation.
- **b. Hidden hyperparameters and scaling issues**: Is there a problem if the losses/likelihoods from the channels are “unbalanced”? E.g. if the behavioral data is 1080p video footage, and you have say 5 EEG channels, then a model with limited capacity may just ignore the EEG data. This is not mentioned anywhere. I think this can be hacked by including a $\lambda$ multiplier on the first term of (6) or raising one of the loss terms to some power (under some sensible regularization), trading off the losses incurred by each channel and making sure the model pays attention to all the data. I am not 100% sure about this though. Please can the authors comment.
- **c. Missing experiments**: There are a couple of experiments/baselines that I think should be added.
- Firstly, in Figure 3, I'd like to see a model that uses the data independently to estimate the latent states and reconstruction. It seems unfair to compare multimodal methods to methods that use just one channel. I’m not 100% sure what this would look like, but an acceptable baseline would be averaging the predictions of image-only and neuron-only models (co-trained with this loss). At least then all models have access to the same data, and it is your novel structure that is increasing the performance.
- Secondly, I would like to see an experiment sweeping over the number of observed neurons in the MNIST experiment. If you have just one neuron, then performance of MM-GP-VAE should be basically equivalent to GP-VAE. If you have 1,000,000 neurons, then you should have near-perfect latent imputations (for a sufficiently large model), which can be attributed solely to the neural module. This should be a relatively easy experiment to add and is a good sanity check.
- Finally, and similarly to above, i’d like to see an experiment where the image is occluded (half of the image is randomly blacked out). This (a) simulates the irregularity that is often present in neural/behavioral data (e.g. keypoint detection failed for some mice in some frames), and (b) would allow us to inspect the long-range “inference” capacity of the model, as opposed to a nearly-supervised reconstruction task.
Again, these should be reasonably easy experiments to run. I’d expect to see all of these experiments included in a final version (unless the authors can convince me otherwise).
- **d. Slightly lacking analysis**: This is not a deal-breaker for me, but the analysis of the inferred latents is somewhat lacking. I’d like to see some more incisive analysis of what the individual and shared features pull out of the data – are there shared latent states that indicate “speed”, or is this confined to the individual behavioral latent? Could we decode a stimulus type from the continuous latent states? How does decoding accuracy from each of the three different $z$ terms differ? etc. I think this sort of analysis is the point of training and deploying models like this, and so I was disappointed to not see any attempt at such an analysis. This would just help drive home the benefits of the method.
### Minor weaknesses / typographical errors:
1. Page 3: why are $\mu_{\psi}$ and $\sigma_{\psi}^2$ indexed by $\psi$? These are variational posteriors and are a function of the data; whereas $\psi$ are static model parameters.
2. Use \citet{} for textual citations (e.g. “GP-VAE, see (Casale et al., 2018).” -> “GP-VAE, see Casale et al. (2018).”)
3. The discussion of existing work is incredibly limited (basically two citations). There is a plethora of work out there tackling computational ethology/neural data analysis/interpretable methods. This notable weakens the paper in my opinion, because it paints a bit of an incomplete picture of the field, and actually obfuscates why this method is so appealing! I expect to see a much more thorough literature review in any final version.
4. Text in Figure 5 is illegible.
5. Only proper nouns should be capitalized (c.f. Pg 2 “Gaussian Process” -> “Gaussian process”), and all proper nouns should be capitalized (c.f. Pg 7 “figure 4(c)”).
6. Figure 1(a): Is there are sampling step to obtain $\tilde{\mu}$ and $\tilde{\sigma}^2$? This sample step should be added, because right now it looks like a deterministic map.
7. I think “truncate” is more standard than “prune” for omitting higher-frequency Fourier terms.
8. I find the use of “A” and “B” very confusing – the fact that A is Behaviour, and B is Neural? I’m not sure what better terms are. I would suggest B for Behavioural – and then maybe A for neural? Or A for (what is currently referred to as) behavioral, but be consistent (sometimes you call it “other”) and refer to it as Auxiliary or Alternative data, and then B is “Brain” data or something.
9. The weakest section in terms of writing is Section 3. The prose in there could do with some tightening. (It’s not terrible, but it’s not as polished as the rest of the text).
10. Use backticks for quotes (e.g. ‘behavioral modality’ -> ``behavioral modality’’).","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. I think figure 6C is a bit awkward, it implies negative rates, which is not the case, I would suggest using a second y-axis or another visualization which is more physically accurate.",NIPS_2016_153,NIPS_2016,"weakness of previous models. Thus I find these results novel and exciting.Modeling studies of neural responses are usually measured on two scales: a. Their contribution to our understanding of the neural physiology, architecture or any other biological aspect. b. Model accuracy, where the aim is to provide a model which is better than the state of the art. To the best of my understanding, this study mostly focuses on the latter, i.e. provide a better than state of the art model. If I am misunderstanding, then it would probably be important to stress the biological insights gained from the study. Yet if indeed modeling accuracy is the focus, it's important to provide a fair comparison to the state of the art, and I see a few caveats in that regard: 1. The authors mention the GLM model of Pillow et al. which is pretty much state of the art, but a central point in that paper was that coupling filters between neurons are very important for the accuracy of the model. These coupling filters are omitted here which makes the comparison slightly unfair. I would strongly suggest comparing to a GLM with coupling filters. Furthermore, I suggest presenting data (like correlation coefficients) from previous studies to make sure the comparison is fair and in line with previous literature. 2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes: 1. Please define the dashed lines in fig. 2A-B and 4B. 2. Why is the training correlation increasing with the amount of training data for the cutout LN model (fig. 4A)? 3. I think figure 6C is a bit awkward, it implies negative rates, which is not the case, I would suggest using a second y-axis or another visualization which is more physically accurate. 4. Please clarify how the model in fig. 7 was trained. Was it on full field flicker stimulus changing contrast with a fixed cycle? If the duration of the cycle changes (shortens, since as the authors mention the model cannot handle longer time scales), will the time scale of adaptation shorten as reported in e.g Smirnakis et al. Nature 1997.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
2. Introducing inverse triples might also be used in other embedding models besides CP. But the authors did not test such cases in their experiments.,NIPS_2018_396,NIPS_2018,1. The only difference between SimplE and CP seems to be that SimplE further uses inverse triples. The technical contributions of this paper are quite limited. 2. Introducing inverse triples might also be used in other embedding models besides CP. But the authors did not test such cases in their experiments. 3. It would be better if the authors could further explain why such a simple step (introducing inverse triples) can improve the CP model that much.,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"• This statement in the abstract is unclear: “ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions”. What does this mean? The abstract should be more high-level. Such technicalities are not necessary.",ICLR_2022_2671,ICLR_2022,", starting from the most critical ones.
Unclear Threat Model. The threat model is provided in Section 4.1, and the authors (implicitly) use the general notation of goal, capabilities, knowledge (the strategy is explained later). However, the capabilities of the attacker are not well defined. Specifically, the paper says that the attacker can control a fraction ( α
) of the M clients. Hence, I ask: what can the attacker do with such clients? Can they perform any manipulation? Are some of the features not modifiable? Is such attacker subject to realistic constraints or feature dependencies? Is the perturbation magnitude bounded? I am referring to the well-known issue of ‘problem vs feature space’ attacks [B], because real attackers are subject to many real world constraints (and especially in networked systems [C, D, E]) and not all adversarial perturbations may be physically realizable ([F]). The authors should elucidate this issue, because it could differentiate between “fictional” and “practical” attacks, therefore defining whether the proposed method is applicable to solve real world problems. Moreover, the following is unclear:
• “There are α
fraction of agents that are malicious and total β
fraction of instances with backdoor trigger across all malicious agents.” I am unable to understand the relationship between α and β
. Is β
a subset of α ?
• From Equation 3 onwards, is h^{j} meant to denote h j
? Is there a need for the braces, or is it a typo?
• Please differentiate from h b e n i g n and h B
. The current notation is very confusing
Tradeoff. A common problem in adversarial ML countermeasures is that they may degrade baseline performance [G, H]. Hence, I am interested in knowing how the proposed method responds when there are no “malicious” clients (or when such clients behave legitimately). In this paper, I am unable to determine what is the baseline performance of the models in “non-adversarial” settings. Does such performance degrade after RVFR is applied? How does RVFR compare to previous works in these circumstances? Even if the baseline performance does not decrease, what is the overhead of the proposed RVFR with respect to past defenses? Figure 3 and 2 only show results for adversarial scenarios. In real circumstances, a defense should have some practical utility. Note that I would not reject the paper even if RVFR does have a significant “cost”. However, such cost must be known.
Very poor Introduction and Abstract. The Introduction fails to provide a concrete justification and enough context for the considered problem. Let me list all the issues I encountered while reading the introductory part of the paper:
• This statement in the abstract is vague: “However, unlike the standard horizontal federated learning, improving the robustness of robust VFL remains challenging.”. Specifically, why is it challenging? Just name a few reasons.
• This statement in the abstract is unclear: “ensure that with a low-rank feature subspace, a small number of attacked samples, and other mild assumptions”. What does this mean? The abstract should be more high-level. Such technicalities are not necessary.
• Please be consistent. In the Introduction: “In FL, a central server coordinates with multiple agents/clients”. Either use “client” or “agent”.
• I suggest mentioning [A] for a practical, recent and useful application of FL in a real world problem.
• This example is unclear: “In VFL, different agents hold different parts of features for the same set of training data. For example, in VFL for credit score prediction, Agent 1 may have the banking data of a user and Agent 2 may have the credit history of the same user, while the server holds corresponding labels.”. To me, it appears that Agent 1 and Agent 2 have different data, and hence represents a HFL problem (and I still do not understand the necessity of the last statement involving the ‘label’). Perhaps the authors should provide a visual example that better explains the difference between HFL and VFL.
• The Introduction still suffers of the same problem as the abstract. “However, it is challenging to defend against malicious attacks in VFL.”. Why? It is very annoying to read the abstract, not get a response to such question; and then re-reading the same concept in the Introduction, and not finding an answer even there. The impression is that the authors are trying to make the problem more difficult than what it currently is: if it is challenging, it should be clear.
• In the Introduction: “The fraction of malicious agents is relatively small”. Can you define such fraction? Does it have an upper boundary?
• The term “backdoor attacks” is never mentioned in the Introduction until the “contribution” paragraph. Such term should be better contextualized: not all poisoning attacks are backdoor attacks.
• I had to reach the Related Work section to understand why poisoning attacks in VFL are “challenging”. However, the motivation provided by the authors is confusing—to say the least. According to the paper, the challenge is “Backdoor attack against VFL is challenging since in the default setting the agent does not have the label information.” First, what is challenging exactly: the attack, or the defense? Second, what is such “default setting”? Third, why does the agent not have the label information? I believe the latter is due to the (poor) explanation provided in the early example in the introduction.
Minor issues:
• Please use the term “stage” (or “phase”) and not “time” to differentiate between training and testing/inference.
• Please add some text between Section 6 and 6.1
• In Section 6.1, the authors state “We study the classification task on two datasets: NUS-WIDE and CIFAR-10. Following (Liu et al., 2020), which proposed the backdoor attack against VFL, we use NUS-WIDE dataset to evaluate our defense.”. Does it mean that the defense is only evaluated on NUS-WIDE?
• In Section 6.2, the authors state “The noise variance is 0:05 for NUS-WIDE and 0:0001 for CIFAR-10 to preserve reasonable utility of the model.”. Please define what “utility” means in this statement.
• Figure 2: please maintain the same range for the y-axis. It does not only varies among the rows, but also among the columns.
• Figure 3 and Figure 2: the range of the y-axis should be the same for both figures.
• Use the same term to refer to figures. If the figures are named “Figures”, then use “Figures” in the text, and not “Fig.”
• What is the difference between “Backdoor Accuracy” and “Clean Accuracy” in Figures 2 and 3?
EXTERNAL REFERENCES
[A]: ""Federated learning for predicting clinical outcomes in patients with COVID-19."" Nature medicine (2021): 1-9.
[B]: ""Intriguing properties of adversarial ml attacks in the problem space."" 2020 IEEE Symposium on Security and Privacy (SP). IEEE, 2020.
[C]: ""Modeling Realistic Adversarial Attacks against Network Intrusion Detection Systems."" ACM Digital Threats: Research and Practice. 2021.
[D]: ""Constrained concealment attacks against reconstruction-based anomaly detectors in industrial control systems."" ACM Annual Computer Security Applications Conference. 2020.
[E]: ""Resilient networked AC microgrids under unbounded cyber attacks."" IEEE Transactions on Smart Grid 11.5 (2020): 3785-3794.
[F]: ""Improving robustness of ML classifiers against realizable evasion attacks using conserved features."" 28th {USENIX} Security Symposium ({USENIX} Security 19). 2019.
[G]: ""Adversarial example defense: Ensembles of weak defenses are not strong."" 11th {USENIX} workshop on offensive technologies ({WOOT} 17). 2017.
[H]: ""Deep reinforcement adversarial learning against botnet evasion attacks."" IEEE Transactions on Network and Service Management 17.4 (2020): 1975-1987.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1) It seems that there is still room to improve the complexity of Algorithm 2;,NIPS_2022_1971,NIPS_2022,"Weakness: No significant weaknesses. Two minor comments/suggestions: 1) It seems that there is still room to improve the complexity of Algorithm 2; 2) some numerical experiments comparing the proposed algorithm with non-adaptive benchmarks (showing the trade-off of adaptivity and performance) could be interesting.
the authors adequately addressed the limitations and potential negative societal impact of their work.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '2']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"2. It would be nice to see some results in other modalities (e.g., maybe some language related tasks. Aside: for language related tasks, people care about OOD performance as well, so maybe expected test loss is not as meaningful?)",XvA1Mn9OFy,ICLR_2025,"1. While I agree that the performance gains in table 1 illustrate that GAM > SAM > SGD, the relative gains of GAM over SAM seem relatively small.
2. It would be nice to see some results in other modalities (e.g., maybe some language related tasks. Aside: for language related tasks, people care about OOD performance as well, so maybe expected test loss is not as meaningful?)","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"1. The motivation of this work should be further justified. In few-shot learning, we usually consider how to leverage a few instances to learn a generalizable model. This paper defines and creates a few-shot situation for graph link prediction, but the proposed method does not consider how to effectively use “few-shot” and how to guarantee the trained model can be generalized well to new tasks with 0/few training steps.",ICLR_2022_1617,ICLR_2022,"Weakness: 1. The motivation of this work should be further justified. In few-shot learning, we usually consider how to leverage a few instances to learn a generalizable model. This paper defines and creates a few-shot situation for graph link prediction, but the proposed method does not consider how to effectively use “few-shot” and how to guarantee the trained model can be generalized well to new tasks with 0/few training steps. 2. The definition of “domain” in this paper is unclear. For instance, why select multiple domains from the same single graph in ogbn-products? Should we consider the selected domains as “different domains”? 3. The application of adversarial learning in few-shot learning is confusing. Adversarial learning in domain adaptation aims to learn domain-invariant representations, but why do we need such kind of representation in few-shot learning?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1 The way of using GP is kind of straightforward and naive. In the GP community, dynamical modeling has been widely investigated, from the start of Gaussian Process Dynamical Model in NIPs 2005.",ICLR_2021_665,ICLR_2021,"Weakness
1 The way of using GP is kind of straightforward and naive. In the GP community, dynamical modeling has been widely investigated, from the start of Gaussian Process Dynamical Model in NIPs 2005.
2 I do not quite get the modules of LSTM Frame Generation and GP Frame Generation in Eq (4). Where are these modules in Fig.3 ? The D in the Stage 3? Using GP to generate Images? Does it make sense? GP is more suitable to work in the latent space, is it?
3 The datasets are not quite representative, due to the simple and experimental scenarios. Moreover, the proposed method is like a fundamental work. But is it useful for high-level research topics, e.g., large-scale action recognition, video caption, etc?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- I find this statement in the supplemental section D.4 questionable: ""Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting"". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states?",NIPS_2019_387,NIPS_2019,"- The main weakness is empirical---scratchGAN appreciably underperforms an MLE model in terms of LM score and reverse LM score. Further, samples from Table 7 are ungrammatical and incoherent, especially when compared to the (relatively) coherent MLE samples. - I find this statement in the supplemental section D.4 questionable: ""Interestingly, we found that smaller architectures are necessary for LM compared to the GAN model, in order to avoid overfitting"". This is not at all the case in my experience (e.g. Zaremba et al. 2014 train 1500-dimensional LSTMs on PTB!), which suggests that the baseline models are not properly regularized. D.4 mentions that dropout is applied to the embeddings. Are they also applied to the hidden states? - There is no comparison against existing text GANs , many of which have open source implentations. While SeqGAN is mentioned, they do not test it with the pretrained version. - Some natural ablation studies are missing: e.g. how does scratchGAN do if you *do* pretrain? This seems like a crucial baseline to have, especially the central argument against pretraining is that MLE-pretraining ultimately results in models that are not too far from the original model. Minor comments and questions : - Note that since ScratchGAN still uses pretrained embeddings, it is not truly trained from ""scratch"". (Though Figure 3 makes it clear that pretrained embeddings have little impact). - I think the authors risk overclaiming when they write ""Existing language GANs... have shown little to no performance improvements over traditional language models"", when it is clear that ScratchGAN underperforms a language model across various metrics (e.g. reverse LM).","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"5. Some of the ablations mentioned in previous sections are hard to locate in the following contents, the writing can be improved in this part.",Ugs2W5XFFo,ICLR_2025,"1.	Note that the paper mainly focuses on SD-based (SD 2.1, SDXL) models. These models are mostly the same styles, e.g., similar network structures and traditional denoising training strategies. Is there any possibility that the MI tuning incorporated with flow-based models like DiT-based models (SD3, Pixart series or so). And it is interesting to see if the proposed MI tuning behaves different with different types of models.
2.	The evaluations on MI mainly focus on only simple semantic concepts like color, shape and texture. Is MI-tuning sensitive to object numbers or so?
3.	The paper fixes the denoising steps to 50 when inferencing an image, are there any differences in performance of MI-tuning when using different steps except 50?
4.	In quantitative analysis of Sect. 3.1, the paper mentions that the point-wise MI ranks images and select 1st, 25th and 50th as the representative images. Why the three images are representative? This needs more detailed explanations. Also, the reason of the selection needs quantitative analysis.
5. Some of the ablations mentioned in previous sections are hard to locate in the following contents, the writing can be improved in this part.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"6) Overall, given2)-5) I think the differential privacy application is a bit too ""half-baked"" at the present time and I would encourage the authors to think through it more clearly. The online algorithm and robustness is significantly interesting and novel on its own. The experimental results in the appendix would be better in the main paper.",NIPS_2016_395,NIPS_2016,"- I found the application to differential privacy unconvincing (see comments below) - Experimental validation was a bit light and felt preliminary RECOMMENDATION: I think this paper should be accepted into the NIPS program on the basis of the online algorithm and analysis. However, I think the application to differential privacy, without experimental validation, should be omitted from the main paper in favor of the preliminary experimental evidence of the tensor method. The results on privacy appear too preliminary to appear in a ""conference of record"" like NIPS. TECHNICAL COMMENTS: 1) Section 1.2: the dimensions of the projection matrices are written as $A_i \in \mathbb{R}^{m_i \times d_i}$. I think this should be $A_i \in \mathbb{R}^{d_i \times m_i}$, otherwise you cannot project a tensor $T \in \mathbb{R}^{d_1 \times d_2 \times \ldots d_p}$ on those matrices. But maybe I am wrong about this... 2) The neighborhood condition in Definition 3.2 for differential privacy seems a bit odd in the context of topic modeling. In that setting, two tensors/databases would be neighbors if one document is different, which could induce a change of something like $\sqrt{2}$ (if there is no normalization, so I found this a bit confusing. This makes me think the application of the method to differential privacy feels a bit preliminary (at best) or naive (at worst): even if a method is robust to noise, a semantically meaningful privacy model may not be immediate. This $\sqrt{2}$ is less than the $\sqrt{6}$ suggested by the authors, which may make things better? 3) A major concern I have about the differential privacy claims in this paper is with regards to the noise level in the algorithm. For moderate values of $L$, $R$, and $K$, and small $\epsilon = 1$, the noise level will be quite high. The utility theorem provided by the author requires a lower bound on $\epsilon$ to make the noise level sufficiently low, but since everything is in ""big-O"" notation, it is quite possible that the algorithm may not work at all for reasonable parameter values. A similar problem exists with the Hardt-Price method for differential privacy (see a recent ICASSP paper by Imtiaz and Sarwate or an ArXiV preprint by Sheffet). For example, setting L=R=100 and K=10, \epsilon = 1, \delta = 0.01 then the noise variance is of the order of 4 x 10^4. Of course, to get differentially private machine learning methods to work in practice, one either needs large sample size or to choose larger $\epsilon$, even $\epsilon \gg 1$. Having any sense of reasonable values of $\epsilon$ for a reasonable problem size (e.g. in topic modeling) would do a lot towards justifying the privacy application. 4) Privacy-preserving eigenvector computation is pretty related to private PCA, so one would expect that the authors would have considered some of the approaches in that literature. What about (\epsilon,0) methods such as the exponential mechanism (Chaudhuri et al., Kapralov and Talwar), Laplace noise (the (\epsilon,0) version in Hardt-Price), or Wishart noise (Sheffet 2015, Jiang et al. 2016, Imtiaz and Sarwate 2016)? 5) It's not clear how to use the private algorithm given the utility bound as stated. Running the algorithm is easy: providing $\epsilon$ and $\delta$ gives a private version -- but since the $\lambda$'s are unknown, verifying if the lower bound on $\epsilon$ holds may not be possible: so while I get a differentially private output, I will not know if it is useful or not. I'm not quite sure how to fix this, but perhaps a direct connection/reduction to Assumption 2.2 as a function of $\epsilon$ could give a weaker but more interpretable result. 6) Overall, given 2)-5) I think the differential privacy application is a bit too ""half-baked"" at the present time and I would encourage the authors to think through it more clearly. The online algorithm and robustness is significantly interesting and novel on its own. The experimental results in the appendix would be better in the main paper. 7) Given the motivation by topic modeling and so on, I would have expected at least an experiment on one real data set, but all results are on synthetic data sets. One problem with synthetic problems versus real data (which one sees in PCA as well) is that synthetic examples often have a ""jump"" or eigenvalue gap in the spectrum that may not be observed in real data. While verifying the conditions for exact recovery is interesting within the narrow confines of theory, experiments are an opportunity to show that the method actually works in settings where the restrictive theoretical assumptions do not hold. I would encourage the authors to include at least one such example in future extended versions of this work.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '4', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1. The contribution of multilingual chain-of-thought is incremental compared to the villa chain-of-though.,ICLR_2023_598,ICLR_2023,"Weakness: 1. The contribution of multilingual chain-of-thought is incremental compared to the villa chain-of-though. 2. It is interesting to see the large gap between Translate-EN, EN-CoT and Native-CoT in MGSM. While the gaps in other benchmarks are not too much. Is it because MGSM benchmark is originated from translation?","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
- It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate.,NIPS_2020_195,NIPS_2020,"My main concerns are: - Feeding the actual pose of one arm (master) and the relative pose of the second arm (slave) with respect to the master and similarly other objects would have been more informative for the network to capture the relational dependencies at the pose level. A baseline comparison with this method would be useful to understand the dependency structure, especially to improve the performance for the second task. Adding other baselines with state-of-the-art methods in the related work would further improve the understanding of the work. - The authors discuss a few examples with the position in table tasks, but the effect of orientation is not explained. Does the approach generalize with randomly sampled orientation of the target object ? Are the orientations normalized to unit quaternions after prediction ? The authors are encouraged to show orientation errors and quantify the performance. - Adding visual pixel information from pixels would help to establish the true merits of graph attention mechanism. - 29 percent accuracy with table assembly tasks is rather low. The Euclidean distance error units in Table 1 seem very high. Are they normalized to per datapoint position errors ? If so, an error of 5 cm with HDR-IL seems unreasonably high. - It is not clear if the proposed methodology is specific to bimanual manipulation. Just using robotic manipulation could be more appropriate. - Experiments with real setup would have been useful to establish the merits of the proposed approach.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. You should provide the METEOR results, which is also reported in recent works [1-5].",51gbtl2VxL,EMNLP_2023,"1. The paper don't compare their models with more recent SOTAs [1-3], so it can not get higher soundness.
2. You should provide the results on more datasets, such as Test2018.
3. You should provide the METEOR results, which is also reported in recent works [1-5].
4. The Figure 5 is not clear, you should give more explanation about it.
[1] Yi Li, Rameswar Panda, Yoon Kim, Chun-Fu Richard Chen, Rogerio S Feris, David Cox, and Nuno Vasconcelos. 2022. VALHALLA: Visual Hallucination for Machine Translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 5216–5226.
[2] Junjie Ye, Junjun Guo, Yan Xiang, Kaiwen Tan, and Zhengtao Yu. 2022. Noiserobust Cross-modal Interactive Learning with Text2Image Mask for Multi-modal Neural Machine Translation. In Proceedings of the 29th International Conference on Computational Linguistics. 5098–5108.
[3] Junjie Ye and Junjun Guo. 2022. Dual-level interactive multimodal-mixup encoder for multi-modal neural machine translation. Applied Intelligence 52, 12 (2022), 14194–14203.
[4] Good for Misconceived Reasons: An Empirical Revisiting on the Need for Visual Context in Multimodal Machine Translation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 6153–6166.
[5] Li B, Lv C, Zhou Z, et al. On Vision Features in Multimodal Machine Translation[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 6327-6337.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. This paper (Section 4) examines G-effects of each unlearning objective independently and in isolation to other learning objectives. Results are also shown and discussed in separate figures and parts of the paper. Studying G-effect of each learning objective in isolation, raises the concern regarding the comparability of G-effect values across various unlearning objectives and approaches.",huo8MqVH6t,ICLR_2025,"1. This paper (Section 4) examines G-effects of each unlearning objective independently and in isolation to other learning objectives. Results are also shown and discussed in separate figures and parts of the paper. Studying G-effect of each learning objective in isolation, raises the concern regarding the comparability of G-effect values across various unlearning objectives and approaches.
- Why empirical analysis of each unlearning approach is shown and discussed in separate parts of the paper?
- Are G-effect values comparable across different unlearning approaches? are values comparable and why?
- Can the proposed G-effect rank unlearning approaches?
2. Section 5 and its Table 1 provide a comprehensive comparison of various unlearning approaches using TOFU unlearning dataset for the removal of fictitious author profiles from LLMs finetuned on them. However, this comparison uses only existing metrics: forget quality, model utility, and PS-scores, and does not report the proposed G-effects.
- Why G-effects are missing in this section?
- How do G-effect values correlate with metrics presented in Table 1?
- Why are the order and ranking of unlearning objectives different across different removal and retention metrics?
3. G-effects need access to intermediate checkpoints during unlearning, especially given the pattern of values in for example Figure 3 (i.e., a peak and then flat close to zero). How does this limit the applicability of the proposed metric?
4. The G-effect definition uses model checkpoints at different time steps and does not directly take into account the risk and unlearning of the initial model.
- Why does this make sense?
- Is this why you need to do accumulative?
- what does the G-effect at each unlearning step mean?
- what does accumulation across unlearning steps mean?
- What does pick mean in Figure 3? Should we stop after that step to have an effective unlearning? what would be the benefit of continuing? is 0 G-effect value the limitation of your method?
5. Some of the claims are not completely supported. For example, the claim ""In terms of the unlearning G-effects, it indicates that the unlearning strength of NPO is weaker; however, for the retaining G-effects, it suggests that NPO better preserves the model integrity."" As an initial step, I would link it to numbers in Table 1.
6. Membership inference attacks are a common approach in the literature for evaluating the removal capability of unlearning approaches [MUSE]. However, this paper does not report the success of membership inference attacks. How the unlearning G-effect is compared to the success of MIA? Are they aligned?
[MUSE] Weijia Shi, Jaechan Lee, Yangsibo Huang, Sadhika Malladi, Jieyu Zhao, Ari Holtzman, Daogao Liu, Luke Zettlemoyer, Noah A Smith, and Chiyuan Zhang. Muse: Machine unlearning six-way evaluation for language models, 2024.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- Advantage of UNIFORM over other procedures is not consistent. The tables show that UNIFORM does not always offer a clear advantage over the results, especially in the 1-shot setting. Do the authors have a theory for why the method is not as effective on the 1 shot setting? Clarity + Experiments are well designed, and the results are clear.",NIPS_2021_138,NIPS_2021,"weakness Originality
+ Interesting and novel insights into the task difficulty for meta-learners. Although the idea of controlling episode difficulty is not very novel [47, 28], the authors go about it in a novel way and propose to use importance sampling for a faster and more efficient way of sampling episodes that is model agnostic. Quality
+ Solid evaluation and analysis. Testing over multiple models, algorithms, and benchmarks makes for a convincing set of results.
+ Confirmed assumptions through experimentation. The paper uses Q-Q plots and Shapiro-Wilk to verify the assumption that episode difficulty follows a normal distribution.
- Advantage of UNIFORM over other procedures is not consistent. The tables show that UNIFORM does not always offer a clear advantage over the results, especially in the 1-shot setting. Do the authors have a theory for why the method is not as effective on the 1 shot setting? Clarity
+ Experiments are well designed, and the results are clear.
+ Paper is organized and well-written. Significance
+ Interesting solution to a relevant problem. Episode difficulty is understudied in few-shot learning, and uniform presents a model agnostic solution to the problem of how to sample tasks during meta-learning. Post-Rebuttal
I have read the rebuttal and other reviews, and I increase my original rating. This paper addresses an interesting and relevant issue of episode difficulty in meta-learning and will be a particularly valuable contribution at the conference. The paper presents a solid set of evaluations and analyses, verifying assumptions (e.g. via Q-Q plots and Shapiro-Wilk tests) and reporting results across multiple benchmarks. The paper presents a novel, simple but effective method that, on the most part, boosts algorithm performance by a couple of additional accuracy points on average. Therefore, I recommend this work for the conference with a score of 8.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Have the author(s) thought about the reason why, information value is ""stronger predictor"" for dialogue(Complementarity in page 7 or discussion in page 8), is there any already existing linguistic theory which could explain it. If so, adding that will make this one a stronger paper.",fkAKjbRvxj,EMNLP_2023,"1. Have the author(s) thought about the reason why, information value is ""stronger predictor"" for dialogue(Complementarity in page 7 or discussion in page 8), is there any already existing linguistic theory which could explain it. If so, adding that will make this one a stronger paper.
2.It turns out to me that different information value serves as a strong predictor among the 5 chosen corpora, for example, in PROVO it is the syntactic information value. Again have the author(s) already had a potential hypothesis for this phenomenon?Is it practical to do a linguistic analysis on this 5 corpora to find the reason?
3. Is it possible that by increasing the set size of A_(x), the generated sentences sampled from ""ancestral sampling"" could have already covered some/all of the samples from other sampling methods, e.g., ""temperature sampling""? As far as I know, both of the two mentioned sampling methods are based on the conditional probability, while typical sampling is a comparatively new sampling method which could cut off the dependence on the conditional probability to some degree and helps to generate more creative and diversify next sentences instead of entering receptive loop(Meister 2023).
Based on that,I would suggest the author(s) making a statistic report about the distributions of generated sentences from different sampling methods and maybe then making a selection of just two representative sampling methods based on the observation. Also a reference to temperature sampling is needed.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- From my point of view the main reason to use AutoML approaches, besides improving raw performances, is extracting hints that can be reused in the design of new network architectures in the future. Unfortunately the authors did not spend much time commenting on these aspects. For example, what might be the biggest takeaways from the found architecture?",NIPS_2020_1857,NIPS_2020,"- The paper is not particularly novel or exciting since it takes algorithms already applied in the field of semantic segmentation and applies them to the stereo depth estimation problem. The idea of using AutoML for stereo is not particularly novel either, as stated by the authors themselves, even if the proposed algorithm outperforms the previous proposal. - From my point of view the main reason to use AutoML approaches, besides improving raw performances, is extracting hints that can be reused in the design of new network architectures in the future. Unfortunately the authors did not spend much time commenting on these aspects. For example, what might be the biggest takeaways from the found architecture? - I would have liked to see an additional ablation study to better highlight the contribution of the proposed method with respect to AutoDispNet. The main differences with respect to the previously published work is the search performed also on the network level and the use of two separate feature and matching networks. Ablating the contribution of one and the other might have been interesting. - The evaluation on Middleburry should include for fairness a test of the found architecture running at quarter resolution to match the testing setup of all the other deep architecture. While it is true that the ability to run at higher resolution is an advantage of the proposed method there is nothing (besides hw limitation) preventing the other networks to run at higher resolution as well. Therefore I think that a fair comparison between networks running on the same test setup will improve the paper highlighting the contribution of the proposed method. - Some minor implementation details are missing from the paper, I will expand this point in my questions to the authors.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"1) The main contribution of this paper lies in the structure-aware encoder-based model for seed lexicon induction, there should be an experiment to study the quality of seed lexicon.",Kjs0mpGJwb,EMNLP_2023,"The experiments is somewhat weak.
1) The main contribution of this paper lies in the structure-aware encoder-based model for seed lexicon induction, there should be an experiment to study the quality of seed lexicon.
2) While the paper focuses on bilingual lexicon induction, it would be beneficial to include a downstream task, such as cross-lingual Natural Language Inference (NLI), to demonstrate the potential impact of the proposed method on downstream applications. This would provide further insights into the effectiveness of the approach beyond the specific lexicon induction task.","{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['3', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['3', '4']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['3', '5']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'boda'], 'labels': ['1', '1']}",,
"* T_a(t) is used in Section 3.1, but only defined in Section 4.",NIPS_2019_991,NIPS_2019,"[Clarity] * What is the value of the c constant (MaxGapUCB algorithm) used in experiments? How was it determined? How does it impact the performance of MaxGapUCB? * The experiment results could be discussed more. For example, should we conclude from the Streetview experiment that MaxGapTop2UCB is better than the other ones? [Significance] * The real-world applications of this new problem setting are not clear. The authors mention applicability to sorting/ranking. It seems like this would require a recursive application of proposed algorithms to recover partial ordering. However, the procedure to find the upper bounds on gaps (Alg. 4) has complexity K^2, where K is the number of arms. How would that translate in computational complexity when solving a ranking problem? Minor details: * T_a(t) is used in Section 3.1, but only defined in Section 4. * The placement of Figure 2 is confusing. --------------------------------------------------------------------------- I have read the rebuttal. Though the theoretical contribution seems rather low given existing work on pure exploration, the authors have convinced me of the potential impacts of this work.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '5', '4']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
1.The main part can be more concise (especially for the introduction part)and including empirical results.,NIPS_2022_1564,NIPS_2022,"1.The main part can be more concise (especially for the introduction part)and including empirical results.
2.Given the new introduced hyper-parameters, it is still not clear whether this new proposed method is empirically useful. How to choose hyper-parameters in a more practical training setting?
3.The empirical evaluations can not well supported their theoretical analysis. As the authors claim running experiments with 24 A100 GPUs, all methods should be compared in a relatively large scaled training task. Only small linear regression experiment results are reported, where communication is not really an issue.
The paper discusses a new variant on a technique in distributed training. As far as I’m concerned, there is no serious issue or limitation that would impact society.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '3', '3']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"* The empirical analysis in Figure 3 is confusing to the reviewer. Could the authors provide additional clarification on how the adjustments to the amplitudes of the input series and forecasting target, based on the Frequency Stability score, affect model prediction accuracy? Additionally, could you explain why these adjustments are effective in enhancing the model's performance? Both Equations (9) and (10) have large spacing from the preceding text. [1] Liu, Yong, et al. ""Non-stationary transformers: Exploring the stationarity in time series forecasting."" Advances in Neural Information Processing Systems 35 (2022): 9881-9893.",5VK1UulEbE,ICLR_2025,"* The contribution of the paper seems limited. According to equation (10), the core approach mainly involves applying a linear transformation to frequency components based on the matrix $\mathrm{S}$. It is unclear how $\mathrm{S}$ functions as a constraint to realize the key motivation of enhancing the weight of stable frequency components.
* In Definition 2, the authors introduce the concept of a Stable Frequency Subset $\mathcal{O}$ but do not provide a clear criterion for determining what qualifies as a stable frequency. Some notations, such as Stable Frequency Subset $\mathcal{O}$, and Theorem 1 presented in Section 2 are not well linked to the design of the method proposed in Section 3.
* The experimental evaluation lacks thoroughness, as it includes only three baseline models (Dlinear, PatchTST, and iTransformer). Although the authors mention other models, such as CrossFormer, they do not include it in their comparisons. Additionally, the Nonstationary Transformer [1] would also be a relevant model for further comparison.
* The empirical analysis in Figure 3 is confusing to the reviewer. Could the authors provide additional clarification on how the adjustments to the amplitudes of the input series and forecasting target, based on the Frequency Stability score, affect model prediction accuracy? Additionally, could you explain why these adjustments are effective in enhancing the model's performance? Both Equations (9) and (10) have large spacing from the preceding text.
[1] Liu, Yong, et al. ""Non-stationary transformers: Exploring the stationarity in time series forecasting."" Advances in Neural Information Processing Systems 35 (2022): 9881-9893.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Currently, this paper only considers binary concepts. This is a common assumption, so not very limiting.",Ku1tUKnAnC,ICLR_2025,"1. This approach requires enumerating all off-target concepts in order to construct the oracle probe for Z_e. However, this is a common assumption in the causal-inference-in-text literature, so not surprising.
2. A crux of this paper is the development of *good* oracle probes. However, there is very little analysis of how to evaluate the oracle probes (which are then used to evaluate causal probing methods).
3. Currently, this paper only considers binary concepts. This is a common assumption, so not very limiting.","{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['2', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO'], 'labels': ['1', '1']}",,
"- The recent related work CoCoOp [1] is not compared in the experiments. Although it is a CVPR'22 work that is officially published after the NeurIPS deadline, as the extended version of CoOp, it is necessary to compare with CoCoOp in the experiments.",NIPS_2022_2813,NIPS_2022,"weakness (insight and contribution), my initial rating is borderline. Strengths:
+ The problem of adapting CLIP under few-shot setting is recent. Compared to the baseline method CoOp, the improvement of the proposed method is significant.
+ The ablation studies and analysis in Section 4.4 is well organized and clearly written. It is easy to follow the analysis and figure our the contribution of each component. Also, Figure 2 is well designed and clear to illustrate the pipeline.
+ The experimental analysis is comprehensive. The analysis on computation time and inference speed is also provided. Weakness:
- (major concern) The contribution is somehow limited. The main contribution is applying optimal transport for few-shot adaptation of CLIP. After reading the paper, it is not clear enough to me why Optimal Transport is better than other distance. Especially, the insight behind the application of Optimal Transport is not clear. I would like to see more analysis and explanation on why Optimal Transport works well. Otherwise, it seems that this work is just an application work on a specific model and a specific task, which limits the contribution.
- The recent related work CoCoOp [1] is not compared in the experiments. Although it is a CVPR'22 work that is officially published after the NeurIPS deadline, as the extended version of CoOp, it is necessary to compare with CoCoOp in the experiments.
- In the approach method, there lacks a separate part or subsection to introduce the inference strategy, i.e., how to use the multiple prompts in the test stage.
- Table 2 mixed different ablation studies (number of prompts, visual feature map, constraint). It would be great if the table can be split into several tables according to the analyzed component.
- The visualization in Figure 4 is not clear. It is not easy to see the attention as it is transparent. References
[1] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In CVPR, 2022.
After reading the authors' response and the revised version, my concerns (especially the contribution of introducing the optimal transport distance for fine-tuning vision-language models) are well addressed and I am happy to increase my rating.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"- Fig. 1 can also be drawn better to show the processing pipeline (prompt generation and manual check, demonstration selection with ground truth scores, and automatic scoring alongwith showing where model training is being used to optimize the selection modules.",PyJ78pUMEE,EMNLP_2023,"- There's over reliance on the LLM to trust the automated scoring with the knowledge that LLMs have their complex biases and sensitivity to prompts (and order).
- It is not clear how the method will perform on long conversations (the dialog datasets used for prompt and demonstration selection seem to contain short conversations)
- The paper can be simplified in writing - the abstract is too long and does not convey the findings well.
- Fig. 1 can also be drawn better to show the processing pipeline (prompt generation and manual check, demonstration selection with ground truth scores, and automatic scoring alongwith showing where model training is being used to optimize the selection modules.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The author only did experiment on two typical games, the ReBeL's performance on more complex problem. Especially when the game has bigger depth which will cause huge inputs of the value and policy function.",NIPS_2020_1430,NIPS_2020,"1. The paper only studied two-player situation, the performence of ReBeL in multi-player games is still to be confirm. 2. There are not enough comparison with other relative method in Section2. 3. The author only did experiment on two typical games, the ReBeL's performance on more complex problem. Especially when the game has bigger depth which will cause huge inputs of the value and policy function. 4. The theortical proof had only been considered two-player situation.","{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '1', '1']}",,,5,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['5', '2', '2']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['qDjzAPQZ', 'JMHUapmO', 'boda'], 'labels': ['1', '1', '0']}",,
"- (136): abbreviations like ""MoCo"" should not appear in the section header, since a reader might not know what it means.",ARR_2022_219_review,ARR_2022,"- The paper hypothesizes that SimCSE suffers from the cue of sentence length and syntax. However, the experiments only targets sentence length but not syntax. - The writing of this paper can benefit by some work (see more below). Specifically, I find Section 3 difficult to understand as someone who does not directly work on this task. Specifically, a good mount of terminologies are introduced without explanations. I suggest a good effort of rewriting this section to be easier understandable by general NLP researchers.
- Though a universal issue in related papers and should not be blamed on the authors, why only consider BERT-base? It is known that other models such as BERT-large, RoBERTa, DeBERTa, etc. could produce better embeddings, and that the observations in these works do not necessarily hold in those larger and better models. - The introduction of the SRL-based discrete augmentation approach (line 434 onwards) is unclear and cannot be possibly understood by readers without any experience in SRL. I suggest at least discussing the following: - Intuitively why relying on semantic roles is better than work like CLEAR - What SRL model you use - What the sequence ""[ARG0, PRED, ARGM − NEG, ARG1]"" mean, and what these PropBank labels mean - What is your reason of using this sequence as opposed to alternatives
- (Line 3-6, and similarly in the Intro) The claim is a finding of the paper, so best prefix the sentence with ""We find that"". Or, if it has been discussed elsewhere, provide citations. - (7): semantics-aware or semantically-aware - (9-10): explore -> exploit - (42): works on -> works by - Figure 1 caption: embeddings o the : typo - Figure 1 caption: ""In a realistic scenario, negative examples have the same length and structure, while positive examples act in the opposite way."" I don't think this is true. Positive or negative examples should have similar distribution of length and structure, so that they don't become a cue during inference. - (99): the first mention of ""momentum-encoder"" in the paper body should immediately come with citation or explanation. - (136): abbreviations like ""MoCo"" should not appear in the section header, since a reader might not know what it means. - (153): what is a ""key""?
- (180): same -> the same - (186-198): I feel that this is a better paragraph describing existing issue and motivation than (63-76). I would suggest moving it to the Intro, and just briefly re-mention the issue here in Related Work. - (245-246): could be -> should be - (248): a -> another - (252-55): Isn't it obvious that ""positive examples in SimCSE have the same length"", since SimCSE enocdes the same sentence differently as positive examples? How does this need ""Through careful observation""?
- (288): ""textual similarity"" should be ""sentence length and structure""? Because the models are predicting textual similarity, after all.
- (300-301): I don't understand ""unchangeable syntax"" - (310-314): I don't understand ""query"", ""key and value"". What do they mean here? Same for ""core"", ""pseudo syntactic"". - (376): It might worth mentioning SimCSE is the state-of-the-art method mention in the Abstract. - (392): Remove ""In this subsection""","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. The technical contribution is unclear. Most of the analysis are quite standard.,QQvhOyIldg,ICLR_2025,"1. This paper is poorly written & presented. A lot of the content can be found in the undergraduate textbook. A substantial part of the results are informal version, say Lemma 6.1 - 6.3. Also, there is hardly any interpretation of the main results. The presentation style does not seem to be serious.
2. The technical contribution is unclear. Most of the analysis are quite standard.
3. There is no numerical experiments to verify its application in real-world dataset.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
- The paper's primary contribution seems to be an incremental advancement in efficiency over the TACTiS approach. More substantial evidence or arguments are needed to establish this as a significant contribution to the field.,xtOydkE1Ku,ICLR_2024,"- The core innovation claimed by the paper is the reduction in computational complexity through a two-stage solution, first estimating marginals and then dependencies. However, this approach isn't novel, as seen in references [1,2]. The paper would benefit from a clearer distinction of how its methodology differs significantly from these existing methods.
- The paper's primary contribution seems to be an incremental advancement in efficiency over the TACTiS approach. More substantial evidence or arguments are needed to establish this as a significant contribution to the field.
- When evaluating the model's efficacy, the improvement in terms of Negative Log-Likelihood (NLL) is notable. However, the Mean Continuous Ranked Probability Score (CRPS) metric indicates that these improvements are only marginal when compared to the TACTiS model.
[1] Andersen, Elisabeth Wreford. ""Two-stage estimation in copula models used in family studies."" Lifetime Data Analysis 11 (2005)
[2] Joe, Harry. ""Asymptotic efficiency of the two-stage estimation method for copula-based models."" Journal of Multivariate Analysis 94.2 (2005).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. The improvements on different datasets are trivial and the novelty of this paper is limited. Lots of previous works focus on this topic. Just adding topic entities seems incremental.,aRlH9AkiEA,EMNLP_2023,"1. It's still unclear how topic entities can improve the relationship representations. This claim is less intuitive.
2. The improvements on different datasets are trivial and the novelty of this paper is limited. Lots of previous works focus on this topic. Just adding topic entities seems incremental.
3. Missed related work.
4. Some methods (e.g., KnowBERT, CorefBERT) in related work are not selected as baselines for comparison.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. The paper lacks discussion on the theoretical guarantee about the approximation ratio of the hierarchical strategy to the global optimal of original QUBO.,yCAigmDGVy,ICLR_2025,"1. As the paper primarily focuses on applying quantum computing to global Lipschitz constant estimation, it is uncertain whether the ICLR community will find this topic compelling.
2. The paper lacks discussion on the theoretical guarantee about the approximation ratio of the hierarchical strategy to the global optimal of original QUBO.
3. The experimental results are derived entirely from simulations under ideal conditions, without consideration for practical aspects of quantum devices such as finite shots, device noise, and limited coherence time. These non-ignorable imperfections could significantly impact the quality of solutions obtained from quantum algorithms in practice.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1) Generally lacking a quantitative measure to evaluate the generated VCEs. Evaluation is mainly performed with visual inspection.,NIPS_2022_246,NIPS_2022,"Weakness: 1) Generally lacking a quantitative measure to evaluate the generated VCEs. Evaluation is mainly performed with visual inspection. 2) As the integration of cone projection shown to be helpful, however it is not clear why this particular projection is chosen. Are there other projections that are also helpful? Is there a theoretical proof that this cone projection resolves the noise of the gradients in non-robust classifiers?
Overall, I think the proposed technique yield better VCE and interesting for the community. I also think that the strengths outweigh the weakness. However, I would be open to hear other reviewers opinion here.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- Only marginal improvements over baselines, mostly within the error bar range. Although the authors claim the method performs better than the baselines, the error range is rather high, suggesting that the performance differences between some methods are not very significant.",NIPS_2021_275,NIPS_2021,"weakness Originality
+ Novel setting. As far as I am aware, the paper proposes a novel setting - Few-shot Hypothesis Adaptation (FHA) - a combination of existing problems - Hypothesis Transfer Learning and the Few-Shot Domain Adaptation.
+/- Somewhat novel method. As far as I am aware, the paper also proposes a novel method - TOHAN - which is a minor adaptation of FADA [23] into the setting. The method architecture seems to be heavily inspired by FADA [23]. Unlike FADA, TOHAN generates and uses the generated intermediate domain instead of the original source domain. However, apart from that, it is not entirely clear what the technical differences are between these methods. Which line in Algorithm 1 would be different for FADA / S-FADA / T-FADA / ST-FADA?
- The relation between FHA and FSL is poorly explained, and FSL is poorly cited. I found the sentence in lines 88-89 particularly vague and poorly written. In addition, line 90 that compares TSN [37] and ProtoNets [29] (""which is relatively weaker than the former"") brings little value to the paper. In my humble opinion, these methods were designed for two different problems (i.e. video action classification [37] and single image classification [29]), and it is inappropriate to compare them in this way. The authors might find the following works from FSL literature more related to their work: (Antreas et al., 2018), (Hariharan & Girshick., 2017), (Wang et al., 2018). Generally, these methods also generate/hallucinate samples from limited target-domain data and should be cited. Quality
+ Some good experiments. The paper performs a solid number of comparisons with representative baselines from HTL and FDA literature ([19] and [23], respectively) and basic fine-tuning baseline proposed by the authors.
+/- Work seems grounded in some theoretical work. However, I did not attempt to verify the proof, so I cannot comment on its correctness.
- Only marginal improvements over baselines, mostly within the error bar range. Although the authors claim the method performs better than the baselines, the error range is rather high, suggesting that the performance differences between some methods are not very significant.
- There might be a possible fundamental flaw to the claim of full data privacy. The authors claim that the TOHAN method (and therefore also the FHA problem) ""strictly"" protects the privacy of the source domain by using the source hypothesis rather than the source data itself (lines 245-247). However, the claim that knowledge is ""completely"" inaccessible may be false. For instance, Ateniese et al. (2015) have shown that it is possible to extract certain types of information from pretrained classifiers and models. In a more recent privacy analysis of deep learning, Nasr et al. (2019) suggest that even well-trained models might leak a significant amount of information about the data they were trained on. Moreover, the proposed TOHAN relies on the leaked source-domain knowledge to generator appropriate source-domain data. Therefore, it seems to me that claim that TOHAN is privacy secure is completely false.
- Lack of sufficient evidence for no source domain features in intermediate data. The authors claim that ""high-level, visual and useful features of source domain are rare in the generated intermediate data (Figure 6)"" (lines 243-244); however, no empirical value is provided to support this claim (i.e. what do authors mean by ""rare"" in this context? how rare is ""rare""?). Figure 6a/b does support this claim as it shows only 4 intermediate-domain images and no examples of the original source-domain data. This is insufficient evidence to draw the conclusion in lines 243-244, and in fact, it points to the contrary. Clarity
- The paper is poorly written. The paper is not particularly easy to read. It contains many spelling and grammatical errors (see below for proposed corrections). There is a large number of acronyms (e.g. FHA, SHOT, HTL, ST+F etc..) and some confusing mathematical notation (e.g. D , D , X and X
refer to different entities) which make this work more confusing to read. The notation in the figure is inconsistent with the main text (uses X
instead of D ). Significance
- The paper presents a novel and interesting problem but could be flawed. This novel problem and method could have important consequences in the context of data privacy - however, to me, the idea seems fundamentally flawed (see my comments in the ""Quality"" subsection, or ""Limitations And Societal Impact"").
- The TOHAN improvements over baselines are mostly marginal. Although the authors claim the method performs better than the baselines, the error range is rather high, suggesting that the performance differences between some methods are not very significant and the improvements are marginal.
Spelling, Grammer, and Other Possible Mistakes.
- Line 32, grammar: ""face of generation"" --> ""face generation""
- Figure 2 caption, grammar: ""which two data come from ... "" --> ""where the two data points come from ... ""
- Line 128, wrong word: ""dependents"" --> ""depends""
- Line 167, redundant word: ""regarding to the ..."" --> ""regarding the ... ""
- Line 210, redundant words: ""which confuses D unable to distinguish between ..."" --> ""which confuses D between ... ""
- Line 213, wrong word: ""initial"" --> ""initialize"" References
Ateniese et al., 2015, ""Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers"", International Journal of Security and Networks, Volume 10, Issue 3
Nasr et al., 2019, ""Comprehensive Privacy Analysis of Deep Learning: Passive and Active White-box Inference Attacks against Centralized and Federated Learning,"" 2019 IEEE Symposium on Security and Privacy
Antoniou et al., 2018, ""Data Augmentation Generative Adversarial Networks"", (ICLR 2018 Workshop)
Hariharan & Girshick., 2017, ""Low-shot Visual Recognition by Shrinking and Hallucinating Features"", (ICCV 2017)
Wang et al., 2018, ""Low-shot learning from imaginary data"" (CVPR 2018) POST-REBUTTAL
After a detailed discussion with the authors, I decided to increase my original rating from 3 to 7. The initial low rating was due to initially hidden assumptions and a poorly defined scope of data privacy which are central in the paper. These have been discussed and clarified by the authors during the rebuttal. The authors also addressed my concerns regarding the novelty and source-domain leakage into the intermediate domain. The authors have agreed to improve the clarity, literature review, dampen down on the privacy claims, and include additional experiments, and I am happy to increase the rating. Although the privacy claims are now not as strong as originally claimed (e.g. the method does not guard against source-information leakage, but rather shelters individual source data points from a possible data leakage), the paper still opens up an interesting area of research and presents a novel method that will likely attract attention in the community.
The authors claim that their method allows for full privacy of the source domain by relying on a well-trained source domain classifier. However, this seems to be based on the false assumption that the source domain classifier does not leak any private information. Recent studies (for example, Nasr et al. (2019) and Ateniese et al. (2015)) suggest that there might be a significant amount of source-domain information that pre-trained models leak. Moreover, TOHAN relies on leaking information from the source domain to generate realistic/compatible intermediate-domain data. This completely invalidates one of the main claims of the paper that private information is protected.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '4', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- The generated videos have significant artifacts. Only some of the beach videos are kind of convincing. The action recognition performance is much below the current state-of-the-art on the UCF dataset, which uses more complex (deeper, also processing optic flow) architectures. Questions:",NIPS_2016_69,NIPS_2016,"- The paper is somewhat incremental. The developed model is a fairly straighforward extension of the GAN for static images. - The generated videos have significant artifacts. Only some of the beach videos are kind of convincing. The action recognition performance is much below the current state-of-the-art on the UCF dataset, which uses more complex (deeper, also processing optic flow) architectures. Questions: - What is the size of the beach/golf course/train station/hospital datasets? - How do the video generation results from the network trained on 5000 hours of video look? Summary: While somewhat incremental, the paper seems to have enough novelty for a poster. The visual results encouraging but with many artifacts. The action classification results demonstrate benefits of the learnt representation compared with random weights but are significantly below state-of-the-art results on the considered dataset.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
4. It is still unclear how to make the new proposed evaluation set more diverse and representative than the previous method and how to select those representative images.,ICLR_2023_4659,ICLR_2023,"Weakness: 1. It would make this paper stronger if the authors can show some adversarial robustness of some SOTA defended recognition models on the new set. 2. I would like to see more clear details of how to use DALL-E2 or stable diffusion models to generate hard examples, e.g., how to design prompts and how to filter out some unrealistic images. 3. The new dataset has some great properties. However, how to make it more scalable to real applications besides evaluating the current models trained on a public dataset? What if we have some new classes in our task, but it is not included in this set? 4. It is still unclear how to make the new proposed evaluation set more diverse and representative than the previous method and how to select those representative images.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors should include a background section to introduce the basic RL framework, including elements of the MDP, trajectories, and policy, to clarify the RL context being considered. Without this, it is difficult to follow the subsequent sections. Additionally, a brief overview of the original DPO algorithm should be provided so that modifications proposed in the methods section are clearly distinguishable.",Y4iaDU4yMi,ICLR_2025,"- The paper's presentation is difficult to follow, with numerous typos and inconsistencies in notation. For example:
- Line 84, ""In summery"" -> ""In summary"".
- In Figure 1, ""LLaVA as dicision model"" -> ""LLaVA as decision model.""
- Line 215, ""donate"" should be ""denote""; additionally, $\pi_{ref}$ is duplicated.
- The definitions of subscripts and superscripts for action (i.e., $a_t^1$ and $a_t^2$) in line 245 and in Equations (4), (6), (7), (8), and (9) are inconsistent.
- Line 213 references the tuple with $o_t$, but it is unclear where $s_{1 \sim t-1}$ originates.
- The authors should include a background section to introduce the basic RL framework, including elements of the MDP, trajectories, and policy, to clarify the RL context being considered. Without this, it is difficult to follow the subsequent sections. Additionally, a brief overview of the original DPO algorithm should be provided so that modifications proposed in the methods section are clearly distinguishable.
- In Section 3.1, the authors state that the VLM is used as a planner; however, it is unclear how the plan is generated. It appears that the VLM functions directly as a policy, outputting final actions to step into the environment, as illustrated in Figure 1. Thus, it may be misleading to frame the proposed method as a ""re-planning framework"" (line 197). Can author clarify this point?
- What type of action space does the paper consider? Is it continuous or discrete? If it is discrete, how is the MSE calculated in Eq. (7)?
- In line 201, what does ""well-fine-tuned model"" refer to? Is this the VLM fine-tuned by the proposed method?
- Throughout the paper, what does $\tau_t^{t-1}$ represent?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. The proposed method may encounter a limitation if the users continuously add new languages because of the limited model capacity.,Gzuzpl4Jje,EMNLP_2023,"1. The original tasks’ performance degenerates to some extent and underperforms the baseline of the Adapter, which indicates the negative influence of removing some parts of the original networks.
2. The proposed method may encounter a limitation if the users continuously add new languages because of the limited model capacity.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', 'X', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- Line 54: Is 'interpretable' program relevant to the notion described in the work of 'Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608'?",NIPS_2020_593,NIPS_2020,"- Line 54: Is 'interpretable' program relevant to the notion described in the work of 'Doshi-Velez, F., & Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608'? - Line 19, 37, 39: A reference for the 'Influence maximization' problem may be provided. The distribution may be more formally given (e.g. which p_{ij} sum to 1). To be able to refer to the joint distributions, there should be a more concrete statement of the p_{ij}. Or maybe a preamble of line 103. - Line 52: Some more details about the polynomial time character of the formulation may clarify your statement about the LP. - Line 103: The strategy space of the adversary implied in the equation is strongly pessimistic (why consider all possible correlations?). This can be used in a follow up work. It seems that it does not reduce the value of the current model.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- The experiments are limited to MNIST and a single real-world dataset.,ICLR_2021_2892,ICLR_2021,"- Proposition 2 seems to lack an argument why Eq 16 forms a complete basis for all functions h. The function h appears to be defined as any family of spherical signals parameterized by a parameter in [-pi/2, pi/2]. If that’s the case, why eq 16? As a concrete example, let \hat{h}^\theta_lm = 1 if l=m=1 and 0 otherwise, so constant in \theta. The only constant associated Legendre polynomial is P^0_0, so this h is not expressible in eq 16. Instead, it seems like there are additional assumptions necessary on the family of spherical functions h to let the decomposition eq 16, and thus proposition 2, work. Hence, it looks like that proposition 2 doesn’t actually characterize all azimuthal correlations. - In its discussion of SO(3) equivariant spherical convolutions, the authors do not mention the lift to SO(3) signals, which allow for more expressive filters than the ones shown in figure 1. - Can the authors clarify figure 2b? I do not understand what is shown. - The architecture used for the experiments is not clearly explained in this paper. Instead the authors refer to Jiang et al. (2019) for details. This makes the paper not self-contained. - The authors appear to not use a fast spherical Fourier transform. Why not? This could greatly help performance. Could the authors comment on the runtime cost of the experiments? - The sampling of the Fourier features to a spherical signal and then applying a point-wise non-linearity is not exactly equivariant (as noted by Kondor et al 2018). Still, the authors note at the end of Sec 6 “This limitation can be alleviated by applying fully azimuthal-rotation equivariant operations.”. Perhaps the authors can comment on that? - The experiments are limited to MNIST and a single real-world dataset. - Out of the many spherical CNNs currently in existence, the authors compare only to a single one. For example, comparisons to SO(3) equivariant methods would be interesting. Furthermore, it would be interesting to compare to SO(3) equivariant methods in which SO(3) equivariance is broken to SO(2) equivariance by adding to the spherical signal a channel that indicates the theta coordinate. - The experimental results are presented in an unclear way. A table would be much clearer. - An obvious approach to the problem of SO(2) equivariance of spherical signals, is to project the sphere to a cylinder and apply planar 2D convolutions that are periodic in one direction and not in the other. This suffers from distortion of the kernel around the poles, but perhaps this wouldn’t be too harmful. An experimental comparison to this method would benefit the paper.
Recommendation: I recommend rejection of this paper. I am not convinced of the correctness of proposition 2 and proposition 1 is similar to equivariance arguments made in prior work. The experiments are limited in their presentation, the number of datasets and the comparisons to prior work.
Suggestions for improvement: - Clarify the issue around eq 16 and proposition 2 - Improve presentation of experimental results and add experimental details - Evaluate the model of more data sets - Compare the model to other spherical convolutions
Minor points / suggestions: - When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex. - In Def 1 in the equation it is confusing to have theta twice on the left-hand side. It would be clearer if h did not have a subscript on the left-hand side.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '1', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.",ICLR_2022_912,ICLR_2022,"1. The paper in general does not read well, and more careful proofreading is needed. 2. In S2D structure, it is not clear why the number of parameters does not change. If the kernel height/width stay the same, then its depth will increase, resulting in more parameters. I agree the efficiency could be improved since the FLOP is quadratic on activation side length. But in terms of parameters, more details are expected.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data. If otherwise, why [10] cannot use these side information?",NIPS_2016_499,NIPS_2016,"- The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data. If otherwise, why [10] cannot use these side information? - The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph? - Second rule in Lemma 2, i.e., Eq (7) and the definition of minimal conditional dependence seem to be conflicting. Taking Zâ in this definition to be the empty set, we should have that x and y are independent given W, but Eq. (7) says otherwise.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
3. The Atari game result (Section 7.2) is limited to a single game and a single baseline. It is very hard to interpret this. Less major weaknesses:,NIPS_2019_390,NIPS_2019,"1. The distinction between modeling uncertainty about the Q-values and modeling stochasticity of the reward (lines 119-121) makes some sense philosophically but the text should make clearer the practical distinction between this and distributional reinforcement learning. 2. It is not explained (Section 5) why the modifications made in Definition 5.1 aren't important in practice. 3. The Atari game result (Section 7.2) is limited to a single game and a single baseline. It is very hard to interpret this. Less major weaknesses: 1. The main text should make it more clear that there are additional experiments in the supplement (and preferably summarize their results). Questions: 1. You define a modified TD learning algorithm in Definition 5.1, for the purposes of theoretical analysis. Why should we use the original proposal (Algorithm 1) over this modified learning algorithm in practice? 2. Does this idea of propagating uncertainty not naturally combine with that of distributional RL, in that stochasticity of the reward might contribute to uncertainty about the Q-value? Typos, etc.: * Line 124, ""... when experimenting a transition ..."" ---- UPDATE: After reading the rebuttal, I have raised my score. I appreciate that the authors have included additional experiments and have explained further the difference between Definition 5.1 and the algorithm used in practice, as well as the distinction between the current work and distributional RL. I hope that all three of these additions will make their way into the final paper.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1) Unlike a factorized model with an IBP prior, the proposed method lacks a sparsity constraint in the number of factors used by subsequent tasks. As such, the model will not be incentivized to use less factors, leading to increasing number of factors and increased computation with more tasks.",ICLR_2022_3248,ICLR_2022,"compared to [1], which are advantages of the IBP. 1) Unlike a factorized model with an IBP prior, the proposed method lacks a sparsity constraint in the number of factors used by subsequent tasks. As such, the model will not be incentivized to use less factors, leading to increasing number of factors and increased computation with more tasks. 2) The IBP prior allows the data to dictate the number of factors to add for each task. The proposed method has no such mechanism, requiring setting the growth rate by hand using heuristics or a pre-determined schedule. Either is liable to over- or under-utilization of model capacity. Table 4 in the Experiments show that this does indeed have a significant impact on performance.
Overall, I think this is an example of convergent ideas rather than plagiarism, but a discussion of the connections is warranted.
Task incremental learning: This method requires knowing the task ID at test time to pick which factor selector weights to use. Without it, the proposed method doesn’t know which subnetwork to use, and would likely have to resort to trying all of them, which isn’t guaranteed to produce the right results. Recent continual learning methods are often evaluated in the more challenging class incremental setting, where task ID is not known.
Experiments 1. (+) Experiments are conducted on a good set of datasets 2. (+) Error bars are shown 3. (+) The proposed method mostly outperforms the baselines, especially on the more complex datasets. 4. (-) More baselines should be compared against, particularly dynamic architecture approaches, as that’s the category that this method falls under. Many of the compared methods don’t operate on the same set of continual learning assumptions as this paper; in particular, the replay-based methods are often using replay because they consider class incremental learning. 5. (-) Why are the results of Multitask learning so bad for S-CIFAR-100 and S-miniImageNet? My understanding is that it trains on all the data jointly, which should actually be the upper bound for a single model. 6. It would have been nice to visualize the factor selection matrices S for each task in order to visualize knowledge transfer.
Miscellaneous: 1. \citep should be used for parenthetical citations. 2. Initial double quote “ is backwards (Related Works). 3. “the first task,rk,1” 4. Figure 3 caption: “abd”
Questions: 1. How would you apply the weight factorization to 4D convolutional kernels?
[1] “Continual Learning using a Bayesian Nonparametric Dictionary of Weight Factors”, AISTATS 2021","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '4', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The presentation of the simulation study is not really doing a favor to the authors. Specifically, the authors do not really comment on why the GPC (benchmark) is performing better than BPC (their method). It would be worth re-iterating that this is b/c of the bandit feedback and not using information about the form of the cost function.",NIPS_2020_902,NIPS_2020,"- The paper could benefit from a better practical motivation, in its current form it will be quite hard for someone who is not at home in this field to understand why they should care about this work. What are specific practical examples in which the proposed algorithm would be beneficial? - The presentation of the simulation study is not really doing a favor to the authors. Specifically, the authors do not really comment on why the GPC (benchmark) is performing better than BPC (their method). It would be worth re-iterating that this is b/c of the bandit feedback and not using information about the form of the cost function. - More generally, the discussion of the simulation study results could be strenghtened. It is not really clear what the reader should take away from the results, and some discussion could help a lot with interpreting them properly.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- I might be helpful to quantify and clarify the claim ""ReLU does not work very well in very deep or in convolutional networks."" ReLUs were used in the AlexNet paper which, at the time, was considered deep and makes use of convolution (with pooling rather than ReLUs for the convolutional layers).",NIPS_2016_117,NIPS_2016,"weakness of this work is impact. The idea of ""direct feedback alignment"" follows fairly straightforwardly from the original FA alignment work. Its notable that it is useful in training very deep networks (e.g. 100 layers) but its not clear that this results in an advantage for function approximation (the error rate is higher for these deep networks). If the authors could demonstrate that DFA allows one to train and make use of such deep networks where BP and FA struggle on a larger dataset this would significantly enhance the impact of the paper. In terms of biological understanding, FA seems more supported by biological observations (which typically show reciprocal forward and backward connections between hierarchical brain areas, not direct connections back from one region to all others as might be expected in DFA). The paper doesn't provide support for their claim, in the final paragraph, that DFA is more biologically plausible than FA. Minor issues: - A few typos, there is no line numbers in the draft so I haven't itemized them. - Table 1, 2, 3 the legends should be longer and clarify whether the numbers are % errors, or % correct (MNIST and CIFAR respectively presumably). - Figure 2 right. I found it difficult to distinguish between the different curves. Maybe make use of styles (e.g. dashed lines) or add color. - Figure 3 is very hard to read anything on the figure. - I think this manuscript is not following the NIPS style. The citations are not by number and there are no line numbers or an ""Anonymous Author"" placeholder. - I might be helpful to quantify and clarify the claim ""ReLU does not work very well in very deep or in convolutional networks."" ReLUs were used in the AlexNet paper which, at the time, was considered deep and makes use of convolution (with pooling rather than ReLUs for the convolutional layers).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- In Figure 1, the reported perplexities are over 30, which looks pretty high. This high perplexity contradicts better BLEU scores in my experience. How did you calculate perplexity?",NIPS_2020_839,NIPS_2020,"- In Table 2, what about the performance of vanilla Transformer with the proposed approach? It's clearer to report the baseline + proposed approach, not only aiming at reporting state-of-the-art performance. - In Figure 1, the reported perplexities are over 30, which looks pretty high. This high perplexity contradicts better BLEU scores in my experience. How did you calculate perplexity?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. The evaluation needs experiments on distributed deployment and a larger model.,fL8AKDvELp,EMNLP_2023,"1. The paper needs a comprehensive analysis of sparse MoE, including the communication overhead (all to all). Currently, it's not clear where the performance gain comes from, basically, different number of experts incurs different communication overhead.
2. The evaluation needs experiments on distributed deployment and a larger model.
3. For the arguments that the existing approach has two key limitations, the authors should present key experiment results for demonstration.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors define rooted patterns (in a similar way to the orbit counting in GSN), but do not elaborate on why it is important for the patterns to be rooted, neither how they choose the roots. A brief discussion is expected, or if non-rooted patterns are sufficient, it might be better for the sake of exposition to discuss this case only in the supplementary material.",NIPS_2021_815,NIPS_2021,"- In my opinion, the paper is a bit hard to follow. Although this is expected when discussing more involved concepts, I think it would be beneficial for the exposition of the manuscript and in order to reach a larger audience, to try to make it more didactic. Some suggestions: - A visualization showing a counting of homomorphisms vs subgraph isomorphism counting. - It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper. - The authors define rooted patterns (in a similar way to the orbit counting in GSN), but do not elaborate on why it is important for the patterns to be rooted, neither how they choose the roots. A brief discussion is expected, or if non-rooted patterns are sufficient, it might be better for the sake of exposition to discuss this case only in the supplementary material. - The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 “Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets”), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes. - Comparison with GSN: The authors mention in section 2 that F-MPNNs are a unifying framework that includes GSNs. In my perspective, given that GSN is a quite similar framework to this work, this is an important claim that should be more formally stated. In particular, as shown by Curticapean et al., 2017, in order to obtain isomorphism counts of a pattern P, one needs not only to compute P-homomorphisms, but also those of the graphs that arise when doing “non-edge contractions” (the spasm of P). Hence a spasm(P)-MPNN would require one extra layer to simulate a P-GSN. I think formally stating this will give the interested reader intuition on the expressive power of GSNs, albeit not an exact characterisation (we can only say that P-GSN is at most as powerful as a spasm(P)-MPNN but we cannot exactly characterise it; is that correct?) - Also, since the concept of homomorphisms is not entirely new in graph ML, a more elaborate comparison with the paper by NT and Maehara, “Graph Homomorphism Convolution”, ICML’20 would be beneficial. This paper can be perceived as the kernel analogue to F-MPNNs. Moreover, in this paper, a universality result is provided, which might turn out to be beneficial for the authors as well.
Additional comments:
I think that something is missing from Proposition 3. In particular, if I understood correctly the proof is based on the fact that we can always construct a counterexample such that F-MPNNs will not be equally strong to 2-WL (which by the way is a stronger claim). However, if the graphs are of bounded size, a counterexample is not guaranteed to exist (this would imply that the reconstruction conjecture is false). Maybe it would help to mention in Proposition 3 that graphs are of unbounded size?
Moreover, there is a detail in the proof of Proposition 3 that I am not sure that it’s that obvious. I understand why the subgraph counts of C m + 1
are unequal between the two compared graphs, but I am not sure why this is also true for homomorphism counts.
Theorem 3: The definition of the core of a graph is unclear to me (e.g., what if P contains cliques of multiple sizes?)
In the appendix, the authors mention they used 16 layers for their dataset. That is an unusually large number of layers for GNNs. Could the authors comment on this choice?
In the same context as above, the experiments on the ZINC benchmark are usually performed with either ~100K or 500K parameters. Although I doubt that changing the number of parameters will lead to a dramatic change in performance, I suggest that the authors repeat their experiments, simply for consistency with the baselines.
The method of Bouritsas et al., arxiv’20 is called “Graph Substructure Networks” (instead of “Structure”). I encourage the authors to correct this.
After rebuttal
The authors have adequately addressed all my concerns. Enhancing MPNNs with structural features is a family of well-performing techniques that have recently gained traction. This paper introduces a unifying framework, in the context of which many open theoretical questions can be answered, hence significantly improving our understanding. Therefore, I will keep my initial recommendation and vote for acceptance. Please see my comment below for my final suggestions which, along with some improvements on the presentation, I hope will increase the impact of the paper.
Limitations: The limitations are clearly stated in section 1, by mainly referring to the fact that the patterns need to be selected by hand. I would also add a discussion on the computational complexity of homomorphism counting.
Negative societal impact: A satisfactory discussion is included in the end of the experimental section.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The paper addresses many times (Line 95-97, Line 308-310) that the consistency between training and inference can be easily satisfied due to the smoothness of neural models. I would suggest giving more explanations on this.",ARR_2022_178_review,ARR_2022,"__1. The relation between instance difficulty and training-inference consistency remains vague:__ This paper seems to try to decouple the concept of instance difficulty and training-inference consistency in current early exiting works. However, I don't think these two things are orthogonal and can be directly decoupled. Intuitively, according to the accuracy of the prediction, there are two main situations for training-inference inconsistency: the inconsistent exit makes the prediction during inference better than that during training, and vice versa. The first case is unlikely to occur. For the second case, considering instance difficulty reflects the decline in the prediction accuracy of the instances during training and inference, it may be regarded as the second case of training-inference consistency. Accordingly, I am still a little bit confused about the relation between instance difficulty and training-inference consistency after reading the paper. I would suggest that the authors calculate the token-level difficulty of the model before and after using the hash function, and perform more analysis on this basis. In fact, if the hash function is instance-level, the sentence-level difficulty of all baselines (including static and dynamic models) can be calculated, which will provide a more comprehensive and fair comparison.
__2. Lack of the analysis of the relation between instance-level consistency and token-level consistency:__ The core idea derived from the preliminary experiments is to enhance the instance-level consistency between training and inference, i.e., mapping semantically similar instances into the same exiting layer. However, the practical method introduces the consistency constrain at the token level. The paper doesn’t show that whether the token-level method can truely address or mitigate the inconsistency problem at the instance level. I would suggest the authors define metrics to reflect the instance-level and token-level consistency, and conduct an experiment to verify that whether they are correlated.
1. I didn’t follow the description of the sentence-level hash function from Line 324 to Line 328: If we use the sequence encoder (e.g., Sentence-BERT) as a hash function to directly map the instances to the exiting layer, why do we still need an internal classifier at that layer? And considering all the instances can be hashed by the pre-trained sequence encoder in advance before training (and early exiting), the appearance of label imbalance should not cause any actual harm? Why does it become a problem?
2. The paper addresses many times (Line 95-97, Line 308-310) that the consistency between training and inference can be easily satisfied due to the smoothness of neural models. I would suggest giving more explanations on this.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters?",NIPS_2017_356,NIPS_2017,"]
My major concerns about this paper is the experiment on visual dialog dataset. The authors only show the proposed model's performance on discriminative setting without any ablation studies. There is not enough experiment result to show how the proposed model works on the real dataset. If possible, please answer my following questions in the rebuttal.
1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters?
2: There are two test settings in visual dialog, while the Table 1 only shows the result on discriminative setting. It's known that discriminative setting can not apply on real applications, what is the result on generative setting?
3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The extra two hyperparameters introduced k and η require finetuning, which depends on availability to the environment or a good OPE method.",NIPS_2022_1598,NIPS_2022,"Weakness:
It is unclear whether the gain of BooT comes from 1. Extra data 2. Different architecture (pretrained gpt2 vs not) 3. Some inherent property in the sequence model as opposed to other world models that may only predict the observation and the reward.
It is unclear from the paper whether bootstrapping is novel beyond supervised learning (e.g. in RL)
There are quite a few additional limitations not mentioned in the paper (l349-350): 1. The extra two hyperparameters introduced k and η require finetuning, which depends on availability to the environment or a good OPE method. 2. As mentioned in l37-39, for other tasks in general, it is unclear whether the dataset available is sufficient to train a BooT, unless we try it, which will incur extra training time and cost, as mentioned in l349-350.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes:",NIPS_2016_153,NIPS_2016,"weakness of previous models. Thus I find these results novel and exciting.Modeling studies of neural responses are usually measured on two scales: a. Their contribution to our understanding of the neural physiology, architecture or any other biological aspect. b. Model accuracy, where the aim is to provide a model which is better than the state of the art. To the best of my understanding, this study mostly focuses on the latter, i.e. provide a better than state of the art model. If I am misunderstanding, then it would probably be important to stress the biological insights gained from the study. Yet if indeed modeling accuracy is the focus, it's important to provide a fair comparison to the state of the art, and I see a few caveats in that regard: 1. The authors mention the GLM model of Pillow et al. which is pretty much state of the art, but a central point in that paper was that coupling filters between neurons are very important for the accuracy of the model. These coupling filters are omitted here which makes the comparison slightly unfair. I would strongly suggest comparing to a GLM with coupling filters. Furthermore, I suggest presenting data (like correlation coefficients) from previous studies to make sure the comparison is fair and in line with previous literature. 2. The authors note that the LN model needed regularization, but then they apply regularization (in the form of a cropped stimulus) to both LN models and GLMs. To the best of my recollection the GLM presented by pillow et al. did not crop the image but used L1 regularization for the filters and a low rank approximation to the spatial filter. To make the comparison as fair as possible I think it is important to try to reproduce the main features of previous models. Minor notes: 1. Please define the dashed lines in fig. 2A-B and 4B. 2. Why is the training correlation increasing with the amount of training data for the cutout LN model (fig. 4A)? 3. I think figure 6C is a bit awkward, it implies negative rates, which is not the case, I would suggest using a second y-axis or another visualization which is more physically accurate. 4. Please clarify how the model in fig. 7 was trained. Was it on full field flicker stimulus changing contrast with a fixed cycle? If the duration of the cycle changes (shortens, since as the authors mention the model cannot handle longer time scales), will the time scale of adaptation shorten as reported in e.g Smirnakis et al. Nature 1997.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. It will be good to see some failure cases and related discussion.,NIPS_2016_43,NIPS_2016,"Weakness: 1. The organization of this paper could be further improved, such as give more background knowledge of the proposed method and bring the description of the relate literatures forward. 2. It will be good to see some failure cases and related discussion.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
5. It's unclear why there is a base layer GNN encoding in the proposed method. An ablation study on the necessity of the base layer GNN encoding would be helpful.,Fg04yPK0BH,ICLR_2025,"1. There is some disconnection between Proposition 2.2 that the adjacency matrix of the line graph has the same support of some unitary matrix and the proposed method which finds the projection of a weighted adjacency matrix to the set of unitary matrices. It is unclear to me if the result in Proposition 2.3 has the same support as the adjacency matrix of the line graph.
2. The computational complexity of the proposed method is very high as it involves taking the square root of a matrix of the size $2e \times 2e$ where $e$ is the number of edges in the graph. Though the block matrix structure can be exploited, but there is no guarantee how many blocks can be found in the matrix. For example, it's likely that the proposed method cannot perform on all of the LRGB datasets.
3. The experiments are not very convincing. They only compared with the one-hop variant of some models that aims to solve the oversquashing problem. Note that the oversquashing problem is intrinsically multi-hop and I don't see the rationale weakening the baseline models to one-hop.
4. The preprocessing time that involves the computation of the block matrix is not reported.
5. It's unclear why there is a base layer GNN encoding in the proposed method. An ablation study on the necessity of the base layer GNN encoding would be helpful.
6. On the Peptide dataset, the GCN can easily achieve the accuracy of the proposed method by some proper data preprocessing or normalization. The authors should provide a comparison following [1].
[1] Tönshoff, Jan, et al. ""Where did the gap go? Reassessing the long-range graph benchmark."" arXiv preprint arXiv:2309.00367 (2023).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. In Section 4.1, \epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \epsilon when (11) is discussed.",ICLR_2021_1189,ICLR_2021,"weakness of the paper is its experiments section. 1. Lack of large scale experiments: The models trained in the experiments section are quite small (80 hidden neurons for the MNIST experiments and a single convolutional layer with 40 channels for the SVHN experiments). It would be nice if there were at least some experiments that varied the size of the network and showed a trend indicating that the results from the small-scale experiments will (or will not) extend to larger scale experiments. 2. Need for more robustness benchmarks: It is impressive that the Lipschitz constraints achieved by LBEN appear to be tight. Given this, it would be interesting to see how LBEN’s accuracy-robustness tradeoff compare with other architectures designed to have tight Lipschitz constraints, such as [1]. 3. Possibly limited applicability to more structured layers like convolutions: Although it can be counted as a strength that LBEN can be applied to convnets without much modification, the fact that its performance considerably trails that of MON raises questions about whether the methods presented here are ready to be extended to non-fully connected architectures. 4. Lack of description of how the Lipschitz bounds of the networks are computed: This critique is self-explanatory.
Decision: I think this paper is well worthy of acceptance just based on the quality and richness of its theoretical development and analysis of LBEN. I’d encourage the authors to, if possible, strengthen the experimental results in directions including (but certainly not limited to) the ones listed above.
Other questions to authors: 1. I was wondering why you didn’t include experiments involving larger neural networks. What are the limitations (if any) that kept you from trying out larger networks? 2. Could you describe how you computed the Lipschitz constant? Given how notoriously difficult it is to compute bounds on the Lipschitz constants of neural networks, I think this section requires more elaboration.
Possible typos and minor glitches in writing: 1. Section 3.2, fist paragraph, first sentence: Should the phrase “equilibrium network” be plural? 2. D^{+} used in Condition 1 is used before it’s defined in Condition 2. 3. Just below equation (7): I think there’s a typo in “On the other size, […]”. 4. In Section 4.1, \epsilon is not used in equation (10), but in equation (11). It might be more clear to introduce \epsilon when (11) is discussed. 5. Section 4.2, in paragraph “Computing an equilibrium”, first sentence: Do you think there’s a grammar error in this sentence? I might also have mis-parsed the sentence. 6. Section 5, second sentence: There are two “the”s in a row.
[1] Anil, Cem, James Lucas, and Roger Grosse. ""Sorting out lipschitz function approximation."" International Conference on Machine Learning. 2019.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 “Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets”), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes.",NIPS_2021_815,NIPS_2021,"- In my opinion, the paper is a bit hard to follow. Although this is expected when discussing more involved concepts, I think it would be beneficial for the exposition of the manuscript and in order to reach a larger audience, to try to make it more didactic. Some suggestions: - A visualization showing a counting of homomorphisms vs subgraph isomorphism counting. - It might be a good idea to include a formal or intuitive definition of the treewidth since it is central to all the proofs in the paper. - The authors define rooted patterns (in a similar way to the orbit counting in GSN), but do not elaborate on why it is important for the patterns to be rooted, neither how they choose the roots. A brief discussion is expected, or if non-rooted patterns are sufficient, it might be better for the sake of exposition to discuss this case only in the supplementary material. - The authors do not adequately discuss the computational complexity of counting homomorphisms. They make brief statements (e.g., L 145 “Better still, homomorphism counts of small graph patterns can be efficiently computed even on large datasets”), but I think it will be beneficial for the paper to explicitly add the upper bounds of counting and potentially elaborate on empirical runtimes. - Comparison with GSN: The authors mention in section 2 that F-MPNNs are a unifying framework that includes GSNs. In my perspective, given that GSN is a quite similar framework to this work, this is an important claim that should be more formally stated. In particular, as shown by Curticapean et al., 2017, in order to obtain isomorphism counts of a pattern P, one needs not only to compute P-homomorphisms, but also those of the graphs that arise when doing “non-edge contractions” (the spasm of P). Hence a spasm(P)-MPNN would require one extra layer to simulate a P-GSN. I think formally stating this will give the interested reader intuition on the expressive power of GSNs, albeit not an exact characterisation (we can only say that P-GSN is at most as powerful as a spasm(P)-MPNN but we cannot exactly characterise it; is that correct?) - Also, since the concept of homomorphisms is not entirely new in graph ML, a more elaborate comparison with the paper by NT and Maehara, “Graph Homomorphism Convolution”, ICML’20 would be beneficial. This paper can be perceived as the kernel analogue to F-MPNNs. Moreover, in this paper, a universality result is provided, which might turn out to be beneficial for the authors as well.
Additional comments:
I think that something is missing from Proposition 3. In particular, if I understood correctly the proof is based on the fact that we can always construct a counterexample such that F-MPNNs will not be equally strong to 2-WL (which by the way is a stronger claim). However, if the graphs are of bounded size, a counterexample is not guaranteed to exist (this would imply that the reconstruction conjecture is false). Maybe it would help to mention in Proposition 3 that graphs are of unbounded size?
Moreover, there is a detail in the proof of Proposition 3 that I am not sure that it’s that obvious. I understand why the subgraph counts of C m + 1
are unequal between the two compared graphs, but I am not sure why this is also true for homomorphism counts.
Theorem 3: The definition of the core of a graph is unclear to me (e.g., what if P contains cliques of multiple sizes?)
In the appendix, the authors mention they used 16 layers for their dataset. That is an unusually large number of layers for GNNs. Could the authors comment on this choice?
In the same context as above, the experiments on the ZINC benchmark are usually performed with either ~100K or 500K parameters. Although I doubt that changing the number of parameters will lead to a dramatic change in performance, I suggest that the authors repeat their experiments, simply for consistency with the baselines.
The method of Bouritsas et al., arxiv’20 is called “Graph Substructure Networks” (instead of “Structure”). I encourage the authors to correct this.
After rebuttal
The authors have adequately addressed all my concerns. Enhancing MPNNs with structural features is a family of well-performing techniques that have recently gained traction. This paper introduces a unifying framework, in the context of which many open theoretical questions can be answered, hence significantly improving our understanding. Therefore, I will keep my initial recommendation and vote for acceptance. Please see my comment below for my final suggestions which, along with some improvements on the presentation, I hope will increase the impact of the paper.
Limitations: The limitations are clearly stated in section 1, by mainly referring to the fact that the patterns need to be selected by hand. I would also add a discussion on the computational complexity of homomorphism counting.
Negative societal impact: A satisfactory discussion is included in the end of the experimental section.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- line 108, the first âfâ should be âgâ in âwe fixed the form of ..â - extra â.â in the middle of a sentence in line 115. One Question: For the baseline MCL with deep learning, how did the author ensure that each of the networks have converged to a reasonable results. Cutting the learners early on might significantly affect the ensemble performance.",NIPS_2016_238,NIPS_2016,"- My biggest concern with this paper is the fact that it motivates âdiversityâ extensively (even the word diversity is in the title) but the model does not enforce diversity explicitly. I was all excited to see how the authors managed to get the diversity term into their model and got disappointed when I learned that there is no diversity. - The proposed solution is an incremental step considering the relaxation proposed by Guzman. et. al. Minor suggestions: - The first sentence of the abstract needs to be re-written. - Diversity should be toned down. - line 108, the first âfâ should be âgâ in âwe fixed the form of ..â - extra â.â in the middle of a sentence in line 115. One Question: For the baseline MCL with deep learning, how did the author ensure that each of the networks have converged to a reasonable results. Cutting the learners early on might significantly affect the ensemble performance.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"* Additional comments after reading the author response Thank you for your kind reply to my comments and questions. I believe that the draft will be further improved in the camera-ready version. One additional suggestion is that the title seems to be too general. The term ""adversarial training"" has a wide range of meanings, so it would be better to include your contribution in the title; for example, ""Improving Word Embeddings by Frequency-based Adversarial Training"" or something.",NIPS_2018_122,NIPS_2018,"- Figure 1 and 2 well motive this work, but in the main body of this paper I cannot see what happens to these figures after applying the proposed adversarial training. It is better to put together the images before and after applying your method in the same place. Figure 2 does not say anything about details (we can understand the very brief overview of the positions of the embeddings), and thus these figures could be smaller for better space usage. - For the LM and NMT models, did you use the technique to share word embedding and output softmax matrices as in [1]? The transformer model would do this, if the transformer implementations are based on the original paper. If so, your method affects not only the input word embeddings, but also the output softmax matrix, which is not a trivial side effect. This important point seems missing and not discussed. If the technique is not used, the strength of the proposed method is not fully realized, because the output word embeddings could still capture simple frequency information. [1] Inan et al., Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling, ICLR 2017. - There are no significance test or discussions about the significance of the score differences. - It is not clear how the BLEU improvement comes from the proposed method. Did you inspect whether rare words are more actively selected in the translations? Otherwise, it is not clear whether the expectations of the authors actually happened. - Line 131: The authors mention standard word embeddings like word2vec-based and glove-based embeddings, but recently subword-based embeddings are also used. For example, fasttex embeddings are aware of internal character n-gram information, which is helpful in capturing information about rare words. By inspecting the character n-grams, it is sometimes easy to understand rare words' brief properties. For example, in the case of ""Peking"", we can see the words start from a uppercase character and ends by the suffix ""ing"", etc. It makes this paper more solid to compare the proposed method with such character n-gram-based methods [2, 3]. [2] Bojanowski et al., Enriching Word Vectors with Subword Information, TACL. [3] Hashimoto et al., A Joint Many-Task Model: Growing a Neural Network for Multiple NLP Tasks, EMNLP 2017. *Minor comments: - Line 21: I think the statement ""Different from classic one-hot representation"" is not necessary, because anyway word embeddings are still based on such one-hot representations (i.e., the word indices). An embedding matrix is just a weight matrix for the one-hot representations. - Line 29: Word2vec is not a model, but a toolkit which implements Skipgram and CBOW with a few training options. - The results on Table 6 in the supplementary material could be enough to be tested on the dev set. Otherwise, there are too many test set results. * Additional comments after reading the author response Thank you for your kind reply to my comments and questions. I believe that the draft will be further improved in the camera-ready version. One additional suggestion is that the title seems to be too general. The term ""adversarial training"" has a wide range of meanings, so it would be better to include your contribution in the title; for example, ""Improving Word Embeddings by Frequency-based Adversarial Training"" or something.","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '0']}",,
"- No study of inference time. Since this is a pose estimation method that is direct and does not require detection or keypoint grouping, it is worth to compare its inference speed to previous top-down and bottom-up pose estimation method.",NIPS_2021_2191,NIPS_2021,"of the paper: [Strengths]
The problem is relevant.
Good ablation study.
[Weaknesses] - The statement in the intro about bottom up methods is not necessarily true (Line 28). Bottom-up methods do have a receptive fields that can infer from all the information in the scene and can still predict invisible keypoints. - Several parts of the methodology are not clear. - PPG outputs a complete pose relative to every part’s center. Thus O_{up} should contain the offset for every keypoint with respect to the center of the upper part. In Eq.2 of the supplementary material, it seems that O_{up} is trained to output the offset for the keypoints that are not farther than a distance \textit{r) to the center of corresponding part. How are the groundtruths actually built? If it is the latter, how can the network parts responsible for each part predict all the keypoints of the pose. - Line 179, what did the authors mean by saying that the fully connected layers predict the ground-truth in addition to the offsets? - Is \delta P_{j} a single offset for the center of that part or it contains distinct offsets for every keypoint? - In Section 3.3, how is G built using the human skeleton? It is better to describe the size and elements of G. Also, add the dimensions of G,X, and W to better understand what DGCN is doing. - Experiment can be improved: - For instance, the bottom-up method [9] has reported results on crowdpose dataset outperforming all methods in Table 4 with a ResNet-50 (including the paper one). It will be nice to include it in the tables - It will be nice to evaluate the performance of their method on the standard MS coco dataset to see if there is a drop in performance in easy (non occluded) settings. - No study of inference time. Since this is a pose estimation method that is direct and does not require detection or keypoint grouping, it is worth to compare its inference speed to previous top-down and bottom-up pose estimation method. - Can we visualize G, the dynamic graph, as it changes through DGCN? It might give an insight on what the network used to predict keypoints, especially the invisible ones.
[Minor comments]
In Algorithm 1 line 8 in Suppl Material, did the authors mean Eq 11 instead of Eq.4?
Fig1 and Fig2 in supplementary are the same
Spelling Mistake line 93: It it requires…
What does ‘… updated as model parameters’ mean in line 176
Do the authors mean Equation 7 in line 212?
The authors have talked about limitations in Section 5 and have mentioned that there are not negative societal impacts.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"27. In Theorem A.3 proof, how the input x has two indices? The input is a vector, not a matrix. Moreover, shouldn’t ∑ k ( W k ( 2 ) ) 2 = 1 / d , not d ?",ICLR_2023_642,ICLR_2023,"Unclear notations. The authors used the same notations to write vectors and scalars. Reading these notations would be challenging to follow for many readers. Please consider updating your notations and refer to the notation section in the Formatting Instructions template for ICLR 23.
The framework impact is unclear. The authors mentioned that the case of intrinsic but known bias and variance is often the case in computational neuroscience and neuromorphic engineering. This is the main motivation for their approach. However, the framework provided is limited to specific cases, namely, white noise and fixed bias. The authors argue that their assumptions are reasonable for most cases computational scientists and neuromorphic engineers face, but they don’t provide evidence for their claims. Clearly, this framework provides an important way for analyzing methods such as perturbed gradient descent methods with Gaussian noise, but it’s unclear how it can help analyze other cases. This suggests that the framework is quite limited.
The authors need to show that their choices and assumption are still useful for computational neuroscience and neuromorphic engineering. This can happen by referring to contributing and important works from these fields having known bias and variance with Gaussian noise.
In the experiments, the used bias is restricted to having the same magnitude for all weights ( b 1 →
). Can we reproduce the results if we use arbitrary biases? It would be better if the authors tried a number of arbitrary biases and averaged the results.
The paper is not well-placed in the literature. The authors didn’t describe the related works fully (e.g., stochastic gradient Langevin dynamics). This makes the work novelty unclear since the authors didn’t mention how analyzing the gradient estimator was done in earlier works and how their contribution is discernible from the earlier contributions. Mentioning earlier contributions increases the quality of your work and makes it distinguishable from other work. Please also refer to my comment in the novelty section.
Missing evidence of some claims and missing details. Here, I mention a few:
It’s not clear how increasing the width and/or depth can lower the trace of the Hessian (Section 2.1). If this comes from a known result/theory, please mention it. Otherwise, please show how it lowers the trace.
The authors mentioned that they use an analytical and empirical framework that is agnostic to the actual learning rule. However, the framework is built on top of a specific learning rule. It’s unclear what is meant by agnostic in this context (Section 1).
The authors mentioned in the abstract that the ideal amount of variance depends on the size and sparsity of the network, the norm of the gradient, and the curvature of the loss landscape. However, the authors didn’t mention the sparsity dependence anywhere in the paper.
The authors mentioned in a note after the proof of Theorem A.5 that it is also valid for Tanh but not Sigmoid. However, the proof assumed that the second derivative is zero. It’s unclear whether a similar derivation can be developed without this assumption. However, the authors only mention the relationship with the gain of ϕ ( . ) .
More information is needed on how the empirical likelihood of descent is computed (Fig. 7).
The use of MSE should be mentioned in Theorem A.3 since it’s not proven for any loss function. In addition, the dataset notation is wrong. It should be D = { ( x 1 , y 1 ) , . . . , ( x M , y M ) }
, where M
is the number of examples since it’s a set containing input-output pairs, not just a single pair.
The argument in Section 2.1 that increasing depth could theoretically make the loss less smooth is not related to the argument being made about variance. It is unclear how this is related to the analyses of how increasing depth affects the impact of the variance. I think it needs to be moved in the discussion on generalization instead.
A misplaced experiment that does not provide convincing evidence to support the theorems and lemmas developed in the paper with less experimental rigor (Fig. 1).
The experiment is misplaced being at the introduction section. This hurts the introduction and makes the reader less focused on your logic to motivate your work.
It’s not clear from the figure what the experiment is. The reader has to read appendix B2 to be able to continue reading your introduction, which is unnecessary.
The results are shown with only three seeds. This is not enough and cannot create any statistical significance in your experiment. I suggest increasing the number of runs to 20 or 30.
It’s unclear why batch gradient descent is used instead of gradient descent with varying bias and variance. Using batch gradient descent might undesirably add to the bias and variance.
The experiment results are not consistent with the rest of the paper. We cannot see the relationship when varying the bias or variance similar to other experiments. Looking at Fig.1B where bias=0, for example, we find that adding a small amount of variance reduces performance, but adding more improves performance up to a limit. This is not the case with the other experiments, though. I suggest following the previous two points to make the results aligned with the rest of your results.
Alternative hypotheses can be made with some experiments. The experiment in Fig. 3.A needs improvement. The authors mention that excessive amounts of variance and/or bias can hinder learning performance. In Fig. 3. A, they only show levels of variance that help decrease loss. An alternative explanation from their figure is that by increasing the variance, the performance improves. This is not the case, of course, so I think the authors need to add more variance curves that hinder performance to avoid alternative interpretations.
Minor issues that didn’t impact the score:
There are nine arXiv references. If they are published, please add this information instead of citing arXiv.
What is a norm N
vector? Can you please add the definition to the paper?
You mentioned that the step size has to be very small. However, in Fig. 1, the step size used is large (0.02). Can you please explain why? Can this be an additional reason why there is no smooth relationship between the values of the variance and performance?
No error bars are added in Fig. 4 or Fig. 7. Can you please add them?
In experiments shown in Fig. 3 and Fig. 5, the number of runs used to create the error bars is not mentioned in Appendix B.2.
A missing 2 D
in Eq. 27.
In Theorem A.3 proof, how the input x
has two indices? The input is a vector, not a matrix. Moreover, shouldn’t ∑ k ( W k ( 2 ) ) 2 = 1 / d
, not d ?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- In section 4.3 and 4.4, words such as “somewhat” and “good generative ability” appears in the description yet I am concerned that even with beam search, only 77% of the result lists contain the ground truth logical forms. If the relationships and entities were replaced, how do we ensure that the plugged-in entities/relationships were the right one? In what percentage were the right entities/relationships being plugged in if no ground truth is available?",tqhAA26vXE,ICLR_2024,"- In section 4.3 and 4.4, words such as “somewhat” and “good generative ability” appears in the description yet I am concerned that even with beam search, only 77% of the result lists contain the ground truth logical forms. If the relationships and entities were replaced, how do we ensure that the plugged-in entities/relationships were the right one? In what percentage were the right entities/relationships being plugged in if no ground truth is available?
- In section 4.5, the authors claim that Graph-Query-of-Thoughts are a way to improve QA’s interpretability and avoid LLM’s hallucinations, which has no evidence support in the result/analysis section. This seems to be an exaggerated claim and I am not convinced.
- Presentation of the paper needs improvement. Multiple grammatical errors, and the description of the method is confusing. Explanation of methods like QLora.etc can be moved to related work, since now it is interrupting the flow of the writing.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- I found the application to differential privacy unconvincing (see comments below) - Experimental validation was a bit light and felt preliminary RECOMMENDATION: I think this paper should be accepted into the NIPS program on the basis of the online algorithm and analysis. However, I think the application to differential privacy, without experimental validation, should be omitted from the main paper in favor of the preliminary experimental evidence of the tensor method. The results on privacy appear too preliminary to appear in a ""conference of record"" like NIPS. TECHNICAL COMMENTS:",NIPS_2016_395,NIPS_2016,"- I found the application to differential privacy unconvincing (see comments below) - Experimental validation was a bit light and felt preliminary RECOMMENDATION: I think this paper should be accepted into the NIPS program on the basis of the online algorithm and analysis. However, I think the application to differential privacy, without experimental validation, should be omitted from the main paper in favor of the preliminary experimental evidence of the tensor method. The results on privacy appear too preliminary to appear in a ""conference of record"" like NIPS. TECHNICAL COMMENTS: 1) Section 1.2: the dimensions of the projection matrices are written as $A_i \in \mathbb{R}^{m_i \times d_i}$. I think this should be $A_i \in \mathbb{R}^{d_i \times m_i}$, otherwise you cannot project a tensor $T \in \mathbb{R}^{d_1 \times d_2 \times \ldots d_p}$ on those matrices. But maybe I am wrong about this... 2) The neighborhood condition in Definition 3.2 for differential privacy seems a bit odd in the context of topic modeling. In that setting, two tensors/databases would be neighbors if one document is different, which could induce a change of something like $\sqrt{2}$ (if there is no normalization, so I found this a bit confusing. This makes me think the application of the method to differential privacy feels a bit preliminary (at best) or naive (at worst): even if a method is robust to noise, a semantically meaningful privacy model may not be immediate. This $\sqrt{2}$ is less than the $\sqrt{6}$ suggested by the authors, which may make things better? 3) A major concern I have about the differential privacy claims in this paper is with regards to the noise level in the algorithm. For moderate values of $L$, $R$, and $K$, and small $\epsilon = 1$, the noise level will be quite high. The utility theorem provided by the author requires a lower bound on $\epsilon$ to make the noise level sufficiently low, but since everything is in ""big-O"" notation, it is quite possible that the algorithm may not work at all for reasonable parameter values. A similar problem exists with the Hardt-Price method for differential privacy (see a recent ICASSP paper by Imtiaz and Sarwate or an ArXiV preprint by Sheffet). For example, setting L=R=100 and K=10, \epsilon = 1, \delta = 0.01 then the noise variance is of the order of 4 x 10^4. Of course, to get differentially private machine learning methods to work in practice, one either needs large sample size or to choose larger $\epsilon$, even $\epsilon \gg 1$. Having any sense of reasonable values of $\epsilon$ for a reasonable problem size (e.g. in topic modeling) would do a lot towards justifying the privacy application. 4) Privacy-preserving eigenvector computation is pretty related to private PCA, so one would expect that the authors would have considered some of the approaches in that literature. What about (\epsilon,0) methods such as the exponential mechanism (Chaudhuri et al., Kapralov and Talwar), Laplace noise (the (\epsilon,0) version in Hardt-Price), or Wishart noise (Sheffet 2015, Jiang et al. 2016, Imtiaz and Sarwate 2016)? 5) It's not clear how to use the private algorithm given the utility bound as stated. Running the algorithm is easy: providing $\epsilon$ and $\delta$ gives a private version -- but since the $\lambda$'s are unknown, verifying if the lower bound on $\epsilon$ holds may not be possible: so while I get a differentially private output, I will not know if it is useful or not. I'm not quite sure how to fix this, but perhaps a direct connection/reduction to Assumption 2.2 as a function of $\epsilon$ could give a weaker but more interpretable result. 6) Overall, given 2)-5) I think the differential privacy application is a bit too ""half-baked"" at the present time and I would encourage the authors to think through it more clearly. The online algorithm and robustness is significantly interesting and novel on its own. The experimental results in the appendix would be better in the main paper. 7) Given the motivation by topic modeling and so on, I would have expected at least an experiment on one real data set, but all results are on synthetic data sets. One problem with synthetic problems versus real data (which one sees in PCA as well) is that synthetic examples often have a ""jump"" or eigenvalue gap in the spectrum that may not be observed in real data. While verifying the conditions for exact recovery is interesting within the narrow confines of theory, experiments are an opportunity to show that the method actually works in settings where the restrictive theoretical assumptions do not hold. I would encourage the authors to include at least one such example in future extended versions of this work.","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
- In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor:,NIPS_2016_283,NIPS_2016,"weakness of the paper are the empirical evaluation which lacks some rigor, and the presentation thereof: - First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what ""error""?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as ""sub-standard"". - The results comparing standard- vs. evolutional dropout on shallow models should be presented as a mean over many runs (at least 10), ideally with error-bars. The plotted curves are obviously from single runs, and might be subject to significant fluctuations. Also the models are small, so there really is no excuse for not providing statistics. - I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark: - In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor: *","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The performance of FedPCL heavily relies on the selection of different pre-trained models, limiting its applications to more wide areas. As shown in Table 4, the model accuracy is quite sensitive to the pre-trained models. This work adequately addressed the limitations. The authors developed a lightweight federated learning framework to reduce the computation and communication costs and integrated pre-trained models to extract prototypes for federated aggregation. The is a new try for federated learning.",NIPS_2022_947,NIPS_2022,"1. Apart from the multiple pre-trained models, FedPCL is built on the idea of prototypical learning and contrastive learning, which are not new in federated learning. 2. The performance of FedPCL heavily relies on the selection of different pre-trained models, limiting its applications to more wide areas. As shown in Table 4, the model accuracy is quite sensitive to the pre-trained models.
This work adequately addressed the limitations. The authors developed a lightweight federated learning framework to reduce the computation and communication costs and integrated pre-trained models to extract prototypes for federated aggregation. The is a new try for federated learning.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
- It would have been interesting to see not only the retrieved and final attentions but also the tentative attention maps in the qualitative figures.,NIPS_2017_356,NIPS_2017,"- I would have liked to see some analysis about the distribution of the addressing coefficients (Betas) with and without the bias towards sequential addressing. This difference seems to be very important for the synthetic task (likely because each question is based on the answer set of the previous one). Also I don't think the value of the trade-off parameter (Theta) was ever mentioned. What was it and how was it selected? If instead of a soft attention, the attention from the previous question was simply used, how would that baseline perform?
- Towards the same point, to what degree does the sequential bias affect the VisDial results?
- Minor complaint. There are a lot of footnotes which can be distracting, although I understand they are good for conserving clarity / space while still providing useful details.
- Does the dynamic weight prediction seem to identify a handful of modes depending on the question being asked? Analysis of these weights (perhaps tSNE colored by question type) would be interesting.
- It would have been interesting to see not only the retrieved and final attentions but also the tentative attention maps in the qualitative figures.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
3. This author should add more description about the contribution of this paper.,ICLR_2023_3208,ICLR_2023,"1.The typesetting in some places is out of order, such as equations (23), (25), and (31). 2. In Page 4, ""A4 bounds the degree of non-stationarity between consecutive iterations"", why this assumption holds? 3. This author should add more description about the contribution of this paper.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"4. Minor comments / suggestions a. The main contributions are introducing two types of attention for deep VAEs, it might help to describe them in a separate section, and only then describe the generative and inference models. Right now the description of the layer-wise attention mechanism is scattered across sections 2.3 and 2.4. b. tricks like normalisation or feature scaling could be referenced in a separate section. c.",ICLR_2022_1012,ICLR_2022,"a. The paper lacks structure and clarity
b. The paper lacks a more qualitative study of the model:
it would be interesting to see what layers the layer-wise attention mechanism attends to.
it would be great to understand how this model uses the latent variables, for instance by measuring the KL divergence at each layer, as done in the previous work (LVAE, BIVA) (connection to ""posterior collapse"").
c. Experiments are limited to CIFAR-10, larger scaler experiments (i.e. ImageNet) would be beneficial to the paper. It is not guaranteed that such an architecture would translate in the same gains for larger datasets (i.e. ImageNet).
3. Clarification needed:
a. Table 5: I interpreted the column ""non-local layers"" as using ""attention across layers"", I hope I was right. The nomenclature needs to be improved.
b. Is the layer-wise attention mechanism specific to deep VAEs, or can it be more generally applied to ResNet architectures?
c. Section 2.3 (paragraph cited bellow): I get the idea, but unless demonstrated, this remains a hypothesis.
""... in practice the network may no longer respect the factorization of the prior p ( z ) = ∏ l p ( z l ∣ z < l )
leading to diminished performance gains as shown in Table 1"":
4. Minor comments / suggestions
a. The main contributions are introducing two types of attention for deep VAEs, it might help to describe them in a separate section, and only then describe the generative and inference models. Right now the description of the layer-wise attention mechanism is scattered across sections 2.3 and 2.4.
b. tricks like normalisation or feature scaling could be referenced in a separate section.
c. eq8: you might want to cite ReZero [1] here
d. Fig 1. a: the lack of arrows going from the activations ( h l , k l q )
to the attention block ( A ( . . . ) )
was confusing on the first read
e. It would be better practice to report likelihoods for multiple random seeds
f. Typo in section 2.1: ""both q ( z x ) and p ( x )
are fully factorized gaussian..."" -> ""both q ( z x ) and p ( z )
are fully factorized gaussian...""
[1] Bachlechner, T., Prasad Majumder, B., Mao, H. H., Cottrell, G. W., and McAuley, J., “ReZero is All You Need: Fast Convergence at Large Depth”, <i>arXiv e-prints</i>, 2020.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- Either I don't understand Figure 5 or the labels are wrong.,ICLR_2022_1393,ICLR_2022,"I think that:
The comparison to baselines could be improved.
Some of the claims are not carefully backed up.
The explanation of the relationship to the existing literature could be improved.
More details on the above weaknesses:
Comparison to baselines:
""We did not find good benchmarks to compare our unsupervised, iterative inferencing algorithm against"" I think this is a slightly unfair comment. The unsupervised and iterative inferencing aspects are only positives if they have the claimed benefits, as compared to other ML methods (more accurate and better generalization). There is a lot of recent work addressing the same ML task (as mentioned in the related work section.) This paper contains some comparisons to previous work, but as I detail below, there seem to be some holes.
FCNN is by far the strongest competitor for the Laplace example in the appendix. Why is this left off of the baseline comparison table in the main paper? Further, is there any reason that FCNN couldn't have been used for the other examples?
Why is FNO not applied to the Chip cooling (Temperature) example?
A major point in this paper is improved generalization across PDE conditions. However, I think that's hard to check when only looking at the test errors for each method. In other words, is CoAE-MLSim's error lower than UNet's error because the approach fit the training data better, or is it because it generalized better? Further, in some cases, it's not obvious to me if the test errors are impressive, so maybe it is having a hard time generalizing. It would be helpful to see train vs. test errors, and ideally I like to see train vs. val. vs. test.
For the second main example (vortex decay over time), looking at Figures 8 and 33 (four of the fifty test conditions), CoAE-MLSim has much lower error than the baselines in the extrapolation phase but noticeably higher in the interpolation phase. In some cases, it's hard to tell how close the FNO line is to zero - it could be that CoAE-MLSim even has orders of magnitude more error. Since we can see that there's a big difference between interpolation and extrapolation, it would be helpful to see the test error averaged over the 50 test cases but not averaged over the 50 time steps. When averaged over all 50 time steps for the table on page 9, it could be that CoAE-MLSim looks better than FNO just because of the extrapolation regime. In practice, someone might pick FNO over CoAE-MLSim if they aren't interested in extrapolating in time. Do the results in the table for vortex decay back up the claim that CoAE-MLSim is generalizing over initial conditions better than FNO, or is it just better at extrapolation in time?
Backing up claims:
The abstract says that the method is tested for a variety of cases to demonstrate a list of things, including ""scalability."" The list of ""significant contributions"" also includes ""This enables scaling to arbitrary PDE conditions..."" I might have missed/forgotten something, but I think this wasn't tested?
""Hence, the choice of subdomain size depends on the trade-off between speed and accuracy."" This isn't clear to me from the results. It seems like 32^3 is the fastest and most accurate?
I noticed some other claims that I think are speculations, not backed up with reported experiments. If I didn't miss something, this could be fixed by adding words like ""might.""
""Physics constrained optimization at inference time can be used to improve convergence robustness and fidelity with physics.""
""The decoupling allows for better modeling of long range time dynamics and results in improved stability and generalizability.""
""Each solution variable can be trained using a different autoencoder to improve accuracy.""
""Since, the PDE solutions are dependent and unique to PDE conditions, establishing this explicit dependency in the autoencoder improves robustness.""
""Additionally, the CoAE-MLSim apprach solves the PDE solution in the latent space, and hence, the idea of conditioning at the bottleneck layer improves solution predictions near geometry and boundaries, especially when the solution latent vector prediction has minor deviations.""
""It may be observed that the FCNN performs better than both UNet and FNO and this points to an important aspect about representation of PDE conditions and its impact on accuracy."" The representation of the PDE conditions could be why, but it's hard to say without careful ablation studies. There's a lot different about the networks.
Similarly: ""Furthermore, compressed representations of sparse, high-dimensional PDE conditions improves generalizability.""
Relationship to literature:
The citation in this sentence is abrupt and confusing because it sounds like CoAE-MLSim is a method from that paper instead of the new method: ""Figure 4 shows a schematic of the autoencoder setup used in the CoAE-MLSim (Ranade et al., 2021a)."" More broadly, Ranade et al., 2021a, Ranade et al., 2021b, and Maleki, et al., 2021 are all cited and all quite related to this paper. It should be more clear how the authors are building on those papers (what exactly they are citing them for), and which parts of CoAE-MLSim are new. (The Maleki part is clearer in the appendix, but the reader shouldn't have to check the appendix to know what is new in a paper.)
I thought that otherwise the related work section was okay but was largely just summarizing some papers without giving context for how they relate to this paper.
Additional feedback (minor details, could fix in a later version, but no need to discuss in the discussion phase):
- The abstract could be clearer about what the machine learning task is that CoAE-MLSim addresses.
- The text in the figures is often too small.
- ""using pre-trained decoders (g)"" - probably meant g_u?
- Many of the figures would be more clear if they said pre-trained solution encoders & solution decoders, since there are multiple types of autoencoders.
- The notation is inconsistent, especially with nu. For example, the notation in Figures 2 & 3 doesn't seem to match the notation in Alg 1. Then on Page 4 & Figure 4, the notation changes again.
- Why is the error table not ordered 8^3, 16^3, 32^3 like Figure 9? The order makes it harder for the reader to reason about the tradeoff.
- Why is Err(T_max) negative sometimes? Maybe I don't understand the definition, but I would expect to use absolute value?
- I don't think the study about different subdomain sizes is an ""ablation"" study since they aren't removing a component of the method.
- Figure 11: I'm guessing that the y-axis is log error, but this isn't labeled as such. I didn't really understand the the legend or the figure in general until I got to the appendix, since there's little discussion of it in the main paper.
- ""Figure 30 shows comparisons of CoAE-MLSim with Ansys Fluent for 4 unseen objects in addition to the example shown in the main paper."" - probably from previous draft. Now this whole example is in the appendix, unless I missed something.
- My understanding is that each type of autoencoder is trained separately and that there's an ordering that makes sense to do this in, so you can use one trained autoencoder for the next one (i.e. train the PDE condition AEs, then the PDE solution AE, then the flux conservation AE, then the time integration AE). This took me a while to understand though, so maybe this could be mentioned in the body of the paper. (Or perhaps I missed that!)
- It seems that the time integration autoencoder isn't actually an autoencoder if it's outputting the solution at the next time step, not reconstructing the input.
- Either I don't understand Figure 5 or the labels are wrong.
- It's implied in the paper (like in Algorithm 1) that the boundary conditions are encoded like the other PDE conditions. In the Appendix (A.1), it's stated that ""The training portion of the CoAE-MLSim approach proposed in this work corresponds to training of several autoencoders to learn the representations of PDE solutions, conditions, such as geometry, boundary conditions and PDE source terms as well as flux conservation and time integration."" But then later in the appendix (A.1.3), it's stated that boundary conditions could be learned with autoencoders but are actually manually encoded for this paper. That seems misleading.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Missing supervised baselines. Since most experiments are done on datasets of scale ~100k images, it is reasonable to assume that full annotation is available for a dataset at this scale in practice. Even if it isn’t, it’s an informative baseline to show where these self-supervised methods are at comparing to a fully supervised pre-trained network.",NIPS_2021_2257,NIPS_2021,"- Missing supervised baselines. Since most experiments are done on datasets of scale ~100k images, it is reasonable to assume that full annotation is available for a dataset at this scale in practice. Even if it isn’t, it’s an informative baseline to show where these self-supervised methods are at comparing to a fully supervised pre-trained network. - The discussion in section 3 is interesting and insightful. The authors compared training datasets such as object-centric versus scene-centric ones, and observed different properties that the model exhibited. One natural question is then what would happen if a model is trained on \emph{combined} datasets. Can the SSL model make use of different kinds of data? - The authors compared two-crop and multi-crop augmentation in section 4, and observed that multi-crop augmentation yielded better performance. One important missing factor is the (possible) computation overhead of multi-crop strategies. My estimation is that it would increase the computation complexity (i.e., slowing the speed) of training. Therefore, one could argue that if we could train the two-crop baseline for a longer period of time it would yield better performance as well. To make the comparison fair, the computation overhead must be discussed. It can also be seen from Figure 7, for the KNN-MoCo, that the extra positive samples are fed into the network \emph{that takes the back-propagated gradients}. It will drastically increase training complexity as the network not only performs forward passing, but also the backward passing as well. - Section 4.2 experiments with AutoAugment as a stronger augmentation strategy. One possible trap is that AutoAugment’s policy is obtained by supervise training on ImageNet. Information leaking is likely.
Questions - In L114 the authors concluded that for linear classification the pretraining dataset should match the target dataset in terms of being object or-scene centric. If this is true, is it a setback for SSL algorithms that strive to learn more generic representations? Then it goes back again to whether by combining two datasets SSL model can learn better representations. - In L157 the authors discussed that for transfer learning potentially only low- and mid-level visual features are useful. My intuition is that low- and mid-level features are rather easy to learn. Then how does it explain the model’s transferability increasing when we scale up pre-training datasets? Or the recent success of CLIPs? Is it possible that \emph{only} MoCo learns low- and mid-level features?
Minor things that don’t play any role in my ratings. - “i.e.” -> “i.e.,”, “e.g.” -> “e.g.,” - In Eq.1, it’s better to write L_{contrastive}(x) = instead of L_{contrastive}. Also, should the equation be normalized by the number of positives? - L241 setup paragraph is overly complicated for an easy-to-explain procedure. L245/246, the use of x+ and x is very confusing. - It’s better to explain that “nearest neighbor mining” in the intro is to mine nearest neighbor in a moving embedding space in the same dataset.
Overall, I like the objective of the paper a lot and I think the paper is trying to answer some important questions in SSL. But I have some reservation to confidently recommend acceptance due to the concerns as written in the “weakness” section, because this is an analysis paper and analysis needs to be rigorous. I’ll be more than happy to increase the score if those concerns are properly addressed in the feedback.
The authors didn't discuss the limitations of the study. I find no potential negative societal impact.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. **Performance differences between methods are minimal across evaluations**. In nearly all results, the performance differences between the methods are less than 1 percentage point, which may be attributable to random variation. Furthermore, the benchmarks selected are outdated and likely saturated. [1] [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)",WzUPae4WnA,ICLR_2025,"1. **The motivation of this paper appears to be questionable.** The authors claim that DoRA increases the risk of overfitting, basing this on two pieces of evidence:
- DoRA introduces additional parameters compared to LoRA.
- The gap between training and test accuracy curves for DoRA is larger than that of BiDoRA.
However, these two points do not convincingly support the claim. First, while additional parameters can sometimes contribute to overfitting, they are not a sufficient condition for it. In fact, DoRA adds only a negligible number of parameters (0.01% of the model size, as reported by the authors) beyond LoRA. Moreover, prior work [1] suggests that LoRA learns less than full fine-tuning and may even act as a form of regularization, implying that the risk of overfitting is generally low across these PEFT methods.
Additionally, the training curves are not necessarily indicative of overfitting, as they can be significantly influenced by factors such as hyperparameters, model architecture, and dataset characteristics. The authors present results from only a single configuration, which limits the generalizability of their findings.
Finally, the authors’ attribution of an *alleged overfitting problem* to DoRA’s concurrent training lacks a strong foundation.
2. **The proposed BiDoRA method is overly complex and difficult to use.** It requires a two-phase training process, with the first phase itself consisting of two sub-steps. It also introduces two additional hyperparameters: the weight of orthogonality regularization and a ratio for splitting training and validation sets. As a result, BiDoRA takes 3.92 times longer to train than LoRA.
3. **Performance differences between methods are minimal across evaluations**. In nearly all results, the performance differences between the methods are less than 1 percentage point, which may be attributable to random variation. Furthermore, the benchmarks selected are outdated and likely saturated.
[1] [LoRA Learns Less and Forgets Less](https://arxiv.org/abs/2405.09673)","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '4', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- Why does the method help on Hopper, which has deterministic dynamics, so given (s, a), there is a unique s', and in this case, it simply reduces to action-conditional masking? Can it be evaluated on some other domains with non-deterministic dynamics to evaluate its empirical efficacy? Otherwise empirically it seems like the method doesn't seem to have much benefit. Why is BEAR missing from baselines?",NIPS_2020_106,NIPS_2020,"I also feel that the paper could have benefited from a discussion of these as compared to just outrightly saying that existing methods do not give us good results. In particular, the conditions under which existing methods work vs do not work should have been discussed more explicitly than what it is right now in the paper. Moreover, I think the experiments on cartpole and hopper are not indicative of their method's performance since these have determnisitc dynamics and the dataset was collected as trajectories (so s' is as frequent as s in the distribution \mu, see my point below) and hence their choice of masking reduces to action conditioned masking only. Some other questions that I have: - From the analysis perspective, the paper says that prior works such as Kumar et al. 2019 that use action conditional and concentrability do not get the same error rate. Is the main issue behind this limitation that the notion of concentrability used in Kumar et al. and other works is trajectory centric and not on the state-action marginal? The latter was shown to be better than trajectory-based concentrability in Chen and Jiang (2019). If this notion of concentrability is used, would that be sufficient to get rid of the concentrability assumptions in your work. - Why does the method help on Hopper, which has deterministic dynamics, so given (s, a), there is a unique s', and in this case, it simply reduces to action-conditional masking? Can it be evaluated on some other domains with non-deterministic dynamics to evaluate its empirical efficacy? Otherwise empirically it seems like the method doesn't seem to have much benefit. Why is BEAR missing from baselines? - It seems like when the batch of data is a set of trajectories, and the state space is continuous, then the density of s_t is the same as s_{t+1} which is 1/N, so in that case does the proposed algorithm which exploits the fact that s_{t+1} may be highly infrequent compared to s_{t} reduce simply to an action conditional? The experiments are done with this setup too it seems. - Building on the previous point, if the data comes from d^\mu: the state visitation distribution of a behavior policy \mu, then d^\mu(s) and d^\mu(s') for a transition (s, a, r, s') observed in the dataset shouldn't be very different, in that case, would the proposed method be not (much) better than action-conditional penalty methods that have mostly been studied in this domain? - How do you compare algorithms, theoretically, for different values of b, and the hyperparameters for other algorithms, such as \eps in BEAR? It seems like for some value of both of these quantities, the algorithms should perform safely and not have much error accumulation. So, how is the proposed method better theoretically than the best configuration of the prior methods? - How will the proposed method compare to residual gradient algorithms which have better guarantees?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. It would be better if the author could provide some theocratical justification in terms of why co-training and weight averaging can improve results, since they are important for the performance.",NIPS_2020_1706,NIPS_2020,"1. The memorization effect is not new to the community. Therefore, the novelty of this paper is not sufficiently demonstrated. The authors need to be clearer what extra insights this paper gives. 2. It would be better if the author could provide some theocratical justification in terms of why co-training and weight averaging can improve results, since they are important for the performance. 3. The empirical performance does not seem to be very strong compared to DivideMix. Some explanations are needed.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"- it would seem to me that in section 4, ""X"" should be a multiset (and [\cal X]**n the set of multisets of size n) instead of a set, since in order the histogram to honestly represent a graph that has repeated vertex or edge labels, you need to include the multiplicities of the labels in the graph as well.",NIPS_2016_182,NIPS_2016,"weakness of the technique in my view is that the kerne values will be dependent on the dataset that is being used. Thus, the effectiveness of the kernel will require a rich enough dataset to work well. In this respect, the method should be compared to the basic trick that is used to allos non-PSD similarity metrics to be used in kernel methods, namely defining the kernel as k(x,x') = (s(x,z_1),...,s(x,z_N))^T(s(x',z_1),...,s(x',z_N)), where s(x,z) is a possibly non-PSD similarity metric (e.g. optimal assignment score between x and z) and Z = {z_1,...,z_n} is a database of objects to compared to. The write-up is (understandably) dense and thus not the easiest to follow. However, the authors have done a good job in communicating the methods efficiently. Technical remarks: - it would seem to me that in section 4, ""X"" should be a multiset (and [\cal X]**n the set of multisets of size n) instead of a set, since in order the histogram to honestly represent a graph that has repeated vertex or edge labels, you need to include the multiplicities of the labels in the graph as well. - In the histogram intersection kernel, it think for clarity, it would be good to replace ""t"" with the size of T; there is no added value to me in allowing ""t"" to be arbitrary.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1) Only the sum of the gradient is taken into account (so it could be that a component a_w_{i,j} has a very negative gradient, but still the sum will be positive), and",ICLR_2022_1998,ICLR_2022,"In Section 3. The paper uses a measure ρ
that is essentially the fraction of examples at which local monotonicity (in any of the prescribed directions in M
) is violated and then show that this measure decreases when using the paper's method over the baselines. However, I'm not certain that this measure corresponds to the global monotonicity requirment that is often desired in practice: namely, the one that appears in Definition 1. For example, consider a 1
-D function over [ 1 , 99.99 ]
whose graph is a piecewise linear curve connecting the points ( 0 , 100 ) , ( 0.99 , 100.99 ) , ( 1 , 99 ) , ( 1.99 , 99.99 ) , ( 2 , 98 ) , ( 2.99 , 98.99 ) , . . . , ( 99 , 1 ) , ( 99.99 , 1.99 )
. This function has nonnegative derivative at about 99
of its domain, yet if one chooses two points x 1 , x 2
uniformaly and independently from the domain, then there's at least a 97
chance that f ( m i n x 1 , x 2 ) > f ( m a x x 1 , x 2 )
. I think, therefore, that it would be good to complement the local ρ
with an estimate of the probability that Definition 1 would not hold over the distribution in question (training, test or random).
Section 4.1 The authors introduce the notion of group monotonicity, but it's unclear how the regularizer introduced in equation 3 helps to encourage that property. Specifically, 1) Only the sum of the gradient is taken into account (so it could be that a component a_w_{i,j} has a very negative gradient, but still the sum will be positive), and 2) the softmax in equation 3 seems to encourage that the total gradient of S y
is larger than the total gradient of all the other S k
's, not that it's positive. Perhaps I'm missing something?
Section 4.2 The paper claims that the fact that a good performance of the ""total activation classifier"" shows evidence that the original classifier satisfies group monotonicity. But that claim is not clear to me. The total activation classifier does not depend on the part of the network that computes the output from the intermediate layer which is critical for the satisfaction of group monotonicity.
Section 4.2.2 The paper doesn't compare their methods to other methods for detecting noisey/adverserial test examples.","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
"2. The authors' derivation falls into classical learning theory-based bounds, which, to the best of my knowledge, does not yield realistic bounds, unless Bayesian considerations are taken into account (e.g. Bayesian-PAC based bounds).",lesQevLmgD,ICLR_2024,"I believe the authors' results merit publication in a specialized journal rather than in ICLR. The main reasons are the following
1. The authors do not give any compelling numerical evidence that their bound is tight or even ""log-tight"".
2. The authors' derivation falls into classical learning theory-based bounds, which, to the best of my knowledge, does not yield realistic bounds, unless Bayesian considerations are taken into account (e.g. Bayesian-PAC based bounds).
3. Even if one maintains that VC-dimension-style learning theory is an important part of the theory of deep learning, my hunch would be that the current work does not contain sufficient mathematical interest to be published in ICLR.
My more minor comments are that
1. The introduction is very wordy and contains many repetitions of similar statements.
2. I found what I believe are various math typos, for instance around Lemma 3.5. I think n and m are used interchangeably. Furthermore calligraphic R with an n sub-script and regular R. Similarly, capital and non-capital l are mixed in assumption 4.8. Runaway subscripts also appear many times in Appendix A2.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"3. Meanwhile, more details about the proposed method should be presented, such as how the implicit distribution characterize the uncertainty of each label value and how the model mitigrate the uncertainty of the label distribution.",ICLR_2023_3693,ICLR_2023,"Weakness: 1. Some details of the proposed method are missing, such as the definition of \mathcal{L}{{kl} and the representation of the augmentation samples in the function. 2. In the proposed method, a novel augmentation strategy is proposed with mask. In the ablation study, the effectiveness of the proposed strategy has not been verified by comparing with mixup and so on. The main contributions of the proposed method should be further verified in the experiments. 3. Meanwhile, more details about the proposed method should be presented, such as how the implicit distribution characterize the uncertainty of each label value and how the model mitigrate the uncertainty of the label distribution.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The proposed method might struggle to detect hallucinations in open-ended responses, for example, the prompt ""introduce a sports celebrity to me"". In this case, the sampled responses could pertain to different individuals, making it challenging to identify shared information for consistency checking.",RwzFNbJ3Ez,EMNLP_2023,"1. The method presented relies on extracting multiple responses from the LLM. For the variant with optimal performance, LLM prompting, 20 samples are needed to achieve the best reported results. Assuming a response contains 5 sentences, this requires 100 API calls to obtain a passage-level score (if I understand correctly), which is cost heavy and ineffective.
2. It remains unclear whether the proposed approach is suitable for detecting hallucinations in responses from other LLMs and across various application scenarios beyond WikiBio. This uncertainty arises because the experiment dataset exclusively encompasses WikiBio responses drawn from text-davinci-003.
3. The proposed method might struggle to detect hallucinations in open-ended responses, for example, the prompt ""introduce a sports celebrity to me"". In this case, the sampled responses could pertain to different individuals, making it challenging to identify shared information for consistency checking.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. It is unclear how these theoretical findings relate to real-world deep learning models, I would suggest the authors verify the conclusion about the label noise and model size on MNIST and CNN as well.",VoI4d6uhdr,ICLR_2025,"1. Although the authors present the exact formulation of the risk in the main text, it is complicated to understand the implications of those formulas. It would be helpful to include more discussion to explain each term to better understand the results.
2. The paper's main contribution is to examine the bias amplification phenomenon using the formula. However, a formal statement about how different components affect the bias amplification is lacking. I would suggest the authors write them in formal theorems.
3. It is unclear how these theoretical findings relate to real-world deep learning models, I would suggest the authors verify the conclusion about the label noise and model size on MNIST and CNN as well.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The paper is well-organized. The writing is good and most of the content is very clear to me. Weaknesses/Feedback 1. The writing could be improved. It would be helpful to draw a table to compare different CoT prompting methods across different dimensions. How and why shall we make an assumption that “questions of all the wrong demonstrations fall into the same frequent-error cluster”? Is the selection criteria in section 4.2 reasonable? Namely, why do we not choose questions with more than 60 tokens and rationales with more than 5 reasoning steps?",ICLR_2023_903,ICLR_2023,"of different chain-of-thought prompting methods: including zero-shot-CoT, few-shot-CoT, manual-CoT, and Auto-CoT. The paper conducts case study experiments to look into the limitations of existing methods and proposes improvement directions. Finally, the paper proposes an improved method for Auto-CoT that could achieve a competitive advantage over Manual-CoT.
2. The experiment part is very detailed and comprehensive.
3. The paper is well-organized.
The writing is good and most of the content is very clear to me. Weaknesses/Feedback
1. The writing could be improved.
It would be helpful to draw a table to compare different CoT prompting methods across different dimensions.
How and why shall we make an assumption that “questions of all the wrong demonstrations fall into the same frequent-error cluster”?
Is the selection criteria in section 4.2 reasonable? Namely, why do we not choose questions with more than 60 tokens and rationales with more than 5 reasoning steps?
2. Some experimental details are missing.
The experimental details for Table 1 are not very clear and lack some details. For example, how the manual-CoT examples are built. What is the number of demonstration examples?
The experimental details for the Codex baseline are missing. I am curious about the instructions you used to prompt the Codex model.
3. It would be better to discuss more recent related work.
I understand some work has been released very recently. Since this work is closely related to the paper, it would be nice to include it in the revised paper. Recent work includes but is not limited to: [1] Calibrate Before Use: Improving Few-Shot Performance of Language Models, 2021 [2] Dynamic Prompt Learning via Policy Gradient for Semi-structured Mathematical Reasoning, 2022 [3] Complexity-Based Prompting for Multi-Step Reasoning, 2022 [4] Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering, 2022 [5] What Makes Pre-trained Language Models Better Zero/Few-shot Learners?, 2022
4. Is it possible to apply Auto-CoT to sample the examples with manually designed CoT?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"• It should be mentioned that p ( y ∣ H f ¯ ( t n ) ) has to be chosen Gaussian, as otherwise Kalman Filtering and Smoothing and CVI is not possible. Later on in the ELBOs this is assumed anyway.",NIPS_2021_1822,NIPS_2021,"of the paper. Organization could definitely be improved and I oftentimes had a bit of a hard time following the discussed steps. But in general, I think the included background is informative and well selected. Though, I could see people having trouble understanding the state-space GP-regression when coming from the more machine-learning like perspective of function/ weight space GP-regression.
Significance: I think there is definitely a significance in this work, as GP-Regression is usually a bit problematic because of the scaling, though it is still used extensively in certain areas, such as Bayesian Optimization or for modelling dynamical systems in robotics.
• Background: f
is a random function which describes a map e.g. f : R × R D s → R
and not a function of X ∈ R N t × N s × D s
as described in eq 1. At least, when one infers the inputs from the definitions of the kernel functions.
• In general, the definitions are confusing and should be checked,e.g. check if f n , k = f ( X n , k )
is correct and properly define X n , k .
• The operator L s
is not mentioned in the text
• 2.1: The description of the process f ¯
is confusing as the relationship to the original process f
is established just at the end.
• It would be helpful to add a bit more background on how the state space model is constructed from the kernel κ t ( ⋅ , ⋅ )
, e.g. why it induces the dimensionality d t
and also describe the limitations that a finite dimensional SDE can only be established, when a suitable spectral decomposition of the kernel exist.
• It should be mentioned that p ( y ∣ H f ¯ ( t n ) )
has to be chosen Gaussian, as otherwise Kalman Filtering and Smoothing and CVI is not possible. Later on in the ELBOs this is assumed anyway. • p ( u ) = N ( u ∣ 0 , K z z )
is a finite dimensional Gaussian not a Gaussian process and p(u) is not a random variable ( = not ∼ ).
• The notations for the covariances, e.g. K z z
are discussed in the appendix. I am fine with it; however, it should be referenced as I was confused in the beginning.
• 2.2: The log
is missing for the Fisher information matrix.
• The acronym CVI is used in the paragraph headline before the definition.
• Some figures and tables are not referenced in the text, such as figure 1.
• 3.1: In line 173 the integration should be carried over f ¯
and not s
, I guess?
• I had a bit of a hard time establishing the connection E p ( f ) [ N ( Y ~ ∣ f , V ~ ) ] = p ( Y ~ )
which is the whole point why one part of the ELBO can be calculated using the Kalman filter. Adding this to a sentence to the text would have helped me a lot.
• One question I had was that for computing the ELBO the matrix exponential is needed. When backpropagating the gradient for the hyper parameters, is this efficient? As I am used to using the adjoint method for computing gradients of the (linear) differential equation.
• Reference to Appendix for the RMSE and NLPD metrics is missing.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. As (suboptimally) weight decay is applied to all layers, we would expect a large training loss and thus suboptimal cosine similarities for large weight decay parameters. Conveniently, cosine similarities for such large weight decay strengths are not reported and the plots end at a weight decay strength where cosine similarities are still close to optimal.",JWwvC7As4S,ICLR_2024,"### Theory
The main theoretical results are Theorem 2.1 and 2.2. They state that if the ""average last-layer feature norm and the last-layer weight matrix norm are both bounded, then achieving near-optimal loss implies that most classes have intra-class cosine similarity near one and most pairs of classes have inter-class cosine similarity near -1/(C-1)"".
Qualitatively, this result is an immediate consequence of continuity of the loss function together with the fact that bounded average last-layer feature norm and bounded last-layer weight matrices implies NC.
Quantitatively, this work proves asymptotic bounds on the proximity to NC as a function of the loss. This quantitative aspect is novel. I am not convinced of its significance however, as I will outline below.
1. The result is only asymptotic, and thus it cannot be used to estimate proximity to NC from a given loss value.
2. The bound is used as basis to argue that *""under the presence of batch normalization
and weight decay of the final layer, larger values of weight decay provide stronger NC guarantees in the sense that the intra-class cosine similarity of most classes is nearer to 1 and the inter-class cosine similarity of most pairs of classes is nearer to -1/(C-1).""*
This is backed up by the observation, that the bounds get more tight if the weight decay parameter $\lambda$ increases. To be more specific, Theorem 2.2 shows that if $L< L{min}+\epsilon$, then the average intra class cosine similarity is smaller than $-1/(C-1) + O(f(C,\lambda,\epsilon,\delta))$ and $f$ decreases with $\lambda$.
The problem with this argument is that the loss function itself depends on the regularization parameter $\lambda$ and so it is a-priori not clear whether values of $\epsilon$ are comparable for different $\lambda$. For example, apply this argument to the more simple loss function $L(x,\lambda)=\lambda x^2$. As $L$ is convex, it is clear that the value of $\lambda>0$ is irrelevant for the minimum and the near optimal solutions. Yet, $L(x,\lambda)<\epsilon$ implies $x^2<\epsilon/\lambda$ which decreases with $\lambda$. By the logic given in this work, the latter inequality suggests that minimizing a loss function with a larger value of $\lambda$ provides stronger guarantees for arriving close to the minimum at $0$. Clearly, this is not the case and an artifact of quantifying closeness to the loss minimum by $\epsilon$, when it should have been adjusted to $\lambda \epsilon$ instead.
I have doubts on how batch normalization is handled. As far as I see, batch normalization enters the proofs only through the condition $\sum_i \| h_i \|^2 =\| h_i \|^2$ (see Prop 2.1). However, this is only an implication and batch normalization induces stronger constraints. The theorems assume that the loss minimizer is a simplex ETF in the presence of batch normalization. This is not obvious, and neither proven nor discussed. It is also not accounted for in the part of the proof of Theorem 2.2, where the loss minimum $m_{reg}$ is derived.
### Experiments
- Theorems 2.1 and 2.2 are not evaluated empirically. It is not tested, whether the average intra / inter class cosine similarities of near optimal solutions follow the exponential dependency in $\lambda$ and the square (or sixth) root dependency on $\epsilon$ as suggested by the theorems.
- Instead, the dependency of cosine similarities at the end of training (200 epochs) on weight decay strength is evaluated. As presumed by the authors, the intra class cosine similarities get closer to the optimum, if the weight decay strength increases. Yet, there are problems with this experiment. It is inconsistent with the setting of the theory part and thus only provides limited insight on if the idealized theoretical results transfer to practice.
1. The theory part depends only on the weight decay strength on the last layer parameters. Yet, in the experiments, weight decay is applied to all layers and its strength varies between experiments (when instead only the strength of the last layer should change).
2. The theorems assume near optimal training loss, but training losses are not reported. Moreover, the reported cosine similarities are far from optimal (e.g. intra class is around 0.2 instead of 1) which suggests that the training loss is also far from optimal. It also suggests that the models are of too small capacity to justify the 'unconstrained-features' assumption.
3. As (suboptimally) weight decay is applied to all layers, we would expect a large training loss and thus suboptimal cosine similarities for large weight decay parameters. Conveniently, cosine similarities for such large weight decay strengths are not reported and the plots end at a weight decay strength where cosine similarities are still close to optimal.
4. On real-world data sets, the inter class cosine similarity increases with weight decay (even for batch norm models VGG11), disagreeing with the theoretical prediction. This observation is insufficiently acknowledged.
### General
The central question that this work wants to answer **What is a minimal set of conditions that would guarantee the emergence of NC?""** is already solved in the sense that it is known that minimal loss plus a norm constraint on the features (explicit via feature normalization or implicit via weight decay) implies neural collapse. The authors argue to add batch normalization to this list but that contradicts minimality.
The first contribution listed by the authors is not a contribution.
1. *""We propose the intra-class and inter-class cosine similarity measure, a simple and geometrically intuitive quantity that measures the proximity of a set of feature vectors to several core
structural properties of NC. (Section 2.2)""*
Cosine similarity (i.e. the normalized inner product) is a well known and an extensively used distance measure on the sphere. In the context of neural collapse, cosine similarities were already used in the foundational paper by Papyan et al. (2020) to empirically quantify closeness to NC (cf. Figure 3 in this reference) and many others. Minor:
- There is a grammatical error in the second sentence of the second paragraph
- There is no punctuation after formulas; In the appendix, multiple rows start with a punctuation
- intra / inter is sometimes written in italics, sometimes upright
- $\beta$ is used multiply with a different meaning
- Proposition 2.1 $N$ = batch site, Theorem 2.2 $N$ = number of samples per class.
- As a consequence, it seems that $\gamma$ needs to be rescaled to account for the number of batches","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"* The title, abstract, introduction, and discussion do not explain that the results are for unsupervised random forests. This is a fairly serious omission, and casual readers would remember the wrong conclusions. This must be fixed for publication, but I think it would be straightforward to fix. Officially, NIPS reviewers are not required to look at the supplementary material. Because of having only three weeks to review six manuscripts, I was not able to make the time during my reviewing. So I worry that publishing this work would mean publishing results without sufficient peer review. DETAILED COMMENTS * p.",NIPS_2018_276,NIPS_2018,". Strengths: * This is the first inconsistency analysis for random forests. (Verified by quick Google scholar search.) * Clearly written to make results (mostly) approachable. This is a major accomplishment for such a technical topic. * The analysis is relevant to published random forest variations; these include papers published at ICDM, AAAI, SIGKDD. Weaknesses: * Relevance to researchers and practitioners is a little on the low side because most people are using supervised random forest algorithms. * The title, abstract, introduction, and discussion do not explain that the results are for unsupervised random forests. This is a fairly serious omission, and casual readers would remember the wrong conclusions. This must be fixed for publication, but I think it would be straightforward to fix. Officially, NIPS reviewers are not required to look at the supplementary material. Because of having only three weeks to review six manuscripts, I was not able to make the time during my reviewing. So I worry that publishing this work would mean publishing results without sufficient peer review. DETAILED COMMENTS * p. 1: I'm not sure it is accurate to say that deep, unsupervised trees grown with no subsampling is a common setup for learning random forests. It appears in Geurts et al. (2006) as a special case, sometimes in mass estimation [1, 2], and sometimes in Wei Fan's random decision tree papers [3-6]. I don't think these are used very much. * You may want to draw a connection between Theorem 3 and isolation forests [7] though. I've heard some buzz around this algorithm, and it uses unsupervised, deep trees with extreme subsampling. * l. 16: ""random"" => ""randomized"" * l. 41: Would be clearer with forward pointer to definition of deep. * l. 74: ""ambient"" seems like wrong word choice * l. 81: Is there a typo here? Exclamation point after \thereexists is confusing. * l. 152; l. 235: I think this mischaracterizes Geurts et al. (2006), and the difference is important for the impact stated in Section 4. Geurts et al. include a completely unsupervised tree learning as a special case, when K = 1. Otherwise, K > 1 potential splits are generated randomly and unsupervised (from K features), and the best one is selected *based on the response variable*. The supervised selection is important for low error on most data sets. See Figures 2 and 3; when K = 1, the error is usually high. * l. 162: Are random projection trees really the same as oblique trees? * Section 2.2: very useful overview! * l. 192: Typo? W^2? * l. 197: No ""Eq. (2)"" in paper? * l. 240: ""parameter setup that is widely used..."" This was unclear. Can you add references? For example, Lin and Jeon (2006) study forests with adaptive splitting, which would be supervised, not unsupervised. * Based on the abstract, you might be interested in [8]. REFERENCES [1] Ting et al. (2013). Mass estimation. Machine Learning, 90(1):127-160. [2] Ting et al. (2011). Density estimation based on mass. In ICDM. [3] Fan et al. (2003). Is random model better? On its accuracy and efficiency. In ICDM. [4] Fan (2004). On the optimality of probability estimation by random decision trees. In AAAI. [5] Fan et al. (2005). Effective estimation of posterior probabilities: Explaining the accuracy of randomized decision tree approaches. In ICDM. [6] Fan el al. (2006). A general framework for accurate and fast regression by data summarization in random decision trees. In KDD. [7] Liu, Ting, and Zhou (2012). Isolation-based anomaly detection. ACM Transactions on Knowledge Discovery from Data, 6(1). [8] Wager. Asymptotic theory for random forests. https://arxiv.org/abs/1405.0352","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2.There is a lack of qualitative experiments to demonstrate the validity of the conditional independence model. a)It is better to provide some illustrative experimental results to demonstrate that minimising HSICcond-i could indeed perform better than minimising HSIC_HOOD. Possibly, one toy dataset can be used to demonstrate the separability of inlier features and outlier features. b)The authors propose a new test metric, however, lacking the correctness test and comparative experiments with other metrices. It may be better to provide some visualization results or schematic diagram, which could make readers easier to understand.",NIPS_2022_1813,NIPS_2022,"1.The innovation of the article seems limited to me, mainly since the work shares the same perspective as [2]. Both the models build upon the probabilistic formulation and applies the Hilbert-Schmidt Independence Criteria (HSIC). It may be good to clarify a bit more on how novel the paper is compared from [2].
2.There is a lack of qualitative experiments to demonstrate the validity of the conditional independence model. a)It is better to provide some illustrative experimental results to demonstrate that minimising HSICcond-i could indeed perform better than minimising HSIC_HOOD. Possibly, one toy dataset can be used to demonstrate the separability of inlier features and outlier features.
b)The authors propose a new test metric, however, lacking the correctness test and comparative experiments with other metrices. It may be better to provide some visualization results or schematic diagram, which could make readers easier to understand.
3.Current experimental results seem not very convincing to me. Some critical comparative results are missing. a)Under the setting of unseen OOD training data, DIN [34], Mahalanobis distance [31], Energy [36], their original papers did not use fake/augmented OOD training data. These settings need to be clarified in the paper. Moreover, the impact of using different augmentation methods on the result could be explored in the ablation.
b)In CIFAR-100, the experimental setup appears to be consistent with that of HOOD [2]. However, in Table 1 (unseen OOD training data), the HOOD’s results are missing. In [2], the results of HOOD are superior to those of Conditional-I's.
c)In Table 2 (unseen OOD training data), the HOOD’s results are also missing.
d)There are missing both Conditional-i-generative and HOOD results for the NLP OOD detection tasks. As missing the results of the most relevant methods, the present experiments could not convince me of the validity of the improvements.
4.The memory bank architecture is one contribution, but the authors do not provide quantitative results of introducing the memory bank architecture.
The authors seemed not to discuss the limitations of the proposed model.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
3. The authors describe an online version of the algorithm because it is impractical to train multiple iterations/epochs with large models and datasets. Is it true that the proposed method requires much more computation than other methods? Please compare the computational complexity with other methods.,NIPS_2018_172,NIPS_2018,"1. The writing is not clear. The descriptions of the techniqual part can not be easily followed by the reviewer, which make it very hard to reimplement the techniques. 2. An incomplete sequence is represented by a finite state automaton. In this paper, only a two out of three finite state automation is used. Is it possible/computational feasible to use more complicated finite state automaton to select more image labels? As there are 12 image labels per image on average, only selecting two labels seems insufficient. 3. The authors describe an online version of the algorithm because it is impractical to train multiple iterations/epochs with large models and datasets. Is it true that the proposed method requires much more computation than other methods? Please compare the computational complexity with other methods. 4. How many estimated complete captions could be obtained for one image? Is it possible to generate over C(12, 3) * C(3, 2) = 660 different captions for one image?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Systematically studying the impact of the cost of incentivization on performance would have been a helpful analysis (e.g., for various values of \alpha, what are the reward incentives each agent receives, and what is the collective return?). It seems like roles between “winners” and “cooperators” emerge because the cost to reward the other agent becomes high for the cooperators. If this cost were lower, it seems like roles would be less distinguished, causing the collective return to be much lower.",NIPS_2020_1274,NIPS_2020,"- It would be helpful if the paper’s definition of “decentralized” is more explicitly stated in the paper, instead of in a footnote. Other ways of defining “decentralized” is where agents do not have access to the global state and actions of other agents during both training and execution which LIO seems to do. - Systematically studying the impact of the cost of incentivization on performance would have been a helpful analysis (e.g., for various values of \alpha, what are the reward incentives each agent receives, and what is the collective return?). It seems like roles between “winners” and “cooperators” emerge because the cost to reward the other agent becomes high for the cooperators. If this cost were lower, it seems like roles would be less distinguished, causing the collective return to be much lower. - In Figure 5d, more explanation as to why the Winner receives about the same incentive as the Cooperator to pull the lever would be helpful; it doesn’t match how the plot is described on lines 286-287.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors discuss how certain methods are significantly different from others, yet no significance testing is done to support these claims. For example, in line 486 the authors write ""The conversational ability of ChatGPT and GPT-4 significantly boosts translation quality and discourse awareness"" -- the difference between zh->en ChatGPT (17.4 d-BLEU; 2.8/2.9 humeval) and GPT-4 (18.8 d-BLEU; 2.6/2.8 humeval) and the scores for FeedME-2 (16.1 d-BLEU; 2.2/2.3 humeval) and PPO (17.2 d-BLEU; 2.6/2.7 humeval) is minimal and it's hard to say whether it is significant without proper testing (including checking the distribution and accounting for multiple comparisons).",sXErPfdA7Q,EMNLP_2023,"UPDATE: The authors addressed most of my concerns however, I believe that the first and second points are still valid and should be discussed as potential limitations (i.e., there are too many confounding variables to claim that one is investigating an impact of different training methods; and the datasets might have been - and probably were - used for RLHF).
UPDATE 2: the concerns were addressed by the authors to the extend it was possible with the current design.
- The authors claim to investigate the effect of different training methods on the processing of discourse-level information (as one of three main experiments), however, it is questionable whether what we see is the effect of different training methods, different training data, or perhaps the data used for RLHF (rather than RLHF alone, that is it is possible that the MT datasets used in this study were used to create RLHF examples). Since the authors research black box models behind an API, I do not think we can make any claims about the effect of the training method (of which we know little) on the model's performance.
- Data contamination might have influenced the evaluation - The authors employ various existing datasets. While two of these datasets do have publication date past the openAI's models' training cutoff point (made public in August 2022), this seems not to be the case for the other datasets employed in this study (including the dataset with contextual errors). It is likely that these were included in the training data of the LLMs being evaluated. Furthermore, with the RLHF models, it is also possible (and quite likely) that MT datasets published post-training were employed to create the RLHF data. For instance the WMT22 dataset was made public in August 2022, which gives companies like OpenAI plenty of time to retrieve it, reformulate it into training examples, and use for RLHF.
- The authors discuss how certain methods are significantly different from others, yet no significance testing is done to support these claims. For example, in line 486 the authors write ""The conversational ability of ChatGPT and GPT-4 significantly boosts translation quality and discourse awareness"" -- the difference between zh->en ChatGPT (17.4 d-BLEU; 2.8/2.9 humeval) and GPT-4 (18.8 d-BLEU; 2.6/2.8 humeval) and the scores for FeedME-2 (16.1 d-BLEU; 2.2/2.3 humeval) and PPO (17.2 d-BLEU; 2.6/2.7 humeval) is minimal and it's hard to say whether it is significant without proper testing (including checking the distribution and accounting for multiple comparisons).
- The main automatic method used throughout this paper is d-BLEU, which works best with multiple references, yet I believe only one is given. I understand that there are limited automatic options for document level evaluation where the sentences cannot be aligned. Some researchers used sliding windows for COMET, but that's not ideal (yet worth trying?). That is why the human evaluation is so important, and the one in this paper is lacking.
- Human evaluation - many important details are missing so it is hard to judge the research design (more questions below); however what bothers me most is that the authors construct an ordinal scale with a clear cutoff point between 2 and 3 (for general quality especially), yet they present only average scores. I do believe that ""5: Translation passes quality control; the overall translation is excellent (...)"" minus ""4: Translation passes quality control; the overall translation is very good"" is not the same ""one"" as ""3: Translation passes quality control; the overall translation is ok. (...)"" minus ""2: Translation does not pass quality control; the overall translation is poor. (...)"". It is clear that the difference between 5 and 4 is minimal, while between 3 and 2 is much bigger. Simple average is not a proper way to analyze this data (a proper analysis would include histograms of scores, possibly a stacked bar with proportion of choices, statistical testing).
- Another issue with the human evaluation is that it appears that the evaluators were asked to evaluate an entire document by assigning it one score. Note that this is a cognitively demanding and difficult task for the evaluators. The results are likely to be unreliable (please see Sheila Castilho's work on this topic). There is also no indication that the annotators were at least given some practice items.
- ""Discourse-aware prompts"" - I am not sure what this experiment was about. It seems that the idea was to evaluated how the availability of discourse information can improve the translation, but if that is so, then all three setups did have discourse level information (hence this evaluation is impossible). The only thing this seems to be doing is checking in which way the information should be presented (one sentence at a time in a chat, all sentences at once but clearly marked, or the entire document at once without sentence boundaries).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- The approach description (§ 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).,ARR_2022_141_review,ARR_2022,"- The approach description (§ 3) is partially difficult to follow and should be revised. The additional page of the camera-ready version should be used to extend the approach description (rather than adding more experiments).
- CSFCube results are not reported with the same metrics as in the original publication making a comparison harder than needed.
- The standard deviation from the Appendix could be added to Table 1 at least for one metric. There should be enough horizontal space.
- Notation of BERT_\theta and BERT_\epsilon is confusing. Explicitly calling them the co-citation sentence encoder and the paper encoder could make it clearer. - How are negative sampled for BERT_\epsilon?
Additional relevant literature: - Luu, K., Wu, X., Koncel-Kedziorski, R., Lo, K., Cachola, I., & Smith, N.A. (2021). Explaining Relationships Between Scientific Documents. ACL/IJCNLP.
- Malte Ostendorff, Terry Ruas, Till Blume, Bela Gipp, Georg Rehm. Aspect-based Document Similarity for Research Papers. COLING 2020.
Typos: -	Line 259: “cotation” -	Line 285: Missing “.”","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3.On the experiments part, the related discussion lacks interpretive insights that would elucidate why the proposed gyro-structures outperform existing methods. In addition, while the paper compares its methods against SPD-based models and a few gyro-structure-based approaches, it lacks comparison with other state-of-the-art methods that might not rely on gyro-structures. This omission makes it unclear whether the proposed approach actually outperforms simpler or more commonly used techniques in manifold-based learning.",ZPwX1FL4yp,ICLR_2025,"1.The application of gyro-structures on SPD manifolds and correlation matrices is indeed novel, but the paper does not clearly articulate the theoretical significance or unique advantages of using Power-Euclidean (PE) geometry over existing approaches like Affine-Invariant (AI) or Log-Euclidean (LE) methods. The work seems incremental without providing substantial theoretical or empirical evidence that PE geometry offers practical improvements beyond computational convenience. Especially, while gyro-structures are presented as an extension to non-Euclidean spaces, the paper does not establish a strong need or motivation for this approach within the broader context of machine learning or geometry-based learning. It lacks a thorough discussion on why gyro-structures would fundamentally enhance SPD or correlation matrix-based learning in a way that current methods do not.
2.Some key theoretical concepts and mathematical operations, such as those in gyrovector space theory and correlation matrix manifold construction, are highly technical and lack intuitive explanations. Additional clarification or simplified summaries would improve accessibility for readers unfamiliar with advanced Riemannian geometry.
3.On the experiments part, the related discussion lacks interpretive insights that would elucidate why the proposed gyro-structures outperform existing methods. In addition, while the paper compares its methods against SPD-based models and a few gyro-structure-based approaches, it lacks comparison with other state-of-the-art methods that might not rely on gyro-structures. This omission makes it unclear whether the proposed approach actually outperforms simpler or more commonly used techniques in manifold-based learning.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1. It is desired to have more evidence or analysis supporting the training effectiveness property of the dataset or other key properties that will explain the importance and possible use-cases of _LMGQS_ over other QFS datasets.,4WrqZlEK3K,EMNLP_2023,"1. It is desired to have more evidence or analysis supporting the training effectiveness property of the dataset or other key properties
that will explain the importance and possible use-cases of _LMGQS_ over other QFS datasets.
2. Several unclear methods affecting readability and reproducibility:
* ""To use LMGQS in the zero-shot setting, it is necessary to convert the queries of diverse formats into natural questions."" (L350) please explain why.
* ""Specifically, we finetune a BART model to generate queries with the document and summary as input."" (L354) - how did you FT? what is the training set and any relevant hyper-parameters for reproducing the results.
* ""we manually create a query template to transform the query into a natural language question."" (L493) - what are the templates? what are the query template and several examples.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method. [a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017 [b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019",ICLR_2021_2527,ICLR_2021,"Duplicate task settings. The proposed new task, cross-supervised object detection, is almost the same as the task defined in (Hoffman et al. 2014, Tang et al. 2016, Uijlings et al. 2018). Both of these previous works study the task of training object detectors on the combination of base class images with instance-level annotations and novel class image with only image-level annotations. The work (Uijlings et al. 2018) also conducts experiments on COCO which contains multi-objects in images. In addition, the work (Khandelwal et al. 2020) unifies the setting of training object detectors on the combination of fully-labeled data and weakly-labeled data, and conducts experiments on multi-object datasets PASCAL VOC and COCO. The task proposed by this paper could be treated as a special case of the task studied in (Khandelwal et al. 2020). We should avoid duplicate task settings.
Limited novelty. The novelty of the proposed method is limited. Combining recognition head and detection head is not new in weakly supervised object detection. The weakly supervised object detection networks (Yang et al. 2019, Zeng et al. 2019) also generate pseudo instance-level annotations from recognition head to train detection head (i.e., head with bounding box classification and regression) for weakly-labeled data.
Review summary: In summary, I would like to give a rejection to this paper due to the duplicate task settings and limited novelty.
Khandelwal et al., Weakly-supervised Any-shot Object Detection, 2020
---------- Post rebuttal ----------
After discussions with authors and reading other reviews, I acknowledge the contribution that this paper advances the performance of cross-supervised object detection.
However, I would like to keep my original reject score. The reasons are as follows.
Extending datasets from PASCAL VOC to COCO is not a significant change comparing to previous tasks. The general object detection papers also evaluated on PASCAL VOC only about five years ago and now evaluate mainly on COCO. With the development of computer vision techniques, it is natural to try more challenging datasets. So although this paper claims that this paper focuses on more challenging datasets, there is no significant difference between the tasks studied in previous works like [a] and this paper.
In addition, apart from ImageNet, the work [b] also evaluates their method on the Open Images dataset which is even larger and more challenging than COCO. The difference between the tasks studied in [b] and this paper is only that, [b] adds a constraint that weakly-labeled classes have semantic correlations with fully-labeled classes and this paper doesn't. This difference is also minor.
Therefore, the task itself cannot be one of the main contributions of this paper (especially the most important contribution of this paper). I would like to suggest the authors change their title / introduction / main paper by 1) giving lower wights to the task parts 2) giving higher weights to intuitions of why previous works fail on challenging datasets like COCO and motivations of the proposed method.
[a] YOLO9000: Better, Faster, Stronger, In CVPR, 2017
[b] Detecting 11K Classes: Large Scale Object Detection without Fine-Grained Bounding Boxes, In ICCV, 2019","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
"2. Figure 5 is hard to comprehend. I would like to see more details about the two baselines presented in Figure 5. The authors only study CATER for the English-centric datasets. However, as we know, the widespread text generation APIs are for translation, which supports multiple languages. Probably, the authors could extend CATER to other languages in the future.",NIPS_2022_2373,NIPS_2022,"weakness in He et al., and proposes a more invisible watermarking algorithm, making their method more appealing to the community. 2. Instead of using a heuristic search, the authors elegantly cast the watermark search issue into an optimization problem and provide rigorous proof. 3. The authors conduct comprehensive experiments to validate the efficacy of CATER in various settings, including an architectural mismatch between the victim and the imitation model and cross-domain imitation. 4. This work theoretically proves that CATER is resilient to statistical reverse-engineering, which is also verified by their experiments. In addition, they show that CATER can defend against ONION, an effective approach for backdoor removal.
Weakness: 1. The authors assume that all training data are from the API response, but what if the adversary only uses the part of the API response? 2. Figure 5 is hard to comprehend. I would like to see more details about the two baselines presented in Figure 5.
The authors only study CATER for the English-centric datasets. However, as we know, the widespread text generation APIs are for translation, which supports multiple languages. Probably, the authors could extend CATER to other languages in the future.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"* Unclear Literature Review: The literature review in the paper needs improvement. It is not adequately clear what the main contribution of the proposed method is, and how it distinguishes itself from existing work, particularly in relation to the utilization of GFlowNet for sequence generation. The paper should provide a more explicit and comparative analysis of related work.",viNQSOadLg,ICLR_2024,"* Lack of Training Details: The paper lacks sufficient information regarding the training process of the policy. It should provide more details on the training data used, the methodology for updating parameters, and the specific hyperparameters employed in the process.
* Unclear Literature Review: The literature review in the paper needs improvement. It is not adequately clear what the main contribution of the proposed method is, and how it distinguishes itself from existing work, particularly in relation to the utilization of GFlowNet for sequence generation. The paper should provide a more explicit and comparative analysis of related work.
* Ambiguity in Key Innovation: The claim that GFNSeqEditor can produce novel sequences with improved properties lacks clarity regarding the key innovation driving these contributions. The paper should better articulate what novel techniques or insights lead to the claimed improvements, thereby enhancing the reader's understanding of the method's unique value.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- I would say that section 3.2 can be eliminated - I think that at this point readers can be presumed to know about the Gumbel-Softmax/Concrete distribution.,NIPS_2018_66,NIPS_2018,"of their proposed method for disentangling discrete features in different datasets. I think that the main of the paper lies in the relatively thorough experimentation. I thought the results in Figure 6 were particularly interesting in that they suggest that there is an ordering in features in terms of mutual information between data and latent variable (for which the KL is an upper bound), where higher mutual information features appear first as the capacity is increased. I also appreciate the explicit discussion of the robust of the degree of disentanglement across restarts, as well as the sensitivity to hyperparameters. Given the difficulties observed in Figure 4 in distinguishing between similar digits (such as 5s and 8s), it would be interesting to see results for this method on a dataset like dSprites, where the shapes are very similar in pixel space. The inferred chair rotations in Figure 7 are also a nice illustration of the ability of the method to generalize to the test set. The main thing that this paper lacks is a more quantitative evaluation. A number of recent papers have proposed metrics for evaluating disentangled representations. In addition the metrics proposed by Kim & Mnih (2018) and Chen et al. (2018), the work by Eastwood & Williams (2017) [1] is relevant in this context. All of these metrics presume that we have access to labels for true latent factors, which is not the case for any of the datasets considered in the experimentation. However, it would probably be worth evaluating one or more of these metrics on a dataset such as dSprites. A minor criticism is that details the training procedure and network architectures are somewhat scarce in the main text. It would be helpful to briefly describe the architectures and training setup in a bit more detail, and explicitly call out the relevant sections of the supplementary material. In particular, it would be good to list key parameters such as Î³ and the schedule for the capacities Cz and Cc, e.g., the figure captions. In Figure 6a, please mark the 25k iterations (e.g. with a vertical dashed line) to indicate that this is where the capacity is no longer increased further. Questions - How robust is the ordering on features Figure 6, given the noted variability across restarts in Section 4.3? I would hypothesize that the discrete variable always emerges first (given that this variable is in some sense given a âgreaterâ capacity than individual dimensions in the continuous variables). Is the ordering on the continuous variables always the same? What happens when you keep increasing the capacity beyond 25k iterations. Does the network eventually use all of the dimensions of the latent variables? - I would also appreciate some discussion of how the hyperparameters in the objective were chosen. In particular, one could imagine that the relative magnitude of Cc and Cz would matter, as well as Î³. This means that there are more parameter to tune than in, e.g., a vanilla Î²-VAE. Can the authors comment on how they chose the reported values, and perhaps discuss the sensitivity to these particular hyperparameters in more detail? - In Figure 2, what is the range of values over which traversal is performed? Related Work In addition to the work by Eastwood & Williams, there are a couple of related references that the authors should probably cite: - Kumar et. al [2] also proposed the total correlation term along with Kim & Mnih (2018) and Chen et al. (2018). - A recent paper by Esmaeli et al. [3] employs an objective based on the Total Correlation, related to the one in Kim & Mnih (2018) and Chen et. al (2018) to induce disentangled representations that can incorporate both discrete and continuous variables. Minor Comments - As the authors write in the introduction, one of the purported advantages of VAEs over GANs is stability of training. However, as mentioned by the author, including multiple variables of different types also makes the representation unstable. Given this observation, maybe it is worth qualifying these statements in the introduction. - I would say that section 3.2 can be eliminated - I think that at this point readers can be presumed to know about the Gumbel-Softmax/Concrete distribution. - Figure 1 could be optimized to use less whitespace. - I would recommend to replace instances of (\citet{key}) with \citep{key}. References [1] Eastwood, C. & Williams, C. K. I. A Framework for the Quantitative Evaluation of Disentangled Representations. (2018). [2] Kumar, A., Sattigeri, P. & Balakrishnan, A. Variational inference of disentangled latent concepts from unlabeled observations. arXiv preprint arXiv:1711.00848 (2017). [3] Esmaeili, B. et al. Structured Disentangled Representations. arXiv:1804.02086 [cs, stat] (2018).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.",XX73vFMemG,EMNLP_2023,"1. The paper studies the case where the student distills knowledge to the teacher, which improves the teacher's performance. However, the improvements could potentially be due to regularization effects rather than distillation as claimed, since all fine-tuning is performed for 10 epochs and without early-stopping. The fine-tuning on GLUE without validation early-stopping usually has very high variances, proper ablation studies are needed to verify.
2. The performance gains (especially, the Community KD) over standard one-way distillation seem quite marginal based on the experiments when compared to other BERT distillation techniques like BERT-PKD, TinyBERT, MobileBERT, or BERT-of-Theseus. This is not a good signal given the training process is more complex with co-training and co-distillation compared to other distillation techniques.
3. Evaluations are limited to BERT models only. Testing on other PLMs would be more convincing.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"* Label the X and Y axes of plots. This paper makes an important step towards safe RL. While this paper builds upon much previous work, it clearly documents and discusses comparisons to previous work. While the results are principally theoretical, I believe it will inspire both more theoretical work and practical applications.",NIPS_2018_688,NIPS_2018,"weakness of the paper is that experiments are limited to a single task. That said, they compare against two reasonable baselines (CPO, and including the constraint as a negative reward). While the formal definition of the constrained objective in L149 - L155 is appreciated, it might be made a bit more clear by avoiding some notation. For example, instead of defining a new function I(x,y) (L151), you could simply use \delta(x >= y), stating that \delta is the Dirac delta function. A visualization of Eq 1 might be helpful to provide intuition. Minutia * Don't start sentences with citation. For example, (L79): ""[32] proposed ..."" * Stylistically, it's a bit odd to use a *lower* bound on the *cost* as a constraint. Usually, ""costs"" are minimized, so we'd want an upper bound. * Second part of Def 4.2 (L148) is redundant. If Pr(X >= \gamma) <= \rho, then it's vacuously true that Pr(X <= \gamma) >= 1 - \rho. Also, one of these two terms should be a strict inequality. * Include the definition of S (L154.5) on its own line (or use \mbox). * Label the X and Y axes of plots. This paper makes an important step towards safe RL. While this paper builds upon much previous work, it clearly documents and discusses comparisons to previous work. While the results are principally theoretical, I believe it will inspire both more theoretical work and practical applications.","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
"3. For experiments, I have 2 comments - (i) addition of performance on word similarity and sentence translation tasks as in the MUSE paper (and others) would lend more credibility to the robustness and effectiveness of the framework. (ii) addition of morphologically rich languages like Finnish, Hebrew, etc and low-resource languages in the experiments would be good to have (minor point).",Kjs0mpGJwb,EMNLP_2023,"1. Although the structural information has not been explicitly used in the current problem statement, it has been implicitly used in few previous works on bilingual mapping induction. Please see:
""Multi-Stage Framework with Refinement based Point Set Registration for Unsupervised Bi-Lingual Word Alignment"". Oprea et al., COLING 2022.
""Point Set Registration for Unsupervised Bilingual Lexicon Induction"". H. Cao and T. Zhao, IJCAI 2018.
As such, a proper discussion and comparison is necessary in the current paper.
2. The GCN that is proposed has no learnable parameters, so it is a static matrix transformation operation. So, why is the term GCN used - isn't it misleading? Further, if it is not learning anything, it is a simple aggregation function, as far as I understood. Then the structural information is not propagating through the entire graph - this is counter-intuitive because the author(s) claim that structural information usage is a key feature of the proposed framework. Am I missing something here?
3. For experiments, I have 2 comments - (i) addition of performance on word similarity and sentence translation tasks as in the MUSE paper (and others) would lend more credibility to the robustness and effectiveness of the framework. (ii) addition of morphologically rich languages like Finnish, Hebrew, etc and low-resource languages in the experiments would be good to have (minor point).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1.The paper consider the node importance among nodes with same label in support set. In 1-shot scenario, how node importance can be used? I also find that the experiment part in the paper does not include the 1-shot paper setting, but related works such as RALE have 1-shot setting, why?",ICLR_2023_2658,ICLR_2023,"Weakness:
1.I think the work is lack of novelty as the work GPN[1] has already proposed to add node importance score in the calculation of class prototype and the paper only give a theoretical analysis on it.
2.The experiment part is not sufficient enough. (1) For few-shot graph node classification problem to predict nodes with novel labels, there are some methods that the paper does not compare with. For example, G-Meta is mentioned in the related works but not compared in the experiments. A recent work TENT[2] is not mentioned in related works. As far as I know, the above two approaches can be applied in the problem setting in the paper. (2) For the approach proposed in the paper, there is no detailed ablation study for the functionalities of each part designed. (3) It is better to add a case study part to show the strength of the proposed method by an example. Concerns:
1.The paper consider the node importance among nodes with same label in support set. In 1-shot scenario, how node importance can be used? I also find that the experiment part in the paper does not include the 1-shot paper setting, but related works such as RALE have 1-shot setting, why?
2.The paper says that the theory of node importance can be applied to other domains. I think there should be an example to verify that conclusion.
3.In section 5.3, ‘we get access to abundant nodes belonging to each class’. I do not think this is always true as there might be a class in the training set that only has few samples given the long-tailed distribution of samples in most graph datasets.
[1] Ding et al. Graph Prototypical Networks for Few-shot Learning on Attributed Networks
[2] Wang et al. Task-Adaptive Few-shot Node Classification","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. You should provide more details about the formula in the text, e.g. $\ell_{BCE}$ ,even if it is simple, give specific details.",Q2IInBu2kz,EMNLP_2023,"1. You should compare your model with more recent models [1-5].
2. Contrastive learning has been widely used in Intent Detection [6-9], although the tasks are not identical. I think the novelty of this simple modification is not suitable for EMNLP.
3. You should provide more details about the formula in the text, e.g. $\ell_{BCE}$ ,even if it is simple, give specific details.
4. You don't provide the value of some hyper-parameters, such as τ.
5. The Figure 1 is blurry, which affects reading.
[1] Qin L, Wei F, Xie T, et al. GL-GIN: Fast and Accurate Non-Autoregressive Model for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021: 178-188.
[2] Xing B, Tsang I. Co-guiding Net: Achieving Mutual Guidances between Multiple Intent Detection and Slot Filling via Heterogeneous Semantics-Label Graphs[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 159-169.
[3] Xing B, Tsang I. Group is better than individual: Exploiting Label Topologies and Label Relations for Joint Multiple Intent Detection and Slot Filling[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 3964-3975.
[4] Song M, Yu B, Quangang L, et al. Enhancing Joint Multiple Intent Detection and Slot Filling with Global Intent-Slot Co-occurrence[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 7967-7977.
[5] Cheng L, Yang W, Jia W. A Scope Sensitive and Result Attentive Model for Multi-Intent Spoken Language Understanding[J]. arXiv e-prints, 2022: arXiv: 2211.12220.
[6] Liu H, Zhang F, Zhang X, et al. An Explicit-Joint and Supervised-Contrastive Learning Framework for Few-Shot Intent Classification and Slot Filling[C]//Findings of the Association for Computational Linguistics: EMNLP 2021. 2021: 1945-1955.
[7] Qin L, Chen Q, Xie T, et al. GL-CLeF: A Global–Local Contrastive Learning Framework for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 2022: 2677-2686.
[8] Liang S, Shou L, Pei J, et al. Label-aware Multi-level Contrastive Learning for Cross-lingual Spoken Language Understanding[C]//Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022: 9903-9918.
[9] Chang Y H, Chen Y N. Contrastive Learning for Improving ASR Robustness in Spoken Language Understanding[J]. arXiv preprint arXiv:2205.00693, 2022.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2.There should be more discussions about why LLMs struggle at fine-grained hard constraints and how to address these problems.,nuPp6jdCgg,EMNLP_2023,"1.While this paper shows many findings, few of them are new to the community.
2.There should be more discussions about why LLMs struggle at fine-grained hard constraints and how to address these problems.
3.It would be better to include vicuna and falcon in Table-2, Table-3, and Table-5.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1- While the experimental results suggest that the proposed approach is valuable for self-supervised learning on 360 video data which have spatial audio, little insights are given about why do we need to do self-supervised learning on this kind of data. In particular,",NIPS_2020_396,NIPS_2020,"1- While the experimental results suggest that the proposed approach is valuable for self-supervised learning on 360 video data which have spatial audio, little insights are given about why do we need to do self-supervised learning on this kind of data. In particular, 1) There are currently several large audio-video datasets such as HowTo100M and VIOLIN, 2) There is not much 360 video data on YouTube in comparison to normal data. 2- For the experimental comparisons, the authors at least should report the performance with using other self-supervised learning losses. For instance, masking features, predicting next video/audio feature, or reconstructing a feature. This will be very useful for understanding the importance of introduced loss in comparison with previous ones. 3- How the videos are divided into 10s segments? 4- It would be interesting to see how this spatial alignment works. For example, aligning an audio to the video and visualizing the corresponding visual region. 5- What's the impact of batch size on performance? batch size of 28 seems small to cover enough positive and negative samples. In this case, using MoCo loss instead of InfoNCE wouldn't help?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- The link between IP and the terms/equations could be explained more explicitly and prominently - Pls include labels for subfigures in Figs 3 and 4, and not just state in the captions.",NIPS_2016_478,NIPS_2016,"weakness is in the evaluation. The datasets used are very simple (whether artificial or real). Furthermore, there is no particularly convincing direct demonstration on real data (e.g. MNIST digits) that the network is actually robust to gain variation. Figure 3 shows that performance is worse without IP, but this is not quite the same thing. In addition, while GSM is discussed and stated as ""mathematically distinct"" (l.232), etc., it is not clear why GSM cannot be used on the same data and results compared to the PPG model's results. Minor comments (no need for authors to respond): - The link between IP and the terms/equations could be explained more explicitly and prominently - Pls include labels for subfigures in Figs 3 and 4, and not just state in the captions. - Have some of the subfigures in Figs 1 and 2 been swapped by mistake?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"17. Cambridge university press, 2004. Or Theorem 14.5 of Fasshauer, Gregory E. Meshfree approximation methods with MATLAB. Vol.",NIPS_2018_543,NIPS_2018,"Weakness: The main idea of the paper is not original. The entire Section 2.1 is classical results in Gaussian process modeling. There are many papers and books described it. I only point out one such source, Chapter 3 and 4 of Santner, Thomas J., Brian J. Williams, and William I. Notz. The design and analysis of computer experiments. Springer Science & Business Media, 2013. The proposed Bayes-Sard framework (Theorem 2.7), which I suspected already exist in the Monte Carlo community, is a trivial application of the Gaussian process model in the numerical integration approximation. The convergence results, Theorem 2.11 and Theorem 2.12, are also some trivial extension of the classic results of RKHS methods. See Theorem 11.11 and 11.13 of Wendland, Holger. Scattered data approximation. Vol. 17. Cambridge university press, 2004. Or Theorem 14.5 of Fasshauer, Gregory E. Meshfree approximation methods with MATLAB. Vol. 6. World Scientific, 2007. Quality of this paper is relatively low, even though the clarity of the technical part is good. This work lacks basic originality, as I pointed out in its weakness. Overall, this paper has little significance.","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
- Results should be averaged over multiple runs to determine statistical significance.,NIPS_2019_1089,NIPS_2019,"- The paper can be seen as incremental improvements on previous work that has used simple tensor products to representation multimodal data. This paper largely follows previous setups but instead proposes to use higher-order tensor products. ****************************Quality**************************** Strengths: - The paper performs good empirical analysis. They have been thorough in comparing with some of the existing state-of-the-art models for multimodal fusion including those from 2018 and 2019. Their model shows consistent improvements across 2 multimodal datasets. - The authors provide a nice study of the effect of polynomial tensor order on prediction performance and show that accuracy increases up to a point. Weaknesses: - There are a few baselines that could also be worth comparing to such as âStrong and Simple Baselines for Multimodal Utterance Embeddings, NAACL 2019â - Since the model has connections to convolutional arithmetic units then ConvACs can also be a baseline for comparison. Given that you mention that âresulting in a correspondence of our HPFN to an even deeper ConACâ, it would be interesting to see a comparison table of depth with respect to performance. What depth is needed to learning âflexible and higher-order local and global intercorrelationsâ? - With respect to Figure 5, why do you think accuracy starts to drop after a certain order of around 4-5? Is it due to overfitting? - Do you think it is possible to dynamically determine the optimal order for fusion? It seems that the order corresponding to the best performance is different for different datasets and metrics, without a clear pattern or explanation. - The model does seem to perform well but there seem to be much more parameters in the model especially as the model consists of more layers. Could you comment on these tradeoffs including time and space complexity? - What are the impacts on the model when multimodal data is imperfect, such as when certain modalities are missing? Since the model builds higher-order interactions, does missing data at the input level lead to compounding effects that further affect the polynomial tensors being constructed, or is the model able to leverage additional modalities to help infer the missing ones? - How can the model be modified to remain useful when there are noisy or missing modalities? - Some more qualitative evaluation would be nice. Where does the improvement in performance come from? What exactly does the model pick up on? Are informative features compounded and highlighted across modalities? Are features being emphasized within a modality (i.e. better unimodal representations), or are better features being learned across modalities? ****************************Clarity**************************** Strengths: - The paper is well written with very informative Figures, especially Figures 1 and 2. - The paper gives a good introduction to tensors for those who are unfamiliar with the literature. Weaknesses: - The concept of local interactions is not as clear as the rest of the paper. Is it local in that it refers to the interactions within a time window, or is it local in that it is within the same modality? - It is unclear whether the improved results in Table 1 with respect to existing methods is due to higher-order interactions or due to more parameters. A column indicating the number of parameters for each model would be useful. - More experimental details such as neural networks and hyperparameters used should be included in the appendix. - Results should be averaged over multiple runs to determine statistical significance. - There are a few typos and stylistic issues: 1. line 2: ""Despite of being compactâ -> âDespite being compactâ 2. line 56: âWe refer multiway arraysâ -> âWe refer to multiway arraysâ 3. line 158: âHPFN to a even deeper ConACâ -> âHPFN to an even deeper ConACâ 4. line 265: ""Effect of the modelling mixed temporal-modality features."" -> I'm not sure what this means, it's not grammatically correct. 5. equations (4) and (5) should use \left( and \right) for parenthesis. 6. and so onâ¦ ****************************Significance**************************** Strengths: - This paper will likely be a nice addition to the current models we have for processing multimodal data, especially since the results are quite promising. Weaknesses: - Not really a weakness, but there is a paper at ACL 2019 on ""Learning Representations from Imperfect Time Series Data via Tensor Rank Regularizationâ which uses low-rank tensor representations as a method to regularize against noisy or imperfect multimodal time-series data. Could your method be combined with their regularization methods to ensure more robust multimodal predictions in the presence of noisy or imperfect multimodal data? - The paper in its current form presents a specific model for learning multimodal representations. To make it more significant, the polynomial pooling layer could be added to existing models and experiments showing consistent improvement over different model architectures. To be more concrete, the yellow, red, and green multimodal data in Figure 2a) can be raw time-series inputs, or they can be the outputs of recurrent units, transformer units, etc. Demonstrating that this layer can improve performance on top of different layers would be this work more significant for the research community. ****************************Post Rebuttal**************************** I appreciate the effort the authors have put into the rebuttal. Since I already liked the paper and the results are quite good, I am maintaining my score. I am not willing to give a higher score since the tasks are rather straightforward with well-studied baselines and tensor methods have already been used to some extent in multimodal learning, so this method is an improvement on top of existing ones.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1) only the SimCLR case is covered and yet, there is no analysis on a seemingly important (see SimCLR-v2 and other recent papers that show that) part of that approach, ie the projection head.",NIPS_2021_2235,NIPS_2021,"(and questions):
A) The biggest weakness I think is that the analysis happens on a very restricted scenario, with no transfer: the authors study only the case where we have a single dataset and learn the encoder without using the label that we know exist and use to learn the classifiers - this is suboptimal and would not make sense in practive. I understand that this evaluation is common practice in SSL paper, however this is only a small part of the evaulations these papers have, and transfer learning is the more important and realisting setting. The authors do discuss this in lines 121-124 but justifying their choice by only citing empirical evidence of correlation of this ""task"" with transfer tasks, but I wouldn't say there are no guarantees there. Calling the second stage of classifier learning on the same dataset as traing as a ""downstream supervised task"" is an exaggeration (I would suggest to the authors to rephrase). Although this task ""correlates"" with transfer tasks, it is not clear to me if also this analysis extends. It would be great to discuss this at least a bit further.
B) Even for this task above, there are further simplifications to facilitate the analysis: 1) only the SimCLR case is covered and yet, there is no analysis on a seemingly important (see SimCLR-v2 and other recent papers that show that) part of that approach, ie the projection head. 2) The MoCo approach which is a very popular variant with a memory queue is not discussed. How does the analysis extend to negatives from a memory queue and dual encoders with exponential moving average? 3) There is a further sumplification by the use of a mean classifier, which is not common practice . Why is that simplification there, and is it central for the analysis?
C) The (absolute) numbers in Table 1 are not so intutive, unbounded and hard to understand. It is really hard to understand what is the main message of Table 1 and some of the rows, eg colisions, could perhaps be made more informative by turning them into probabilities. It is unclear what is meant in line 269 by ""10 sampled data augmentation per sample"" and unclear what reporting the Collision bound without the alpha and beta constants offer (section 4.2 is very unclear to me).
Some more notes/questions:
The discussion on clustering based SSL methods and Sec 4.4 is very restricted to this unrealistic task, that becomes even more unrealistic for clustering based pretraining. It is uncler to me what it offers.
A missing ref (Kalantidis et al ""Hard Negative Mixing for Contrastive Learning"" NeuriPS 2020) synthesizes hard negatives for contrastive SSL. Same as MoCo, it would be interesting to discuss how this analysis extends to synthetic negatives. Rating
Although an interesting study, the paper has limitations (see ""weaknesses"" section above). I would say that the current version of the paper is marginally below the acceptance threshold, but I am looking forward to the authors addressing my concerns above in their rebuttal.
Post-rebutal thoughts
The authors provided extensive responses to my questions, answering many in a satisfactory way. I still think however that a central concern listed in the original review stand: the fact that Arora et al study the same task that first learns without labels and the with labels on the same dataset (and only that task) doesn't mean that this is what should be the only task to study for ""Understanding Negative Samples in Instance Discriminative Self-supervised Representation Learning"".
In their response, the authors claim that
The self-supervised learning setting of our analysis is practical because the setting is quite similar to a semi-supervised learning setting, where we can access massive unlabeled samples and a few labeled samples.
With all due respect, I wouldn't compare this to semi-supervised learning for one key reason: as the authors also say here, in semi-supervised learning you have few labeled examples, a key property of the task. So, I would totally understand this analysis if the proposed bound was evaluated in a semi-supervised setting. This is not the case here, ie more than few labeled examples per class are used for learning the classifiers in this case.
Similarly, wrt the answer on the usage of a mean classifier:
a few-shot learning setting uses a mean classifier, namely, Prototypical Networks [9], which has been cited more than 2700 times, according to Google Scholar.
Again, in the same way, the use of a mean classifier is indeed justified for few-shot learning, but it is well known that in the case of datasets with many labels, a logistic regression classifiers is superior.
Overall, I do see some merit in this paper, yet I think the breadth of the analysis is not enough; I will keep my score to 5.
The authors do discuss some limitations, but not potential societal impacts. Given the nature of the work, the latter is not easy to assess and in my opinion it is fine to skip for a theoretical paper on SSL.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. The observation and conclusions are hidden in the experimental section. It would be great if the paper can highlight those observations and conclusions, which is very useful for understanding the trade-offs of annotation effort and corresponding training performance.",NIPS_2018_87,NIPS_2018,"weakness/questions: 1. Description of the framework: It's not very clear what Bs is in the formulation. It's not introduced in the formulation, but later on the paper talks about how to form Bs along with Os and Zs for different supervision signals. And it;s very confusing what is Bs's role in the formulation. 2. computational cost: it would be great to see an analysis about the computation cost. 3. Experiment section: it seems that for the comparison with other methods, the tracklets are also generated using different process. So it's hard to draw conclusions based on the results. Is it possible to apply different algorithms to same set of tracklets? For example, for the comparison of temporal vs temp+BB, the conclusion is not clear as there are three ways of generating tracklets. It seems that the conclusion is -- when using same tracklet set, the temp + BB achieves similar performance as using temporal signal only. However, this is not explicitly stated in the paper. 4. The observation and conclusions are hidden in the experimental section. It would be great if the paper can highlight those observations and conclusions, which is very useful for understanding the trade-offs of annotation effort and corresponding training performance. 5. comparison with fully supervised methods: It would be great if the paper can show comparison with other fully supervised methods. 6. What is the metric used for the video level supervision experiment? It seems it's not using the tracklet based metrics here, but the paper didn't give details on that.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. As several modifications mentioned in Section 3.4 were used, it would be better to provide some ablation experiments of these tricks to validate the model performance further.",ICLR_2021_243,ICLR_2021,"Weakness: 1. As several modifications mentioned in Section 3.4 were used, it would be better to provide some ablation experiments of these tricks to validate the model performance further. 2. The model involves many hyperparameters. Thus, the selection of the hyperparameters in the paper needs further explanation. 3. A brief conclusion of the article and a summary of this paper's contributions need to be provided. 4. Approaches that leveraging noisy label noise label regularization and multi-label co-regularization were not reviewed or compared in this paper.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. As mentioned in Remark 1, the KDE would requires more data when the classifier space is beyond binary. However for other approach, Zhang et. al.[44], this seems not a problem. I wonder whether it is possible to show the comparison of the performance on datasets that the decision space is beyond binary?",NIPS_2020_1264,NIPS_2020,"Weakness: 1. As mentioned in Remark 1, the KDE would requires more data when the classifier space is beyond binary. However for other approach, Zhang et. al.[44], this seems not a problem. I wonder whether it is possible to show the comparison of the performance on datasets that the decision space is beyond binary? 2. Is it possible to show the relation between the size of decision space and the amount of data required for KDE? 3. Experiments: a. My first concerns in the experiment section is whether the dataset is challenging enough? The setting of all real-world datasets seems very simplified. Specifically, the decision space is binary and only ONE attribute is considered as sensitive information. Therefore, I wonder whether the proposed approach would work in a more complex setting where more attributes are considered as sensitive information. b. I have another concerns about the potential impact of the proposed approach. Specifically, I wonder would this approach be generalized to other tasks? Comparing with the experiments in [44], the approach in [44] seems could be applied into a more broad use case, which is the de-biasing on word embedding. However, it seems non-trivial for the proposed approach to generalize beyond the binary decision problem. c. Comparing with [44], apart from stabilized training, what is the advantage of the proposed approach over [44]. Performance wise, it seems the proposed approach achieves similar results as [44] in Credit Card dataset and COMPAS dataset. Computational time wise, the proposed approach is actually slower than [44] on Law School dataset and adult census datasets. Application wise, it seems [44] could be applied to a broader application domain than the proposed approach.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2) How the capacity of the SR model affects the FID. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.",ICLR_2021_1310,ICLR_2021,"Lack of some critical comparisons: 1) Does the proposed method also outperform the approach that first generates using a pre-trained BigGAN-256 then upsamples using an officially pre-trained ESRGAN? 2) What about the inference time (or complexity) of NSB-GAN compared to BigGAN?
If the decoders are trained with real samples only (drawn from the imagenet dataset), the upsampled outputs may have visual artifacts due to the mismatched distribution between the train (real) and test (fake gen from sampler) images. For example, the images of the cabinet (Fig3, 4, 5th row) have overly sharpened artifacts that BigGAN does not suffer.
Overall, the suggested work effectively decreases the training time of the BigGAN using a simple idea. However, there are missing comparisons and analyses such as 1) comparison with pre-trained BigGAN -> ESRGAN 2) How the capacity of the SR model affects the FID. And lastly, since the proposed method is pipelining, there are some unexpected artifacts.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"6. Appendix A is left blank, and the purpose of Proposition B.1 in Appendix B is unclear—is it merely meant to illustrate the classic partitioning principle of K-means? This is a well-known concept in machine learning, and furthermore, the authors’ so-called “proof” is missing.",JXvEzl8YkS,ICLR_2025,"1. Although I am quite familiar with regime-switching, identification, and financial markets, the logical structure of this paper is challenging to follow. The introduction directly presents model definitions and the authors’ proposed improvements without first providing an overview of related work, the motivation, or the current state of research. This structure makes it difficult for readers to understand the unique contributions of this work and distinguish them from existing models. A clearer, more organized flow would greatly enhance readability and clarify the authors’ contributions.
2. The paper lacks a detailed motivation explaining why Regularised K-means, specifically, is optimal for adapting into the Jump Model framework. A comparison of Regularised K-means with alternative approaches could clarify why it is most suitable in this context.
3. The paper briefly mentions Hidden Markov Models (HMM) and other regime-switching models but does not provide a thorough comparison, such as the latest model - RHINE: A Regime-Switching Model with Nonlinear Representation for Discovering and Forecasting Regimes in Financial Markets (SIAM SDM2024). Besides, although feature selection is central to the model, there is limited discussion on alternative feature selection methods in time series analysis.
4. The formulas (such as those in Eq. 1.6) lack detailed explanations for how each component, including the penalty terms $ P(\mu) $, specifically enhances feature selection and regime accuracy.
5. The experimental part does not demonstrate how regime identification or interpretability is achieved. Additionally, there are no actual experimental results presented in the main text, yet the pseudo-code of the algorithms takes up several pages.
6. Appendix A is left blank, and the purpose of Proposition B.1 in Appendix B is unclear—is it merely meant to illustrate the classic partitioning principle of K-means? This is a well-known concept in machine learning, and furthermore, the authors’ so-called “proof” is missing.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"5) The paper lacks additional necessary experiments, including comparison experiments, ablation studies, and hyperparameter analysis, etc.",zBrjRswpkg,ICLR_2025,"1) There is no clear representation of the motivation and contributions of the paper.
2) The theoretical analysis is limited, aspects such as convergence analysis and constraint violations related to constraint learning are not addressed.
3) The overall presentation of the paper might benefit from improvements, as it does not clearly convey its main claims and contains some expression errors.
4) The experimental results do not seem to adequately support the theoretical analysis. For example, Figure 3 shows significant constraint violations in CCPG w/PC.
5) The paper lacks additional necessary experiments, including comparison experiments, ablation studies, and hyperparameter analysis, etc.
6) The baselines and experimental environments in the paper are too few to illustrate the validity of the method.
7) There is no code or detailed implementation description provided to support the reproducibility of the results.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The authors introduce several approximations (i -iii) which leaves loose ends. It is understandable that approximations are necessary to derive clean results, but the possible vulnerability (e.g., due to the assumption of attacks being in the feasible set only in lines 107-110) needs to be expanded to reassure the readers that it is not a real concern.",NIPS_2017_337,NIPS_2017,"Not many, but there are several approximations made in the derivation. Also the strong assumptions of convexity and norm-based defense were made.
Qualitative evaluation Quality:
The paper is technically sound for the most part, except for severals approximation steps that perhaps needs more precise descriptions. Experiments seem to support the claims well. Literature review is broad and relevant. Clarity:
The paper is written and organized very well. Perhaps the sections on data-dependent defenses are a bit complex to follow. Originality:
While the components such as online learning by regret minimization and minimax duality may be well-known, the paper uses them in the context of poisoning attack to find a (approximate) solution/certificate for norm-based defense, which is original as far as I know. Significance:
The problem addressed in the paper has been gaining importance, and the paper presents a clean analysis and experiments on computing an approximate upperbound on the risk, assuming a convex loss and/or a norm-based defense.
Detailed comments
I didn't find too many things to complain. To nitpick, there are two concerns about the paper.
1. The authors introduce several approximations (i -iii) which leaves loose ends. It is understandable that approximations are necessary to derive clean results, but the possible vulnerability (e.g., due to the assumption of attacks being in the feasible set only in lines 107-110) needs to be expanded to reassure the readers that it is not a real concern.
2. Secondly, what relevance does the framework have with problems of non-convex losses and/or non-norm type defenses? Would the non-vanishing duality gap and the difficulty of maximization over non-norm type constraints make the algorithm irrelevant? Or would it still give some intuitions on the risk upperbound?
p.3, binary classification: If the true mean is known through an oracle, can one use the covariance or any other statistics to design a better defense?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"5. In my opinion, the contribution of this paper appears somewhat limited, and the proposed model seems incremental in its approach.",iQHL76NqJT,ICLR_2024,"1. In the Introduction, the authors assert that ""i) To the best of our knowledge, we are the first to learn node embeddings using the abstention-based GAT architecture."" This claim seems overstated.
2. In Section 3.1, the authors introduce NodeCwR-Cov and mention that ""There are two more fully connected layers after the softmax layer (with 512 nodes and one node) to model the selection function g."" The meaning of ""having 512 nodes and one node"" is unclear in this context.
Additionally, the selection function threshold is set to 0.5, but the rationale behind choosing this value and its impact on the model or performance is not explained.
This threshold serves to filter eligible candidates. It is essential to consider the accuracy of these candidates for each threshold, as they significantly impact the overall performance.
3. The presentation of results in tables and figures is unclear. For instance, in Table 1, the meanings of Cov and LS are not explained.
The experimental analysis lacks depth and clarity.
4. GAT is chosen as the backbone for the proposed model. How does it compare to other graph neural network models?
5. In my opinion, the contribution of this paper appears somewhat limited, and the proposed model seems incremental in its approach.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"* The proposed model demonstrate impressive performance on many benchmarks (setting new SoTA scores) but more careful analysis probably is needed, especially for some pretty ""old"" benchmarks that the data might have been indirectly seen by the model via the ""data curation"" process. More details about the evaluation procedures would be helpful.",Ux0BEP46fd,ICLR_2025,"* The key techinique behind the dataset collection is to enrich existing video datasets with aligned, cross-modality representations, which is achieved by leveraging off-the-shelf pre-trained models, e.g. captions to video frames and audio. The quality of the multi-modal datasets is directly affected by the selected models. However, this part is only briefly mentioned in Section 3.1. More ablations or discussions would be beneficial regarding this part, as I presume this affects the generic applicablity of the proposed dataset collection approach, i.e. whether the potential distribution shift between the pre-trained model checkpoint and the source video dataset would affect the quality of the generated dataset.
* The proposed model demonstrate impressive performance on many benchmarks (setting new SoTA scores) but more careful analysis probably is needed, especially for some pretty ""old"" benchmarks that the data might have been indirectly seen by the model via the ""data curation"" process. More details about the evaluation procedures would be helpful.
* There exists quite some grammatical errors and typos (e.g. L181-182, L212, L216, etc.) The paper would benefit from more careful revisions.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. In the experimental part, this paper verifies different metrics for different OPE methods. However, in Figure 4 and Figure 5, the different methods in the two sets of benchmarks proposed in this article are quite different in different OPE methods. I hope the author can give some comments on the differences between the two sets of evaluation methods.",ICLR_2021_375,ICLR_2021,"1. The biggest problem with this article is that the contribution of the article is insufficient and lacks originality. This article proposes a benchmark for off-policy evaluation and verifies different OPE methods, but this article does not compare with other similar benchmarks to verify whether the benchmark proposed in this article is effective. 2. In the experimental part, this paper verifies different metrics for different OPE methods. However, in Figure 4 and Figure 5, the different methods in the two sets of benchmarks proposed in this article are quite different in different OPE methods. I hope the author can give some comments on the differences between the two sets of evaluation methods. 3. The author uses the value function in formula 1 to estimate the effect of the strategy. I doubt this method. Because of the attenuation factor here, I think it has an impact on the final value calculated by different methods. Because bellman equation is only an estimate of the value of a certain state, not an absolute strategy benefit, I hope the author will give some explanations here.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
". From the experimental results, without the prior information (the same as the benchmarks) the proposed method has no advantage compared to the SOTA. The advantage only shows when using the prior knowledge. Such comparison is a bit unfair, because in this case the proposed method essentially requires two representation models learned based on each dataset, ie., VAE/GAN + CL. Such extra complexity and cost need to be considered.",ICLR_2023_2851,ICLR_2023,"Loss function: The proposed Decoupled Uniformity loss extends the uniformity loss in [54] and does not explicitly include the alignment loss in [54], which is elegant. The authors claimed that the proposed loss function can implicitly encourage alignment (also showed theoretically in Theorem 1 with insufficient training samples). The reviewer wonder what if you add the alignment loss similar to [54]. Would that increase or degrade the performance in practice, ie., when the number of the training samples is neither too low nor infinity? As shown in the experiment, without the prior information, the proposed method cannot beat the SOTA.
Assumptions: The authors introduced the Weak-aligned encoder, which is claimed to be weaker than previous assumptions such as L-smoothness. In the implementation, how to ensure this assumption is satisfied?
Assumptions: How to ensure Assumption 2 in implementation?
Definition 3.7: How to find the value of \lambda? Experiments:
. From the experimental results, without the prior information (the same as the benchmarks) the proposed method has no advantage compared to the SOTA. The advantage only shows when using the prior knowledge. Such comparison is a bit unfair, because in this case the proposed method essentially requires two representation models learned based on each dataset, ie., VAE/GAN + CL. Such extra complexity and cost need to be considered.
. Kernel quality is crucial to the downstream performance as shown experimentally in Appendix Fig. 4. In the implementation, how to choose a proper kernel with optimal parameters? Are these parameters jointly learned or picked via the validation sets?
. In Table 2, the performance of the proposed method is shown with 4 views. Do the benchmarks also use 4 views? If not, please provide results with 2 views for the fair comparison.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.,NIPS_2017_401,NIPS_2017,"Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.
Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility:
Appears to be reproducible.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1) experimental settings for Figure 1 to Figure 9 are totally missing, which makes them hard to be convincing;",ICLR_2023_4768,ICLR_2023,".
The method.
Although I like the experimental findings in Section 4. I have a lot of concerns: 1) experimental settings for Figure 1 to Figure 9 are totally missing, which makes them hard to be convincing; 2) how about the observations when using different teacher-student pairs (with either similar or different network architectures? 3) the metrics (Eq. 4 and its reduced version Eq. 7) are not explicitly used in the designs of FT-KD and PESF-KD. This breaks the flow of the method section. Why not to add a related loss in the optimization?
The proposed FT-KD and PESF-KD are heavily based on existing works deep mutual learning (DML) [1] and parameter-efficient transfer learning methods [2-3], leading to limited technical novelty. What's more, they actually need huge training costs instead of more efficient claimed by the authors, in my understanding (see below comments for details).
To me, claiming FT-KD as a new contribution (the authors call it an adaptive knowledge transfer learning method) is misleading to some degree. As FT-KD merely removes the online knowledge transfer path from the student to the teacher, in DML. Besides, in FT-KD, is the teacher model pre-trained beforehand or trained from scratch jointly the student model? I think, it should be pre-trained beforehand as the authors state that it will be fine-tuned during the training of the student model, is it true?
Note that PESF-KD (may also include FT-KD) uses the pre-trained teacher model. That is, PESF-KD is not a real online KD method. It not only needs the pre-trained stage, but also the online joint training stage. As a result, the claimed 'more efficient' advantage is totally misleading.
[1] Deep Mutual Learning, CVPR 2018.
[2] TinyTL: Reduce Memory, Not Parameters for Efficient On-Device Learning, NeurIPS 2020.
[3] Towards a Unified View of Parameter-Efficient Transfer Learning, ICLR 2022.
The experiments.
The improvement of the proposed methods is mostly marginal, compared to DML, or other competitive methods.
Note that PESF-KD (may also include FT-KD) uses the pre-trained teacher model, and thus the experimental comparisons of training cost in Table 2, Table 3, etc. are wrong. That is, PESF-KD is not a real online KD method. It not only needs the pre-trained stage, but also the online joint training stage. As a result, the claimed 'more efficient' advantage is totally misleading.
In performance comparison, the authors only consider relatively old KD methods, many recently proposed high-performance KD methods are ignored. Some representative methods are SSKD, SRRL, SemCKD, ReviewKD and SimKD. Others.
The writing can be improved significantly: 1) there are so many bolded descriptions; 2) the organization of figures is bad; 3) there are many typos and grammar errors. ----Update----
I keep my original score as my concerns are not well addressed or ignored in the rebuttal.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The rationale behind this work stems from the observation that current parameter isolation methods often hinder the acquisition of new task knowledge. Thus, the authors suggest pathway protection based on the sparsity exhibited by activation channels in deep networks. However, some parameter isolation methods are specifically tailored to leverage this sparsity. The clarification on how the proposed method avoids impeding the learning of new task knowledge remains ambiguous.",tVNZj27pb3,ICLR_2025,"1. The rationale behind this work stems from the observation that current parameter isolation methods often hinder the acquisition of new task knowledge. Thus, the authors suggest pathway protection based on the sparsity exhibited by activation channels in deep networks. However, some parameter isolation methods are specifically tailored to leverage this sparsity. The clarification on how the proposed method avoids impeding the learning of new task knowledge remains ambiguous.
2. The terms ""pathway"" and ""channel"" lack clarity. A precise definition of these concepts and a comparison with prior works such as Piggyback, Packnet, and MEAT [1] would enhance the understanding. The proposed method appears conceptually similar to these existing works.
3. The language used in the paper is perplexing. For instance, in the abstract: ""Given the sparsity of activation channels in a deep network, we introduce a novel misaligned fusion method within the context of continuous learning."" The meaning of ""misaligned fusion"" and its relevance is unclear.
4. Line #80~81: ""Therefore, pathways protection is all you need."" Such a statement lacks empirical support. It would be beneficial for the authors to provide more rigorous evidence beyond mere examples.
5. Line #239~241: ""Graph matching bears resemblance to a quadratic assignment problem (QAP) (Loiola et al., 2007), with the objective of establishing correspondences between the nodes in an image and the edges connecting these nodes."" The reference to ""nodes in an image"" requires clarification. Additionally, line #246: ""In our framework, a deep network is conceptualized as an image."" The authors offer no elucidation on how a deep network can be likened to an image.
Given the uncertainties raised, evaluating the work poses a challenge. The authors appear to lack sufficient writing and publishing experience. I will proceed with the review and reassess the work following a response from the authors.
### Reference:
[1] Meta-Attention for ViT-Backed Continual Learning, CVPR 2022.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2) The regularization term seems a bit ad-hoc. Although the author has provided some intuitive explanation of the regularization, it seems lack of theoretical support. There are some other statistics which may be used to replace role of the mean and standard derivation in the regularization. Why they are not adapted in the regularization? For example, the median which is not sensitive to outlier value of data can be used to replace mean value.",NIPS_2018_177,NIPS_2018,"weakness and issues which should be clarified: 1) The novelty of this paper is incremental. The proposed method is developed based on the MDNet framework. It seems that the only difference is that the proposed method further incorporate the attention regularization for backward propagation. 2) The regularization term seems a bit ad-hoc. Although the author has provided some intuitive explanation of the regularization, it seems lack of theoretical support. There are some other statistics which may be used to replace role of the mean and standard derivation in the regularization. Why they are not adapted in the regularization? For example, the median which is not sensitive to outlier value of data can be used to replace mean value. 3) The author claims that the proposed method can enable the classifier attend to temporal invariant motion patterns. It seems that no explanation is provided about what motion patterns mean in this paper. Although some figures show the evolvement of attention during training, no motion pattern is illustrated. In addition, some large variations may happen during the tracking process, such as out-plane-rotation, how can the proposed method ensure that the temporal motion invariant pattern can be found and the classifiers can attend to them? [POST-REBUTTAL COMMENTS] I have read the rebuttal and still have the concerns on the theoretical support for the regularization term. I keep my rating.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. In the experimental section, the authors have not conducted comparisons with existing fairness algorithms. Integrating benchmark comparisons against state-of-the-art fairness algorithms would significantly enhance the paper. It would offer tangible evidence of the proposed method's performance and effectively position the ManyFairHPO framework within the existing FairML research landscape.",LIBZ7Mp0OJ,ICLR_2024,"1.	While the authors propose optimizing model fairness through the Pareto frontier and simultaneous measurement of multiple fairness indicators, they do not provide a theoretical demonstration of how trading off one fairness metric for another could lead to an overall improvement in model fairness. A deeper theoretical exploration in this area could strengthen the paper, offering clearer guidelines on how to navigate fairness trade-offs effectively.
2.	The paper lacks theoretical analysis on how to select among different Pareto-optimal outcomes, especially when one fairness metric is already at its optimal is one of the Pareto-optimal outcomes, i.e., there is no difference from a single optimization outcome. A theoretical framework or set of criteria for making these choices would be beneficial, providing practitioners with a robust method for decision-making in situations with multiple optimal fairness solutions.
3.	The authors only use one performance to evaluate the model performance. In the context of FairML, where the applications are intricate and multifaceted, relying on a single performance metric may not sufficiently capture the model’s overall performance and impact. A diverse set of performance metrics would provide a more holistic view, ensuring a balanced and thorough evaluation.
4.	In the experimental section, the authors have not conducted comparisons with existing fairness algorithms. Integrating benchmark comparisons against state-of-the-art fairness algorithms would significantly enhance the paper. It would offer tangible evidence of the proposed method's performance and effectively position the ManyFairHPO framework within the existing FairML research landscape.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2 The authors should discuss the iteration cost (computational budge) of the proposed method. It will be great if the authors discuss the iteration cost of all related methods including baseline methods.,NIPS_2022_1292,NIPS_2022,"1 Discuss whether the proposed method can be applied to a discrete distribution with infinite support such as a Poisson distribution.
2 The authors should discuss the iteration cost (computational budge) of the proposed method. It will be great if the authors discuss the iteration cost of all related methods including baseline methods.
3 The running time instead of the number of training steps should be included in at least one of the plots.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Experimental section: a. Need to report average over multiple runs. Results are very close together and it is hard to favor one method. b. Sec. 3.1: Since this is the toy-dataset, a discussion why the decision boundaries look as they do, would be interesting. c. Sec. 3.3: What information is in Fig. 9 middle and right?",ICLR_2021_1966,ICLR_2021,", Suggestions, Questions: 1. A theoretical discussion about following points will improve the contribution of the paper: a. Why do large margins result in higher adversarial robustness? What happens if I change the attack type? b. Benefits compared over other adversarial training methods are not clear. c. A more detailed discussion about the equilibrium state is necessary, as currently provided in Sec. 2.3. This is rather an example. 2. Experimental section: a. Need to report average over multiple runs. Results are very close together and it is hard to favor one method. b. Sec. 3.1: Since this is the toy-dataset, a discussion why the decision boundaries look as they do, would be interesting. c. Sec. 3.3: What information is in Fig. 9 middle and right? 3. Formatting and writing: a. Detailed proofreading required. e.g. on p. 3 “using cross-entropy loss and clean data for training” b. Some variables are used but not introduced. e.g. x_n1, x_n2 in Sec. 2.3. c. Figures are too small and not properly labeled in experimental section. d. References to prior work are missing as e.g. “Virtual Adversarial Training: A Regularization Method for Supervised and Semi-Supervised Learning” e. Algorithms need rework, e.g. information of Alg. 1 can be written in 2,3 lines.
Though the idea of adaptive adversarial noise magnitude is in general appealing, the paper has some weaknesses: (i) theoretical contribution is relatively minor, (ii) the paper does not present the material sufficiently clearly to the reader, and (iii) experimental evaluation is not sufficiently conclusive in favor of the paper's central hypothesis.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- The new proposed model can be used only with a small number of dimensions because of the curse of dimensionality imposed by the core tensor C.,NIPS_2022_2592,NIPS_2022,"- (major) I don’t agree with the limitation (ii) of current TN models: “At least one Nth-order factor is required to physically inherit the complex interactions from an Nth-order tensor”. TT and TR can model complex modes interactions if the ranks are large enough. The fact that there is a lack of direct connections from any pair of nodes is not a limitation because any nodes are fully connected through a TR or TT. However, the price to pay with TT or TR to model complex modes interactions is having bigger core tensor (larger number of parameters). The new proposed topology has also a large price to pay in terms of model size because the core tensor C grows exponentially with the number of dimensions, which makes it intractable in practice. The paper lacks from a comparison of TR/TT and TW for a fixed size of both models (see my criticism to experiments below). - The new proposed model can be used only with a small number of dimensions because of the curse of dimensionality imposed by the core tensor C. - (major) I think the proposed TW model is equivalent to TR by noting that, if the core tensor C is represented by a TR (this can be done always), then by fusing this TR with the cores G_n we can reach to TR representation equivalent to the former TW model. I would have liked to see this analysis in the paper and a discussion justifying TW over TR. - (major) Comparison against other models in the experiments are unclear. The value of the used ranks for all the models are omitted which make not possible a fair comparison. To show the superiority of TW over TT and TR, the authors must compare the tensor completion results for all the models but having the same number of model parameters. The number of model parameters can be computed by adding the number of entries of all core tensors for each model (see my question about experiment settings below). - (minor) The title should include the term “tensor completion” because that is the only application of the new model that is presented in the paper. - (minor) The absolute value operation in the definition of the Frobenius norm in line 77 is not needed because tensor entries are real numbers. - (minor) I don’t agree with the statement in line 163: “Apparently, the O(NIR^3+R^N) scales exponentially”. The exponential grow is not apparent, it is a fact.
I updated my scores after rebuttal. See my comments below
Yes, the authors have stated that the main limitation of their proposed model is its exponentionally grow of model parameters with the number of dimensions.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', 'X', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"• Supervised pretraining based on the prediction of homo-lumo gap may lead to negative transfer. For example, on QM9 in downstream experiments, Transformer-M performs poorly on most tasks other than homo, lumo, and gap. This may be contradictory to the description ""general-purpose neural network model"" claimed in this paper.",ICLR_2023_1503,ICLR_2023,"• 2D and 3D information encoding are from previous work. This work just simply combines them, and the model architecture is the same as Graphormer[1]. The novelty is not enough.
• Supervised pretraining based on the prediction of homo-lumo gap may lead to negative transfer. For example, on QM9 in downstream experiments, Transformer-M performs poorly on most tasks other than homo, lumo, and gap. This may be contradictory to the description ""general-purpose neural network model"" claimed in this paper.
• Lack of description of PDBBind data processing and splitting in downstream tasks.
• Absence of some ablation experiments: ①(p2D, p3D, p2D&3D) = (1:0:0) / (0:1:0) / (0:0:1); ②Only using the 3D position denoising task while pretraining.
Other questions：
• Do the authors consider encoding 1D molecular data mode, e.g., SMILES, simultaneously?
• What do the authors think about the possibility of negative transfer on downstream tasks due to the supervised signal introduced during pretraining?
• Whether there is data leakage during finetuning on PDBBind, because we know that the general, refined, and core sets have overlapping parts.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2.a) The authors say on lines 80-82 that the center correlation was not insightful for discriminating model defenses, but then use that metric in figure 4 A&B. I’m wondering why they found it useful here and not elsewhere? Or what they meant by the statement on lines 80-82.",NIPS_2021_1222,NIPS_2021,"Claims: 1.a) I think the paper falls short of the high-level contributions claimed in the last sentence of the abstract. As the authors note in the background section, there are a number of published works that demonstrate the tradeoffs between clean accuracy, training with noise perturbations, and adversarial robustness. Many of these, especially Dapello et al., note the relevance with respect to stochasticity in the brain. I do not see how their additional analysis sheds new light on the mechanisms of robust perception or provides a better understanding of the role stochasticity plays in biological computation. To be clear - I think the paper is certainly worthy of publication and makes notable contributions. Just not all of the ones claimed in that sentence.
1.b) The authors note on lines 241-243 that “the two geometric properties show a similar dependence for the auditory (Figure 4A) and visual (Figure 4B) networks when varying the eps-sized perturbations used to construct the class manifolds.” I do not see this from the plots. I would agree that there is a shared general upward trend, but I do not agree that 4A and 4B show “similar dependence” between the variables measured. If nothing else, the authors should be more precise when describing the similarities.
Clarifications: 2.a) The authors say on lines 80-82 that the center correlation was not insightful for discriminating model defenses, but then use that metric in figure 4 A&B. I’m wondering why they found it useful here and not elsewhere? Or what they meant by the statement on lines 80-82.
2.b) On lines 182-183 the authors note measuring manifold capacity for unperturbed images, i.e. clean exemplar manifolds. Earlier they state that the exemplar manifolds are constructed using either adversarial perturbations or from stochasticity of the network. So I’m wondering how one constructs images for a clean exemplar manifold for a non-stochastic network? Or put another way, how is the denominator of figure 2.c computed for the ResNet50 & ATResNet50 networks?
2.c) The authors report mean capacity and width in figure 2. I think this is the mean across examples as well as across seeds. Is the STD also computed across examples and seeds? The figure caption says it is only computed across seeds. Is there a lot of variability across examples?
2.d) I am unsure why there would be a gap between the orange and blue/green lines at the minimum strength perturbation for the avgpool subplot in figure 2.c. At the minimum strength perturbation, by definition, the vertical axis should have a value of 1, right? And indeed in earlier layers at this same perturbation strength the capacities are equal. So why does the ResNet50 lose so much capacity for the same perturbation size from conv1 to avgpool? It would also be helpful if the authors commented on the switch in ordering for ATResNet and the stochastic networks between the middle and right subplots.
General curiosities (low priority): 3.a) What sort of variability is there in the results with the chosen random projection matrix? I think one could construct pathological projection matrices that skews the MFTMA capacity and width scores. These are probably unlikely with random projections, but it would still be helpful to see resilience of the metric to the choice of random projection. I might have missed this in the appendix, though.
3.b) There appears to be a pretty big difference in the overall trends of the networks when computing the class manifolds vs exemplar manifolds. Specifically, I think the claims made on lines 191-192 are much better supported by Figure 1 than Figure 2. I would be interested to hear what the authors think in general (i.e. at a high/discussion level) about how we should interpret the class vs exemplar manifold experiments.
Nitpick, typos (lowest priority): 4.a) The authors note on line 208 that “Unlike VOneNets, the architecture maintains the conv-relu-maxpool before the first residual block, on the grounds that the cochleagram models the ear rather than the primary auditory cortex.” I do not understand this justification. Any network transforming input signals (auditory or visual) would have to model an entire sensory pathway, from raw input signal to classification. I understand that VOneNets ignore all of the visual processing that occurs before V1. I do not see how this justifies adding the extra layer to the auditory network.
4.b) It is not clear why the authors chose a line plot in figure 4c. Is the trend as one increases depth actually linear? From the plot it appears as though the capacity was only measured at the ‘waveform’ and ‘avgpool’ depths; were there intermediate points measured as well? It would be helpful if they clarified this, or used a scatter/bar plot if there were indeed only two points measured per network type.
4.c) I am curious why there was a switch to reporting SEM instead of STD for figures 5 & 6.
4.c) I found typos on lines 104, 169, and the fig 5 caption (“10 image and”).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. Another minor comment is on the name used to describe the phenomenon: distributional generalization sounds a bit strong to capture the empirical phenomenon presented. It represents the ideal of the total variation between the test and train distributions of the network’s outputs vanishing to zero which might not be the case. It is hard to draw this conclusion from a few test functions on which the outputs match.,ICLR_2022_2213,ICLR_2022,"1. Based on some efforts to reproduce the results on my end, it is not clear how strongly the proposed observation holds which might limit the significance of the contributions of the paper.
Comments and questions: 1. What would constitute distributional generalization in the setting of regression? If I consider the setting of regression for a moment, the phenomenon appears to be less surprising: a reasonably smooth regression model which interpolates the train data would necessarily exhibit distributional generalization in that setting. 2. It would help to see some error bars on the plots in Figure 2B. I tried replicating the toy example (classify CIFAR-10 classes as objects vs animals with label noise on cats) and observed that when label noise is 30% on the train data only 2-5% of cats in test set were being labeled as objects. The network used was a ResNet50 trained to train accuracy 96.2% using SGD with learning rate = 0.1, momentum = 0.9 and weight decay of 5e-4 and trained for ~160 epochs. However, I did observe that when the label noise is increased to 70%, the distributional generalization effect was seen more strongly (test cats labeled objects 60-80% of the time). 3. It would make for a stronger case if the paper reports the numbers observed when the label noise experiment is performed on image-net with 1000 classes as well (at least on the non-tail classes). This would further stress test the conjecture. Even if the phenomenon significantly weakens in this setting, the numbers are worth seeing.
Minor comments: 1. AlexNet top-1 accuracy on ImageNet reported as 56.5%. Isn’t this 63.3%? 2. Another minor comment is on the name used to describe the phenomenon: distributional generalization sounds a bit strong to capture the empirical phenomenon presented. It represents the ideal of the total variation between the test and train distributions of the network’s outputs vanishing to zero which might not be the case. It is hard to draw this conclusion from a few test functions on which the outputs match.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1) I still find the theoretical contribution ok but not particularly strong, given existing results. As mentioned in the review, it is a weak, unpractical bound, and the proof, in itself, does not provide particular mathematical novelty.",NIPS_2019_1364,NIPS_2019,"weakness of the paper. Questions: * In which cases the assumptions of theorems 3,4 hold? In addition to SLC, they have some matroid related assumptions. Since these results intend to demonstrate the power of the SLC class, these should be discussed in more detail. * How the diversity related \alpha enters the mixing bounds? It seems that the bound depends very weakly on \alpha only through \nu(S_0). Edit following author's response: I'm inclined to keep my score of 6. This is due to following reasons: 1) I still find the theoretical contribution ok but not particularly strong, given existing results. As mentioned in the review, it is a weak, unpractical bound, and the proof, in itself, does not provide particular mathematical novelty. 2) The claims that ""in practice the mixing time is even better"" are not nearly sufficiently supported by the experiments, and therefore the evidence provided to practitioners is very limited. 3) My question regarding dependence on $\alpha$ was not answered in a satisfactory manner. I would expect a more explicit dependence on $\alpha$, since with higher diversity the problem should be more complicated. If this is not reflected in the bounds, it means the bounds are very loose.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '0', '0']}",,
"2. Experimental Evaluation 2.1. Ablations 2.1.1. The paper claims that âAnother distinction of our approach is the âpickingâ step â. However, this aspect is not ablated. 2.2. Experiments on CIFAR. The comparison on CIFAR is not convincing 2.2.1. The continual learning literature has extensive experiments on this dataset and the paper only compares to one approach (DEN). 2.2.2. It is unclear if DEN is correctly used/evaluated. It would have been more convincing if the authors used the same setup as in the DEN paper to make sure the comparison is fair/correct.",NIPS_2019_1225,NIPS_2019,"1. Determining hyperparameters and reporting complexity 1.1. The paper requires setting âaccuracy goalsâ when encountering a new task. However, it might be unclear which accuracy can be reached and the paper is opaque how these accuracy goals are determined e.g. when comparing to prior work. To reach optimal performance algorithm 1 might need significant manual intervention. 1.1.1. How are the âaccuracy goalsâ determined (especially for Table 6,7)? 1.1.2. What happens if growing the network does not lead to achieving the accuracy goal? E.g. increasing the network capacity might lead to stronger overfitting and a reduced accuracy? 1.2. The approach may need many iterations to retrain the model to meet the âaccuracy goalâ (both w.r.t. growing and compressing) 1.3. How much is the model grown, how much is picked, how much is compressed? It would be interesting to see this for the different models in Table 6, as well as the accuracy targets. 1.4. It would be good to report the memory overhead from the binary masks and relate this to memory-based approached such as GEM, A-GEM, and generative replay. 2. Experimental Evaluation 2.1. Ablations 2.1.1. The paper claims that âAnother distinction of our approach is the âpickingâ step â. However, this aspect is not ablated. 2.2. Experiments on CIFAR. The comparison on CIFAR is not convincing 2.2.1. The continual learning literature has extensive experiments on this dataset and the paper only compares to one approach (DEN). 2.2.2. It is unclear if DEN is correctly used/evaluated. It would have been more convincing if the authors used the same setup as in the DEN paper to make sure the comparison is fair/correct. 3. Motivation 3.1. The paper claims forgetting is fully avoided due to the usage of a mask. While it is true that *after* model compression no further forgetting happens, but there is an accuracy drop during pruning, in contrast to e.g. regularization-based methods. Specifically, the original value (before pruning) is not recoverable and hence should be reported as forgetting. 4. The checklist is not fully accurate. The paper does not provide error bars and std-deviation for experiments. 5. Minor: 5.1. Grammar issue in word âdeterminingâ in the 4th paragraph on page 3. 5.2. On page 3, in âMethod overviewâ it says âAn overview of our method is depicted belowâ whereas it should directly refer to Figure 1 because Figure 1 is on page 2 5.3. On page 6, right below Figure 2, it says âin all experiments, but realize DENâ. Word ârealizeâ does not fit into the context. 5.4. In future, please use the submission template (not the camera-ready version) so that line numbers on the margins can be used to easily refer to the text. I lean more towards accept: The overall convincing results (especially Table 6) and overall novel model outweigh the limitations discussed above.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Nitpicking: I find ""above/below diagonal"" (add a thin gray diagonal to the plot) easier to interpret than ""above/below 45 degree"", which sounds like a local property (e.g., not the case where the red line saturates and has ""0 degrees"").",NIPS_2018_567,NIPS_2018,"(bias against subgroups, uncertainty on certain subgroups), in applications for fair decision making. The paper is clearly structured, well written and very well motivated. Except for minor confusions about some of the math, I could easily follow and enjoyed reading the paper. As far as I know, the framework and particularly the application to fairness is novel. I believe the general idea of incorporating and adjusting to human decision makers as first class citizens of the pipeline is important for the advancement of fairness in machine learning. However, the framework still seems to encompass a rather minimal technical contribution in the sense that both a strong theoretical analysis and exhaustive empirical evaluation are lacking. Moreover, I am concerned about the real world applicability of the approach, as it mostly seems to concern situations with a rather specific (but unknown) behavior of the decision maker, which typically does not transfer across DMs, needs to be known during training. I have trouble thinking of situations where sufficient training data, both ground truth and the DMs predictions, are available simultaneously. While the authors do a good job evaluating various aspects of their method (one question about this in the detailed comments), those are only two rather simplistic synthetic scenarios. Because of the limited technical and experimental contribution, I heavy-heartedly tend to vote for rejection of the submission, even though I am a big fan of the motivation and approach. Detailed Comments - I like the setup description in Section 2.1. It is easy to follow and clearly describes the technical idea of the paper. - I have trouble understanding (the proof of) the Theorem (following line 104). You show that eq (6) and eq (7) are equal for appropriately chosen $\gamma_{defer}$. However, (7) is not the original deferring loss from eq (3). Shouldn't the result be that learning to defer and rejection learning are equivalent if for the (assumed to be) constant DM loss, $\alpha$ happens to be equal to $\gamma_{reject}$? In the theorem it sounds as if they were equivalent independent of the parameter choices for $\gamma_{reject}$ and $\alpha$. The main takeaway, namely that there is a one-to-one correspondence between rejection learning with cost $\gamma_{reject}$ and learning to defer with a DM with constant loss $\alpha$, is still true. Is there a specific reason why the authors decided to present the theorem and proof in this way? - The authors highlight various practical scenarios in which learning to defer is preferable and detail how it is expected to behave. However, this practicability seems to be heavily impaired by the strong assumptions necessary to train such model, i.e., availability of ground truth and DM's decisions for each DM of interest, where each is expected to have their own specific biases/uncertainties/behaviors during training. - What does it mean for the predictions \hat{Y} to follow an (independent?) Bernoulli equation (12) and line 197? How is p chosen, and where does it enter? Could you improve clarity by explicitly stating w.r.t. what the expectations in the first line in (12) are taken (i.e., where does p enter explicitly?) Shouldn't the expectation be over the distribution of \hat{Y} induced by the (training) distribution over X? - In line 210: The impossibility results only hold for (arguably) non-trivial scenarios. - When predicting the Charlson Index, why does it make sense to treat age as a sensitive attribute? Isn't age a strong and ""fair"" indicator in this scenario? Or is this merely for illustration of the method? - In scenario 2 (line 252), does $\alpha_{fair}$ refer to the one in eq (11)? Eq. (11) is the joint objective for learning the model (prediction and deferral) given a fixed DM? That would mean that the autodmated model is encouraged to provide unfair predictions. However, my intuition for this scenario is that the (blackbox) DM provides unfair decisions and the model's task is to correct for it. I understand that the (later fixed) DM is first also trained (semi synthetic approach). Supposedly, unfairness is encouraged only when training DM as a pre-stage to learning the model? I encourage the authors to draw the distinction between first training/simulating the DM (and the corresponding assumptions/parameters) and then training the model (and the corresponding assumptions/parameters) more clearly. - The comparison between the deferring and the rejecting model is not quite fair. The rejecting model receives a fixed cost for rejecting and thus does not need access to DM during training. This already highlights that it cannot exploit specific aspects (e.g., additional information) of the DM. On the other hand, while the deferring model can adaptively pass on those examples to DM, on which the DM performs better, this requires access to DM's predictions during training. Since DMs typically have unique/special characteristics that could vary greatly from one DM to the next, this seems to be a strong impairment for training a deferring model (for each DM individually) in practice? While the adaptivity of learning to defer unsurprisingly constitutes an advantage over rejection learning, it comes at the (potentially large) cost of relying on more data. Hence, instead of simply showing its superiority over rejection learning, one should perhaps evaluate this tradeoff? - Nitpicking: I find ""above/below diagonal"" (add a thin gray diagonal to the plot) easier to interpret than ""above/below 45 degree"", which sounds like a local property (e.g., not the case where the red line saturates and has ""0 degrees""). - Is the slight trend of the rejecting model on the COMPAS dataset in Figure 4 to defer less on the reliable group a property of the dataset? Since rejection learning is non-adaptive, it is blind to the properties of DM, i.e., one would expect it to defer equally on both groups if there is no bias in the data (greater variance in outcomes for different groups, or class imbalance resulting in higher uncertainty for one group). - In lines 306-307 the authors argue that deferring classifiers have higher overall accuracy at a given minimum subgroup accuracy (MSA). Does that mean that at the same error rate for the subgroup with the largest error rate (minimum accuracy), the error rate on the other subgroups is on average smaller (higher overall accuracy)? This would mean that the differences in error rates between subgroups are larger for the deferring classifier, i.e., less evenly distributed, which would mean that the deferring classifier is less fair? - Please update the references to point to the conference/journal versions of the papers (instead of arxiv versions) where applicable. Typos line 10: learning to defer ca*n* make systems... line 97: first ""the"" should be removed End of line 5 of the caption of Figure 3: Fig. 3a (instead of Figs. 3a) line 356: This reference seems incomplete?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"* L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards.",NIPS_2016_386,NIPS_2016,", however. For of all, there is a lot of sloppy writing, typos and undefined notation. See the long list of minor comments below. A larger concern is that some parts of the proof I could not understand, despite trying quite hard. The authors should focus their response to this review on these technical concerns, which I mark with ** in the minor comments below. Hopefully I am missing something silly. One also has to wonder about the practicality of such algorithms. The main algorithm relies on an estimate of the payoff for the optimal policy, which can be learnt with sufficient precision in a ""short"" initialisation period. Some synthetic experiments might shed some light on how long the horizon needs to be before any real learning occurs. A final note. The paper is over length. Up to the two pages of references it is 10 pages, but only 9 are allowed. The appendix should have been submitted as supplementary material and the reference list cut down. Despite the weaknesses I am quite positive about this paper, although it could certainly use quite a lot of polishing. I will raise my score once the ** points are addressed in the rebuttal. Minor comments: * L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0? * L177: ""(OCO )"" -> ""(OCO)"" and similar things elsewhere * L176: You might want to mention that the learner observes the whole concave function (full information setting) * L223: I would prefer to see a constant here. What does the O(.) really mean here? * L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards. * L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes? * The algorithm only stops /after/ it has exhausted its budget. Don't you need to stop just before? (the regret is only trivially affected, so this isn't too important). * L213: \tilde \mu is undefined. I guess you mean \tilde \mu_t, but that is also not defined except in Corollary 1, where it just given as some point in the confidence ellipsoid in round t. The result holds for all points in the ellipsoid uniformly with time, so maybe just write that, or at least clarify somehow. ** L435: I do not see how this follows from Corollary 2 (I guess you meant part 1, please say so). So first of all mu_t(a_t) is not defined. Did you mean tilde mu_t(a_t)? But still I don't understand. pi^*(X_t) is (possibly random) optimal static strategy while \tilde \mu_t(a_t) is the optimistic mu for action a_t, which may not be optimistic for pi^*(X_t)? I have similar concerns about the claim on the use of budget as well. * L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else. * L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee? * L200: ""for every arm a"" implies there is a single optimistic parameter, but of course it depends on a ** L303: Why not choose T_0 = m Sqrt(T)? Then the condition becomes B > Sqrt(m) T^(3/4), which improves slightly on what you give. * It would be nice to have more interpretation of theta (I hope I got it right), since this is the most novel component of the proof/algorithm.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- It is unclear to me what scientific insight we get from this model and formalism over the prior task-optimized approaches. For instance, this model (as formulated in Section 2.3) is not shown to be a prototype approximation to these non-linear RNN models that exhibit emergent behavior. So it is not clear that your work provides any further “explanation” as to how these nonlinear models attain such solutions purely through optimization on a task.",ICLR_2021_1208,ICLR_2021,"- It is unclear to me what scientific insight we get from this model and formalism over the prior task-optimized approaches. For instance, this model (as formulated in Section 2.3) is not shown to be a prototype approximation to these non-linear RNN models that exhibit emergent behavior. So it is not clear that your work provides any further “explanation” as to how these nonlinear models attain such solutions purely through optimization on a task. - Furthermore, I am not really sure how “emergent” the hexagonal grid patterns really are in this model. Given partitioning of the generator matrices into blocks in Section 2.5, it almost seems by construction we would get hexagonal grid patterns and it would be very hard for the model to learn anything different.
While the ideas of this paper are mathematically elegant, I do not see the added utility these models provide over prior approaches nor how they provide a deeper explanation of the surprising emergent grid firing patterns observed in task-optimized nonlinear RNNs. For these reasons, I recommend rejection.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- PPG outputs a complete pose relative to every part’s center. Thus O_{up} should contain the offset for every keypoint with respect to the center of the upper part. In Eq.2 of the supplementary material, it seems that O_{up} is trained to output the offset for the keypoints that are not farther than a distance \textit{r) to the center of corresponding part. How are the groundtruths actually built? If it is the latter, how can the network parts responsible for each part predict all the keypoints of the pose.",NIPS_2021_2191,NIPS_2021,"of the paper: [Strengths]
The problem is relevant.
Good ablation study.
[Weaknesses] - The statement in the intro about bottom up methods is not necessarily true (Line 28). Bottom-up methods do have a receptive fields that can infer from all the information in the scene and can still predict invisible keypoints. - Several parts of the methodology are not clear. - PPG outputs a complete pose relative to every part’s center. Thus O_{up} should contain the offset for every keypoint with respect to the center of the upper part. In Eq.2 of the supplementary material, it seems that O_{up} is trained to output the offset for the keypoints that are not farther than a distance \textit{r) to the center of corresponding part. How are the groundtruths actually built? If it is the latter, how can the network parts responsible for each part predict all the keypoints of the pose. - Line 179, what did the authors mean by saying that the fully connected layers predict the ground-truth in addition to the offsets? - Is \delta P_{j} a single offset for the center of that part or it contains distinct offsets for every keypoint? - In Section 3.3, how is G built using the human skeleton? It is better to describe the size and elements of G. Also, add the dimensions of G,X, and W to better understand what DGCN is doing. - Experiment can be improved: - For instance, the bottom-up method [9] has reported results on crowdpose dataset outperforming all methods in Table 4 with a ResNet-50 (including the paper one). It will be nice to include it in the tables - It will be nice to evaluate the performance of their method on the standard MS coco dataset to see if there is a drop in performance in easy (non occluded) settings. - No study of inference time. Since this is a pose estimation method that is direct and does not require detection or keypoint grouping, it is worth to compare its inference speed to previous top-down and bottom-up pose estimation method. - Can we visualize G, the dynamic graph, as it changes through DGCN? It might give an insight on what the network used to predict keypoints, especially the invisible ones.
[Minor comments]
In Algorithm 1 line 8 in Suppl Material, did the authors mean Eq 11 instead of Eq.4?
Fig1 and Fig2 in supplementary are the same
Spelling Mistake line 93: It it requires…
What does ‘… updated as model parameters’ mean in line 176
Do the authors mean Equation 7 in line 212?
The authors have talked about limitations in Section 5 and have mentioned that there are not negative societal impacts.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"* The figures are small and almost unreadable * It doesn't clearly state how equation 5, follows from equation 4 * It is not clear how \theta^{t+1/2} come into the picture. Explain * S^{*} and S^{~} are very important parameters in the paper, yet they were not properly defined. In line 163 it is claimed that S^{~} is defined in Assumption 1, but one can see the definition is not proper, it is rather cyclic.",NIPS_2017_262,NIPS_2017,"that are addressed below:
* Most of the theoretical work presented here are built upon prior work, it is not clear what is the novelty and research contribution of the paper.
* The figures are small and almost unreadable
* It doesn't clearly state how equation 5, follows from equation 4
* It is not clear how \theta^{t+1/2} come into the picture. Explain
* S^{*} and S^{~} are very important parameters in the paper, yet they were not properly defined. In line 163 it is claimed that S^{~} is defined in Assumption 1, but one can see the definition is not proper, it is rather cyclic.
* Since the comparison metric here is wall-clock time, it is imperative that the implementation of the algorithms be the same. It is not clear that it is guaranteed. Also, the size of the experimental data is quite small.
* If we look into the run-times of DCPN for sim_1k, sim_5k, and sim_10k and compare with DC+ACD, we see that DCPN is performing better, which is good. But the trend line tells us a different story; between 1k and 10k data the DCPN run-time is about 8x while the competitor grows by only 2x. From this trend it looks like the proposed algorithm will perform inferior to the competitor when the data size is larger e.g., 100K.
* Typo in line 106","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1: texts in legends and axis labels should be larger At the beginning of page 6: Proposition (1) -> Proposition 1. --> (1) is confused with Equation 1. Captions and legend's font should be larger (similar to text size) in Fig. 2 and 3.,ICLR_2022_1887,ICLR_2022,"W1: The proposed method is a combination of existing loss. The novelty of technical contribution is not very strong.
W2: The proposed hybrid loss is argued that it is beneficial as the Proxy-NCA loss will promotes learning new knowledge better (first paragraph in Sec. 3.4), rather than less catastrophic forgetting. But the empirical results show that the proposed method exhibits much less forgetting than the prior arts (Table. 2). The argument and the empirical results are not well aligned.
Also, as the proposed method seems promoting learning new knowledge, it is suggested to empirically validate the benefit of the proposed approach by a measure to evaluate the ability to learn new knowledge (e.g., intransigence (Chaudhry et al., 2018)).
W3: Missing important comparison to Ahn et al.'s method in Table 3 (and corresponding section, titled ""comparison with Logits Bias Solutions for Conventional CIL setting"").
W4: Missing analyses of ablated models (Table 4). The proposed hybrid loss exhibits meaningful empirical gains only in CIFAR100 (and marginal gain in CIFAR10), comparing ""MS loss with NCM (Gen)"" and ""Hybrid loss with NCM (Gen)"". But there is no descriptive analysis for it.
W5: Lack of details of Smooth datasets in Sec. 4.3.
W6: Missing some citation (or comparison) using logit bias correction in addition to Wu et al., 2019 and Anh et al., 2020
Kang et al., 2020: https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9133417
Mittal et al., 2021: https://openaccess.thecvf.com/content/CVPR2021W/CLVision/papers/Mittal_Essentials_for_Class_Incremental_Learning_CVPRW_2021_paper.pdf
W7: Unclear arguments or arguments lack of supporting facts
4th para in Sec.1
'It should be noticed that in online CIL setting the data is seen only once, not fully trained, so it is analogous to the low data regime in which the generative classifier is preferable.' Why?
5th line of 2nd para in Sec. 3.1.3
'This problem becomes more severe as the the number of classes increases.'
Lack of supporting facts
W8: Some mistakes in text (see details in notes below) and unclear presentations Note
Mistakes in text
End of 1st para in Sec.1: intelligence agents -> intelligent agents
3th line of 1 para in Sec. 3.2: we can inference with -> we can conduct inference with
1st line of 1st para in Sec. 3.3
we choose MS loss as training... -> we choose the MS loss as a training...
MS loss is a state-of... -> MS loss is the state-of...
6th line of Proposition 1: 'up to an additive constant and L' -> 'up to an additive constant c and L'
Improvement ideas in presentations
Fig. 1: texts in legends and axis labels should be larger
At the beginning of page 6: Proposition (1) -> Proposition 1. --> (1) is confused with Equation 1.
Captions and legend's font should be larger (similar to text size) in Fig. 2 and 3.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- For the counterfactual experiments, I would have liked to see a comparison against Journey TRAK [1] used at a particular step of the the sampling trajectory. In particular, [1, Figure 2] shows a much larger effect of removing high-scoring images according to Journey TRAK, in comparison with CLIP cosine similarity.",vKViCoKGcB,ICLR_2024,"- Out of the listed baselines, to the best of my knowledge only Journey TRAK [1] has been explicitly used for diffusion models in previous work. As the authors note, Journey TRAK is not meant to be used to attribute the *final* image $x$ (i.e., the entire sampling trajectory). Rather, it is meant to attribute noisy images $x_t$ (i.e., specific denoising steps along the sampling trajectory). Thus, the direct comparison with Journey TRAK in the evaluation section is not on equal grounds.
- For the counterfactual experiments, I would have liked to see a comparison against Journey TRAK [1] used at a particular step of the the sampling trajectory. In particular, [1, Figure 2] shows a much larger effect of removing high-scoring images according to Journey TRAK, in comparison with CLIP cosine similarity.
- Given that the proposed method is only a minor modification of existing methods [1, 2], I would have appreciated a more thorough attempt at explaining/justifying the changes proposed by the authors.
[1] Kristian Georgiev, Joshua Vendrow, Hadi Salman, Sung Min Park, and Aleksander Madry. The
journey, not the destination: How data guides diffusion models. In Workshop on Challenges in
Deployable Generative AI at International Conference on Machine Learning (ICML), 2023.
[2] Sung Min Park, Kristian Georgiev, Andrew Ilyas, Guillaume Leclerc, and Aleksander Madry. Trak:
Attributing model behavior at scale. In International Conference on Machine Learning (ICML), 2023.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.",NIPS_2017_370,NIPS_2017,"- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?
- From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.
- The improvements on image deconvolution is minimal with CNN-X working better than ACNN when all the dataset is considered. This shows that the adaptive convolutions are not universally applicable when the side information is available. Also, there are no comparisons with state-of-the-art network architectures for digit recognition and image deconvolution. Suggestions:
- It would be good to move some visual results from supplementary to the main paper. In the main paper, there is almost no visual results on crowd density estimation which forms the main experiment of the paper. At present, there are 3 different figures for illustrating the proposed network architecture. Probably, authors can condense it to two and make use of that space for some visual results.
- It would be great if authors can address some of the above weaknesses in the revision to make this a good paper.
Review Summary:
- Despite some drawbacks in terms of experimental analysis and the general applicability of the proposed technique, the paper has several experiments and insights that would be interesting to the community. ------------------
After the Rebuttal: ------------------
My concern with this paper is insufficient analysis of 'filter manifold network' architecture and the placement of adaptive convolutions in a given CNN. Authors partially addressed these points in their rebuttal while promising to add the discussion into a revised version and deferring some other parts to future work.
With the expectation that authors would revise the paper and also since other reviewers are fairly positive about this work, I recommend this paper for acceptance.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph?",NIPS_2016_499,NIPS_2016,"- The proposed method is very similar in spirit to the approach in [10]. It seems that the method in [10] can also be equipped with scoring causal predictions and the interventional data. If otherwise, why [10] cannot use these side information? - The proposed method reduces the computation time drastically compared to [10] but this is achieved by reducing the search space to the ancestral graphs. This means that the output of ACI has less information compared to the output of [10] that has a richer search space, i.e., DAGs. This is the price that has been paid to gain a better performance. How much information of a DAG is encoded in its corresponding ancestral graph? - Second rule in Lemma 2, i.e., Eq (7) and the definition of minimal conditional dependence seem to be conflicting. Taking Zâ in this definition to be the empty set, we should have that x and y are independent given W, but Eq. (7) says otherwise.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Also, the theoretical discussions can use some improvements. Although directly related to fair representation, the current theorems follow directly from the algorithm design itself and the well-known property of mutual information to $\Delta_{DP}$. For instance, I was expecting some sample complexity-type results for not returning NSF, e.g., given confidence levels, what is the sufficient amount of training data points that would not return NSF.",fkvdewFFN6,ICLR_2024,"1. The overall framework seems quite similar to the Seldonian algorithm design of Thomas et al. (2019), e.g., see Fig. 1 of Thomas et al. (2019)). Although it is true that Thomas et al. (2019) only considered fair classification experiments, as mentioned in this paper's related works, the proposed FRG also has an objective function related to the expressiveness of the representation, and some of the details even match; for instance, the discussions on ""$1 - \delta$ confidence upper bound"" on pg. 4 are quite similar to the caption of Fig. 1 of Thomas et al. (2019). Then, the question boils down to what is the novel contribution of this work, and my current understanding is that this is a simple application of Thomas et al. (2019) to fair representation learning. Of course, there are theoretical analysis, practical considerations and good experimental results that are specific to fair representation learning, but I believe that (as I will elaborate below) there are some problems that need to be addressed. Lastly, I believe that Thomas et al. (2019) should be given much more credit than the current draft.
2. Although the paper focuses on the high-probability fair representation construction (which should be backed theoretically in a solid way, IMHO), there are too many components (for ""practicality"") that are theoretically unjustified. There are three such main components: doubling the width of the confidence interval to ""avoid"" overfitting, introducing the hyperparameters $\gamma$ and $v$ for upper bounding $\Delta_{DP}$, and approximating the candidate optimization.
3. Also, the theoretical discussions can use some improvements. Although directly related to fair representation, the current theorems follow directly from the algorithm design itself and the well-known property of mutual information to $\Delta_{DP}$. For instance, I was expecting some sample complexity-type results for not returning NSF, e.g., given confidence levels, what is the sufficient amount of training data points that would not return NSF.
4. Lastly, if the authors meant for this paper to be a practical paper, then it should be clearly positioned that way. For instance, the paper should allocate much more space to the experimental verifications and do more experiments. Right now, the experiments are only done for two datasets, both of which consider binary sensitive attributes. In order to show the proposed FRG's versatility, the paper should do a more thorough experimental evaluation of various datasets of different characteristics with multiple group and/or nonbinary sensitive attributes, trade-off (Pareto front) between fairness and performance (or any of such kind), even maybe controlled synthetic experiments.
**Summary**. Although the framework is simple and has promising experiments, I believe that there is still much to be done. In its current form, the paper's contribution seems to be incremental and not clear.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"• Your VAD description is puzzling. What is stated in the paper simply discards any TF bins that have a magnitude of less than epsilon. If this is what you do I wouldn't call it a VAD, you are simply discarding TF bins with zero magnitude that will result in a division by zero. A VAD is supposed to look for the presence of speech (not just energy), and is also very unlikely to be defined over frequency, it's usually only over time.",ICLR_2021_2896,ICLR_2021,"of paper
On the plus side: This paper is well written, the experiments are fairly well done, the results are good, and the outlined approach is sensible.
On the minus side: It isn't clear what the scientific contribution is. Using a known network structure on a known feature to perform a known process doesn't really provide much insight. I do feel that there is room for insight in this paper, but the authors stick to a very high-level description of their experiments. Recommendation
I would recommend rejection. I think this is good work, but it doesn't convey to me anything that I didn't already know (other than this particular combination of feature and network seems to work well). I expect papers to teach me something, not just report a finding without any analysis or explanation. As is, the paper is really not much more than what is simply contained in table 1. This paper could have in it a lot more information and analysis to make it a much stronger submission, but the current approach is simply a report on a single point estimate. Yes it seems to work well, but I don't believe that just that is the standard for a strong publication.
Questions for clarifications:
• A hugely influential factor in multi-source localization is how one defines a peak. This is not described adequately in this paper and I think needs some clarification. I understand that for each time frame we obtain a location posterior. But it isn't clear how to a) Find out how many speakers are active, and b) finding out the peaks that correspond to each speaker. If we have e.g. two speakers and six substantial nonzero posterior values, how does one pick the two that correspond to a speaker? We cannot use the two loudest ones, they might be both from the same speaker. What if the amplitude difference between the speakers is large? This is not a question that can be brushed away so easily and should be directly addressed.
• It would be interesting to see something akin to a confusion matrix. Arrays are not equally good at detecting all directions and understanding how this algorithm behaves with respect to classic array behavior would be an interesting bit of information.
Provide additional feedback with the aim to improve the paper
• I wouldn't use the terms ""spectral and phase"". The distinction should be ""magnitude and phase""; ""spectral"" usually implies the whole complex-valued output of the DFT. But that entire sentence is a little out of place, since you claim that phase is more useful than magnitudes, and then you use the complex data directly. I would simply state which features you use and leave it at that. The extra discussion there is an unnecessary distraction.
• ""since it is known to encapsulate the spatial fingerprint for each sound source"", I assume when you say source you imply a spatial location. I believe ""source"" tends to be usually interpreted as the actual sound (hence ""source separation""), which is independent of the location.
• Your VAD description is puzzling. What is stated in the paper simply discards any TF bins that have a magnitude of less than epsilon. If this is what you do I wouldn't call it a VAD, you are simply discarding TF bins with zero magnitude that will result in a division by zero. A VAD is supposed to look for the presence of speech (not just energy), and is also very unlikely to be defined over frequency, it's usually only over time.
• I understand that for the sake of conforming to current APIs, complex numbers are being avoided here. But, for something on paper, it strikes me as quite strange to stack real and imaginary parts when a complex-valued representation is clearly much easier to describe and manipulate. I can predict your rebuttal to this point, but papers are supposed to set forth the science from which we will write code, and not directly describe the code.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying Q ^ t and S t , as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of α t and also on the average lengths of the predictive intervals when we let Q ^ t and S t vary with time?",NIPS_2021_40,NIPS_2021,"/Questions:
I only have minor suggestions:
1.) In the discussion, it may be worth including a brief discussion on the empirical motivation for a time-varying Q ^ t and S t
, as opposed to a fixed one as in Section 4.2. For example, what is the effect on the volatility of α t
and also on the average lengths of the predictive intervals when we let Q ^ t and S t
vary with time?
2.) I found the definition of the quantile a little confusing, an extra pair of brackets around the term ( 1 | D | ∑ ( X r , Y r ) ∈ D 1 S ( X r , Y r ) ≤ s )
might help, or maybe defining the bracketed term separately if space allows.
3.) I think there are typos in Lines 93, 136, 181 (and maybe in the Appendix too): should it be Q ^ t ( 1 − α t ) instead? ##################################################################### Overall:
This is a very interesting extension to conformal prediction that no longer relies on exchangeability but is still general, which will hopefully lead to future work that guarantees coverage under weak assumptions. I believe the generality also makes this method useful in practice.
The authors have described the limitations of their theory, e.g. having a fixed Q ^
with time.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. I have some doubts about the definations in Table1. What's the different between anchor-based regression and the regression in RepPoints? in RetinaNet, there is also only a one-shot regression. And in ATSS, this literature has proved that the regression methods do not influence a lot. The method that directly regresses [w, h] to the center point is good enough. While RepPoints regresses distance to the location of feature maps. I think there is no obvious difference between the two methods. I hope the authors can clarify this problem. If not, the motivations here is not solid enough.",NIPS_2020_471,NIPS_2020,"1. The descriptions in joint inference is not very clear. I cannot get how the refine process do according to Equ. 2. It would be great if the authors can clear this part during the rebuttal and polish this part in the final version. 2. I have some doubts about the definations in Table1. What's the different between anchor-based regression and the regression in RepPoints? in RetinaNet, there is also only a one-shot regression. And in ATSS, this literature has proved that the regression methods do not influence a lot. The method that directly regresses [w, h] to the center point is good enough. While RepPoints regresses distance to the location of feature maps. I think there is no obvious difference between the two methods. I hope the authors can clarify this problem. If not, the motivations here is not solid enough. 3. It would be great if the authors can analyze the computational costs and inference speeds for the proposed method.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.",ICLR_2022_2810,ICLR_2022,"The clarity of the writing could be improved substantially. Descriptions are often vague, which makes the technical details harder to understand. I think it's fine to give high-level intuitions separate from low-level details, but the current writing invites confusion. For example, at the start of Section 3, the references to buffers and clusters are vague. The text refers readers to where these concepts are described, but the high-level description doesn't really give a clear picture, making the text that follows harder to understand.
Ideas are not always presented clearly. For example:
may only exploit a small part of it, making most of the goals pointless.```
- Along the same lines, at the start of the Experiments section, when reading ```the ability of DisTop to select skills to learn``` I am left to wonder what this ""ability"" and ""selection"" refers to. This is not a criticism of word choice. The issue is that the previous section did not set up these ideas.
- Sections of the results do not seem to actually address the experimental question they are motivated by (that is, the question at the paragraph header). In general, this paper tends to draw conclusions that seem only speculatively supported by the results.
- Overall, the paper is not particularly easy to follow. The presentation lacks a clear intuition for how the pieces fit together and the experiments have little to hang on to as a result.
- The conclusions drawn from the experiments are not particularly convincing. While there is some positive validation, demonstration of the *topology* learning's success is lacking. There are some portions of the appendix that get at this, but the analysis feels incomplete. Personally, I am much more convinced by a demonstration that the underlying pieces of the algorithm are viable than by seeing that, when they are all put together, the training curves look better.
### Questions/Comments:
- The second paragraph of 2.1 is hard to follow. If the technical details are important, it may make more sense to work them into a different area of the text.
- The same applies to 2.2. The technical details are hard to follow.
- You claim ""In consequence, we avoid using a hand engineered environment-specific scheduling"" on page 4. Does this suggest that the $\beta$ parameter and the $\omega'$ update rate are environment independent?
- Why do DisTop and Skew-Fit have such different starting distances for Visual Pusher (Figure 1, left middle)?
- It is somewhat strange phrasing to describe Skew-Fit as having ""favorite"" environments (page 6).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
5. The student and refinement networks are trained simultaneously. Which may improve the performance of the teacher network. Is the comparison fair? Please provide KID/FID metrics of your teacher network.,ICLR_2023_4699,ICLR_2023,"1. The guidance over SIFT feature space is good. However, the perceptual losses (such as VGG feature loss) are also considered effective. The authors should clarify their choice, otherwise this contribution is weakened. 2. Assumption 3.1. says the loss of TKD is assumed less than IYOR. However, eqn.7 tells a different story. 3. Assumption 3.1. may not hold in real cases. One cannot increase the parameter number of the teacher network when applying a KD algorithm.
4. This paper can become more solid if IYOR is used in some modern i2i methods. i.e, StyleFlow, EGSDE etc. 5. The student and refinement networks are trained simultaneously. Which may improve the performance of the teacher network. Is the comparison fair? Please provide KID/FID metrics of your teacher network.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help?",NIPS_2018_553,NIPS_2018,"- This paper misses a few details in model design and experiments: A major issue is the ""GTA"" / ""DET"" feature representation in Table 1. As stated in section 4.1, image regions are extracted from ground-truth / detection methods. But what is the feature extractor used on top of those image regions? Comparing resnet / densenet extracted features with vgg / googlenet feature is not fair. - The presentation of this paper can be further improved. E.g. paragraph 2 in intro section is a bit verbose. Also breaking down overly-long sentences into shorter but concise ones will improve fluency. Some additional comments: - Figure 3: class semantic feature should be labeled as ""s"" instead of ""c""? - equation 1: how v_G is fused from V_I? please specify. - equation 5: s is coming from textual representations (attribute / word to vec / PCA'ed TFIDF). It might have positive / negative values? However the first term h(W_{G,S}, v_G) is post ReLU and can only be non-negative? - line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help? - line 170: class semantic information is [not directly] embedded into the network? - Equation 11: v_s and u_G are both outputs from trained-network, and they are not normalized? So minimize L-2 loss could be simply reducing the magnitude of both vectors? - Line 201: the dimensionality of each region is 512: using which feature extractor? - Section 4.2.2: comparing number of attention layers is a good experiment. Another baseline could be not using Loss_G? So attention is only guided by global feature vector. - Table 4: what are the visual / textual representations used in each method? otherwise it is unclear whether the end-to-end performance gain is due to proposed attention model.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Goal Misspecification: Failures on the ALFRED benchmark often occurred due to goal misspecification, where the LLM did not accurately recover the formal goal predicate, especially when faced with ambiguities in human language.",qJ0Cfj4Ex9,ICLR_2024,"- Goal Misspecification: Failures on the ALFRED benchmark often occurred due to goal misspecification, where the LLM did not accurately recover the formal goal predicate, especially when faced with ambiguities in human language.
- Policy Inaccuracy: The learned policies sometimes failed to account for low-level, often geometric details of the environment.
- Operator Overspecification: Some learned operators were too specific, e.g., the learned SliceObject operator specified a particular type of knife, leading to planning failures if that knife type was unavailable.
- Limitations in Hierarchical Planning: The paper acknowledges that it doesn't address some core problems in general hierarchical planning. For instance, it assumes access to symbolic predicates representing the environment state and doesn't tackle finer-grained motor planning. The paper also only considers one representative pre-trained LLM and not others like GPT-4.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. I think the improvement of this method over SOTA methods such as IGEV is small. Does this mean that there is no multi-peak distribution problem in iterative optimization schemes similar to IGEV? I suggest that the author analyze the distribution of disparities produced by IGEV compared to other baselines to determine why the effect is not significantly improved on IGEV. And I have another concern. Currently, SOTA schemes are basically iterative frameworks similar to IGEV. Is it difficult for Sampling-Gaussian to significantly improve such frameworks?",lGDmwb12Qq,ICLR_2025,"1. I think the innovation of this paper is limited. In this paper, I think the main improvement comes from taking the disparity range below 0 into consideration, eliminating the negative impact on the scheme based on distributed supervision in the disparity range below 0. But with a fixed extended disparity range, i.e., 16, I think it's hard to fit the distribution of the scenarios. Do I need to set a new extended range to fit the distribution range in a new scenario? I think this is an offset that is highly relevant to the scene.
2. I think the improvement of this method over SOTA methods such as IGEV is small. Does this mean that there is no multi-peak distribution problem in iterative optimization schemes similar to IGEV? I suggest that the author analyze the distribution of disparities produced by IGEV compared to other baselines to determine why the effect is not significantly improved on IGEV. And I have another concern. Currently, SOTA schemes are basically iterative frameworks similar to IGEV. Is it difficult for Sampling-Gaussian to significantly improve such frameworks?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Model-Specific Insights: The paper focuses on generic findings across models, but a deeper investigation into how specific models (e.g., GPT-4o vs. InternVL2) behave differently when ReGuide is applied could add nuance to the conclusions. For example, the differences in false positive rates (FPR) between models with and without ReGuide should be presented for a better comparison.",R4h5PXzUuU,ICLR_2025,"1. Limited Explanation of Failures: Although the paper provides examples of failure cases, it does not fully delve into the underlying causes or offer detailed solutions for these issues. While the authors acknowledge the problem of overconfidence in models such as GPT-4o with ReGuide, further exploration of these limitations could strengthen the study. There is still not a robust method to effectively introduce LVLMs to the OoD tasks.
2. Model-Specific Insights: The paper focuses on generic findings across models, but a deeper investigation into how specific models (e.g., GPT-4o vs. InternVL2) behave differently when ReGuide is applied could add nuance to the conclusions. For example, the differences in false positive rates (FPR) between models with and without ReGuide should be presented for a better comparison.
3. Scalability and Practicality: While the ReGuide method shows a promising direction, the computational overhead and API limitations mentioned in the paper could present challenges for practical, large-scale implementation. This issue is touched upon but not sufficiently addressed in terms of how ReGuide might be optimized for deployment at scale. Meanwhile, the inference cost analysis can further improve the paper's quality and inspire further work.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex.",ICLR_2021_2892,ICLR_2021,"- Proposition 2 seems to lack an argument why Eq 16 forms a complete basis for all functions h. The function h appears to be defined as any family of spherical signals parameterized by a parameter in [-pi/2, pi/2]. If that’s the case, why eq 16? As a concrete example, let \hat{h}^\theta_lm = 1 if l=m=1 and 0 otherwise, so constant in \theta. The only constant associated Legendre polynomial is P^0_0, so this h is not expressible in eq 16. Instead, it seems like there are additional assumptions necessary on the family of spherical functions h to let the decomposition eq 16, and thus proposition 2, work. Hence, it looks like that proposition 2 doesn’t actually characterize all azimuthal correlations. - In its discussion of SO(3) equivariant spherical convolutions, the authors do not mention the lift to SO(3) signals, which allow for more expressive filters than the ones shown in figure 1. - Can the authors clarify figure 2b? I do not understand what is shown. - The architecture used for the experiments is not clearly explained in this paper. Instead the authors refer to Jiang et al. (2019) for details. This makes the paper not self-contained. - The authors appear to not use a fast spherical Fourier transform. Why not? This could greatly help performance. Could the authors comment on the runtime cost of the experiments? - The sampling of the Fourier features to a spherical signal and then applying a point-wise non-linearity is not exactly equivariant (as noted by Kondor et al 2018). Still, the authors note at the end of Sec 6 “This limitation can be alleviated by applying fully azimuthal-rotation equivariant operations.”. Perhaps the authors can comment on that? - The experiments are limited to MNIST and a single real-world dataset. - Out of the many spherical CNNs currently in existence, the authors compare only to a single one. For example, comparisons to SO(3) equivariant methods would be interesting. Furthermore, it would be interesting to compare to SO(3) equivariant methods in which SO(3) equivariance is broken to SO(2) equivariance by adding to the spherical signal a channel that indicates the theta coordinate. - The experimental results are presented in an unclear way. A table would be much clearer. - An obvious approach to the problem of SO(2) equivariance of spherical signals, is to project the sphere to a cylinder and apply planar 2D convolutions that are periodic in one direction and not in the other. This suffers from distortion of the kernel around the poles, but perhaps this wouldn’t be too harmful. An experimental comparison to this method would benefit the paper.
Recommendation: I recommend rejection of this paper. I am not convinced of the correctness of proposition 2 and proposition 1 is similar to equivariance arguments made in prior work. The experiments are limited in their presentation, the number of datasets and the comparisons to prior work.
Suggestions for improvement: - Clarify the issue around eq 16 and proposition 2 - Improve presentation of experimental results and add experimental details - Evaluate the model of more data sets - Compare the model to other spherical convolutions
Minor points / suggestions: - When talking about the Fourier modes as numbers, perhaps clarify if these are reals or complex. - In Def 1 in the equation it is confusing to have theta twice on the left-hand side. It would be clearer if h did not have a subscript on the left-hand side.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. On line 205, it should be Fig. 1 instead of Fig. 5.1. In latex, please put '\label' after the '\caption', and the bug will be solved.",NIPS_2016_431,NIPS_2016,"1. It seems the asymptotic performance analysis (i.e., big-oh notation of the complexity) is missing. How is it improved from O(M^6)? 2. On line 205, it should be Fig. 1 instead of Fig. 5.1. In latex, please put '\label' after the '\caption', and the bug will be solved.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"* The notations $h, g$ almost appear out of the blue. What are their input, output domains. It is the same issue with $\delta$ but it at least has some brief description.",IQ0BBfbYR2,ICLR_2025,"1. Writing should be seriously improved. It is really tedious to get through the paper. Often terms are not defined properly and the reader really has to rely on the context to understand used terms. This is not possible always. See a list of writing issues below:
* I find parts of Fig. 2 unclear. The caption should describe the key features of the model. Why is there a connection between Decoder and External classifier but no arrow. What is it supposed to mean?
* line 224 Should describe $d, \kappa$ clearly and what they denote. Is the feature encoding coming from an external network, the classifier $f$ itself etc. What is the distance metric? Euclidean distance?
* Implicit/Explicit classifier should be described clearly when describing CoLa-DCE in Sec. 4. What is their input, output domains, what are their roles etc. I guess one of them is $f$. I assume they originate from prior literature but they also seem central to your method so need clear concise description.
* line 249: Should define $\mathcal{N}$ or state its the set of natural numbers (can be confused with normal distribution as you use it in Eq. 1).
* line 259 Why is the external classifier modelled as p(x|y) (which should be for diffusion model). Wouldn't the classifier be modelled as p(y|x)?
* The notations $h, g$ almost appear out of the blue. What are their input, output domains. It is the same issue with $\delta$ but it at least has some brief description.
* line 260. You provide absolutely no explanation of notations about concepts, how they are represented, what the binary constraints denote exactly. It is difficult to understand $\lambda_1, ..., \lambda_k$ and $\theta_1, ..., \theta_k$. Are the lambda's just subset of natural numbers from 1 to K? Your notation for $\theta$ also does not seem consistent. For starters, line 236 has it going from 0 to k while at other places it is 1 to k. More importantly, Eq. 7 makes it seem like $\theta$'s denote subset of indices but they are supposed to be binary masks. I am not sure what are these representations exactly, beyond the basic idea that they control which concepts to condition on.
* There are two terms for datasets (line 219, 226) $X', \hat{X}$. What is the difference between the two? Is the reference data classification training/validation dataset and the other test data?
* line 222 ""As the model perception of the data shall be represented, the class predictions of the model are used to determine class affiliation."" Really odd phrasing, please make it more clear.
* line 319-320 ""Using the intermediate ... high flip ratios"" I am not sure how you are drawing this conclusion from Tab. 1. Please elaborate on this how you come to this conclusion.
* line 469 What is $attr$ supposed to denote exactly. The attribution map for a particular feature/concept? If yes, why are you computing absolute magnitude for relative alignment? Is it the Frobenius norm of the difference of attributions? It should be described more clearly.
* Please explain more clearly what the ""confidence"" metric is? Is it the difference between classifier's probability for the initially predicted class before and after the modifications?
* line 295 mentions l2 norm between original and counterfactual image. Was it supposed to be a metric in Tab. 1?
2. The quantitative metrics of CoLa-DCE seem weak. LDCE seems to clearly outperform CoLA-DCE on ""Flip-ratio"" and ""Confidence"" metrics while being close on FID.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"6.The result comparison of ""Iteratively greedy Search"" versus ""random search"" on the model structure should be supplemented.",ICLR_2022_2421,ICLR_2022,"Weakness: 1.Some typos such as “TRAFE-OFFS” in the title of section 4.1. 2.The 24 different structures generated by random premutation in section 4.1 should be explained in more detail. 3.The penultimate sentence of Section 3.3 states that ""iterative greedy search can avoid the suboptimality of the resulting scaling strategy on a particular model"", which is not a serious statement because the results of the iterative greedy search are also suboptimal solutions. 4.The conclusion of ""Cost breakdown can indicate the transferability effectiveness"" in Figure 7 is not sufficient. We cannot extend the conclusion obtained from a few specific experiments to any different hardware devices or different architectures. 5.Why not use the same cost for different devices instead of flops, latency, and 1/FPS for different hardware? 6.The result comparison of ""Iteratively greedy Search"" versus ""random search"" on the model structure should be supplemented.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations.",ARR_2022_205_review,ARR_2022,"- Missing ablations: It is unclear from the results how much performance gain is due to the task formulation, and how much is because of pre-trained language models. The paper should include results using the GCPG model without pre-trained initializations. - Missing baselines for lexically controlled paraphrasing: The paper does not compare with any lexically constrained decoding methods (see references below). Moreover, the keyword control mechanism proposed in this method has been introduced in CTRLSum paper (He et al, 2020) for keywork-controlled summarization.
- Related to the point above, the related work section is severely lacking (see below for missing references). Particularly, the paper completely omits lexically constrained decoding methods, both in related work and as baselines for comparison.
- The paper is hard to follow and certain sections (particularly the Experimental Setup) needs to made clearer. It was tough to understand exactly where the exemplar/target syntax was obtained for different settings, and how these differed between training and inference for each of those settings.
- The paper should include examples of generated paraphrases using all control options studies (currently only exemplar-controlled examples are included in Figure 5). Also, including generation from baseline systems for the same examples would help illustrate the differences better. Missing References: Syntactically controlled paraphrase generation: Goyal et al., ACL2020, Neural Syntactic Preordering for Controlled Paraphrase Generation Sun et al. EMNLP2021, AESOP: Paraphrase Generation with Adaptive Syntactic Control <-- this is a contemporaneous work, but would be nice to cite in next version. Keyword-controlled decoding strategies: Hokamp et al. ACL2017, Lexically Constrained Decoding for Sequence Generation Using Grid Beam Search Post et al, NAACL 2018, Fast Lexically Constrained Decoding with Dynamic Beam Allocation for Neural Machine Translation Other summarization work that uses similar technique for keyword control: He et al, CTRLSum, Towards Generic Controllable Text Summarization","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"- It's recommended to add reference next to baseline names in tables (e.g. place citation next to 'FF Positional' if that refers a paper method) - In Corollary 1, $\Omega$ is not explicitly defined (though it's not hard to infer what it means).",ICLR_2021_329,ICLR_2021,"Weakness
- I do not see how MFN 'largely outperforms' existing baseline methods. It is difficult to identify the quality difference between output from the proposed method and SIREN -- shape representation seems to even prefer SIREN's results (what is the ground truth for Figure 5a). The paper is based on the idea of replacing compositional models with recursive, multiplicative ones, though neither the theory nor the results are convincing to prove this linear approximation is better. I have a hard time getting the intuition of the advantages of the proposed method.
- this paper, and like other baselines (e.g. SIREN) do not comment much on the generalization power of these encoding schemes. Apart from image completion, are there other experiments showing the non-overfitting results, for example, on shape representation or 3D tasks?
- the proposed model has shown to be more efficient in training, and I assume it is also more compact in size, but there is no analysis or comments on that? Suggestions
- Result figures are hard to spot differences against baselines. It's recommended to use a zoom or plot the difference image to show the difference.
- typo in Corollary 2 -- do you mean linear combination of Gabor bases?
- It's recommended to add reference next to baseline names in tables (e.g. place citation next to 'FF Positional' if that refers a paper method)
- In Corollary 1, $\Omega$ is not explicitly defined (though it's not hard to infer what it means).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1. It is hard to understand what the axes are for Figure 1.,NIPS_2022_2286,NIPS_2022,"Weakness 1. It is hard to understand what the axes are for Figure 1. 2. It is unclear what the major contributions of the paper are. Analyzing previous work does not constitute as a contribution. 3. It is unclear how the proposed method enables better results. For instance, Table 1 reports similar accuracies for this work compared to the previous ones. 4. The authors talk about advantages over the previous work in terms of efficiency however the paper does not report any metric that shows it is more efficient to train with this proposed method. 5. Does the proposed method converge faster compared to previous algorithms? 6. How does the proposed methods compare against surrogate gradient techniques? 7. The paper does not discuss how the datasets are converted to spike domain.
There are no potential negative societal impacts. One major limitation of this work is applicability to neuromorphic hardware and how will the work shown on GPU translate to neuromorphic cores.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', 'X', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. If possible, results on ImageNet can be more convincing of the proposed method",ICLR_2022_2971,ICLR_2022,"The experiment part is kind of weak. Only experiments of CIFAR-100 are conducted and only DeiT-small/-base are compared. 1. For comparison with DeiT, can you add DeiT variants with the same number of layers, heads, hidden dimension as CMHSA to do a fair comparison? DeiT with 12 layers performs worse than CMHSA with 6 layers on CIFAR-100 is as expected, thus not a convincing comparison.
2. If possible, can you add CNN models to show if CMHSA would actually make ViT performs better/on-par with CNN models, which should be the ultimate goal of training ViT in low-data regime, otherwise one would pretrain ViT on large scale dataset or just use CNN models.
3. If possible, results on ImageNet can be more convincing of the proposed method","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2 Direct runtime comparisons with existing methods are missing. The proposed approach is based on implicit differentiation which usually requires additional computational costs. Thus, the direct runtime comparison is necessary to demonstrate the efficiency of the proposed approach.",NIPS_2022_2138,NIPS_2022,"Weakness:
1 The main contribution of this paper is about the software, but the theoretical contribution is overstated. The proof of the theorem is quite standard and I do not get some new insight from it.
2 Direct runtime comparisons with existing methods are missing. The proposed approach is based on implicit differentiation which usually requires additional computational costs. Thus, the direct runtime comparison is necessary to demonstrate the efficiency of the proposed approach.
3 Recently, implicit deep learning has attracted many attentions, which is very relevant to the topic of this paper. An implementation example of implicit deep neural networks should be included. Moreover, many Jacobian-free methods e.g., [1-3] have been proposed to reduce the computational cost. The comparisons (runtime and accuracy) with these methods are preferred.
[1] Fung, Samy Wu, et al. ""Fixed point networks: Implicit depth models with Jacobian-free backprop."" (2021).
[2] Geng, Zhengyang, et al. ""On training implicit models."" Advances in Neural Information Processing Systems 34 (2021): 24247-24260.
[3] Ramzi, Zaccharie, et al. ""SHINE: SHaring the INverse Estimate from the forward pass for bi-level optimization and implicit models."" arXiv preprint arXiv:2106.00553 (2021).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1) the proposed framework is a simple combination of meta-learning and federated learning. I cannot see any technical contribution.,ICLR_2021_1744,ICLR_2021,"Weakness:
This work simply applies the meta-learning method into the federated learning setting. I can’t see any technical contribution, either in the meta-learning perspective or the federated perspective. The experimental results are not convincing because the data partition is not for federated learning. Reusing data partition in a meta-learning context is unrealistic for a federated learning setting.
The title is misleading or over-claimed. Only the adaptation phase costs a few rounds, but the communication cost of the meta-training phase is still high.
The non-IID partition is unrealistic. The authors simply reuse the dataset partitions used in the meta-learning context, which is not a real federated setting. Or in other words, the proposed method can only work in the distribution which is similar to the meta-learning setting.
Some meta earning-related benefits are intertwined with reducing communication costs. For example, the author claimed the proposed method has better generalization ability, however, this is from the contribution of the meta-learning. More importantly, this property can only be obvious when the data distribution cross-clients meet the assumption in the context of meta-learning.
The comparison is unfair to FedAvg. At least, we should let FedAvg use the same clients and dataset resources as those used in Meta-Training and Few-Rounds adaptation.
“Episodic training” is a term from meta-learning. I suggest the authors introduce meta-learning and its advantage first in the Introduction.
Few-shot FL-related works are not fully covered. Several recent published knowledge distillation-based few-shot FL should be discussed.
Overall Rating
I tend to clearly reject this paper because: 1) the proposed framework is a simple combination of meta-learning and federated learning. I cannot see any technical contribution. 2) Claiming the few round adaptations can reduce communication costs for federated learning is misleading, since the meta-training phase is also expensive. 3) the data partition is directly borrowed from meta-learning, which is unrealistic in federated learning.
---------after rebuttal--------
The rebuttal does not convince me with evidence, thus I keep my overall rating. I hope the author can obviously compare the total cost of meta-learning phase plus FL fine-tuning phase with other baselines.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', 'X', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"1. However, my major concern is that the contribution is insufficient. In general, the authors studied the connection between the complementary and the model robustness but without further studies on how to leverage such characteristics to improve model robustness. Even though this paper could be the first work to study this connection, the conclusion could be easily and intuitively obtained, i.e., when multimodal complementary is higher, the robustness is more delicate when one of the modalities is corrupted. Except for the analysis of the connection between complementary and robustness, it is expected to see more insightful findings or possible solutions.",ICLR_2023_2406,ICLR_2023,"1. However, my major concern is that the contribution is insufficient. In general, the authors studied the connection between the complementary and the model robustness but without further studies on how to leverage such characteristics to improve model robustness. Even though this paper could be the first work to study this connection, the conclusion could be easily and intuitively obtained, i.e., when multimodal complementary is higher, the robustness is more delicate when one of the modalities is corrupted. Except for the analysis of the connection between complementary and robustness, it is expected to see more insightful findings or possible solutions. 2. The proposed metric is calculated on the features extracted by some pre-trained models. So the pre-trained models are necessary for metric computing which is contradictory to that the metric is used to measure the multimodal data complementary. In addition, in my opinion, the metric is unreliable since the model participates in the metric calculation and will inevitably affect the calculation results. 3. There are many factors that will affect the model's robustness. The multimodal data complementary is one of them. However, multimodal data complementary is not solely determined by the data itself. For example, classification on MS-COCO data is obviously less complementary than VQA on MS-COCO data. As mentioned by the author, the VQA task requires both modalities for question answering, accordingly the complementary is determined by the multimodal and the target task. However, I didn't see much further discussion about these possible factors.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- Focusing on which clusters are ""best"" rather than what the differences in representation are between them, seems an odd choice given the motivation of the paper.",hbon6Jbp9Q,ICLR_2025,"- The pruning method does not appear to offer much beyond the method of feature reweighted representational similarity analysis, which is quite popular (see Kaniuth and Hebert, 2022; NeuroImage). In fact, it is essentially a particular limited case of FR-RSA, where the weights of features are either 0 or 1. The authors do not appear well aware of the literature, as only 20 references are made.
- I found the technique of using multiple different feature spaces (the 25 feature space of Mitchell et. al to fit voxel encoding models, then the full/pruned glove model to analyze similarities within clusters) to be convoluted and potentially circular.
- As the technique is not particularly novel, it is important that the authors deliver some clear novel findings about brain function. The abstract only lists one ""From a neurobiological perspective, we find that brain regions encoding social and cognitive aspects
of lexical items consistently also represent their sensory-motor features, though the reverse does not hold."" I did not find the case for this finding to be particularly strong. I welcome the authors to make the case more strongly.
- The figures are poorly made, certainly well below the bar of ICLR, and do not communicate much if anything that will affect how researcher's think about semantic organization in the brain. For example, while an interesting approach of clustering brain regions is used, these regions are never visualized. In the one plot that attempts to explain some differences across brain regions, the authors use arbitrary number indices for brain regions; at minimum, anatomical labels are needed. However, in 2024, it is expected that a strong paper on this topic can make elegant visualizations of the cortical surface. Practitioners in this field understand the importance of such visualizations for relating findings to pre-existing conceptual notions of cortical organization, and for driving further intuition that will affect future research.
- The authors waste precious space presenting fits to training data (see Table 1, ""complete dataset"", which reports the representational similarity after selecting the features that optimize to improve that representational similarity). Only the cross-validated results are worth presenting.
- Focusing on which clusters are ""best"" rather than what the differences in representation are between them, seems an odd choice given the motivation of the paper.
- Averaging voxels across subjects is likely to drastically reduce the granularity of the possible findings, since there is no expectation in voxel-level alignment of fine-grained conceptual information, but only larger-scale information. I believe it would be better to construct the clusters using all subject's individual data in a group-aligned space, where the same methods can otherwise be used, but individual voxel's are kept independent and not averaged across subjects.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The caption for Figure 7 is incorrect, and should be corrected to ""Edge Dynamics"" from ""Node Dynamics"".",vXSCD3ToCS,ICLR_2025,"- The paper appears to require daily generation of the dynamic road network topology using the tree-based adjacency matrix generation algorithm. The efficiency of this process remains unclear. Additionally, since the topologies undergo minimal changes between consecutive days, and substantial information is shared across these days, it raises the question of whether specialized algorithms are available to accelerate this topology generation.
- The authors present performance results only for two districts, D06 and D11. It is recommended to extend the reporting to include experimental results from the remaining seven districts.
- There is an inconsistency in the layout of the document: Figure 5 referred to on line 215 of Page 4, yet it is located on Page 7.
- The caption for Figure 7 is incorrect, and should be corrected to ""Edge Dynamics"" from ""Node Dynamics"".
- It is recommended that some recent related studies be discussed in the paper, particularly focusing on their performance with this dataset.
[1] UniST: A Prompt-Empowered Universal Model for Urban ST Prediction. KDD2024.
[2] Fine-Grained Urban Flow Prediction. WWW2021.
[3] When Transfer Learning Meets Cross-City Urban Flow Prediction: Spatio-Temporal Adaptation Matters. IJCAI2022.
[4] Spatio-Temporal Self-Supervised Learning for Traffic Flow Prediction. AAA2023.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. It could be convincing to discuss case studies and error studies to highlight the effectiveness of each proposed component. For example, this paper mentions that the Element-level Graph Pre-training abandons the strategy of capturing the complex structure but focuses directly on the core elements. However, without case study, it is less convincing to figure it out. An example of case study can be found in “Graph pre-training for AMR parsing and generation”.",o6D5yTpK8w,EMNLP_2023,"1. An incremental study of this topic based on previous works.
- [Bao IJCAI 2022] Aspect-based Sentiment Analysis with Opinion Tree Generation, IJCAI 2022 (OTG method)
- [Bao ACL Finding 2023] Opinion Tree Parsing for Aspect-based Sentiment Analysis, Findings of ACL 2023
The techniques involved in the proposed framework have been commonly used for ABSA, including graph pre-training, opinion tree generation and so on, and it seems not surprising enough to combine them together. The experimental results only show the performance can be improved, but lack of the explanations of why the performance can be improved.
2. The experimental results are not exactly convincing, by comparing with the main results in [Bao IJCAI 2022] and [Bao ACL Finding 2023]. For example,
- For the scores of OTG method, [Bao ACL Finding 2023] < this paper < [Bao IJCAI 2022]. Note that this is a significant difference, for example, on the Restaurant dataset, for F1 score, [Bao ACL Finding 2023] 0.6040 < this paper 0.6164 < [Bao IJCAI 2022] 0.6283; on the laptop dataset, for F1 score, [Bao ACL Finding 2023] 0.3998 < this paper 0.4394 < [Bao IJCAI 2022] 0.4544
- On the laptop dataset, for F1 score, although the scores of OTG method in this paper 0.4394 < the scores of proposed method in this paper 0.4512; The scores of OTG method in [Bao IJCAI 2022] 0.4544 > the scores of proposed method in this paper 0.4512;
There are also other significant differences on the performance of the baseline methods in these papers.
3. It could be convincing to discuss case studies and error studies to highlight the effectiveness of each proposed component. For example, this paper mentions that the Element-level Graph Pre-training abandons the strategy of capturing the complex structure but focuses directly on the core elements. However, without case study, it is less convincing to figure it out. An example of case study can be found in “Graph pre-training for AMR parsing and generation”.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1 The traditional DCI framework may already be considered explicitness(E) and size(S). For instance, to evaluate the disentanglement (D) of different representation methods, you may need to use a fixed capacity of probing (f), and the latent size should also be fixed. DCI and ES may be entangled with each other. For instance, if you change the capacity of probing or the latent size, then the DCI evaluation also changes correspondingly. The reviewer still needs clarification on the motivation for considering explicitness(E) and size(S) as extra evaluation.",ICLR_2023_935,ICLR_2023,"1 The traditional DCI framework may already be considered explicitness(E) and size(S). For instance, to evaluate the disentanglement (D) of different representation methods, you may need to use a fixed capacity of probing (f), and the latent size should also be fixed. DCI and ES may be entangled with each other. For instance, if you change the capacity of probing or the latent size, then the DCI evaluation also changes correspondingly. The reviewer still needs clarification on the motivation for considering explicitness(E) and size(S) as extra evaluation.
2 Intuitively, explicitness(E) and size(S) may be highly related to the given dataset. The different capacity requirements in the 3rd paragraph may be due to the input modality difference. Given a fixed dataset, the evaluation of disentanglement should provide enough capacity and training time which is powerful enough to achieve the DCI evaluation. If the capacity of probing needs to be evaluated, then the training time, cost, and learning rate may also be considered because they may influence the final value of DCI.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. In the experimental section of the paper, the standard deviation after multiple experiments is not provided. The improvement brought by SoRA compared with the baseline is quite limited, which may be due to random fluctuations. The author should clarify which effects are within the range of standard deviation fluctuations and which are improvements brought by the SoRA method.",jxgz7FEqWq,EMNLP_2023,"1. The comparison experiment with the MPOP method is lacking (See Missing References [1]). MPOP method is a lightweight fine-tuning method based on matrix decomposition and low-rank approximation, and it has a strong relevance to the method proposed in the paper. However, there is no comparison with this baseline method in the paper.
2. In Table 4, only the training time of the proposed method and AdaLoRA is compared, lacking the efficiency comparison with LoRA, Bitfit, and Adapter.
3. In the experimental section of the paper, the standard deviation after multiple experiments is not provided. The improvement brought by SoRA compared with the baseline is quite limited, which may be due to random fluctuations. The author should clarify which effects are within the range of standard deviation fluctuations and which are improvements brought by the SoRA method.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. This paper is not well organized. The layout of this paper is a bit rushed. For example, The font size of some annotations of Figure1 and Figure 2 is relatively small. And these two figures are not drawn explicitly enough. Table 2 is inserted wrongly inside of a paragraph. Top two lines on page 6 are in the wrong format.",ICLR_2022_3010,ICLR_2022,"1. Missing important related works
The related works are not well-reviewed. To be specific, only two papers of 2020 are discussed and recent papers of 2021 are missing. For example, the following papers are very close to the topic this paper addresses:
Dynamic Fusion With Intra-and Inter-Modality Attention Flow for Visual Question Answering, CVPR 2019.
Deep Modular Co-Attention Networks for Visual Question Answering, CVPR 2019.
Uniter: Universal image-text representation learning, ECCV 2020.
Visualbert: A simple and performant baseline for vision and language.
ViLT: Vision-and-Language Transformer Without Convolution or Region Supervision.
2. Compared baselines are not advanced enough.
VQA is a very popular research topic and plenty of approaches are benchmarked on the VQA-v2 dataset and the GQA dataset. However, many recent advanced methods are missed. For example, only one method after 2019 is compared in Table 3.
3. Missing some details of model implementation.
What are the models used to extract entity-level, noun phrase level, and sentence level features in section 3.1.2?
4. This paper is not well organized.
The layout of this paper is a bit rushed. For example,
The font size of some annotations of Figure1 and Figure 2 is relatively small. And these two figures are not drawn explicitly enough.
Table 2 is inserted wrongly inside of a paragraph.
Top two lines on page 6 are in the wrong format.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"- While the types of interventions included in the paper are reasonable computationally, it would be important to think about whether they are practical and safe for querying in the real world.",NIPS_2019_1049,NIPS_2019,"- While the types of interventions included in the paper are reasonable computationally, it would be important to think about whether they are practical and safe for querying in the real world. - The assumption of disentangled factors seems to be a strong one given factors are often dependent in the real world. The authors do include a way to disentangle observations though, which helps to address this limitation. Originality: The problem of causal misidentification is novel and interesting. First, identifying this phenomenon as an issue in imitation learning settings is an important step towards improved robustness in learned policies. Second, the authors provide a convincing solution as one way to address distributional shift by discovering the causal model underlying expert action behaviors. Quality: The quality of the work is high. Many details are not included in the main paper, but the appendices help to clarify some of the confusion. The authors evaluated the approach on multiple domains with several baselines. It was particularly helpful to see the motivating domains early on with an explanation of how the problem exists in these domains. This motivated the solution and experiments at the end. Clarity: The work was very well-written, but many parts of the paper relied on pointers to the appendices so it was necessary to go through them to understand the full details. There was a typo on page 3: Z_t â Z^t. Significance: The problem and approach can be of significant value to the community. Many current learning systems fail to identify important features relevant for a task due to limited data and due to the training environment not matching the real world. Since there will almost always be a gap between training and testing, developing approaches that learn the correct causal relationships between variables can be an important step towards building more robust models. Other comments: - What if the factors in the state are assumed to be disentangled but are not? What will the approach do/in what cases will it fail? - It seems unrealistic to query for expert actions at arbitrary states. One reason is because states might be dangerous, as the authors point out. But even if states are not dangerous, parachuting to a particular state would be hard practically. The expert could instead be simply presented a state and asked what they would do hypothetically (assuming the state representations of the imitator and expert match, which may not hold), but it could be challenging for an expert to hypothesize what he or she would do in this scenario. Basically, querying out of context can be challenging with real users. - In the policy execution mode, is it safe to execute the imitatorâs learned policy in the real world? The expert may be capable of acting safely in the world, but given that the imitator is a learning agent, deploying the agent and accumulating rewards in the real world can be unsafe. - On page 7, there is a reference to equation 3, which doesnât appear in the main submission, only in the appendix. - In the results section for intervention by policy execution, the authors indicate that the current model is updated after each episode. How long does this update take? - For the Atari game experiments, how is the number of disentangled factors chosen to be 30? In general, this might be hard to specify for an arbitrary domain. - Why is the performance for DAgger in Figure 7 evaluated at fewer intervals? The line is much sharper than the intervention performance curve. - The authors indicate that GAIL outperforms the expert query approach but that the number of episodes required are an order of magnitude higher. Is there a reason the authors did not plot a more equivalent baseline to show a fair comparison? - Why is the variance on Hopper so large? - On page 8, the authors state that the choice of the approach for learning the mixture of policies doesnât matter, but disc-intervention obtains clearly much higher reward than unif-intervention in Figures 6 and 7, so it seems like it does make a difference. ----------------------------- I read the author response and was happy with the answers. I especially appreciate the experiment on testing the assumption of disentanglement. It would be interesting to think about how the approach can be modified in the future to handle these settings. Overall, the work is of high quality and is relevant and valuable for the community.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"7) in the same section, the notation {\cal P} with a subscript is used several times without being defined.",NIPS_2016_537,NIPS_2016,"weakness of the paper is the lack of clarity in some of the presentation. Here are some examples of what I mean. 1) l 63, refers to a ""joint distribution on D x C"". But C is a collection of classifiers, so this framework where the decision functions are random is unfamiliar. 2) In the first three paragraphs of section 2, the setting needs to be spelled out more clearly. It seems like the authors want to receive credit for doing something in greater generality than what they actually present, and this muddles the exposition. 3) l 123, this is not the definition of ""dominated"" 4) for the third point of definition one, is there some connection to properties of universal kernels? See in particular chapter 4 of Steinwart and Christmann which discusses the ability of universal kernels two separate an arbitrary finite data set with margin arbitrarily close to one. 5) an example and perhaps a figure would be quite helpful in explaining the definition of uniform shattering. 6) in section 2.1 the phrase ""group action"" is used repeatedly, but it is not clear what this means. 7) in the same section, the notation {\cal P} with a subscript is used several times without being defined. 8) l 196-7: this requires more explanation. Why exactly are the two quantities different, and why does this capture the difference in learning settings? ---- I still lean toward acceptance. I think NIPS should have room for a few ""pure theory"" papers.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- l148: known, instead of know - l156: please define \gamma_0^{***} - Figure 1: Please specify the meaning of the colors in the caption as well as the text.",NIPS_2017_110,NIPS_2017,"of this work include that it is a not-too-distant variation of prior work (see Schiratti et al, NIPS 2015), the search for hyperparameters for the prior distributions and sampling method do not seem to be performed on a separate test set, the simultion demonstrated that the parameters that are perhaps most critical to the model's application demonstrate the greatest relative error, and the experiments are not described with adequate detail. This last issue is particularly important as the rupture time is what clinicians would be using to determine treatment choices. In the experiments with real data, a fully Bayesian approach would have been helpful to assess the uncertainty associated with the rupture times. Paritcularly, a probabilistic evaluation of the prospective performance is warranted if that is the setting in which the authors imagine it to be most useful. Lastly, the details of the experiment are lacking. In particular, the RECIST score is a categorical score, but the authors evaluate a numerical score, the time scale is not defined in Figure 3a, and no overall statistics are reported in the evaluation, only figures with a select set of examples, and there was no mention of out-of-sample evaluation.
Specific comments:
- l132: Consider introducing the aspects of the specific model that are specific to this example model. For example, it should be clear from the beginning that we are not operating in a setting with infinite subdivisions for \gamma^1 and \gamma^m and that certain parameters are bounded on one side (acceleration and scaling parameters).
- l81-82: Do you mean to write t_R^m or t_R^{m-1} in this unnumbered equation? If it is correct, please define t_R^m. It is used subsequently and it's meaning is unclear.
- l111: Please define the bounds for \tau_i^l because it is important for understanding the time-warp function.
- Throughout, the authors use the term constrains and should change to constraints.
- l124: What is meant by the (*)?
- l134: Do the authors mean m=2?
- l148: known, instead of know
- l156: please define \gamma_0^{***}
- Figure 1: Please specify the meaning of the colors in the caption as well as the text.
- l280: ""Then we made it explicit"" instead of ""Then we have explicit it""","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1. What exactly do the authors mean by the “upper faces” of the convex hull? The dual subdivision and projection π need to be explained better. Decision boundaries of neural networks: The variable ‘p’ is not explicitly defined. This is rather problematic since it has been used extensively throughout the rest of the paper. It would make sense to move def.,ICLR_2021_1832,ICLR_2021,"The paper perhaps bites off a little more than it can chew. It might be best if the authors focused on their theoretical contributions in this paper, added more text and intuition about the extensions of their current bias-free NNs, fleshed out their analyses of the lottery ticket hypothesis and stopped at that.
The exposition and experiments done with tropical pruning need more work. Its extension to convolutional layers is a non-trivial but important aspect that the authors are strongly encouraged to address. This work could possibly be written up into another paper. Similarly, the work done towards generating adversarial samples could definitely do with more detailed explanations and experiments. Probably best left to another paper.
Contributions: The theoretical contributions of the work are significant and interesting. The fact that the authors have been able to take their framework and apply it to multiple interesting problems in the ML landscape speaks to the promise of their theory and its resultant perspectives. The manner in which the tropical geometric framework is applied to empirical problems however, requires more work.
Readability: The general organization and technical writing of the paper are quite strong, in that concepts are laid out in a manner that make the paper approachable despite the unfamiliarity of the topic for the general ML researcher. The language of the paper however, could do with some improvement; Certain statements are written such that they are not the easiest to follow, and could therefore be misinterpreted.
Detailed comments:
While there are relatively few works that have explicitly used tropical geometry to study NN decision boundaries, there are others such as [2] which are similar in spirit, and it would be interesting to see exactly how they relate to each other.
Abstract: It gets a little hard to follow what the authors are trying to say when they talk about how they use the new perspectives provided by the geometric characterizations of the NN decision boundaries. It would be helpful if the tasks were clearly enumerated.
Introduction: “For instance, and in an attempt to…” Typo – delete “and”. Similar typos found in the rest of the section too, addressing which would improve the readability of the paper a fair bit.
Preliminaries to tropical geometry: The preliminaries provided by the authors are much appreciated, and it would be incredibly helpful to have a slightly more detailed discussion of the same with some examples in the appendix. To that end, it would be a lot more insightful to discuss ex. 2 in Fig. 1, in addition to ex. 1. What exactly do the authors mean by the “upper faces” of the convex hull? The dual subdivision and projection π
need to be explained better.
Decision boundaries of neural networks: The variable ‘p’ is not explicitly defined. This is rather problematic since it has been used extensively throughout the rest of the paper. It would make sense to move def. 6 to the section discussing preliminaries.
Digesting Thm. 2: This section is much appreciated and greatly improves the accessibility of the paper. It would however be important, to provide some intuition about how one would study decision boundaries when the network is not bias-free, in the main text. In particular, how would the geometry of the dual subdivision δ ( R ( x ) )
change? On a similar note, how do things change in practice when studying deep networks that are not bias free, given that, “Although the number of vertices of a zonotope is polynomial in the number of its generating line segments, fast algorithms for enumerating these vertices are still restricted to zonotopes with line segments starting at the origin”? Can Prop. 1 and Cor. 1 be extended to this case trivially?
Tropical perspective to the lottery ticket hypothesis: It would be nice to quantify the (dis)similarity in the shape of the decision boundaries polytopes across initializations and pruning using something like the Wasserstein metric.
Tropical network pruning: How are λ 1 , λ 2
chosen? Any experiments conducted to decide on the values of the hyper-parameters should be mentioned in the main text and included in the appendix. To that end, is there an intuitive way to weight the two hyper-parameters relative to each other?
Extension to deeper networks: Does the order in which the pruning is applied to different layers really make a difference? It would also be interesting to see whether this pruning can be parallelized in some way. A little more discussion and intuition regarding this extension would be much appreciated. Experiments:
The descriptions of the methods used as comparisons are a little confusing – in particular, what do the authors mean when they say “pruning for all parameters for each node in a layer” Wouldn’t these just be the weights in the layer?
“…we demonstrate experimentally that our approach can outperform all other methods even when all parameters or when only the biases are fine-tuned after pruning” – it is not immediately obvious why one would only want to fine-tune the biases of the network post pruning and a little more intuition on this front might help the reader better appreciate the proposed work and its contributions.
Additionally, it might be an unfair comparison to make with other methods, since the objective of the tropical geometry-based pruning is preservation of decision boundaries while that of most other methods is agnostic of any other properties of the NN’s representational space.
Going by the results shown in Fig. 5, it would perhaps be better to say that the tropical pruning method is competitive with other pruning methods, rather than outperforming them (e.g., other methods seem to do better with the VGG16 on SVHN and CIFAR100)
“Since fully connected layers in DNNs tend to have much higher memory complexity than convolutional layers, we restrict our focus to pruning fully connected layers.” While it is true that fully connected layers tend to have higher memory requirements than convolutional ones, the bulk of the parameters in modern CNNs still belong to convolutional layers. Moreover, the most popular CNNs are now fully convolutional (e.g., ResNet, UNet) which would mean that the proposed methods in their current form would simply not apply to them.
Comparison against tropical geometry approaches for network pruning – why are the accuracies for the two methods different when 100% of the neurons are kept and the base architecture used is the same? The numbers reported are à (100, 98.6, 98.84) Tropical adversarial attacks: Given that this topic is not at all elaborated upon in the main text (and none of the figures showcase any relevant results either), it is strongly recommended that the authors either figure out a way to allocate significantly more space to this section, or not include it in this paper. (The idea itself though seems interesting and could perhaps make for another paper in its own right.)
References: He et al. 2018a and 2018b seem to be the same.
[1] Zhang L. et al., “Tropical Geometry of Deep Neural Networks”, ICML 2018. [2] Balestriero R. and Baraniuk R., “A Spline Theory of Deep Networks”, ICML 2018.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- Originality: the work seems to be sufficiently original with respect to its predecessor (SUCRL) and with respect to other published works in NIPS;,NIPS_2017_303,NIPS_2017,"of their approach with respect to the previous SUCRL. The provided numerical simulation is not conclusive but supports the above considerations;
- Clarity: the paper could be clearer but is sufficiently clear. The authors provide an example and a theoretical discussion which help understanding the mathematical framework;
- Originality: the work seems to be sufficiently original with respect to its predecessor (SUCRL) and with respect to other published works in NIPS;
- Significance: the motivation of the paper is clear and relevant since it addresses a significant limitation of previous methods;
Other comments:
- Line 140: here the first column of Qo is replaced by vo to form P'o, so that the first state is not reachable anymore but from a terminating state. I assume that either Ass.1 (finite length of an option) or Ass. 2 (the starting state is a terminal state) clarify this choice. In the event this is the case, the authors should mention the connection between the two;
- Line 283: ""four"" -> ""for"";
- Line 284: ""where"" s-> ""were"";","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '0']}",,,"{'annotators': ['hgdl9t28'], 'labels': ['1']}",,
"- I am also not convinced to the idea that the images and their augmentations need to be treated separately, they can be interchangeable.",NIPS_2021_2445,NIPS_2021,"and strengths in their analysis with sufficient experimental detail, it is admirable, but they could provide more intuition why other methods do better than theirs.
The claims could be better supported. Some examples and questions(if I did not miss out anything)
Why using normalization is a problem for a network or a task (it can be thought as a part of cosine distance)? How would Barlow Twins perform if their invariance term is replaced with a euclidean distance?
Your method still uses 2048 as the batch size, I would not consider it as small. For example, Simclr uses examples in the same batch and its batch size changes between 256-8192. Most of the methods you mentioned need even much lower batch size.
You mentioned not sharing weights as an advantage, but you have shared weights in your results, except Table 4 in which the results degraded as you mentioned. What stops the other methods from using different weights? It should be possible even though they have covariance term between the embeddings, how much their performance would be affected compared with yours?
My intuition is that a proper design might be sufficient rather than separating variance terms.
- Do you have a demonstration or result related to your model collapsing less than other methods? In line 159, you mentioned gradients become 0 and collapse; it was a good point, is it commonly encountered, did you observe it in your experiments?
- I am also not convinced to the idea that the images and their augmentations need to be treated separately, they can be interchangeable.
- Variances of the results could be included to show the stability of the algorithms since it was another claim in the paper(although ""collapsing"" shows it partly, it is a biased criteria since the other methods are not designed for var/cov terms).
- How hard is it to balance these 3 terms?
- When someone thinks about gathering two batches from two networks and calculate the global batch covariance in this way; it includes both your terms and Barlow Twins terms. Can anything be said based on this observation, about which one is better and why? Significance:
Currently, the paper needs more solid intuition or analysis or better results to make an impact in my opinion. The changes compared with the prior work are minimal. Most of the ideas and problems in the paper are important, but they are already known.
The comparisons with the previous work is valuable to the field, they could maybe extend their experiments to the more of the mentioned methods or other variants.
The authors did a great job in presenting their work's limitations, their results in general not being better than the previous works and their extensive analysis(tables). If they did a better job in explaining the reasons/intuitions in a more solid way, or include some theory if there is any, I would be inclined to give an accept.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim.",NIPS_2016_313,NIPS_2016,"Weakness: 1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim. 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc. 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods. 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work. 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Another issue of the paper is that the disentangling is done manually. For example, the semantic segmentation network is the first module in the pipeline. Why is that? Why not something else? It would be interesting if the paper did not have this type of manual disentangling, and everything was learned.",NIPS_2018_25,NIPS_2018,"- My understanding is that R,t and K (the extrinsic and intrinsic parameters of the camera) are provided to the model at test time for the re-projection layer. Correct me in the rebuttal if I am wrong. If that is the case, the model will be very limited and it cannot be applied to general settings. If that is not the case and these parameters are learned, what is the loss function? - Another issue of the paper is that the disentangling is done manually. For example, the semantic segmentation network is the first module in the pipeline. Why is that? Why not something else? It would be interesting if the paper did not have this type of manual disentangling, and everything was learned. - ""semantic"" segmentation is not low-level since the categories are specified for each pixel so the statements about semantic segmentation being a low-level cue should be removed from the paper. - During evaluation at test time, how is the 3D alignment between the prediction and the groundtruth found? - Please comment on why the performance of GTSeeNet is lower than that of SeeNetFuse and ThinkNetFuse. The expectation is that groundtruth 2D segmentation should improve the results. - line 180: Why not using the same amount of samples for SUNCG-D and SUNCG-RGBD? - What does NoSeeNet mean? Does it mean D=1 in line 96? - I cannot parse lines 113-114. Please clarify.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Although the authors derive PAC-Bayesian bound for GNNs in the transductive setting and show how the interplay between training and testing sets influences the generalization ability, I fail to see the strong connection between the theoretical analysis and the proposed method. The proposed method seems to simply adopt the idea of the self-attention mechanism from the transformer and apply it to the graph. It's not clear to me how the proposed method enhances the generalization for the distant nodes.",pwW807WJ9G,ICLR_2024,"1. Although the authors derive PAC-Bayesian bound for GNNs in the transductive setting and show how the interplay between training and testing sets influences the generalization ability, I fail to see the strong connection between the theoretical analysis and the proposed method. The proposed method seems to simply adopt the idea of the self-attention mechanism from the transformer and apply it to the graph. It's not clear to me how the proposed method enhances the generalization for the distant nodes.
2. My major concern about the proposed method is the graph partition as partitioning the graph usually leads to information loss. Though node2vec is used for positional encoding purposes, it only encodes the local topological structure, and it cannot compensate for the lost information between different subgraphs. Based on algorithm 1 in Appendix E, there is no information exchange between different subgraphs. The nodes in a subgraph can only receive the information from other nodes within this subgraph and these nodes are isolated from the nodes from other subgraphs. The performance seems to highly depend on the quality of the graph partition algorithms. However, it's unclear whether different graph partitions will influence the performance of the proposed method or not.
3. Some experimental setups are not quite clear. See questions below.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '3', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
1) It is not clear how the method behaves without Lipschitz Hessian assumption.,ICLR_2021_1705,ICLR_2021,"and suggestions for improvement: I have several concerns about the potential impact of both theoretical and practical results. Mainly:
By referring to Wilson et al., 2017, the authors argue that diagonal step sizes in adaptive algorithms hurt generalization. First, I find this claim rather vague, as there has been many followups to Wilson et al., 2017, so I suggest the authors to be more precise and include more recent observations. Moreover, one can use non-diagonal versions of these algorithms. For example, see [2 and Adagrad-norm from Ward et al., 2019], it is easy to consider similar non-diagonal versions of Adam/AMSGrad/Adagrad with first order momentum (a.k.a. AdamNC or AdaFOM), then, are these algorithms also supposed to have good generalization? I think it is important to see how these non-diagonal adaptive methods behave in practice compared to SGD/Adam+ for generalization to support the authors' claim.
I think the algorithm seems more like an extension of momentum SGD, than Adam.
It is nice to improve \eps^{-4} complexity with Lipschitz Hessian assumption, but what happens when this assumption fails? Does Adam+ get standard \epsilon^{-4}?
From what I understand in remark after Lemma 1, the variance reduction is ensured by taking β to 0
. The authors use 1 / T a
for some a ∈ ( 0 , 1 )
. Here, I have several questions. First, how does such a small β
work in practice? If in practice, a larger β
works well and theory requires β → 0
for working, it shows to me that theoretical analysis of the paper does not translate to the practical performance. When one uses β
values that work well in practice, does the theory show convergence?
Related to the previous part, I am also not sure about ""adaptivity"" of the method. The authors need to use Lipschitz constants L , L H
to set step sizes. Moreover β
is also fixed in advance, depending on horizon T
, which is the main reason to have variance reduction on z t − ∇ f ( w k )
. So, I do not understand what is adaptive in the step size or in the variance reduction mechanism of the method.
For experiments, the authors say that Adam+ is comparable with ""tuned"" SGD. However, from the explanations in the experimental part, I understand that Adam+ is also tuned similar to SGD. Then, what is the advantage compared to SGD? If one needs the same amount of tuning for Adam+, and the performance is similar, I do not see much advantage compared to SGD. On this front, I suggest the authors to show what happens when the step size parameter is varied, is Adam+ more robust to non-tuned step sizes compared to SGD?
To sum up, I vote for rejection since 1) the analysis and parameters require strict condition, 2) it is not clear if the analysis illustrates the practical performance (very small β
is needed in theory), 3) practical merit is unclear since the algorithm needs to be tunes similar to SGD and the results are also similar to SGD.
[1] Zhang, Lin, Jegelka, Jadbabaie, Sra, Complexity of Finding Stationary Points of Nonsmooth Nonconvex Functions, ICML 2020. [2] Levy, Online to Offline Conversions, Universality and Adaptive Minibatch Sizes, NIPS 2017.
======== after discussion phase ==========
I still think that the merit of the method is unclear due to reasons: 1) It is not clear how the method behaves without Lipschitz Hessian assumption. 2) The method only obtains the state-of-the-art complexity of ϵ − 3.5
with large mini-batch sizes and the complexity with small mini-batch sizes (section 2.1) is suboptimal (in fact drawbacks such as this needs to be presented explicitly, right now I do not see enough discussions about this.). 3) Adaptive variance reduction property claimed by the authors boils down to picking ""small enough"" β
parameter, which in my opinion takes away the adaptivity claim and is for example not the case in adaptive methods such as AdaGrad. 4) The comparison with AdaGrad and Adam with scalar step sizes is not included (the authors promised to include it later, but I cannot make a decision about these without seeing results) and I am not sure if Adam+ will bring benefits over them. 5) Presentation of the paper needs major improvements. I recommend making the remarks after Lemma 1 and theorems clearer, by writing down exact expressions and the implications of these (for example remarks such as ""As the algorithm converges with E [ ∇ F ( w ) 2 ] and β
decreases to zero, the variance of z t
will also decrease"" can be made more rigorous and clearer, by writing down exactly the bound for the variance of z t
by iterating the recursion written with E δ t + 1
and highlighting what each term does in the bound. This way will be much easier for readers to understand your paper).
Therefore, I am keeping my score.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3 Some of the pieces are simply using existing methods, such as equation (12), the presentation of these methods are also vague (can only be understood after checking the original paper).",IksoHnq4rC,EMNLP_2023,"TL;DR I appreciate the efforts and observations / merits found by the authors. However, this paper poorly presents the methodology (both details and its key advantage), and it’s hard to validate the conclusion with such little hyperparameter analysis. I would love to see more detailed results, but I could not accept this version to be accepted as an EMNLP paper.
1 There are too many missing details when presenting the methodology: e.g., what will be the effect if I remove one or two losses presented by the authors? Though their motivations are clear, they do not validate the hypothesis clearly.
2 A lot of equations look like placeholders, such as equations (1, 2, 3, 5, 6).
3 Some of the pieces are simply using existing methods, such as equation (12), the presentation of these methods are also vague (can only be understood after checking the original paper).
4 The pipeline misses a lot of details. For example, how long does it take to pre-train each module? How adding pre-training will benefit the performance? How to schedule the training of the discriminator and the main module? Not mentioning the detail design of the RNN network used.
5 Why do we need to focus on the four aspects? They are just listed there. Also, some of the results presentation does not seem to be thorough and valid. For example, in table 2, the Quora datasets have the highest perturbation ratio, but the downgraded performance is the least among the three. Is it really because the adversarial samples are effective instead of the task variance or dataset variance? Also, we didn’t see the attack performance of other comparison methods. And how is the test set generated? What is the size of the adversarial test set and why is that a good benchmark?
6 In table 4, it’s actually hard to say which is better, A^3 or A^2T, if you count the number of winners for each row and column.
7 In table 5, is the computation time also considered the pre-training stage? If not, why? Does the pre-training stage can serve as a unified step which is agnostic to the dataset and tasks?
8 I don’t quite understand the point of section 4.6, and its relationship to the effectiveness of A^3. This influence of /rho seems to be really obvious. I would rather be more interested in changing the six hyperparameters mentioned in line 444 and test their effectiveness.
9 The related work section is also not very well-written. I couldn’t understand what is the key difference and key advantage of A^3 compared to the other methods.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?",ARR_2022_12_review,ARR_2022,"I feel the design of NVSB and some experimental results need more explanation (more information in the section below).
1. In Figure 1, given experimental dataset have paired amateur and professional recordings from the same singer, what are the main rationals for (a) Having a separate timbre encoder module (b) SADTW takes outputs of content encoder (and not timbre encoder) as input?
2. For results shown in Table 3, how to interpret: (a) For Chinese MOS-Q, NVSB is comparable to GT Mel A. (b) For Chinese and English MOS-V, Baseline and NVSB have overlapping 95% CI.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- Table 4 is incomplete. It should include the results for all four datasets.,NIPS_2017_71,NIPS_2017,"- The paper is a bit incremental. Basically, knowledge distillation is applied to object detection (as opposed to classification as in the original paper).
- Table 4 is incomplete. It should include the results for all four datasets.
- In the related work section, the class of binary networks is missing. These networks are also efficient and compact. Example papers are:
* XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, ECCV 2016
* Binaryconnect: Training deep neural networks with binary weights during propagations, NIPS 2015
Overall assessment: The idea of the paper is interesting. The experiment section is solid. Hence, I recommend acceptance of the paper.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- As I said above, I found the writing / presentation a bit jumbled at times.",NIPS_2017_351,NIPS_2017,"- As I said above, I found the writing / presentation a bit jumbled at times.
- The novelty here feels a bit limited. Undoubtedly the architecture is more complex than and outperforms the MCB for VQA model [7], but much of this added complexity is simply repeating the intuition of [7] at higher (trinary) and lower (unary) orders. I don't think this is a huge problem, but I would suggest the authors clarify these contributions (and any I may have missed).
- I don't think the probabilistic connection is drawn very well. It doesn't seem to be made formally enough to take it as anything more than motivational which is fine, but I would suggest the authors either cement this connection more formally or adjust the language to clarify.
- Figure 2 is at an odd level of abstraction where it is not detailed enough to understand the network's functionality but also not abstract enough to easily capture the outline of the approach. I would suggest trying to simplify this figure to emphasize the unary/pairwise/trinary potential generation more clearly.
- Figure 3 is never referenced unless I missed it.
Some things I'm curious about:
- What values were learned for the linear coefficients for combining the marginalized potentials in equations (1)? It would be interesting if different modalities took advantage of different potential orders.
- I find it interesting that the 2-Modalities Unary+Pairwise model under-performs MCB [7] despite such a similar architecture. I was disappointed that there was not much discussion about this in the text. Any intuition into this result? Is it related to swap to the MCB / MCT decision computation modules?
- The discussion of using sequential MCB vs a single MCT layers for the decision head was quite interesting, but no results were shown. Could the authors speak a bit about what was observed?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The stability definition needs better justified, as the left side can be arbitrarily small under some construction of \tilde{g}. A more reasonable treatment is to make it also lower bounded.",ICLR_2022_537,ICLR_2022,"1. The stability definition needs better justified, as the left side can be arbitrarily small under some construction of \tilde{g}. A more reasonable treatment is to make it also lower bounded. 2. It is expected to see a variety of tasks beyond link predict where PE is important.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- I wonder how crucial the annealing scheme from the last paragraph in Section 4 is. Especially when $\alpha$ is not decreased to $0$ I imagine this could induce a bias which may be so large that it outweighs the bias reductions attained by using IWAE in the first place.,NIPS_2020_1391,NIPS_2020,"- I wonder how crucial the annealing scheme from the last paragraph in Section 4 is. Especially when $\alpha$ is not decreased to $0$ I imagine this could induce a bias which may be so large that it outweighs the bias reductions attained by using IWAE in the first place. - The only other weakness is related to the clarity of the exposition, especially around the ""OVIS_~"" estimator (see further details below). ==== EDIT: 2020-08-24 ===== replaced ""$\alpha$ is not increased to $1$"" by ""$\alpha$ is not decreased to $0$"" as I had had a typo in this part of my review","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- I wonder what is the total computational complexity compared to other methods (e.g., emerging convolutions). If I imagine the Woodbury flow working on a mobile device, the number of operations could cause a significant power demand.",NIPS_2020_486,NIPS_2020,"- I wonder what is the total computational complexity compared to other methods (e.g., emerging convolutions). If I imagine the Woodbury flow working on a mobile device, the number of operations could cause a significant power demand. - Following on that, I am worried that the total computational complexity is much higher for other approaches. This could limit the usability of the proposed transformation.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors write “In the base IOI circuit, the Induction, Duplicate Token, and Previous Token heads primarily attend to the S2 token” this is incorrect according to Section 3 of Wang et al., 2023. These heads are _active_ at the S2 token, but do not primarily attend to it.",FbZSZEIkEU,ICLR_2025,"- The experimental reports are lacking in many details about the experimental methodology, making it difficult to be confident that the claims are robust.
- The explanations throughout the paper should be clearer to fully communicate the ideas and experiments of the authors.
- The S2 hacking hypothesis is quite vague and the author do not present any deep understanding that would explain the mechanisms by which certain attention heads pay extra attention to the S2 token.
- In the experiments on the DoubleIO and other prompt variations, it is unclear at which token positions paths are being ablated, as this is unspecified by the original circuit.
- The authors write: “Given the algorithm inferred from the IOI circuit, it is clear that the full model should completely fail on this task”. However this is a misunderstanding of the original work. The IOI circuit was discovered using mean ablations that keep most of the prompt intact. Therefore Wang et al. don’t expect it generalize to different prompt formats.
- The authors write “In the base IOI circuit, the Induction, Duplicate Token, and Previous Token heads primarily attend to the S2 token” this is incorrect according to Section 3 of Wang et al., 2023. These heads are _active_ at the S2 token, but do not primarily attend to it.
- The authors write: ""The proposed IOI circuit is shown to perform very well while still being faithful to the full model"" In fact, the IOI circuit is known to have severe limitations, as shown in concurrent work by Miller et al. (2024) [[2]](https://arxiv.org/abs/2407.08734). Nitpicks:
- In Figure 2, it is not clear what Head 1, 2, 3 and 4 refer to.
- The paper should include Figure 2 from Wang et al. 2023 [[1]](https://arxiv.org/pdf/2211.00593#page=4.37) to make it easier to follow discussions about the circuit.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1). Technically speaking, the contribution of this work is incremental. The proposed pipeline is not that impressive or novel; rather, it seems to be a pack of tricks to improve defense evaluation.",NIPS_2022_389,NIPS_2022,"1). Technically speaking, the contribution of this work is incremental. The proposed pipeline is not that impressive or novel; rather, it seems to be a pack of tricks to improve defense evaluation. 2). Although IoFA is well supported by cited works and described failures, its introduction lacks practical cases, where Figures 1 and 2 do not provide example failures and thus do not lead to a better understanding. 3). The reported experimental results appear to evidence the proposed methods, while there is a missing regarding the case analysis and further studies.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain.",NIPS_2019_653,NIPS_2019,"of the method. Clarity: The paper has been written in a manner that is straightforward to read and follow. Significance: There are two factors which dent the significance of this work. 1. The work uses only binary features. Real world data is usually a mix of binary, real and categorical features. It is not clear if the method is applicable to real and categorical features too. 2. The method does not seem to be scalable, unless a distributed version of it is developed. It's not reasonable to expect a single instance can hold all the training data that the real world datasets ususally contain.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
3. It is not clear why eta_ri term is non-central chi-squared distribution.,NIPS_2020_284,NIPS_2020,"1. A major concern that I have is, the authors consider only shifts in annotations as the noise. However, real-world annotations include other types of noises like missing annotations or duplicate annotations. The authors do not consider this in their discussion. From the outset, it seems that their current method cannot accommodate these additional noises. From this perspective, I would say that the paper is incomplete in modeling different types of annotation noises. 2. It is not clear why the authors approximate pdf phi and Psi with Gaussian distribution. 3. It is not clear why eta_ri term is non-central chi-squared distribution. 4. As far as I understand, small shifts in annotations will not affect performance much, since neural networks can be robust if receptive size of the network is large enough. Can the authors discuss this more in detail. 5. The proposed method seems to be too specific to the counting problem. Can this method be extended to other problems in vision like object detection.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players. Initial Evaluation: This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility: Appears to be reproducible.",NIPS_2017_401,NIPS_2017,"Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.
Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility:
Appears to be reproducible.","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
"- L15: Again, too vague, certain RNNs work well for certain natural language reasoning tasks. See for instance the literature on natural language inference and the leaderboard at https://nlp.stanford.edu/projects/snli/ - L16-18: The reinforcement learning / agent analogy seems a bit out-of-place here. I think you generally point to generalization capabilities which I believe are better illustrated by the examples you give later in the paper (from lines 229 to 253).",NIPS_2018_917,NIPS_2018,"- Results on bAbI should be taken with a huge grain of salt and only serve as a unit-test. Specifically, since the bAbI corpus is generated from a simple grammar and sentence follow a strict triplet structure, it is not surprising to me that a model extracting three distinct symbol representations from a learned sentence representation (therefore reverse engineering the underlying symbolic nature of the data) would solve bAbI tasks. However, it is highly doubtful this method would perform well on actual natural language sentences. Hence, statements such as ""trained [...] on a variety of natural language tasks"" is misleading. The authors of the baseline model ""recurrent entity networks"" [12] have not stopped at bAbI, but also validated their models on more real-world data such as the Children's Book Test (CBT). Given that RENs solve all bAbI tasks and N2Ns solve all but one, it is not clear to me what the proposed method adds to a table other than a small reduction in mean error. Moreover, the N2N baseline in Table 2 is not introduced or referenced in this paper, so I am not sure which system the authors are referring to here. Minor Comments - L11: LSTMs have only achieved on some NLP tasks, whereas traditional methods still prevail on others, so stating they have achieved SotA in NLP is a bit too vague. - L15: Again, too vague, certain RNNs work well for certain natural language reasoning tasks. See for instance the literature on natural language inference and the leaderboard at https://nlp.stanford.edu/projects/snli/ - L16-18: The reinforcement learning / agent analogy seems a bit out-of-place here. I think you generally point to generalization capabilities which I believe are better illustrated by the examples you give later in the paper (from lines 229 to 253). - Eq. 1: This seems like a very specific choice of combining the information from entity representations and their types. Why is this a good choice? Why not keep the concatenation of the kitty/cat outer product and the mary/person outer product? Why is instead the superposition of all bindings a good design choice? - I believe section four could benefit from a small overview figures illustrating the computation graph that is constructed by the method. - Eq. 7: At first, I found it surprising why three distinct relation representation are extracted from the sentence representation, but it became clearer later with the write, move and backling functions. Maybe already mention at this point why the three relation representations are going to be used for. - Eq. 15: s_question has not been introduced before. I imagine it is a sentence encoding of the question and calculated similarly to Eq. 5? - Eq. 20: A bit more details for readers unfamiliar with bAbI or question answering would be good here. ""valid words"" here means possible answer words for the given story and question, correct? - L192: ""glorot initalization"" -> ""Glorot initialization"". Also, there is a reference for that method: Glorot, X., & Bengio, Y. (2010, March). Understanding the difficulty of training deep feedforward neural networks. In Proceedings of the thirteenth international conference on artificial intelligence and statistics (pp. 249-256). - L195: Î±=0.008, Î²â=0.6 and Î²â=0.4 look like rather arbitrary choices. Where does the configuration for these hyper-parameters come from? Did you perform a grid search? - L236-244: If I understand it correctly, at test time stories with new entities (Alex etc.) are generated. How does your model support a growing set of vocabulary words given that MLPs have parameters dependent on the vocabulary size (L188-191) and are fixed at test time? - L265: If exploding gradients are a problem, why don't you perform gradient clipping with a high value for the gradient norm to avoid NaNs appearing? Simply reinitializing the model is quite hacky. - p.9: Recurrent entity networks (RENs) [12] is not just an arXiv paper but has been published at ICLR 2017.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. In Fig.5, as shown in the third figure, the proposed sensitive-layer selection against randomized selection does not make too much difference in terms of StableDiffusion and the authors do not further discuss such an observation. Besides, there is a lack of mathematical or theoretical justification for the proposed Algorithm.1.",m8ERGrOf1f,ICLR_2025,"1. While the unified low-precision quantization strategy is effective, the scalability of the approach—especially when applied to larger diffusion models beyond those tested—is unclear. Including runtime and memory trade-offs when scaling to more complex models (e.g., SDXL) or higher-resolution tasks would enhance the practical utility of the work.
2. There is a lack of BF16 (baseline) when the authors try to demonstrate the effectiveness of the purposed method on FP8 configurations.
3. In Fig.5, as shown in the third figure, the proposed sensitive-layer selection against randomized selection does not make too much difference in terms of StableDiffusion and the authors do not further discuss such an observation. Besides, there is a lack of mathematical or theoretical justification for the proposed Algorithm.1.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.",ARR_2022_1_review,ARR_2022,"- Using original encoders as baselines might not be sufficient. In most experiments, the paper only compares with the original XLM-R or mBERT trained without any knowledge base information. It is unclear whether such encoders being fine-tuned towards the KB tasks would actually perform comparable to the proposed approach. I would like to see experiments like just fine tuning the encoders with the same dataset but the MLM objective in their original pretraining and comparing with them. Such baselines can leverage on input sequences as simple as `<s>X_s X_p X_o </s>` where one of them is masked w.r.t. MLM training.
- The design of input formats is intuitive and lacks justifications. Although the input formats for monolingual and cross-lingual links are designed to be consistent, it is hard to tell why the design would be chosen. As the major contribution of the paper, justifying the design choice matters. In other words, it would be better to see some comparisons over some variants, say something like `<s>[S]X_s[S][P]X_p[P][O]X_o[O]</s>` as wrapping tokens in the input sequence has been widely used in the community.
- The abstract part is lengthy so some background and comparisons with prior work can be elaborated in the introduction and related work. Otherwise, they shift perspective of the abstract, making it hard for the audience to catch the main novelties and contributions.
- In line 122, triples denoted as $(e_1, r, e_2)$ would clearly show its tuple-like structure instead of sets.
- In sec 3.2, the authors argue that the Prix-LM (All) model consistently outperforms the single model, hence the ability of leveraging multilingual information. Given the training data sizes differ a lot, I would like to see an ablation that the model is trained on a mix of multilingual data with the same overall dataset size as the monolingual. Otherwise, it is hard to justify whether the performance gain is from the large dataset or from the multilingual training.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1). Lack of speed analysis, the experiments have compared GFLOPs of different segmentation networks. However, there is no comparisons of inference speed between the proposed network and prior work. The improvement on inference speed will be more interesting than reducing FLOPs.",NIPS_2021_2247,NIPS_2021,"1). Lack of speed analysis, the experiments have compared GFLOPs of different segmentation networks. However, there is no comparisons of inference speed between the proposed network and prior work. The improvement on inference speed will be more interesting than reducing FLOPs. 2). For the detail of the proposed NRD, it is reasonable that the guidance maps are generated from the low-level feature maps. And the guidance maps can be predicted from the the first-stage feature maps or the second-stage feature maps. It is better to provide one ablation study about the effect for them. 3). Important references are missing. The GFF[1] and EfficientFCN[2] both aims to implement the fast semantic segmentation method in the encode-decoder architecture. I encourage the authors to have a comprehensive comparison with these work.
[1]. Gated Fully Fusion for Semantic Segmentation, AAAI'20. [2]. EfficientFCN: Holistically-guided Decoding for Semantic Segmentation, ECCV'20.
See above. The societal impact is shown one the last page of the manuscript.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"1. Optimal quantization is not scalable (which is mentioned in the paper as well). Even with clustering before, it is costly to both N(number of data) and M(the dimension). The paper (in abstract and intro) aims to speed up VI by fast convergence which is needed for big data/big model setting, which the quantization is a bottleneck for it, which makes the method loses its point.",NIPS_2020_1634,NIPS_2020,"1. Optimal quantization is not scalable (which is mentioned in the paper as well). Even with clustering before, it is costly to both N(number of data) and M(the dimension). The paper (in abstract and intro) aims to speed up VI by fast convergence which is needed for big data/big model setting, which the quantization is a bottleneck for it, which makes the method loses its point. 2. Apart form the scalability, I wonder about the effectiveness in high dimensional space as well where everything is far away from each other. 3. The experiments are only with very simple small UCI datasets and very simple/small models (linear regression). I would be great to see with more ""real-life"" experiments. 4. There is also limited baselines. [a] is discussed in the paper but not compared. Only the basic BBVI is compared. It would be good to see at least baselines such as [a] and [b] in the experiments. 5. For algorithm 2, it would be insanely expensive if quantization needs be to computed every round. but it is explained with exponential family, it only need once. But if it limits to be exponential family, then the point of whole BBVI is lost. 5. Small things: line 2, minimize->maximize; can you explicitly discuss about the optimal quantization computational complexity. [a]Alexander Buchholz, Florian Wenzel, and Stephan Mandt. “Quasi-Monte Carlo Variational 238 Inference”. [b] Stochastic Learning on Imbalanced Data: Determinantal Point Processes for Mini-batch Diversification","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"3. Contrastive Response Tuning: as part of the core methodology, the paper should compare its effectiveness against existing methods, such as contrastive decoding [3][4]. Issues mentioned above should be addressed. Otherwise this work should aim for a more application-oriented venue. The notations issues.",lja4JMesmC,ICLR_2025,"The biggest issue of the paper is the lack of depth. While it ablates the impact of each of the algorithmic components, they authors spent little effort trying to understand why each of them work and to compare them against existing methods.
1. It’s not clear what makes EP successful.
- I strongly suspect the performance gain is mostly due to the fine-tuning of the connector module. The critical experiment of simply having both the connector and the LLM (LoRA params) trainable is missing.
- Additionally, an experiment comparing EP with prefixing tuning [1] will tell whether it’s necessary to condition the prefix (additional tokens to the LLM’s embedding space) on the image at all to get good performance. Essentially, I need to see experiments showing me EP > fine-tuning the original VLM’s connector + prefix tuning to be convinced it’s novel.
- I also don’t buy the claim that fine-tuning the Vision model in VLM will distort vision language alignment at all. If fine-tuning the Vision model is harmful, wouldn’t the trained LoRA weights be more harmful as well? A controlled experiment where the vision encode is also trained is needed. I am confident this will make EP perform even better.
- Finally, other works with the same core methodology should be discussed. For example, Graph Neural Prompting [2] builds a knowledge graph based on the prompt and multiple choice candidates and generates a conditional prefix to prompt the LLM. I think the idea is extremely similar to EP.
2. Regarding RDA: this is essentially a fancy way of saying knowledge distillation but no relevant papers are cited. Regarding implementation, the author mentions gradient detachment. If I understood it correctly, this just means the TSM, or the “teacher”, is not trained while the goal is to train the student. Shouldn’t this be the default setting anyway?
3. Contrastive Response Tuning: as part of the core methodology, the paper should compare its effectiveness against existing methods, such as contrastive decoding [3][4].
Issues mentioned above should be addressed. Otherwise this work should aim for a more application-oriented venue.
The notations issues.
- In equations (1), (2), (3), (5), (6), why is there a min() operator on the left hand side? The author seems to mix it up with the argmin notation. I think the author should remove the min() and avoid argmin() like notation since not all parameters are trained.
Minor grammar issues
- For example, Takeaway #1: TSM features can prompts (prompt) VLMs to generate desired responses. References:
[1] Li, Xiang Lisa, and Percy Liang. ""Prefix-Tuning: Optimizing Continuous Prompts for Generation."" Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.
[2] Tian, Yijun, et al. ""Graph neural prompting with large language models."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 17. 2024.
[3] Leng, Sicong, et al. ""Mitigating object hallucinations in large vision-language models through visual contrastive decoding."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
[4] Favero, Alessandro, et al. ""Multi-modal hallucination control by visual information grounding."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2. The effectiveness and problem of the algorithm are that it requires access to the entire training dataset. Have the authors considered how the algorithm should operate effectively when the training dataset is not fully perceptible? Overall: The trigger proposed in this paper is novel, but the related validation experiments are not comprehensive, and the time complexity of the computation and the efficiency of the algorithm are not clearly analyzed. In addition, I expect the authors to further elucidate the technical contribution rather than the form of the attack.",ICLR_2023_3291,ICLR_2023,"although this paper uses formal data symbolic description for the proposed method, there is still no framework diagram to help the method understanding, which makes the algorithm of the article slightly inferior in the narration and implementation process.
although this paper introduces various attack methods in detail, it does not show more attack methods in experimental comparison, such as ISSBA. as a novel attack method, the authors should give more experimental comparison and analysis of the attack. 3, the author mentioned in the paper the advantages of the algorithm can also be mentioned in the attack on high efficiency. But for this part, I don't seem to see more theoretical analysis (convergence) and related experimental proofs. I have reservations about this point.
What is the main difference between the authors and ISSBA in terms of the formulation of the method? I would like the authors further to explain the contribution in conjunction with the formulas.
Some Questions: 1.How is the computational efficiency of extracting the trigger? Unlike previous backdoor attack algorithms, the method needs to analyze and extract data from the entire training dataset. Does this result in exponential time growth as the dataset increases? 2. The effectiveness and problem of the algorithm are that it requires access to the entire training dataset. Have the authors considered how the algorithm should operate effectively when the training dataset is not fully perceptible?
Overall: The trigger proposed in this paper is novel, but the related validation experiments are not comprehensive, and the time complexity of the computation and the efficiency of the algorithm are not clearly analyzed. In addition, I expect the authors to further elucidate the technical contribution rather than the form of the attack.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"104 it is said that every kernel can be described by a feature space parameterized by a neural network, but this is trivially not true. For instance, for RBF kernels, the RKHS is famously infinite-dimensional, such that one would need an NN with infinite width to represent it. So at most, NNs can represent finite-dimensional RKHSs in practice. This limitation should be made more clear. l.",NIPS_2021_895,NIPS_2021,"The description of the method is somewhat unclear and it is hard to understand all the design choices.
Some natural baselines and important related work seem to be missing.
Major comments:
The lack of flexibility of standard GPs is not a new observation as has been approached in the past, possibly most famously by the deep GP [1]. These models have recently become a mainstream tool with easily usable frameworks [e.g., 2], so that it would seems like a natural baseline to compare against.
Generally, a lot of related work seems to be missed by this paper. For instance, meta-learning kernels for GPs for few-shot tasks has already been done by [3] and then later also by [4,5,6]. These should probably be mentioned and it should be discussed how the proposed method compares against them.
The paper proposes to use CNFs, but these require solving a complex-looking integral (e.g., Eq. 9). It should be discussed how tractable this integral is or how it is approximated in practice. Moreover, it seems like an easier choice would be standard NFs, so it should be discussed why CNFs are assumed to be better here. Possibly, one should also directly compare against a model with a standard NF as an ablation study.
In l. 257ff it is claimed that the proposed GP methods are less prone to memorization. How does this compare to the results in [4], where DKT seems to memorize as well? Could the regularization proposed in [4] be combined with the proposed model?
Minor comments:
In l. 104 it is said that every kernel can be described by a feature space parameterized by a neural network, but this is trivially not true. For instance, for RBF kernels, the RKHS is famously infinite-dimensional, such that one would need an NN with infinite width to represent it. So at most, NNs can represent finite-dimensional RKHSs in practice. This limitation should be made more clear.
l. 151 with GP -> with a GP
l. 152 use invertible mapping -> use an invertible mapping
l. 161 the ""marginal log-probability"" is more commonly called ""log marginal likelihood"" or ""log evidence""
Eq. (8): should it be z
instead of y ?
In the tables, it would be more helpful to also bolden the fonts of the entries where the error bars overlap with the best entry.
[1] Damianou & Lawrence 2012, https://arxiv.org/abs/1211.0358
[2] Dutordoir et al. 2021, https://arxiv.org/abs/2104.05674
[3] Fortuin et al. 2019, https://arxiv.org/abs/1901.08098
[4] Rothfuss et al. 2020, https://arxiv.org/abs/2002.05551
[5] Venkitaraman et al. 2020, https://arxiv.org/abs/2006.07212
[6] Titsias et al. 2020, https://arxiv.org/abs/2009.03228
The limitations of the method are hard to assess, mostly because the choice of CNFs over NFs or any other flexible distribution family is not well motivated and because (theoretical and empirical) comparisons to many relevant related methods are missing. This should be addressed.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"* The proposed method is not well-positioned in literature. It's worth pointing out that the key idea of representing the marginal score as the expectation of scores of distributions conditioned on inputs is actually quite well-known. It has been used, for example, to develop the original denoising score matching objective [1]. It is also used in the literature as ""score-interpolation"" [2]. I just named a few but I would recommend the authors to do a thorough literature review as I believe this property is used in many more works.",f5juXkyorf,ICLR_2024,"* The proposed method is not well-positioned in literature. It's worth pointing out that the key idea of representing the marginal score as the expectation of scores of distributions conditioned on inputs is actually quite well-known. It has been used, for example, to develop the original denoising score matching objective [1]. It is also used in the literature as ""score-interpolation"" [2]. I just named a few but I would recommend the authors to do a thorough literature review as I believe this property is used in many more works.
* The definition of the notation \hat{c}_k (baycenters) is missing.
* The exponential dependence of the sampling error on T is concerning. Although empirical evidence is provided to justify that this error bound is pessimistic, it also renders the bound unnecessary. Meanwhile, it's unclear if the conclusion that under sigma < 0.4, a large starting T is harmless will generalize to other datasets.
* The 3D point cloud experiment is interesting but I don't understand why the proposed method fills the gap there. Could the authors elaborate on this?
* The practical utility of the proposed closed-form models is also unclear. Given that the model can only sample from baycenters of data point tuples, is there a clear case where we would prefer such a model over a trained score model?
* This is minor, but the readability of section 3 can be greatly improved if not going to the notation convention used by rectified flow, as the proposed method can be described using the standard diffusion model formulation (where the time is reversed and stochastic transition is used).
[1] Vincent, Pascal. ""A connection between score matching and denoising autoencoders."" Neural computation 23.7 (2011): 1661-1674.
[2] Dieleman, Sander, et al. ""Continuous diffusion for categorical data."" arXiv preprint arXiv:2211.15089 (2022).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- "" The second is that the neuron state of ANNs is represented in binary format but that of SNNs is represented in float format, leading to the precision mismatch between ANNs and SNNs"", wrong value formats of SNNs and ANNs.",cdwXPlM4uN,ICLR_2024,"major concerns:
- This paper proposes a knowledge distillation framework from ANN to SNN. The only contribution is the loss function used here is different from previous work. This contribution is not sufficient for conferences like ICLR.
- It is hard to determine that the usage of this loss function is unique in the literature. In the domain of knowledge distillation in ANNs, there are numerous works studying different types of loss functions. Whether this loss function is original remains questioned.
- I find the design choice of simply averaging the time dimension in SNN featuremaps inappropriate. The method in this paper is not the real way to compute the similarity matrix. Instead, to calculate the real similarity matrix of SNNs, the authors should flatten the SNN activation to $[B, TCHW]$ and then compute the $B\times B$ covariance matrix. For detailed explanation, check [1].
- Apart from accuracy, there are not many insightful discoveries for readers to understand the specific mechanism of this loss function.
- The experiments are not validated on ImageNet, which largely weakens the empirical contribution of this paper.
minor concerns:
- "" The second is that the neuron state of ANNs is represented in binary format but that of SNNs is represented in float format, leading to the precision mismatch between ANNs and SNNs"", wrong value formats of SNNs and ANNs.
- The related work should be placed in the main text, rather than the appendix. There are many spaces left on the 9th page.
[1] Uncovering the Representation of Spiking Neural Networks Trained with Surrogate Gradient","{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['4', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV'], 'labels': ['1', '1']}",,
"1. How does your linear attention handle the autoregressive decoding? As of training, you can feed the network with a batch of inputs with long token dimensions. But when it comes to the generation phase, I am afraid that only limited tokens are used to generate the next token. Then do you still have benefits for inference?",OROKjdAfjs,ICLR_2024,"1. How does your linear attention handle the autoregressive decoding? As of training, you can feed the network with a batch of inputs with long token dimensions. But when it comes to the generation phase, I am afraid that only limited tokens are used to generate the next token. Then do you still have benefits for inference?
2. The paper reads like a combination of various tricks as a lot of techniques were discussed in the previous paper, like LRPE, Flash, and Flash Attention. Especially for the Lightning Attention vs. Flash Attention, I did not find any difference between these two. The gated mechanism was also introduced in Flash paper. These aspects leave us a question in terms of the technical novelty of this paper.
3. It looks like during training, you are still using the quadratic attention computational order as indicated in Equ. 10? I suppose it was to handle the masking part. But that loses the benefits of training with linear attention complexity.
4. In terms of evaluation, although in the abstract, the authors claim that the linearized LLM extends to 175B parameters, most experiments are conducted on 375M models. For the large parameter size settings, the author only reports the memory and latency cost savings. The accuracy information is missing, without which I feel hard to evaluate the linearized LLMs.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1) One of the key components is the matching metric, namely, the Pearson correlation coefficient (PCC). However, the assumption that PCC is a more relaxed constraint compared with KL divergence because of its invariance to scale and shift is not convincing enough. The constraint strength of a loss function is defined via its gradient distribution. For example, KL divergence and MSE loss have the same optimal solution while MSE loss is stricter than KL because of stricter punishment according to its gradient distribution. From this perspective, it is necessary to provide the gradient comparison between KL and PCC.",NIPS_2022_1590,NIPS_2022,"1) One of the key components is the matching metric, namely, the Pearson correlation coefficient (PCC). However, the assumption that PCC is a more relaxed constraint compared with KL divergence because of its invariance to scale and shift is not convincing enough. The constraint strength of a loss function is defined via its gradient distribution. For example, KL divergence and MSE loss have the same optimal solution while MSE loss is stricter than KL because of stricter punishment according to its gradient distribution. From this perspective, it is necessary to provide the gradient comparison between KL and PCC. 2) The experiments are not sufficient enough. 2-1) There are limited types of teacher architectures. 2-2) Most compared methods are proposed before 2019 (see Tab. 5). 2-3) The compared methods are not sufficient in Tab. 3 and 4. 2-4) The overall performance comparisons are only conducted on the small-scale dataset (i.e., CIFAR100). Large datasets (e.g., ImageNet) should also be evaluated. 2-5) The performance improvement compared with SOTAs is marginal (see Tab. 5). Some students only have a 0.06% gain compared with CRD. 3) There are some typos and some improper presentations. The texts of the figure are too small, especially the texts in Fig.2. Some typos, such as “on each classes” in the caption of Fig. 3, should be corrected.
The authors have discussed the limitations and societal impacts of their works. The proposed method cannot fully address the binary classification tasks.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- In Fig. 4, is it possible that GPI with noise added could reproduce the data similarly well or are there other measures to show that GPI cannot have as good fit with behavioural data (e.g. behavioural trajectories? time to goal?) - Finally this approach seems to be suitable for modelling pattern separation tasks, for which there is also behavioural data available - it would be nice to have some discussion on this.",NIPS_2019_810,NIPS_2019,"weakness of this paper in its current form is clarity, which hopefully can be improved, as although successor representation is an increasingly popular area in RL, it's also fairly complicated, hence it needs to be well explained (as e.g. is done in Gershman, J. Neurosci 2018). I also have a few more technical comments: - It's not exactly clear where is reward in section 2. Tabular case and rewards being linear in the state representation are mentioned; however, how exactly this is done should be explained more explicitly (or at least referred to where it's explained in the supplementary information - currently SI has information about parameters, algorithm and task settings details, but not methodological explanations) - It is mentioned that the mixture model is learned by gradient descent - it would be nice to see further discussion about how exactly this is done and why that is biologically realistic (as gradient descent is not something typically performed in the brain). - It would be nice to see not only summary statistics, but also typical trajectories performed by the model (and other candidate models) at different stages of learning - It is mentioned that epsilon = 0 works best for BSR, but in section 4.2 it's stated that for the puddle world epsilon = 0.2 was used for all models - why is that? Normally when comparing different models/algorithms, effort should be taken to find the best performing parameters (or more generally most suitable formalisations) for each model. - What exactly is the correlation coefficient in section 5.1 (0.90 or 0.95) between? - In Fig. 4, is it possible that GPI with noise added could reproduce the data similarly well or are there other measures to show that GPI cannot have as good fit with behavioural data (e.g. behavioural trajectories? time to goal?) - Finally this approach seems to be suitable for modelling pattern separation tasks, for which there is also behavioural data available - it would be nice to have some discussion on this. - There are a number of typos throughout the paper, which although don't obscure meaning should be corrected in the final version.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. If you have the resources, I would be very interested to see how the “small learning rate for attention parameters” benchmark (described above) would compare with the proposed approach.",ICLR_2021_318,ICLR_2021,"weakness, thought this is shared in RIM as well, and not that relevant to what is being evaluated/investigated in the paper.
Decision: This paper makes an algorithmic contribution to the systemic generalization literature, and many in the NeurIPS community who are interested in this literature would benefit from having this paper accepted to the conference. I'm in favour of acceptance.
Questions to authors: 1. Is there a specific reason why you reported results on knowledge transfer (i.e. section 4.3) only on a few select environments? 2. As mentioned in the “weak points” section, it would be nice if you could elaborate on 3. Is it possible that the caption of Figure 4 is misplaced? That figure is referenced in Section 4.1 (Improved Sample Efficiency), but the caption suggests it has something to do with better knowledge transfer. 4. If you have the resources, I would be very interested to see how the “small learning rate for attention parameters” benchmark (described above) would compare with the proposed approach. 5. In Section 4, 1st paragraph, you write “Do the ingredients of the proposed method lead to […] a better curriculum learning regime[…]”. Could you elaborate on what you mean by this?
[1] Beaulieu, Shawn, et al. ""Learning to continually learn."" arXiv preprint arXiv:2002.09571 (2020).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '1', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
* L106: This seems to carelessly resolve a debate that the paper was previously careful to leave open (L29). Why can't it be that the distribution has changed? Do any experiments disentangle changes in distribution from removal of information? Things I didn't understand:,NIPS_2019_873,NIPS_2019,"--- I think human studies in interpretability research are mis-represented at L59. * These approaches don't just ask people whether they think an approach is trustworthy. They also ask humans to do things with explanations and that seems to have a better connection to whether or not an explanation really explains model behavior. This follows the version of interpretability from [1]. This paper laments a lack of theoretical foundation to interpretability approaches (e.g., at L241,L275-277) and it acknowledges at multiple points that we don't know what ground truth for feature importance estimates should look like. Doesn't a person have to interpret an explanation at some point a model for it to be called interpretable? It seems like human studies may offer a way to philosophically ground interpretability, but this part of the paper mis-represents that research direction in contrast with its treatment of the rest of the related work. Minor evaluation problems: * Given that there are already multiple samples for all these experiments, what is the variance? How significant are the differences between rankings? I only see this as a minor problem because the differences on the right of figure 4 are quite large and those are what matter most. * I understand why more baseline estimators weren't included: it's expensive. It would be interesting to incorporate lower frequency visualizations like Grad-CAM. These can sometimes give significantly different performance (e.g., as in [3]). I expect it may have significant impact here because a more coarse explanation (e.g., 14x14 heatmap) may help avoid noise that comes from the non-smooth, high frequency, per-pixel importance of the explanations investigated. This seems further confirmed by the visualizations in figure 1 which remove whole objects as pointed out at L264. The smoothness of coarse visualization method seems like it should do something similar, so it would further confirm the hypothesis about whole objects implied at L264. * It would be nice to summarize ROAR into one number. It would probably have much more impact that way. One way to do so would be to look at the area under the test accuracy curves of figure 4. Doing so would obscure richer insights that ROAR would provide, but this is a tradeoff made by any aggregate statistic. Presentation: * L106: This seems to carelessly resolve a debate that the paper was previously careful to leave open (L29). Why can't it be that the distribution has changed? Do any experiments disentangle changes in distribution from removal of information? Things I didn't understand: * L29: I didn't get this till later in the paper. I think I do now, but my understanding might change again after the rebuttal. More detail here would be useful. * L85: Wouldn't L1 regularization be applied to the weights? Is that feature selection? What steps were actually taken in the experiments used in this paper? Did the ResNet50 used have L1 regularization? * L122: What makes this a bit unclear is that I don't know what is and what is not a random variable. Normally I would expect some of these (epsilon, eta) to be constants. Suggestions --- * It would be nice to know a bit more about how ROAR is implemented. Were the new datasets dynamically generated? Were they pre-processed and stored? * Say you start re-training from the same point. Train two identical networks with different random seeds. How similar are the importance estimates from these networks (e.g. using rank correlation similarity)? How similar are the sets of the final 10% of important pixels identified by ROAR across different random seeds? If they're not similar then whatever importance estimator isn't even consistent with itself in some sense. This could be thought of as an additional sanity check and it might help understand why the baseline estimators considered don't do well. [1]: Doshi-Velez, F., & Kim, B. (2017). A Roadmap for a Rigorous Science of Interpretability. ArXiv, abs/1702.08608. Final Evaluation --- Quality: The experiments were thorough and appropriately supported the conclusions. The paper really only evaluate importance estimators using ROAR. It doesn't really evaluate ROAR itself. I think this is appropriate given the strong motivation the paper has and the lack of concensus about what methods like ROAR should be doing. Clarity: The paper could be clearer in multiple places, but it ultimately gets the point across. Originality: The idea is similar to [30] as cited. ROAR uses a similar principle with re-training and this makes it new enough. Significance: This evaluation could become popular, inspire future metrics, and inspire better importance estimators. Overall, this makes a solid contribution. Post-rebuttal Update --- After reading the author feedback, reading the other reviews, and participating in a somewhat in-depth discussion I think we reached some agreement, though not everyone agreed about everything. In particular, I agree with R4's two recommendations for the final version. These changes would address burning questions about ROAR. I still think the existing contribution is a pretty good contribution to NeurIPS (7 is a good rating), though I'm not quite as enthusiastic as before. I disagree somewhat with R4's stated main concern, that ROAR does not distinguish enough between saliency methods. While it would be nice to have more analysis about the differences between these methods, ROAR is only one way to analyze these explanations and one analysis needn't be responsible for identifying differences between all the approaches it analyzes.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The proposed method uses AdamW with cosine lr for training, while comparing methods only use adam with fixed lr. Directly comparing with their numbers in paper is unfair. It would be better to reproduce their results using the same setting, since most of the recent methods have their code released.",NIPS_2021_2050,NIPS_2021,"1. Transformer has been adopted for lots of NLP and vision tasks, and it is no longer novel in this field. Although the authors made a modification on the transformer, i.e. cross-layer, it does not bring much insight in aspect of machine learning. Besides, in ablation study (table4 and 5), the self-cross attention brings limited improvement (<1%). I don’t think this should be considered as significant improvement. It seems that the main improvements over other methods come from using a naïve transformer instead of adding the proposed modification. 2. This work only focuses on a niche task, which is more suitable for CV conference like CVPR rather than machine learning conference. The audience should be more interested in techniques that can work for general tasks, like general image retrieval. 3. The proposed method uses AdamW with cosine lr for training, while comparing methods only use adam with fixed lr. Directly comparing with their numbers in paper is unfair. It would be better to reproduce their results using the same setting, since most of the recent methods have their code released.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"5)Though this is not necessary, I'm curious about the performance of the SOTA method (e.g. LST) combined with the adaptive metric.",ICLR_2021_2846,ICLR_2021,"Weakness: There are some concerns authors should further address: 1)The transductive inference stage is essentially an ensemble of a serial of models. Especially, the proposed data perturbation can be considered as a common data augmentation. What if such an ensemble is applied to the existing transductive methods? And whether the flipping already is adopted in the data augmentation before the inputs fed to the network? 2)During meta-training, only the selected single path is used in one transductive step, what about the performance of optimizing all paths simultaneously? Given during inference all paths are utilized. 3)What about the performance of MCT (pair + instance)? 4)Why the results of Table 6 is not aligned with Table 1 (MCT-pair)? Also what about the ablation studies of MCT without the adaptive metrics. 5)Though this is not necessary, I'm curious about the performance of the SOTA method (e.g. LST) combined with the adaptive metric.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what ""error""?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as ""sub-standard"".",NIPS_2016_283,NIPS_2016,"weakness of the paper are the empirical evaluation which lacks some rigor, and the presentation thereof: - First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what ""error""?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as ""sub-standard"". - The results comparing standard- vs. evolutional dropout on shallow models should be presented as a mean over many runs (at least 10), ideally with error-bars. The plotted curves are obviously from single runs, and might be subject to significant fluctuations. Also the models are small, so there really is no excuse for not providing statistics. - I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark: - In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor: *","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The performance gains are not very high, more most of the metrics the different between the baseline (w/o caption + w/o warmup) and best approach (with caption + warmup) is less than 1%.",p2P1Q4FpEB,EMNLP_2023,"1. The performance gains are not very high, more most of the metrics the different between the baseline (w/o caption + w/o warmup) and best approach (with caption + warmup) is less than 1%.
2. The paper lack error analysis and model output examples.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
3. How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?,NIPS_2017_486,NIPS_2017,"1.	The paper is motivated with using natural language feedback just as humans would provide while teaching a child. However, in addition to natural language feedback, the proposed feedback network also uses three additional pieces of information â which phrase is incorrect, what is the correct phrase, and what is the type of the mistake. Using these additional pieces is more than just natural language feedback. So I would like the authors to be clearer about this in introduction.
2.	The improvements of the proposed model over the RL without feedback model is not so high (row3 vs. row4 in table 6), in fact a bit worse for BLEU-1. So, I would like the authors to verify if the improvements are statistically significant.
3.	How much does the information about incorrect phrase / corrected phrase and the information about the type of the mistake help the feedback network? What is the performance without each of these two types of information and what is the performance with just the natural language feedback?
4.	In figure 1 caption, the paper mentions that in training the feedback network, along with the natural language feedback sentence, the phrase marked as incorrect by the annotator and the corrected phrase is also used. However, from equations 1-4, it is not clear where the information about incorrect phrase and corrected phrase is used. Also L175 and L176 are not clear. What do the authors mean by âas an exampleâ?
5.	L216-217: What is the rationale behind using cross entropy for first (P â floor(t/m)) phrases? How is the performance when using reinforcement algorithm for all phrases?
6.	L222: Why is the official test set of MSCOCO not used for reporting results?
7.	FBN results (table 5): can authors please throw light on why the performance degrades when using the additional information about missing/wrong/redundant?
8.	Table 6: can authors please clarify why the MLEC accuracy using ROUGE-L is so low? Is that a typo?
9.	Can authors discuss the failure cases of the proposed (RLF) network in order to guide future research?
10.	Other errors/typos:
a.	L190: complete -> completed
b.	L201, âWe use either â¦ feedback collectionâ: incorrect phrasing
c.	L218: multiply -> multiple
d.	L235: drop âbyâ
Post-rebuttal comments:
I agree that proper evaluation is critical. Hence I would like the authors to verify that the baseline results [33] are comparable and the proposed model is adding on top of that.
So, I would like to change my rating to marginally below acceptance threshold.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- Table 1 does not show standard deviations. Overall, this would be a far stronger submission, if the experiments were more extensive. This includes:",rtx8B94JMS,ICLR_2024,"From my point of view, there are 2 main weaknesses in this submission (for details, see below).
1. The method and the experiments are insufficiently described, and I have some questions in this regard. However, I am convinced, that the manuscript can be updated to be much more clear.
2. The empirical evaluation is of limited scope. Qualitatively, the method is evaluated on 2 toy problems (fOU & Hurst index); quantitatively on a single synthetic dataset (stochastic moving MNIST). For the latter, only two baseline models from 2018 and 2020 are compared to. In consequence, the usefulness of the method is not established. On the plus side, there are 3 ablations / further studies.
Minor weaknesses.
- The method appears to be inefficient, with a training time of 39 hours on an NVIDIA GeForce RTX 4090 for one model per model trained on stochastic moving MNIST.
- A detailed comparison with Tong et al. (2022) (who also learn approximations to fBM) is missing. So far, it is only stated that Tong et al. did not apply their model to video data and that it is completely different. A clear illustration of the conceptual differences and a comparison of the pros and cons of each approach would be appreciated (summary in main part + mathematical details in supplementary material).
# Summary
For me, this is a borderline submission. On the one hand, the proposed method is novel, significant and of theoretical interest. On the other hand, there are clarity issues, a weak empirical evaluation and no clear use case. Expecting the clarity issues to be resolved, I rate the submission as a marginally above the acceptance threshold, as the theoretical strengths outweigh the empirical flaws. ----
# Details on major weaknesses
## Point 1 (clarity).
**Regarding the method.** After reading the method and experiment section multiple times, I still have no idea, how to implement it. I am aware of the provided source code, nevertheless I found the paper to be insufficient in this regard.
What I got from section E and Figure 4 is that:
- First, there is an encoding step that returns $h$, a sequence of vectors/matrices over time. Somehow, these vectors are used to compute $\omega$. I have no idea how this $\omega$ is related to the optimal one from Prop. 5.
- $h$ is given to a temporal convolution layer that returns $g$ and this $g$ has as many 'frames' as the input and is used as input to the control function $u$.
- The control, drift and the diffusion function are implemented as neural networks.
- An SDE solution is numerically approximated with a Stratonovich–Milstein solver.
- $\omega$ is used in the decoding step, I do not understand why and how.
- Where do the approximation processes $Y$ enter. How are they parametrized, is $\gamma$ as in Prop 5? Are they integrated separately from $X$?
- The ELBO contains an expectation over sample paths. How many paths are sampled to estimate the mean?
- Fig. 6: Why do the samples from the prior always show a 4 and a 7? Does the prior depend on the observations?
**Regarding Moving MNIST.**
What precisely is the task / evaluation protocol in the experiment on the stoch. moving MNIST dataset? I did not see it specified, but from the overall description it appears that a sequence of 25 frames is given to the model and the task is to return the same 25 frames again (with the goal of learning a generative model).
## Point 2 (empirical evaluation)
- The method is not evaluated on real world data.
- Quantitatively, the method is only evaluated on one synthetic dataset (stoch. moving MNIST).
- While the method is motivated by ""*Unfortunately, for many practical scenarios, BM falls short of capturing the full complexity and richness of the observed real data, which often contains long-range dependencies, rare events, and intricate temporal structures that cannot be faithfully represented by a Markovian process""*, the moving MNIST dataset is not of this kind. It is not long range (only 25 frames) and there is no correlated noise.
- The method is only compared to 2 baselines (SVG, SLRVP) on moving MNIST.
- Table 1 does not show standard deviations.
Overall, this would be a far stronger submission, if the experiments were more extensive. This includes:
- Evaluation on more task and datasets, and specifically on datasets were this method is expected to shine, i.e., in the presence of correlated noise. The pendulum dataset of [Becker et al., Factorized inference in high-dimensional deep feature space, ICML 2019] would be one example.
- Comparison with more baselines. In particular, more recent / state-of-the-art methods that do not model a differential equation and the fBM model by Tong et al. ----
There is a typo in Proposition 1: ""Markov rocesses""","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"- Performance boost due to more parameters? In Tab 1,2,3, if we think carefully, LinearTop and NLTop adds additional parameters, while Unary performs much worse comparing to the numbers reported e.g. in [14], where they used a different and probably better neural network. This raises a question: if we use a better Unary baseline, is there still a performance boost?",NIPS_2018_583,NIPS_2018,"Weakness: - (4) or (5) are nonconvex saddle point problems, there is no convergence guarantee for Alg 1. Moreover, as a subroutine for (7), it is not clear how many iterations (the hyperparameter n) should be taken to make sure (7) is convergent. Previously in structured SVM, people noticed that approximate inference could make the learning diverges. - Performance boost due to more parameters? In Tab 1,2,3, if we think carefully, LinearTop and NLTop adds additional parameters, while Unary performs much worse comparing to the numbers reported e.g. in [14], where they used a different and probably better neural network. This raises a question: if we use a better Unary baseline, is there still a performance boost? - In Table 1, the accuracies are extremely poor: testing accuracy = 0.0? Something must be wrong in this experiment. - Scalability: since H(x,c) outputs the whole potential vector with length O(K^m), where m is the cardinality of the largest factor, which could be extremely long to be an input for T. - The performance of NLTop is way behind the Oracle (which uses GT as input for T). Does this indicate (3) is poorly solved or because of the learning itself? [*] N Komodakis Efficient training for pairwise or higher order CRFs via dual decompositio. CVPR 2011. [**] D Sontag et al. Learning efficiently with approximate inference via dual losses. ICML 2010.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"• The paper is a bit hard to follow, and several sections were needed more than one reading pass. I suggest improving the structure (introduction->method->experiments), and put more focus on the IEM in Fig 3, which is in my view the main figure in this paper. Also, to improve the visualization of the Fig 7, and Fig.",NIPS_2022_2041,NIPS_2022,"• The paper is a bit hard to follow, and several sections were needed more than one reading pass. I suggest improving the structure (introduction->method->experiments), and put more focus on the IEM in Fig 3, which is in my view the main figure in this paper. Also, to improve the visualization of the Fig 7, and Fig. 10.
It will be good to exemplify few failure cases of your model (e.g., on the FG or medical datasets). Perhaps other FG factors are needed? (e.g., good continuation?).","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Related Work: As the available space allows it, the paper would benefit from a more detailed discussion of related work, by not only describing the related works, but also discussing the differences to the presented work.",NIPS_2017_302,NIPS_2017,"1.	Related Work: As the available space allows it, the paper would benefit from a more detailed discussion of related work, by not only describing the related works, but also discussing the differences to the presented work.
2.	Qualitative results: To underline the success of the work, the paper should include some qualitative examples, comparing its generated sentences to the ones by related work.
3.	Experimental setup: For the Coco image cations, the paper does not rely on the official training/validation/test split used in the COCO captioning challenge.
3.1.	Why do the authors not use the entire training set?
3.2.	It would be important for the automatic evaluation to report results using the evaluation sever and report numbers on the blind test set (for the human eval it is fine to use the validation set). Conclusion:
I hope the authors will include the coco caption evaluation server results in the rebuttal and final version as well as several qualitative results.
Given the novelty of the approach and strong experiments without major flaws I recommend accepting the paper.
It would be interesting if the authors would comment on which problems and how their approach can be applied to non-sequence problems.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
- The experiments of the paper are done only on neural networks and image classification tasks. It would be interesting to see the performance of attack on other architecture and classification tasks.,NIPS_2020_1012,NIPS_2020,"-The scenarios that success of the attack is less than 50%, a simple ensemble method could be used to defend the attack. It seems that the success of attack in attacking the Google model is around 20% which could be circumvented by using multiple models. -The attack seems to be unstable when changing the architecture. For instance the attack on VGG does not succeed as much as the attack on other architectures. -On novelty of the paper: The ideas behind the attack seem to be simple and borrow ideas from the Metalearing literature. However, this is not necessary a bad thing as it shows simple ideas can be used to attack models. - The experiments of the paper are done only on neural networks and image classification tasks. It would be interesting to see the performance of attack on other architecture and classification tasks.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark:",NIPS_2016_283,NIPS_2016,"weakness of the paper are the empirical evaluation which lacks some rigor, and the presentation thereof: - First off: The plots are terrible. They are too small, the colors are hard to distinguish (e.g. pink vs red), the axis are poorly labeled (what ""error""?), and the labels are visually too similar (s-dropout(tr) vs e-dropout(tr)). These plots are the main presentation of the experimental results and should be much clearer. This is also the reason I rated the clarity as ""sub-standard"". - The results comparing standard- vs. evolutional dropout on shallow models should be presented as a mean over many runs (at least 10), ideally with error-bars. The plotted curves are obviously from single runs, and might be subject to significant fluctuations. Also the models are small, so there really is no excuse for not providing statistics. - I'd like to know the final used learning rates for the deep models (particularly CIFAR-10 and CIFAR-100). Because the authors only searched 4 different learning rates, and if the optimal learning rate for the baseline was outside the tested interval that could spoil the results. Another remark: - In my opinion the claim about evolutional dropout addresses the internal covariate shift is very limited: it can only increase the variance of some low-variance units. Batch Normalization on the other hand standardizes the variance and centers the activation. These limitations should be discussed explicitly. Minor: *","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- I am not convinced that transformer free of locality-bias is indeed the best option. In fact, due to limited speed of information propagation, the neighborhood agents should naturally have more impacts on each other, compared to far away nodes. I hope the authors to explain more why transformer’s no-locality won’t make a concern here.",NIPS_2021_1343,NIPS_2021,"Weakness - I am not convinced that transformer free of locality-bias is indeed the best option. In fact, due to limited speed of information propagation, the neighborhood agents should naturally have more impacts on each other, compared to far away nodes. I hope the authors to explain more why transformer’s no-locality won’t make a concern here. - Due to the above, I feel graph networks seem to capture this better than the too-free transformer, and their lack of global context/ the “over-squashing” might be mitigated by adding non-local blocks (e.g., check “Non-Local Graph Neural Networks” or several other works proposing “global attention” for GNNs). - The authors also claimed “traditional GNNs” cannot handle direction-feature coupling: that is not true. See a latest work “MagNet: A Neural Network for Directed Graphs” and I am sure there were more prior arts. Authors are asked to consider whether those directional GNNs can possibly suit their task well too. - Transform is introduced as a centralized agent. Its computational overhead can become formidable when the network gets larger. Authors shall discuss how they prepare to address the scalability bottleneck.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
2. It appears that the output from the algorithm depends on the order in which the data are processed. This should be clarified.,NIPS_2019_1145,NIPS_2019,"The paper has the following main weaknesses: 1. The paper starts with the objective of designing fast label aggregation algorithms for a streaming setting. But it doesnât spend any time motivating the applications in which such algorithms are needed. All the datasets used in the empirical analysis are static datasets. For the paper to be useful, the problem considered should be well motivated. 2. It appears that the output from the algorithm depends on the order in which the data are processed. This should be clarified. 3. The theoretical results are presented under the assumption that the predictions of FBI converge to the ground truth. Why should this assumption be true? It is not clear to me how this assumption is valid for finite R. This needs to clarified/justified. 3. The takeaways from the empirical analysis are not fully clear. It appears that the big advantage of the proposed methods is their speed. However, the experiments donât seem to be explicitly making this point (the running times are reported in the appendix; perhaps they should be moved to the main body). Plus, the paper is lacking the key EM benchmark. Also, perhaps the authors should use a different dataset in which speed is most important to showcase the benefits of this approach. Update after the author response: I read the author rebuttal. I suggest the authors to add the clarifications they detailed in the rebuttal to the final paper. Update after the author response: I read the author rebuttal. I suggest the authors to add the clarifications they detailed in the rebuttal to the final paper. Also, the motivating crowdsourcing application where speed is really important is not completely clear to me from the rebuttal. I suggest the authors clarify this properly in the final paper.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"1. While the mitigation strategies aim to reduce memorization, it's unclear what impact they might have on the overall performance of the model. Often, there's a trade-off between reducing a particular behavior and maintaining high performance. If these mitigation strategies significantly impair the model's utility, it might deter their adoption.",84n3UwkH7b,ICLR_2024,"1. While the mitigation strategies aim to reduce memorization, it's unclear what impact they might have on the overall performance of the model. Often, there's a trade-off between reducing a particular behavior and maintaining high performance. If these mitigation strategies significantly impair the model's utility, it might deter their adoption.
2. As stated in the paper, a weakness of the proposed method is the lack of interpretability in the detection strategy of memorized prompts. The current approach requires the model owners to select an empirical threshold based on a predetermined false positive rate, but the outcomes generated lack clear interpretability. This lack of clarity can make it difficult for model owners to fully understand and trust the detection process. The authors acknowledge that developing a method that produces interpretable p-values could significantly assist model owners by providing a confidence score quantifying the likelihood of memorization.
3. Advising users on modifying or omitting trigger tokens might be effective in theory, but in practice, it could be cumbersome. Users might need to understand what these tokens are, why they need to modify them, and how they affect the output. This could make the user experience less intuitive, especially for those unfamiliar with the inner workings of AI models.
4. The paper assumes that all prompts can be modified or that users will be willing to modify them. In real-world scenarios, some prompts might be non-negotiable, and changing them might not be an option.
5. While the paper suggests that the method is computationally efficient, implementing the strategies during the training and inference phases might still introduce computational or operational overheads for model owners.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '1', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"• Although it is provided that a 6-fold cross-validation is used for every dataset, the reason for cross-validation is not understood because other papers that this work compares to did not use the cross- validation in their papers. Therefore, it is not clear why 6-fold cross-validation is required for this problem.",ICLR_2023_4079,ICLR_2023,"• There are considerable similarities with another paper [1] (see below references). The work in this paper is not novel and there are no citation given to [1]. EM approach, regeneration approach is mentioned in [1]. • The experimental results are not convincing. The paper provides that joint learning on CIFAR- 100 dataset gives 39.97% accuracy when tested on class incremental learning. However, there seems to be more accurate results obtained with CIFAR-100 dataset on class incremental learning. For example, the paper [2] obtains 58.4% accuracy. In addition, the memory size is 10 times lower than this setup. The experiments do not contain the paper [2]. Other relevant papers [3, 4] whose accuracies are listed higher for this dataset are not compared and referenced either. • Although it is provided that a 6-fold cross-validation is used for every dataset, the reason for cross-validation is not understood because other papers that this work compares to did not use the cross- validation in their papers. Therefore, it is not clear why 6-fold cross-validation is required for this problem. • The notation for results is not clear. The paper claims the improvement for CIFAR-10 is 3%p but it is not clear what %p stands for. • Although there is a reference to [2] and other types of rehearsal based continual learning methods, the experiments do not contain any of the rehearsal methods. • The setup for the experiments is missing. The code is not provided. • The effect of memory size is ambiguous. An ablation study containing the effect of memory size should be added for justifying the memory size selection. • In Table-1, the experimental results for CelebA dataset are written in caption. However, there are not any experiments with CelebA dataset. [1] Overcoming Catastrophic Forgetting with Gaussian Mixture Replay (Pfülb and Geppert, 2021) [2] Gdumb: A simple approach that questions our progress in continual learning (Prabhu et al., 2020) [3] Supervised Contrastive Learning (Khosla et al., 2020) [4] Split-and-Bridge: Adaptable Class Incremental Learning within a Single Neural Network (Kim and Choi, 2021)","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it’s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.",ARR_2022_51_review,ARR_2022,"1. The choice of the word-alignment baseline seems odd. The abstract claims that “Word alignment has proven to benefit many-to-many neural machine translation (NMT).” which is supported by (Lin et al., 2020). However, the method proposed by Lin et al was used as baseline. Instead, the paper compared to an older baseline proposed by (Garg et al., 2019). Besides, this baseline by Garg et al (+align) seems to contradict the claim in the abstract since it always performs worse than the baseline without word-alignment (Table 2). If for some practical reason, the baseline of (Lin et al., 2020) can’t be used, it needs to be explained clearly.
2. In Table 2, the proposed approaches only outperform the baselines in 1 setup (out of 3). In addition, there is no consistent trend in the result (i.e. it’s unclear which proposed method (+w2w) or (+FA) is better). Thus, the results presented are insufficient to prove the benefits of the proposed methods. To better justify the claims in this paper, additional experiments or more in-depth analysis seem necessary.
3. If the claim that better word-alignment improves many-to-many translation is true, why does the proposed method have no impact on the MLSC setup (Table 3)? Section 4 touches on this point but provides no explanation.
1. Please provide more details for the sentence retrieval setup (how sentences are retrieved, from what corpus, is it the same/different to the setup in (Artetxe and Schwenk, 2019) ? ).
From the paper, “We found that for en-kk, numbers of extracted word pairs per sentence by word2word and FastAlign are 1.0 and 2.2, respectively. In contrast, the numbers are 4.2 and 20.7 for improved language pairs”. Is this because word2word and FastAlign fail for some language pairs or is this because there are few alignments between these language pairs? Would a better aligner improve result further?
2. For Table 3, are the non-highlighted cells not significant or not significantly better? If it’s the latter, please also highlight cells where the proposed approaches are significantly worse. For example, from Kk to En, +FA is significantly better than mBART (14.4 vs 14.1, difference of 0.3) and thus the cell is highlighted. However, from En to Kk, the difference between +FA and mBART is -0.5 (1.3 vs 1.8) but this cell is not highlighted.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"1. This paper presents a highly effective engineering method for ReC. However, it should be noted that the proposed framework incorporates some combinatorial and heuristic aspects. In particular, the Non-Ambiguous Query Generation procedure relies on a sophisticated filtering template. It would be helpful if the author could clarify the impact of these heuristic components.",FgEM735i5M,EMNLP_2023,"1. This paper presents a highly effective engineering method for ReC. However, it should be noted that the proposed framework incorporates some combinatorial and heuristic aspects. In particular, the Non-Ambiguous Query Generation procedure relies on a sophisticated filtering template. It would be helpful if the author could clarify the impact of these heuristic components.
2. Since the linguistic expression rewriting utilizes the powerful GPT-3.5 language model, it would be interesting to understand the extent of randomness and deviation that may arise from the influence of GPT-3.5. Is there any studies or analyses on this aspect?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- I doubt the proposed method can be trained without using any camera information (Line 223, the so called ""knowledge of CAD model correspondences""). Without knowing the viewpoint, how is it possible to perform ray marching? How do you know where the ray comes from?",NIPS_2020_960,NIPS_2020,"- The writing of this paper is very misleading. First of all, it claims that it can be trained only using a single viewpoint of the object. In fact, all previous diffrentiable rendering techniques can be trained using a single view of object at training time. However, the reason why multi-view images are used for training in prior works is that single-view images usually lead to ambiguity in the depth direction. The proposed method also suffers from this problem -- it cannot resolve the ambiguity of depth using a single image either. The distrance-transformed silhouette can only provide information on the xy plane - the shape perpendicular to the viewing direction. - I doubt the proposed method can be trained without using any camera information (Line 223, the so called ""knowledge of CAD model correspondences""). Without knowing the viewpoint, how is it possible to perform ray marching? How do you know where the ray comes from? - The experiments are not comprehensive and convincing. 1) The comparisons do not seem fair. The performance of DVR is far worse than that in the original DVR paper. Is DVR trained and tested on the same data? What is the code used for evaluation? Is it from the original authors or reimplementation? 2) Though it could be interesting to see how SoftRas performs, it is not very fair to compare SoftRas here as it use a different 3D representation -- mesh. It is well known that mesh representation cannot model arbitrary topology. Thus it is not surprising to see it is outperformed. Since this paper works on implicit surface, it would be more interesting to compare with more state-of-the-art differentiable renderers for implicit surface, i.e. [26],[27],[14], or at least the baseline approach [38]. However, no direct comparisons with these approaches are provided, making it difficult to verify the effectivenss of the proposed approach.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '2', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"4. The paper would benefit from a more detailed comparison with related work, in particular making a detailed comparison to the time complexity and competitiveness of prior art. Minor:",NIPS_2018_813,NIPS_2018,"(the simulations seem to address whether or not an improvement is actually seen in practice), the paper would benefit from a discussion of the fact that the targeted improvement is in the (relatively) small n regime. 4. The paper would benefit from a more detailed comparison with related work, in particular making a detailed comparison to the time complexity and competitiveness of prior art. Minor: 1. The proofs repeatedly refer to the Cauchy inequality, but it might be better given audience familiarity to refer to it as the Cauchy-Schwarz inequality. Post-rebuttal: I have read the authors' response and am satisfied with it. I maintain my vote for acceptance.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"• It seems that ODA, one of the methods of solving the MOIP problem, has learned the policy to imitate the problem-solving method, but it did not clearly suggest how the presented method improved the performance and computation speed of the solution rather than just using ODA.",NIPS_2022_532,NIPS_2022,"• It seems that ODA, one of the methods of solving the MOIP problem, has learned the policy to imitate the problem-solving method, but it did not clearly suggest how the presented method improved the performance and computation speed of the solution rather than just using ODA.
• In order to apply imitation learning, it is necessary to obtain labeled data by optimally solving various problems. There are no experiments on whether there are any difficulties in obtaining the corresponding data, and how the performance changes depending on the size of the labeled data.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2. Some figures are not self-explanatory. For instance, in Figure 4, the line of No adapt or Finetune are covered by other lines, without additional explanation.",ICLR_2023_91,ICLR_2023,"1. Some confusions. In Parameter Transformation part, you state that “The number of adaptation parameters is given by k (2 d2 + d + 2). This is typically much smaller than the number of MDN parameters (weights and biases from all layers)”. In previous part you state that “The MDN output with all the mixture parameters has dimension p = k (d(d + 1)/2 + d + 1).” Why the adaptation parameters is much smaller than the number of MDN parameters? 2. Some figures are not self-explanatory. For instance, in Figure 4, the line of No adapt or Finetune are covered by other lines, without additional explanation. 3. More experiments. How the unsupervised domain adaptation performs based on the baseline model and how it compares with the proposed approach?","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '4']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', 'X', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"- The sampling performed to obtain different initializations x_0 seems important for the convergence to optimum. This is not experimentally evaluated carefully on the proposed benchmarks, except for Tab. 1 in supplementary where it is compared to sampling from uniform distribution.",NIPS_2020_3,NIPS_2020,"- Unlike Tandem Model [4,5] and cVAE based methods the proposed method uses gradient updates and therefore is slow. The authors acknowledge this in the manuscript and demonstrate study the method as a function of inference budget. - The sampling performed to obtain different initializations x_0 seems important for the convergence to optimum. This is not experimentally evaluated carefully on the proposed benchmarks, except for Tab. 1 in supplementary where it is compared to sampling from uniform distribution.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '2']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '0']}",,
"2. What’s the logic between the proposed method and [9] and [16]? Why the authors compare the proposed method with [9] first, then [16]? Why the authors only compare the computational cost with [9], but [16]? Is the computational cost a big contribution to this paper? Is that a big issue in a practical scenario? That part is weird to me and there is no further discussion about it in the rest of this paper.",NIPS_2020_541,NIPS_2020,"Some concerns: 1. There is too much-overleaped information in Table 1, Table 2, and Figure 1. Figure 1 includes all information presented in Tables 1 and 2. 2. What’s the logic between the proposed method and [9] and [16]? Why the authors compare the proposed method with [9] first, then [16]? Why the authors only compare the computational cost with [9], but [16]? Is the computational cost a big contribution to this paper? Is that a big issue in a practical scenario? That part is weird to me and there is no further discussion about it in the rest of this paper. 3. Why the proposed column smoothing method produces better result compare with block smoothing method? 4. The accuracy drop for the Imagenet dataset is a concern, which makes the proposed method in-practical.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.,ARR_2022_311_review,ARR_2022,"__1. Lack of significance test:__ I'm glad to see the paper reports the standard deviation of accuracy among 15 runs. However, the standard deviation of the proposed method overlaps significantly with that of the best baseline, which raises my concern about whether the improvement is statistically significant. It would be better to conduct a significance test on the experimental results.
__2. Anomalous result:__ According to Table 3, the performance of BARTword and BARTspan on SST-2 degrades a lot after incorporating text smoothing, why?
__3. Lack of experimental results on more datasets:__ I suggest conducting experiments on more datasets to make a more comprehensive evaluation of the proposed method. The experiments on the full dataset instead of that in the low-resource regime are also encouraged.
__4. Lack of some technical details:__ __4.1__. Is the smoothed representation all calculated based on pre-trained BERT, even when the text smoothing method is adapted to GPT2 and BART models (e.g., GPT2context, BARTword, etc.)?
__4.2__. What is the value of the hyperparameter lambda of the mixup in the experiments? Will the setting of this hyperparameter have a great impact on the result?
__4.3__. Generally, traditional data augmentation methods have the setting of __augmentation magnification__, i.e., the number of augmented samples generated for each original sample. Is there such a setting in the proposed method? If so, how many augmented samples are synthesized for each original sample?
1. Some items in Table 2 and Table 3 have Spaces between accuracy and standard deviation, and some items don't, which affects beauty.
2. The number of BARTword + text smoothing and BARTspan + text smoothing on SST-2 in Table 3 should NOT be in bold as they cause degeneration of the performance.
3. I suggest Listening 1 to reflect the process of sending interpolated_repr into the task model to get the final representation","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '5', '3']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 'boda'], 'labels': ['1', '1']}",,
"2. Additionally, it was not clear how the generic argument task and the random argument task proved what the authors claimed. All in all, the whole dataset transformation and the ensuing experimental setup felt very cumbersome, and not very clear.",rJhk7Fpnvh,EMNLP_2023,"1.	It was quite unclear how the experiments performed in the work to corroborate the authors’ theory did that. In the random premise task – how could the authors ensure that the random predicate indeed resulted in NO-ENTAIL? I understand that such random sampling has a very small probability of resulting in something that is not NO-ENTAIL, but given that many predicates have synonyms, and other predicates whom they entail, and hence by proxy the current hypothesis might also entail, it feels like it is crucial to ensure that indeed NO-ENTAIL was the case for all the instances (as this is not the train set, but rather the evaluation set).
2.	Additionally, it was not clear how the generic argument task and the random argument task proved what the authors claimed. All in all, the whole dataset transformation and the ensuing experimental setup felt very cumbersome, and not very clear.","{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '2', '1']}",,,6,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['hgdl9t28', 's46d5sbV', 'boda'], 'labels': ['1', '1', '1']}",,
"2) How does the PL condition you use compare with the PL conditions proposed in âGlobal Convergence of Arbitrary-Block Gradient Methods for Generalized Polyak-Åojasiewicz Functionsâ, arXiv:1709.03014 ?",NIPS_2018_514,NIPS_2018,"and Questions: 1) Can the results be improved so that ProxSVRG+ becomes better than SCSG for all minibatch sizes b? 2) How does the PL condition you use compare with the PL conditions proposed in âGlobal Convergence of Arbitrary-Block Gradient Methods for Generalized Polyak-Åojasiewicz Functionsâ, arXiv:1709.03014 ? 3) Line 114: Is the assumption really necessary? Why? Or just sufficient? 4) I think the paper would benefit if some more experiments were included in the supplementary, on some other problems and with other datasets. Otherwise the robustness/generalization of the observations drawn from the included experiments is unclear. Small issues: 1) Lines 15-16: The sentence starting with âBesidesâ is not grammatically correct. 2) Line 68: What is a âsuper constantâ? 3) Line 75: âmatchesâ -> âmatchâ 4) Page 3: âorcaleâ â âoracleâ (twice) 5) Caption of Table 1: âare definedâ -> âare givenâ 6) Eq (3): use full stop 7) Line 122: The sentence is not grammatically correct. 8) Line 200: ârestatedâ -> ârestartedâ 9) Paper [21]: accents missing for one authorâs name ***** I read the rebuttal and the other reviews, and am keeping my score.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. While the idea of jointly discovering, hallucinating, and adapting is interesting, there is a complete lack of discussing the impact of adding additional parameters and additional computational effort due to the multi-stage training and the multiple discriminators. The authors should provide this analysis for a fair comparison with the baseline [31, 33, *].",NIPS_2020_911,NIPS_2020,"1. While the idea of jointly discovering, hallucinating, and adapting is interesting, there is a complete lack of discussing the impact of adding additional parameters and additional computational effort due to the multi-stage training and the multiple discriminators. The authors should provide this analysis for a fair comparison with the baseline [31, 33, *]. 2. Splitting the target data into easy and hard is already explored in the context of UDA. 3. Discovering the latent domain from the target domain is already proposed in [24]. 4. The problem of Open Compound Domain Adaptation is already presented in [**]. 5. Hallucinating the latent target domains is achieved through an image translation network adapted from [5]. 6. Style consistency loss to achieve diverse target styles has been used in previous works. 7. While the existing UDA methods [31,33] only use one discriminator, it is unclear to me why authors have applied multiple discriminators. 8. The details of the discriminator have not been discussed. 9. I was wondering why including the hallucination part reduces the performance in Table 1(b). It seems like the Discover module with [31] performs better than (Discover + Hallucinate + [31]). Also, the complex adapting stage where the authors used multiple discriminators mostly brings performance improvement. More importantly, did authors try to run the baseline models [17, 25, 31, 33, 39] with a similar longer training scheme? Otherwise, it is unfair to compare with the baselines. 10. Since the authors mentioned that splitting the training process helps to achieve better performance, It could be interesting to see the results of single-stage and multi-stage training. 11. It is not well explained why the adaptation performance drops when K > 3. Also, the procedure of finding the best K seems ad hoc and time-consuming. 12. I am just curious to see how the proposed method performs in a real domain adaptation scenario (GTA5->CityScapes). [*] Fei Pan, Inkyu Shin, François Rameau, Seokju Lee, In So Kweon. Unsupervised Intra-domain Adaptation for Semantic Segmentation through Self-Supervision. In CVPR 2020. [**] Liu, Ziwei and Miao, Zhongqi and Pan, Xingang and Zhan, Xiaohang and Lin, Dahua and Yu, Stella X. and Gong, Boqing. Open Compound Domain Adaptation. In CVPR 2020.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The analysis of the correlation between dataset size and the Frobenius norm and the singular values is underwhelming. It is not clear if this trend holds across different model architectures, and if so, no theoretical evidence is advanced for this correlation.",2RQokbn4B5,ICLR_2025,"1. The analysis of the correlation between dataset size and the Frobenius norm and the singular values is underwhelming. It is not clear if this trend holds across different model architectures, and if so, no theoretical evidence is advanced for this correlation.
2. The proposed method for dataset size recovery is way too simple to offer any insights.
3. The authors only study dataset size recovery for foundation models fine-tuned with a few samples. However, this problem is very general and should be explored in a broader framework.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- Section 4.2 experiments with AutoAugment as a stronger augmentation strategy. One possible trap is that AutoAugment’s policy is obtained by supervise training on ImageNet. Information leaking is likely. Questions - In L114 the authors concluded that for linear classification the pretraining dataset should match the target dataset in terms of being object or-scene centric. If this is true, is it a setback for SSL algorithms that strive to learn more generic representations? Then it goes back again to whether by combining two datasets SSL model can learn better representations.",NIPS_2021_2257,NIPS_2021,"- Missing supervised baselines. Since most experiments are done on datasets of scale ~100k images, it is reasonable to assume that full annotation is available for a dataset at this scale in practice. Even if it isn’t, it’s an informative baseline to show where these self-supervised methods are at comparing to a fully supervised pre-trained network. - The discussion in section 3 is interesting and insightful. The authors compared training datasets such as object-centric versus scene-centric ones, and observed different properties that the model exhibited. One natural question is then what would happen if a model is trained on \emph{combined} datasets. Can the SSL model make use of different kinds of data? - The authors compared two-crop and multi-crop augmentation in section 4, and observed that multi-crop augmentation yielded better performance. One important missing factor is the (possible) computation overhead of multi-crop strategies. My estimation is that it would increase the computation complexity (i.e., slowing the speed) of training. Therefore, one could argue that if we could train the two-crop baseline for a longer period of time it would yield better performance as well. To make the comparison fair, the computation overhead must be discussed. It can also be seen from Figure 7, for the KNN-MoCo, that the extra positive samples are fed into the network \emph{that takes the back-propagated gradients}. It will drastically increase training complexity as the network not only performs forward passing, but also the backward passing as well. - Section 4.2 experiments with AutoAugment as a stronger augmentation strategy. One possible trap is that AutoAugment’s policy is obtained by supervise training on ImageNet. Information leaking is likely.
Questions - In L114 the authors concluded that for linear classification the pretraining dataset should match the target dataset in terms of being object or-scene centric. If this is true, is it a setback for SSL algorithms that strive to learn more generic representations? Then it goes back again to whether by combining two datasets SSL model can learn better representations. - In L157 the authors discussed that for transfer learning potentially only low- and mid-level visual features are useful. My intuition is that low- and mid-level features are rather easy to learn. Then how does it explain the model’s transferability increasing when we scale up pre-training datasets? Or the recent success of CLIPs? Is it possible that \emph{only} MoCo learns low- and mid-level features?
Minor things that don’t play any role in my ratings. - “i.e.” -> “i.e.,”, “e.g.” -> “e.g.,” - In Eq.1, it’s better to write L_{contrastive}(x) = instead of L_{contrastive}. Also, should the equation be normalized by the number of positives? - L241 setup paragraph is overly complicated for an easy-to-explain procedure. L245/246, the use of x+ and x is very confusing. - It’s better to explain that “nearest neighbor mining” in the intro is to mine nearest neighbor in a moving embedding space in the same dataset.
Overall, I like the objective of the paper a lot and I think the paper is trying to answer some important questions in SSL. But I have some reservation to confidently recommend acceptance due to the concerns as written in the “weakness” section, because this is an analysis paper and analysis needs to be rigorous. I’ll be more than happy to increase the score if those concerns are properly addressed in the feedback.
The authors didn't discuss the limitations of the study. I find no potential negative societal impact.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Tabular data (seeing each attribute dimension as a modality) is another popular form of multi-modal data. It would interesting, although not necessary, to see how this model works for tabular data.",NIPS_2019_1408,NIPS_2019,"Weakness: 1. Although the four criteria (proposed by the author of this paper) for multi-modal generative models seem reasonable, they are not intrinsic generic criteria. Therefore, the argument that previous works fail for certain criteria is not strong. 2. Tabular data (seeing each attribute dimension as a modality) is another popular form of multi-modal data. It would interesting, although not necessary, to see how this model works for tabular data.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2.For the language-agnostic characters of entity representations, the paper has weak analysis on the alignment of entity representations. The authors could add more analysis about the multilingual alignment of entity representations and it would be better to have visualizations or case studies for different types of languages such as language family. We are also interested in whether entities from low-resourced languages are well aligned with the high-resourced ones.",ARR_2022_64_review,ARR_2022,"1. The idea is a bit incremental and simply the extension of previous monolingual LUKE.
2.For the language-agnostic characters of entity representations, the paper has weak analysis on the alignment of entity representations.
The authors could add more analysis about the multilingual alignment of entity representations and it would be better to have visualizations or case studies for different types of languages such as language family. We are also interested in whether entities from low-resourced languages are well aligned with the high-resourced ones.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- More details on using attention would be useful, perhaps as an extra appendix.",NIPS_2018_831,NIPS_2018,"- I wasn't fully clear about the repeat/remember example in Section 4. I understand that the unrolled reverse computation of a TBPTT of an exactly reversible model for the repeat task is equivalent to the forward pass of a regular model for the remember task, but aren't they still quite different in major ways? First, are they really equivalent in terms of their gradient updates? In the end, they draw two different computation graphs? Second, at *test time*, the former is not auto-regressive (i.e., it uses the given input sequence) whereas the latter is. Maybe I'm missing something simple, but a more careful explanation of the example would be helpful. Also a minor issue: why are an NF-RevGRU and an LSTM compared in Appendix A? Shouldn't an NF-RevLSTM be used for a fairer comparison? - I'm not familiar with the algorithm of Maclaurin et al., so it's difficult to get much out of the description of Algorithm 1 other than its mechanics. A review/justification of the algorithm may make the paper more self-contained. - As the paper acknowledges, the reversible version has a much higher computational cost during training (2-3 times slower). Given how cheap memory is, it remains to be seen how actually practical this approach is. OTHER COMMENTS - It'd still be useful to include the perplexity/BLEU scores of a NF-Rev{GRU, LSTM} just to verify that the gating mechanism is indeed necessary. - More details on using attention would be useful, perhaps as an extra appendix.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- The references list contains duplicates and the publication venues and/or the publication years of many of the papers are missing.,NIPS_2017_496,NIPS_2017,"weakness of the paper is the experimental evaluation. The only experimental results are reported on synthetic datasets, i.e. MNIST and MNIST multi-set. As the objects are quite salient on the black background, it is difficult to judge the advantage of the proposed attention mechanism. In natural images, as discussed in the limitations, detecting saliency with high confidence is an issue. However, this work having been motivated partially as a framework for an improvement to existing saliency models should have been evaluated in more realistic scenarios. Furthermore, the proposed attention model is not compared with any existing attention models. Also, a comparison with human gaze based as attention (as discussed in the introduction) would be interesting. A candidate dataset is CUB annotated with human gaze data in Karessli et.al, Gaze Embeddings for Zero-Shot Image Classification, CVPR17 (another un-cited related work) which showed that human gaze based visual attention is class-specific.
Minor comment:
- The references list contains duplicates and the publication venues and/or the publication years of many of the papers are missing.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
1: The theoretical analysis in Theorem 1 is unclear and weak. It is unclear that what the error bound in Theorem 1 means. The authors need to analyze and compare the theoretical results to other comparable methods.,ICLR_2022_2403,ICLR_2022,"1: The theoretical analysis in Theorem 1 is unclear and weak. It is unclear that what the error bound in Theorem 1 means. The authors need to analyze and compare the theoretical results to other comparable methods.
2: The title is ambiguous and may lead to inappropriate reviewers.
3: I see no code attached to this submission, which makes me a bit concerned about reproducibility.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2) why the explicit methods perform better than implicit methods on the locomotion tasks. The pseudo-code of the proposed method is missing. [1] Søren Asmussen and Peter W Glynn. Stochastic simulation: algorithms and analysis, volume 57. Springer, 2007. [2] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Proceedings of the 5th Conference on Robot Learning. PMLR, 2022.",ICLR_2023_3381,ICLR_2023,"The authors claim that they bridge an important gap between IBC [2] and RvS by modeling the dependencies between the state, action, and return with an implicit model on Page 6. However, noticing that IBC proposes to use the implicit model to model the dependencies between the state and action, I think the contribution of this paper is to introduce the return from RvS to the implicit model. Thus, the proposed method looks like a combination of IBC and RvS.
The authors conduct experiments in Section 5.1 to show the advantages of the implicit model. However, such advantages are similar to IBC, which could hurt the novelty of this paper. The authors may want to highlight the novelty of the proposed method against IBC.
The discussions of the empirical results in Sections 5.1 and 5.2.2 are missing. The authors may want to explain: 1) why the RvS method fails to reach either goal and converges to the purple point in Figure 4(b); 2) why the explicit methods perform better than implicit methods on the locomotion tasks.
The pseudo-code of the proposed method is missing.
[1] Søren Asmussen and Peter W Glynn. Stochastic simulation: algorithms and analysis, volume 57. Springer, 2007.
[2] P. Florence, C. Lynch, A. Zeng, O. A. Ramirez, A. Wahid, L. Downs, A. Wong, J. Lee, I. Mordatch, and J. Tompson. Implicit behavioral cloning. In Proceedings of the 5th Conference on Robot Learning. PMLR, 2022.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2) the use of low-resource language pairs further finetune the multilingual model and use the method like R3F to maintain the generalization ability of the model. In some low-resource language translations from 1.2->2.0, although the improvement of 0.8 can be claimed, it is insignificant in a practical sense. Missing References: Aghajanyan, Armen, et al. ""Better Fine-Tuning by Reducing Representational Collapse."" International Conference on Learning Representations. 2020.",NIPS_2021_1527,NIPS_2021,"Weakness:
The unbalanced data scenario has not been properly explored by experiments. Under what circumstances can it be counted as an unbalanced data scenario, and what is the data ratio? Therefore, the experiments should not pay more attention to one given setting like TED, WMT, etc., but should construct unbalanced scenarios of different ratios by sampling data in one setting like WMT to verify this important issue.
There is a lack of a reasonable ablation study on the upsampling parameter T, so we cannot confirm whether the oversampling overfit phenomenon will occur, and to what extent will the upsampling reach.
Some baselines are missing in the experimental comparison, such as 1) giving different weights to the loss of unbalanced translation pairs so that in the later stages of training, there will be no situation where rich-resource pairs dominate the training loss; 2) the use of low-resource language pairs further finetune the multilingual model and use the method like R3F to maintain the generalization ability of the model.
In some low-resource language translations from 1.2->2.0, although the improvement of 0.8 can be claimed, it is insignificant in a practical sense.
Missing References:
Aghajanyan, Armen, et al. ""Better Fine-Tuning by Reducing Representational Collapse."" International Conference on Learning Representations. 2020.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The paper mentions that their model can work well for a variety of image noise, but they show results only on images corrupted using Gaussian noise. Is there any particular reason for the same?",NIPS_2016_314,NIPS_2016,"I found in the paper includes: 1. The paper mentions that their model can work well for a variety of image noise, but they show results only on images corrupted using Gaussian noise. Is there any particular reason for the same? 2. I can't find details on how they make the network fit the residual instead of directly learning the input - output mapping. - Is it through the use of skip connections? If so, this argument would make more sense if the skip connections exist after every layer (not every 2 layers) 3. It would have been nice if there was an ablation study on what plays the most important factor on the improvement in performance. Whether it is the number of layers or the skip connections, and how does the performance vary when the skip connections are used for every layer. 4. The paper says that almost all existing methods estimate the corruption level at first. There is a high possibility that the same is happening in the initial layers of their Residual net. If so, the only advantage is that theirs is end to end. 5. The authors mention in the Related works section that the use of regularization helps the problem of image- restoration, but they donât use any type of regularization in their proposed model. It would be great if the authors can address these points (mainly 1, 2 and 3) in the rebuttal.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
1. The authors claim that the existing PU learning methods will suffer a gradual decline in performance as the dimensionality of the data increases. It would be better if the authors can visualize this effect. This is very important as this is the research motivation of this paper.,VwyTrglgmW,ICLR_2024,"1.	The authors claim that the existing PU learning methods will suffer a gradual decline in performance as the dimensionality of the data increases. It would be better if the authors can visualize this effect. This is very important as this is the research motivation of this paper.
2.	Since the authors claim that the high dimensionality is harmful for the PU methods, have the authors tried to firstly implement dimension reduction via some existing approaches and then deploy traditional PU classifiers?
3.	In problem setup, the authors should clarify whether their method belongs to case-control PU learning or censoring PU learning, as their generation ways of P data and U data are quite different.
4.	The proposed algorithm contains Kmeans operation. Note that if there are many examples with high dimension, Kmeans will be very inefficient.
5.	The authors should compare their algorithm with SOTA methods and typical methods on these benchmark datasets.
6.	The figures in this paper are in low quality. Besides, the writing of this paper is also far from perfect.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials.",NIPS_2019_1397,NIPS_2019,"weakness of the manuscript. Clarity: The manuscript is well-written in general. It does a good job in explaining many results and subtle points (e.g., blessing of dimensionality). On the other hand, I think there is still room for improvement in the structure of the manuscript. The methodology seems fully explainable by Theorem 2.2. Therefore, Theorem 2.1 doesn't seem necessary in the main paper, and can be move to the supplement as a lemma to save space. Furthermore, a few important results could be moved from the supplement back to the main paper (e.g., Algorithm 1 and Table 2). Originality: The main results seem innovative to me in general. Although optimizing information-theoretic objective functions is not new, I find the new objective function adequately novel, especially in the treatment of the Q_i's in relation to TC(Z|X_i). Relevant lines of research are also summarized well in the related work section. Significance: The proposed methodology has many favorable features, including low computational complexity, good performance under (near) modular latent factor models, and blessing of dimensionality. I believe these will make the new method very attractive to the community. Moreover, the formulation of the objective function itself would also be of great theoretical interest. Overall, I think the manuscript would make a fairly significant contribution. Itemized comments: 1. The number of latent factors m is assumed to be constant throughout the paper. I wonder if that's necessary. The blessing of dimensionality still seems to hold if m increases slowly with p, and computational complexity can be still advantageous compared to GLASSO. 2. Line 125: For completeness, please state the final objective function (empirical version of (3)) as a function of X_i and the parameters. 3. Section 4.1: The simulation is conducted under a joint Gaussian model. Therefore, ICA should be identical with PCA, and can be removed from the comparisons. Indeed, the ICA curve is almost identical with the PCA curve in Figure 2. 4. In the covariance estimation experiments, negative log likelihood under Gaussian model is used as the performance metric for both stock market data and OpenML datasets. This seems unreasonable since the real data in the experiment may not be Gaussian. For example, there is extensive evidence that stock returns are not Gaussian. Gaussian likelihood also seems unfair as a performance metric, since it may favor methods derived under Gaussian assumptions, like the proposed method. For comparing the results under these real datasets, it might be better to focus on interpretability, or indirect metrics (e.g., portfolio performance for stock return data). 5. The equation below Line 412: the p(z) factor should be removed in the expression for p(x|z). 6. Line 429: It seems we don't need Gaussian assumption to obtain Cov(Z_j, Z_k | X_i) = 0. 7. Line 480: Why do we need to combine with law of total variance to obtain Cov(X_i, X_{l != i} | Z) = 0? 8. Lines 496 and 501: It seems the Z in the denominator should be p(z). 9. The equation below Line 502: I think the '+' sign after \nu_j should be a '-' sign. In the definition of B under Line 503, there should be a '-' sign before \sum_{j=1}^m, and the '-' sign after \nu_j should be a '+' sign. In Line 504, we should have \nu_{X_i|Z} = - B/(2A). Minor comments: 10. The manuscript could be more reader-friendly if the mathematical definitions for H(X), I(X;Y), TC(X), and TC(X|Z) were state (in the supplementary material if no space in the main article). References to these are necessary when following the proofs/derivations. 11. Line 208: black -> block 12. Line 242: 50 real-world datasets -> 51 real-world datasets (according to Line 260 and Table 2) 13. References [7, 25, 29]: gaussian -> Gaussian Update: Thanks to the authors' for the response. A couple minor comments: - Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials. - Regarding the Gaussian evaluation metric, I think it would be helpful to include the comments as a note in the paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Lot of result description is needlessly convoluted e.g. ""less likely to produce less easier to teach and less structured languages when no listener gets reset"". ** Suggestions - A related idea of speaker-listener communication from a teachability perspective was studied in [1] - In light of [2], it's pertinent that we check that useful communication is actually happening. The differences in figures seem too small. Although the topography plots do seem to indicate something reasonable going on. [1]: https://arxiv.org/abs/1806.06464 [2]: https://arxiv.org/abs/1903.05168",NIPS_2019_1420,NIPS_2019,"Weakness - Not completely sure about the meaning of the results of certain experiments and the paper refuses to hypothesize any explanations. Other results show very little difference between the alternatives and unclear whether they are significant. - Lot of result description is needlessly convoluted e.g. ""less likely to produce less easier to teach and less structured languages when no listener gets reset"". ** Suggestions - A related idea of speaker-listener communication from a teachability perspective was studied in [1] - In light of [2], it's pertinent that we check that useful communication is actually happening. The differences in figures seem too small. Although the topography plots do seem to indicate something reasonable going on. [1]: https://arxiv.org/abs/1806.06464 [2]: https://arxiv.org/abs/1903.05168","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table. It would be better to provide a mathematical characterization.",ICLR_2022_497,ICLR_2022,"I have the following questions to which I wish the author could respond in the rebuttal. If I missed something in the paper, I would appreciate it if the authors could point them out.
Main concerns: - In my understanding, the best scenarios are those generated from the true distribution P (over the scenarios), and therefore, the CVAE essentially attempts to approximate the true distribution P. In such a sense, if the true distribution P is independent of the context (which is the case in the experiments in this paper), I do not see the rationale for having the scenarios conditioned on the context, which in theory does not provide any statistical evidence. Therefore, the rationale behind CVAE-SIP is not clear to me. If the goal is not to approximate P but to solve the optimization problem, then having the objective values involved as a predicting goal is reasonable; in this case, having the context involved is justified because they can have an impact on the optimization results. Thus, CVAE-SIPA to me is a valid method. - While reducing the scenarios from 200 to 10 is promising, the quality of optimization has decreased a little bit. On the other hand, in Figure 2, using K-medoids with K=20 can perfectly recover the original value, which suggests that K-medoids is a decent solution and complex learning methods are not necessary for the considered settings. In addition, I am also wondering the performance under the setting that the 200 scenarios (or random scenarios of a certain number from the true distributions) are directly used as the input of CPLEX. In addition, to justify the performance, it is necessary to provide information about robustness as well as to identify the case where simple methods are not satisfactory (such as larger graphs).
Minor concerns: - Given the structure of the proposed CVAE, the generation process takes the input of z and c where z
is derived from w
. This suggests that the proposed method requires us to know a collection of scenarios from the true distribution. If this is the case, it would be better to have a clear problem statement in Sec 3. Based on such understanding, I am wondering about the process of generating scenarios used for getting K representatives - it would be great if codes like Alg 1 was provided. - I would assume that the performance is closely related to the number of scenarios used for training, and therefore, it is interesting to examine the performance with different numbers of scenarios (which is fixed as 200 in the paper). - The structure of the encoder is not clear to me. The notation q_{\phi} is used to denote two different functions q(z w,D) and q ( c , D )
. Does that mean they are the same network? - It would be better to experimentally justify the choice of the dimension of c and z. - It looks to me that the proposed methods are designed for graph-based problems, while two-stage integer programming does not have to be graph problems in general. If this is the case, it would be better to clearly indicate the scope of the considered problem. Before reaching Sec 4.2, I was thinking that the paper could address general settings. - The paper introduces CVAE-SIP and CVAE-SIPA in Sec 5 -- after discussing the training methods, so I am wondering if they follow the same training scheme. In particular, it is not clear to me by saying “append objective values to the representations” at the beginning of Sec 5. - The approximation error is defined as the gap between the objective values, which is somehow ambiguous unless one has seen the values in the table. It would be better to provide a mathematical characterization.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate.,NIPS_2018_641,NIPS_2018,"weakness. First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a ""good"" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless. Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the ""learning rate"" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD. The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes. I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication. Other comments: 1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. ""Gradient methods for minimizing composite functions"" by Yu. Nesterov. 2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate. 4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties. 5. The proofs in the supplementary material are too terse.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- The proposed model produces only 1 node changing cluster per time step on average because the reassignment probability is 1/n. This allows for only very slow dynamics. Furthermore, the proposed evolution model is very simplistic in that no other edges are changed aside from edges with the (on average) 1 node changing cluster.",NIPS_2016_394,NIPS_2016,"- The theoretical results don't have immediate practical implications, although this is certainly understandable given the novelty of the work. As someone who is more of an applied researcher who occasionally dabbles in theory, it would be ideal to see more take-away points for practitioners. The main take-away point that I observed is to query a cluster proportionally to the square root of its size, but it's unclear if this is a novel finding in this paper. - The proposed model produces only 1 node changing cluster per time step on average because the reassignment probability is 1/n. This allows for only very slow dynamics. Furthermore, the proposed evolution model is very simplistic in that no other edges are changed aside from edges with the (on average) 1 node changing cluster. - Motivation by the rate limits of social media APIs is a bit weak. The motivation would suggest that it examines the error given constraints on the number of queries. The paper actually examines the number of probes/queries necessary to achieve a near-optimal error, which is a related problem but not necessarily applicable to the social media API motivation. The resource-constrained sampling motivation is more general and a better fit to the problem actually considered in this paper, in my opinion. Suggestions: Please comment on optimality in the general case. From the discussion in the last paragraph in Section 4.3, it appears that the proposed queue algorithm would is a multiplicative factor of 1/beta from optimality. Is this indeed the case? Why not also show experiment results for just using the algorithm of Theorem 4 in addition to the random baselines? This would allow the reader to see how much practical benefit the queue algorithm provides. Line 308: You state that you show the average and standard deviation, but standard deviation is not visible in Figure 1. Are error bars present but just too small to be visible? If so, state that it is the case. Line 93: ""asymptoticall"" -> ""asymptotically"" Line 109: ""the some relevant features"" -> Remove ""the"" or ""some"" Line 182: ""queries per steps"" -> ""queries per step"" Line 196-197: ""every neighbor of neighbor of v"" -> ""neighbor of"" repeated Line 263: Reference to Appendix in supplementary material shows ?? Line 269: In the equation for \epsilon, perhaps it would help to put parentheses around log n, i.e. (log n)/n rather than log n/n. Line 276: ""issues query"" -> I believe this should be ""issues 1 query"" Line 278: ""loosing"" -> ""losing"" I have read the author rebuttal and other reviews and have decided not to change my scores.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
2) Noisy label tolerance: A new perspective of Partial Multi-Label Learning,NIPS_2021_1954,NIPS_2021,"Some state-of-the-art partial multi-label references are missing, such as 1) Partial Multi-Label Learning with Label Distribution 2) Noisy label tolerance: A new perspective of Partial Multi-Label Learning 3) Partial multi-label learning with mutual teaching.
The explanation of Theorem 1 is weak; the author should provide more explanations.
Can the author do the experiments on the image data set?","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"* significance: As mentioned above this is a very specialized paper likely to interest some experts in the online convex optimization communities. Although narrow in scope, it contains interesting theoretical results advancing the state of the art in dealing with these specific problems.",NIPS_2019_168,NIPS_2019,"of the submission. * originality: This is a highly specialized contribution building up novel results on two main fronts: The derivation of the lower bound on the competitive ratio of any online algorithm and the introduction of two variants of an existing algorithm so as to meet this lower bound. Most of the proofs and techniques are natural and not surprising. In my view the main contribution is the introduction of the regularized version which brings a different, and arguably more modern interpretation, about the conditions under which these online algorithms perform well in these adversarial settings. * quality: The technical content of the paper is sound and rigorous * clarity: The paper is in general very well-written, and should be easy to follow for expert readers. * significance: As mentioned above this is a very specialized paper likely to interest some experts in the online convex optimization communities. Although narrow in scope, it contains interesting theoretical results advancing the state of the art in dealing with these specific problems. * minor details/comments: - p.1, line 6-7: I would rewrite the sentence to simply express that the lower bound is $\Omega(m^{-1/2})$ \- p.3, line 141: cost an algorithm => cost of an algorithm \- p.4, Algorithm 1, step 3: mention somewhere that this is the projection operator (not every reader will be familiar with this notation \- p.5, Theorem 2: remind the reader that the $\gamma$ in the statement is the parameter of OBD as defined in Algorithm 1 \- p.8, line 314: why surprisingly?","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"1.There are missing details about division to train and test sets, numbers as well as how the division was made (simply random? Any other considerations?). These details should be added.",ICLR_2021_457,ICLR_2021,"My main concern is that it is not completely clear to me how the authors suggest using the dataset for developing AI that is more ethical. I can clearly understand that one can use it to train an auxiliary model that will test/verify/give value for RL etc. I can also see that using it to fine tune language models and test them as done in the paper, can give an idea of how the language representation is aligned with or represents well ethical concepts. But it seems that the authors are trying to claim something broader when they say ““By defining and benchmarking a model’s understanding of basic concepts in ETHICS…” and “To do well on the ETHICS dataset, models must know about the morally relevant factors emphasized by each of these ethical systems”. It sounds as if they claim that given a model one can benchmark it on the dataset. If that is the case, they should explain how (for example say I develop a model that filters CVs and I want to see if it is fair, how can I use the dataset to test that model?). If not, I would suggest being clearer about the way the dataset can be used.
In addition, I personally do not like using language such as “With the ETHICS dataset, we find that current language models have a promising but incomplete understanding of basic ethical knowledge.” Or “By defining and benchmarking a model’s understanding of basic concepts in ETHICS, we enable future research necessary for ethical AI”. I think that even if a model can perform well on the ETHICS dataset, it is far from clear that it has understanding of ethical concepts. It is a leap of faith in my mind to conclude from what is essentially learning a classification task to ethical understanding. I would like to see the authors make more precise claims in that respect.
Recommendation: I vote for accepting this paper, at its current state marginally above threshold but provided some clarifications, I find this a clear accept. I think the area of ethical AI is important, releasing a well-constructed dataset is an important step forward and overall this paper should be of interest to the ICLR community.
Questions and minor comments: 1.There are missing details about division to train and test sets, numbers as well as how the division was made (simply random? Any other considerations?). These details should be added. 2. In the Impartiality section there is missing reference to Fig 2 – it is given only later so one does not see the relevant examples.
Post-rebuttal comments: My concerns are resolved. I have changed my vote to acceptance. (7).","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Clarity: The submission is well written and easy to follow, the concept of coresets is well motivated and explained. While some more implementation details could be provided (source code is intended to be provided with camera-ready version), a re-implementation of the method appears feasible.",NIPS_2019_651,NIPS_2019,"(large relative error compared to AA on full dataset) are reported. - Clarity: The submission is well written and easy to follow, the concept of coresets is well motivated and explained. While some more implementation details could be provided (source code is intended to be provided with camera-ready version), a re-implementation of the method appears feasible. - Significance: The submission provides a method to perform (approximate) AA on large datasets by making use of coresets and therefore might be potentially useful for a variety of applications. Detailed remarks/questions: 1. Algorithm 2 provides the coreset C and the query Q consists of the archetypes z_1, â¦, z_k which are initialised with the FurthestSum procedure. However, it is not quite clear to me how the archetype positions are updated after initialisation. Could the authors please comment on that? 2. The presented theorems provide guarantees for the objective functions phi on data X and coreset C for a query Q. Table 1 reporting the relative errors suggests that there might be a substantial deviation between coreset and full dataset archetypes. However, the interpretation of archetypes in a particular application is when AA proves particularly useful (as for example in [1] or [2]). Is the archetypal interpretation of identifying (more or less) stable prototypes whose convex combinations describe the data still applicable? 3. Practically, the number of archetypes k is of interest. In the presented framework, is there a way to perform model selection in order to identify an appropriate k? 4. The work in [3] might be worth to mention as a related approach. There, the edacious nature of AA is approached by learning latent representation of the dataset as a convex combination of (learnt) archetypes and can be viewed as a non-linear AA approach. [1] Shoval et al., Evolutionary Trade-Offs, Pareto Optimality, and the Geometry of Phenotype Space, Science 2012. [2] Hart et al., Inferring biological tasks using Pareto analysis of high-dimensional data, Nature Methods 2015. [3] Keller et al., Deep Archetypal Analysis, arxiv preprint 2019. ---------------------------------------------------------------------------------------------------------------------- I appreciate the authorsâ response and the additional experimental results. I consider the plot of the coreset archetypes on a toy experiment insightful and it might be a relevant addition to the appendix. In my opinion, the submission constitutes a relevant contribution to archetypal analysis which makes it more feasible in real-world applications and provides some theoretical guarantees. Therefore, I raise my assessment to accept.","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"1) Building text descriptions for each task still requires human labor. We do not know what textual format is optimal for policy learning. It varies from task to task, model to model. On the other hand, as I stated in Question 1, the long-text input could restrict the scalability of this framework.",NIPS_2022_1708,NIPS_2022,"Scalability: The proposed encoding method is templated-based (Line 155-156). Although the input encoding scheme (Section 7.1) may be a trivial problem, the encoding scheme may still affect the performance. Searching for the optimal encoding scheme is an expensive process, which may bring a high cost of hand-crafted engineering. Besides, the data gathering method also relies on hand-designed templates (Line 220).
Presentation: The related work of PLM is adequately cited. But the authors should also introduce the background of policy learning so that the significance of this work can be highlighted.
Performance: Compared to the work that uses traditional networks like DQN, the integration of PLM may affect the inference speed.
Clarity: Most parts of this paper are well written. However, there are some typos in the paper:
Line 53: pretrained LMs -> pre-trained LMs
Line 104: language -> language. (missing full stop mark)
Some papers should be cited in a proper way: Line 108: [23], Line 109: [36], Line 285:[15], Line 287 [15]. For example, in Line 108, ""[23] show that"" needs to be rewritten as ""Frozen Pretrained Transformer (FPT) [23] show that"".
[Rebuttal Updates] The authors provided the additional experiments for addressing my concern of scalability. The authors also revised the typos and added the related works.
Societal Impact: No potential negative societal impact. The authors provide a new perspective to aid policy learning with a pre-trained language model.
Limitation: 1) Building text descriptions for each task still requires human labor. We do not know what textual format is optimal for policy learning. It varies from task to task, model to model. On the other hand, as I stated in Question 1, the long-text input could restrict the scalability of this framework. 2) The proposed methods also need humans to design some templates/rules, as the authors mentioned in the conclusion part.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. As shown in figure 3, the performance improvement of proposed methods seems not so significant, the biggest improvement in the bank dataset was ~0.02. Additionally, using some tables to directly show the key improvements may be more intuitive and detailed.",NIPS_2022_1637,NIPS_2022,"1. The examples of scoring systems in the Introduction seem out of date, there are many newer and recognized clinical scoring systems. It also should briefly introduce the traditional framework of the scoring system and its difference in methodology and performance with the proposed method. 2. As shown in figure 3, the performance improvement of proposed methods seems not so significant, the biggest improvement in the bank dataset was ~0.02. Additionally, using some tables to directly show the key improvements may be more intuitive and detailed. 3. Although extensive experiments and discussion on performance, in my opinion, its most significant improvement would be efficiency, and there are few discussions or ablation experiments on efficiency. 4. The model AUC can assess the model discriminant ability, i.e., the probability of a positive case is bigger than that of a negative case, but may be hard to show its consistency between predicted score and actual risk. However, this consistency may be more crucial to the clinical scoring system (differentiated with classification task). Therefore, the related studies are encouraged to conduct calibration curves to show the agreement. It would be better to prove the feasibility of the generated scoring system? The difference between the traditional method and our method can also be discussed in this paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"• Experimental validation are not convincing. Only shallow networks are considered (2 or 3 layers), and the optimization strategy, including the grid search strategy for hyperparameters selection, is not described. Minor issue: positioning with respect to related works is limited. For example, layer redundancy (which is the opposite of diversity) has been considered in the context of network pruning: https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.pdf",ICLR_2021_1043,ICLR_2021,", which justify the score: • The theoretical developments presented in the paper build on the Rademacher complexity, but ignore the conclusions drawn by Zhang et al. in Section 2.2 of their ICLR 2017 paper (Understanding deep learning requires rethinking generalization). • The theoretical developments build on the assumption that (i) there exists a lower bound, valid for any input, to the distance between the output of each pair of neurons, and (ii) the proposed diversity loss increases this lower bound. Those two assumptions are central to the theoretical developments, but are quite arguable. For example, a pair of neuron that is not activated by a sample, which is quite common, leads to a zero lower bound. • Experimental validation are not convincing. Only shallow networks are considered (2 or 3 layers), and the optimization strategy, including the grid search strategy for hyperparameters selection, is not described.
Minor issue: positioning with respect to related works is limited. For example, layer redundancy (which is the opposite of diversity) has been considered in the context of network pruning: https://openaccess.thecvf.com/content_CVPR_2019/papers/He_Filter_Pruning_via_Geometric_Median_for_Deep_Convolutional_Neural_Networks_CVPR_2019_paper.pdf","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Although the use of this type of loss in this setting might be new, this work does not prove any new theoretical results.",z62Xc88jgF,ICLR_2024,"1. Although the use of this type of loss in this setting might be new, this work does not prove any new theoretical results.
2. That being said, experiment is a very important component in this paper, however, I find the evaluation metric of the solution very interesting. More specifically, let $u$ be the output of neural networks and $u^*$ be the exact solution. The test error is usually computed using relative $L^2$ norm (See for example [1][2]), i.e.
$$|| u - u^*||_2^2 / ||u^*||_2^2 = \int|u - u^*|^2dx / \int |u^*|^2 dx.$$
However, in Figure 4, when evaluating solutions, the mean error is computed using equation (15), the energy norm.
(i). why not using the relative $L^2$ norm? How does Astral loss perform if the evaluation is done in $L^2$?
(ii). The a posteriori error bound is in the energy norm, i.e.
$$L(u, w_L) \leq |||u-u^*||| \leq U(u, w_U).$$
so I would naturally expect Astral loss to achieve fairly small error in this energy norm, but this does not necessarily imply the solution is ""better"". Equations can be solved in different spaces. In fact, I think the space $L^2$ is more commonly used when people study existence and uniqueness of PDE solutions.
(iii). There could be a relation between the energy norm and $L^2$ norm. More explanation is needed for the specific choice of the evaluation metric since it differs from the previous literature.
[1] Li et al., Physics-Informed Neural Operator for Learning Partial Differential Equations
[2] Wang et al., An Expert's Guide to Training Physics-informed Neural Networks","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. It seems to me that a very straightforward hypothesis about these two parts would be that the trivial part is what’s very simple, either highly consistent to what’s in the training set, or the images with very typical object pose in the center of the images; and for the impossible part, it might be the images with ambiguous labels, atypical object pose or position. I think the human test results would support this hypothesis, but I wonder whether the authors could provide more evidence to either prove or disprove this hypothesis.",ICLR_2022_1014,ICLR_2022,"1. It seems to me that a very straightforward hypothesis about these two parts would be that the trivial part is what’s very simple, either highly consistent to what’s in the training set, or the images with very typical object pose in the center of the images; and for the impossible part, it might be the images with ambiguous labels, atypical object pose or position. I think the human test results would support this hypothesis, but I wonder whether the authors could provide more evidence to either prove or disprove this hypothesis. 2. The figure 6 is very confusing to me. The caption says that the right part is original ImageNet test set, but the texts on the image actually say it’s the left part. If the texts on the image are right, then the right panel is the consistency on the validation images between the two parts. If I understand the experiments correctly, these results are for models trained on ImageNet training set without the trivial or the impossible part and then tested on ImageNet validation set without the two parts. Although it’s good to see the lower consistency, it should be compared to the consistency between models trained on the whole ImageNet training set and tested on ImageNet validation set without the two parts, which I cannot find. Is the consistency lower because of the changed training process or the changed validation set? 3. It is also unclear how surprising we should be towards the consistency distribution, is this a result of an exponential distribution of the general “identification” difficulty (most images are simple, then less and less are more difficult)?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- For bAbI, it seems the model was only tested on single supporting fact dataset (Task 1 of bAbI). How about other tasks?",NIPS_2016_93,NIPS_2016,"/ Major concerns: - It is difficult to evaluate whether the MovieQA result should be considered significant given that +10% gap exists between MemN2N on dataset with explicit answers (Task 1) and RBI + FP on dataset with other forms of supervision, especially Task 3. If I understood correctly, the different tasks are coming from the same data, but authors provide different forms of supervision. Also, Task 3 gives full supervision of the answers. Then I wonder why RBI + FP on task 3 (69%) is doing much worse than MemN2N on task 1 (80%). Is it because the supervision is presented in a more implicit way (""No, the answer is kitchen"" instead of ""kitchen"")? - For RBI, they only train on rewarded actions. Then this means rewardless actions that get useful supervision (such as ""No, the answer is Timothy Dalton."" in Task 3) is ignored as well. I think this could be one significant factor that makes FP + RBI better than RBI alone. If not, I think the authors should provide stronger baseline than RBI (that is supervised by such feedback) to prove the usefulness of FP. Questions / Minor concerns: - For bAbI, it seems the model was only tested on single supporting fact dataset (Task 1 of bAbI). How about other tasks? - How is dialog dataset obtained from QA datasets? Are you using a few simple rules? - Lack of lexical / syntactic diversity of teacher feedback: assuming the teacher feedback was auto-generated, do you intend to turk the teacher feedback and / or generate a few different kinds of feedback (which is more real-life situation)? - How does other models than MemN2N do on MovieQA?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
3. It is hard to follow Sec. 3.2. The author may improve it and give more illustrations and examples.,ICLR_2022_2318,ICLR_2022,"Weakness： 1. This paper is built on the SPAIR framework and focuses on point cloud data, which is somehow incremental. 2. There is no ablation study to validate the effectiveness of the proposed components and the loss. 3. It is hard to follow Sec. 3.2. The author may improve it and give more illustrations and examples. 4. It is unclear how the method can work and decompose a scene into different objects. I did not see how Chamfer Mixture loss can achieve this goal. More explanation should go here.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2 The technical contribution is limited, i.e., there is no significant technical contribution and extension based on a typical model for the cross-domain recommendation setting.",ICLR_2022_2070,ICLR_2022,"Weakness:
1 The idea is a bit too straightforward, i.e., using the attributes of the items/users and their embeddings to bridge any two domains.
2 The technical contribution is limited, i.e., there is no significant technical contribution and extension based on a typical model for the cross-domain recommendation setting.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
1. Adding fully-supervised baselines for small models in table 1 will be useful in understanding the gap between full supervision and SSL for these models.,ICLR_2021_842,ICLR_2021,"1. Performance gains on downstream tasks of detection and instance segmentation are much lower -- how would the authors propose to improve these? 2. If the primary goal is to improve SSL performance on small models, I would have liked to see more analysis on how different design choices of setting up contrastive learning affect model performance and if these could aid performance improvement, in addition to knowledge distillation.
Questions and suggestions: 1. Adding fully-supervised baselines for small models in table 1 will be useful in understanding the gap between full supervision and SSL for these models. 2. In figure 3, does 100% (green line) represent the student network trained with 100% of labeled imagenet supervised data? It is hard to interpret what these numbers represent. 3. Minor point: Some citations, which should not be in parentheses, are in parentheses (e.g., Romero et al. page 8). Please fix this in the revision.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"4. Time Complexity: What is the time complexity of the proposed algorithm? In each step of LaMOO, it has to repeatedly calculate the hypervolume of different regions for promising region selection. However, the computation of hypervolume could be time-consuming, especially for problems with many objectives (e.g., >3). Would it make LaMOO impractical for those problems?",ICLR_2022_445,ICLR_2022,"Weakness: Method:
1. Novelty:
Incremental Contribution: The proposed LaMOO is a direct generalization from the LaMCTS method to multi-objective optimization (MOO). The novel part is to use dominance number as criteria for search space partition and hypervolume for promising region selection. These are all straightforward generalizations for MOO. The contribution of this work is somewhat incremental along the line of LaNAS, LaMCTS, and LaP^3.
Missing Closely Related Approaches: This work claims the proposed approach to learn the promising region is fundamentally different from the previous works. However, many classification-based search space partition methods have been proposed in the machine learning community, see [1][2][3] (classification + random sampling). (Tree-based) space partition methods have been widely used for black-box optimization [4][5][6]. In addition, there are also different works on classification-based MOO [7] (SVM + NSGA-II/MO-CMA-ES) [8] (Ordinal SVM + NSGA-II) [9].
2. Theoretical Analysis:
A large part of this work is on the theoretical understanding for space partition and LaMCTS. However, the analysis is mostly for single-objective optimization, and the extension to multi-objective optimization is much less promising.
3. Why LaMOO Works:
Further discussions are needed to clearly clarify the properties of LaMOO.
Dominance-based Approach for Many Objective Optimization: LaMOO uses the dominance number as the split criteria to train the SVM models and partition the search space. However, the dominance-based method is typically not good for many objective optimization due to the lack of dominance pressure (e.g., all solutions are non-dominated with each other, and all have the same dominated number). Why is LaMOO still good for many objective optimization?
Combination with Multi-Objective Bayesian Optimization (MOBO): It is straightforward to see the benefit of using LaMOO with model-free optimization (e.g., NSGA-II and MO-CMA-ES). However, it is not so clear to understand why it also works for MOBO (e.g., qEHVI). The qEHVI approach already builds (global) Gaussian process models to approximate each objective function, and uses hypervolume-based criteria to select the most promising solution(s) (e.g., maximizing the expected hypervolume improvement) for evaluation. Therefore, its selected solution(s) should be already on the approximate Pareto front without the LaMOO approach. Is the good performance due to only use solutions in the promising region to build the models (but I think GP would work well with all data as in the setting considered in this work)? Or because LaMOO restricts the search in the region close to the current best non-dominated solutions (then what is the relation to the trust-region approach [10])?
Exploitation v.s. Exploration: With LaMOO, the solutions can only be selected from the most promising region (e.g., around the current Pareto front), which is good for exploitation. However, will this approach lead to worse overall performance due to the lack of exploration (e.g., cannot find more diverse Pareto solutions far from the current Pareto front)?
4. Time Complexity:
What is the time complexity of the proposed algorithm? In each step of LaMOO, it has to repeatedly calculate the hypervolume of different regions for promising region selection. However, the computation of hypervolume could be time-consuming, especially for problems with many objectives (e.g., >3). Would it make LaMOO impractical for those problems?
5. Inaccurate Description for MOO Methods:
CMA-ES: CMA-ES is a widely-used single objective optimization algorithm [11]. The multi-objective version proposed in (Igel et al., 2007a) is usually called MO-CMA-ES. It is also confusing why most citation for the MO-CMA-ES (in the main paper and Table 1) is for the steady-state updated version (Igel et al., 2007b) but not for the original paper (Igel et al., 2007a).
ParEGO: The seminal algorithm proposed in Knowles (2006) is called ParEGO and the qParEGO is a parallel extension recently proposed in Daulton et al. (2020). It is not suitable to refer the algorithm in Knowles (2006) as qParEGO in Table 1 and the main text.
MOEA/D: In my understanding, MOEA/D is suitable for many objective optimization (objectives > 3), see its performance in the NSGA-III paper (Deb & Jain, 2014), while the main challenge is how to specify the weight vector for a new problem with unknown Pareto front as correctly pointed out in this work.
Hypervolume-based Method: This work indicates the indicator-based method is better for many objective optimization. However, the time complexity and expensive calculation could make the hypervolume-based method impractical for many-objective optimization. Experiment:
6. Missing Experimental Setting:
Many important experiment settings are missing in this work, such as the number of initial solutions for MOBO (and its generation method), the number of batched solutions for MOBO (e.g., q), the reference point for hypervolume (during the optimization, and for the final evaluation), the ground truth Pareto front used for calculating the log hypervolume difference for real-world problems (e.g., Nasbench 201).
7. Comparison to Model-Free Evolutionary Algorithm:
It is reasonable that LaMOO can improve the MO-CMA-ES performance since it builds extra models to allocate computation to the most promising region. However, in my understanding, the model-free evolutionary algorithms are not designed for expensive optimization, and their typical use case is with a large number of cheap evaluations with a fast run time. It is more interesting to directly compare LaMOO with other model-based methods (e.g., MO-CMA-ES with GP models).
8. MOBO Performance:
What are the hyperparameters for qEHVI? It seems its performance on VehicleSafty problem is worse than those reported in the original paper Daulton et al. (2020).
9. Wall Clock Run Time:
Please report the wall clock run time for both LaMOO and other model-free/model-based algorithms, as in Daulton et al. (2020).
Minor Issues:
When citing multiple works, please put them in chronological order. Reference:
[1] Hashimoto, Tatsunori, Steve Yadlowsky, and John Duchi. Derivative free optimization via repeated classification. AISTATS 2018.
[2] Kumar, Manoj, George E. Dahl, Vijay Vasudevan, and Mohammad Norouzi. Parallel architecture and hyperparameter search via successive halving and classification. arXiv:1805.10255.
[3] Yu, Yang, Hong Qian, and Yi-Qi Hu. Derivative-free optimization via classification. AAAI 2016.
[4] Munos, Rémi. Optimistic optimization of a deterministic function without the knowledge of its smoothness. NeurIPS 2011.
[5] Ziyu Wang, Babak Shakibi, Lin Jin, and Nando de Freitas. Bayesian multi-scale optimistic optimization. AISTATS 2014.
[6] Kenji Kawaguchi, Leslie Pack Kaelbling, and Tomas Lozano-Perez. Bayesian optimization with exponential convergence. NeurIPS 2015.
[7] Loshchilov, Ilya, Marc Schoenauer, and Michèle Sebag. A mono surrogate for multiobjective optimization. In Proceedings of the 12th annual conference on Genetic and evolutionary computation, 2010.
[8] Seah, Chun-Wei, Yew-Soon Ong, Ivor W. Tsang, and Siwei Jiang. Pareto rank learning in multi-objective evolutionary algorithms. In 2012 IEEE Congress on Evolutionary Computation, 2012.
[9] Pan, Linqiang, Cheng He, Ye Tian, Handing Wang, Xingyi Zhang, and Yaochu Jin. A classification-based surrogate-assisted evolutionary algorithm for expensive many-objective optimization. IEEE Transactions on Evolutionary Computation 2018.
[10] Daulton, Samuel, David Eriksson, Maximilian Balandat, and Eytan Bakshy. Multi-Objective Bayesian Optimization over High-Dimensional Search Spaces. arXiv:2109.10964, 2021.
[11] Hansen, Nikolaus, and Andreas Ostermeier. Completely derandomized self-adaptation in evolution strategies. Evolutionary Computation 2001.
[12] Ishibuchi, Hisao, Yu Setoguchi, Hiroyuki Masuda, and Yusuke Nojima. Performance of decomposition-based many-objective algorithms strongly depends on Pareto front shapes. IEEE Transactions on Evolutionary Computation 21, no. 2 (2016): 169-190.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"5. No conclusion is provided Updated after Author response: I am still not happy that the authors did not do any expts. While theoretical results only provide a bound, the usefulness can only be found by thorough evaluation. I would also urge the authors to add a conclusion section since the takeaways become more informative after reading the whole paper.",NIPS_2018_219,NIPS_2018,"Weakness: 1. I found the paper hard to follow. Unfamiliar with local differential privacy, I found it hard to comprehend. The definition is in Section 2. I would urge the authors to present it in Section 1 2. The accuracy estimates provided in the paper are probabilistic. Without proper experiments it is impossible to judge the tradeoff between privacy and accuracy. This paper does not provide any expt results 3. Since this is an iterative system, how scalable is the method? This is very important to understand this, since the authors guarantee diff privacy after each epoch. There is a cost to pay for this in terms of the ""delay"" 4. From the simple problem of average of bits, how can we do go more complex data at each user? 5. No conclusion is provided Updated after Author response: I am still not happy that the authors did not do any expts. While theoretical results only provide a bound, the usefulness can only be found by thorough evaluation. I would also urge the authors to add a conclusion section since the takeaways become more informative after reading the whole paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,
"- As mentioned before, the dataset used in the experiments are all very small. It would be more convincing to see some result on medium or even large dataset such as ImageNet. But this is just a minor issue and it will not affect the overall quality of the paper.",NIPS_2020_1476,NIPS_2020,"- As mentioned before, the dataset used in the experiments are all very small. It would be more convincing to see some result on medium or even large dataset such as ImageNet. But this is just a minor issue and it will not affect the overall quality of the paper. - Which model did you used in section 5 for image recognition task? To some extend it show the capability of Augerino on this task. However, on image recognition the network architecture strongly affect the result. It is interested to see what kind of chemical reaction will take place between Augrino and difference DNN architectures. --------------- after rebuttal ----------------- - Regarding the authors' response and all the other review comments I am agree with R4, that in this paper there is still some important issues needed to be re-worked before publication. I thus decided to lower my rating. I would like to encourage the authors to re-submit after revision.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- you have listed some of the limitations of evolutionary methods, but I think there are much deeper things to say regarding leveraging state, reactiveness, and learning during an episode. Being honest and direct would work well for this work - the title is way to generic and vague - be precise when being critical. What does ""brittle convergence properties mean"" - I would say DeepRL methods are widely adopted. Consider the landscape 10 years ago.",NIPS_2018_109,NIPS_2018,"that limit the contribution. In particular: 1) the method does not seem to improve on the robustness and sensitivity as claimed in the motivation of using evolutionary methods in the first place. In fig 3 the new method is noisier in 2 domains and equally noisy as the competitors in the rest. 2) The paper claims SOTA in these domains compared to literature results. The baseline results reported in the paper under review in HalfCheetah, Swimmer and Hopper are worse the state of the art reported in the literature [1,2]. Either because the methods used achieved better results in [1,2] or because the SOTA in the domain was TRPO which was not reported in the paper under review. SOTA is a big claim; support it carefully. 3) There is significant over-claiming throughout the paper. E.g line 275 ""best of both approaches"", line 87 ""maximal information extraction"" 4) It is not clear why async methods like A3C were not discussed or compared against. This is critical given the SOTA claim 5) The paper did not really attempt to tease apart what was going on in the system. When evaluating how often was DDPG agent chosen for evaluation trials or did you prohibit this? What does the evaluation curve look like for the DDPG agent? That is do everything you have done, but evaluate the DDPG agent as the candidate from the population. Or allowing the population to produce the data for the DDPG and training it totally off-policy to see how well the DDPG learnings (breaking the bottom right link in fig 1). Does adding more RL agents help training? The paper is relatively clear and the is certainly original. However my concerns above highlight potential issues with quality and significance of the work. [1] https://arxiv.org/abs/1709.06560 [2] https://arxiv.org/pdf/1708.04133.pdf ++++++++++++ Ways to improve the paper that did not impact the scoring above: - you have listed some of the limitations of evolutionary methods, but I think there are much deeper things to say regarding leveraging state, reactiveness, and learning during an episode. Being honest and direct would work well for this work - the title is way to generic and vague - be precise when being critical. What does ""brittle convergence properties mean"" - I would say DeepRL methods are widely adopted. Consider the landscape 10 years ago. - claim V-trace is too expensive. I have no idea why - its important to note that evolutionary methods can be competitive but not better than RL methods - discussion starting on line 70 is unclear and seems not well supported by data. Say something more plain and provide data to back it up - definition of policy suggests deterministic actions - not sure what state space s = 11 means? typo - section at line 195 seems repetitive. omit","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?",ICLR_2021_2674,ICLR_2021,"Though the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:
- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?
- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
- According to Held et al ""Using Blur to Affect Perceived Distance and Size"", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
- in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
- the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
- what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?
Minor suggestions
- figure text should be rasterized, and figures should maintain its aspect ratio.
- figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
- the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
- the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The first claimed contribution of the paper is that unlike other existing algorithms, the proposed algorithm does not take as many points or does not need apriori knowledge about dimensions of subspaces. It would have been better if there were some empirical justification about this.",NIPS_2016_9,NIPS_2016,"Weakness: The authors do not provide any theoretical understanding of the algorithm. The paper seems to be well written. The proposed algorithm seems to work very all on the experimental setup, using both synthetic and real-world data. The contributions of the papers are enough to be considered for a poster presentation. The following concerns if addressed properly could raise to the level of oral presentation: 1. The paper does not provide an analysis on what type of data the algorithm work best and on what type of data the algorithm may not work well. 2. The first claimed contribution of the paper is that unlike other existing algorithms, the proposed algorithm does not take as many points or does not need apriori knowledge about dimensions of subspaces. It would have been better if there were some empirical justification about this. 3. It would be good to show some empirical evidence that the proposed algorithm works better for Column Subset Selection problem too, as claimed in the third contribution of the paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C.",NIPS_2021_537,NIPS_2021,"Weakness: The main weakness of the approach is the lack of novelty. 1. The key contribution of the paper is to propose a framework which gradually fits the high-performing sub-space in the NAS search space using a set of weak predictors rather than fitting the whole space using one strong predictor. However, this high-level idea, though not explicitly highlighted, has been adopted in almost all query-based NAS approaches where the promising architectures are predicted and selected at each iteration and used to update the predictor model for next iteration. As the authors acknowledged in Section 2.3, their approach is exactly a simplified version of BO which has been extensively used for NAS [1,2,3,4]. However, unlike BO, the predictor doesn’t output uncertainty and thus the authors use a heuristic to trade-off exploitation and exploration rather than using more principled acquisition functions.
2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C. 3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper.
Detailed Comments: 1. The authors conduct some ablation studies in Section 3.2. However, a more important ablation would be to modify the proposed predictor model to get some uncertainty (by deep-ensemble or add a BLR final output layer) and then use BO acquisition functions (e.g. EI) to do the sampling. The proposed greedy sampling strategy works because the search space for NAS-Bench-201 and 101 are relatively small and as demonstrated in [6], local search even gives the SOTA performance on these benchmark search spaces. For a more realistic search space like NAS-Bench-301[7], the greedy sampling strategy which lacks a principled exploitation-exploration trade-off might not work well. 2. Following the above comment, I’ll suggest the authors to evaluate their methods on NAS-Bench-301 and compare with more recent BO methods like BANANAS[2] and NAS-BOWL[4] or predictor-based method like BRP-NAS [5] which is almost the same as the proposed approach. I’m aware that the authors have compared to BONAS and shows better performance. However, BONAS uses a different surrogate which might be worse than the options proposed in this paper. More importantly, BONAS use weight-sharing to evaluate architectures queried which may significantly underestimate the true architecture performance. This trades off its performance for time efficiency. 3. For results on open-domain search, the authors perform search based on a pre-trained super-net. Thus, the good final performance of WeakNAS on MobileNet space and NASNet space might be due to the use of a good/well-trained supernet; as shown in Table 6, OFA with evalutinary algorithm can give near top performance already. More importantly, if a super-net has been well-trained and is good, the cost of finding the good subnetwork from it is rather low as each query via weight-sharing is super cheap. Thus, the cost gain in query efficiency by WeakNAS on these open-domain experiments is rather insignificant. The query efficiency improvement is likely due to the use of a predictor to guide the subnetwork selection in contrast to the naïve model-free selection methods like evolutionary algorithm or random search. A more convincing result would be to perform the proposed method on DARTS space (I acknowledge that doing it on ImageNet would be too expensive) without using the supernet (i.e. evaluate the sampled architectures from scratch) and compare its performance with BANANAS[2] or NAS-BOWL[4]. 4. If the advantage of the proposed method is query-efficiency, I’d love to see Table 2, 3 (at least the BO baselines) in plots like Fig. 4 and 5, which help better visualise the faster convergence of the proposed method. 5. Some intuitions are provided in the paper on what I commented in Point 3 in Weakness above. However, more thorough experiments or theoretical justifications are needed to convince potential users to use the proposed heuristic (a simplified version of BO) rather than the original BO for NAS. 6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.
7. The results in Table 2 which show linear-/exponential-decay sampling clearly underperforms uniform sampling confuse me a bit. If the predictor is accurate on the good subregion, as argued by the authors, increasing the sampling probability for top-performing predicted architectures should lead to better performance than uniform sampling, especially when the performance of architectures in the good subregion are rather close. 8. In Table 1, what does the number of predictors mean? To me, they are simply the number of search iterations. Do the authors reuse the weak predictors from previous iterations in later iterations like an ensemble?
I understand that given the time constraint, the authors are unlikely to respond to my comments. Hope those comments can help the authors for future improvement of the paper.
References: [1] Kandasamy, Kirthevasan, et al. ""Neural architecture search with Bayesian optimisation and optimal transport."" NeurIPS. 2018. [2] White, Colin, et al. ""BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search."" AAAI. 2021. [3] Shi, Han, et al. ""Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS."" NeurIPS. 2020. [4] Ru, Binxin, et al. ""Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels."" ICLR. 2020. [5] Dudziak, Lukasz, et al. ""BRP-NAS: Prediction-based NAS using GCNs."" NeurIPS. 2020. [6] White, Colin, et al. ""Local search is state of the art for nas benchmarks."" arXiv. 2020. [7] Siems, Julien, et al. ""NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search."" arXiv. 2020.
The limitation and social impacts are briefly discussed in the conclusion.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
".1. The proposed S1DB-ED algorithm is too similar to RMED (Komiyama et al. 2015), so I think the novelty of this part is limited. The paper needs to give a sufficient discussion on the comparison with RMED.",NIPS_2021_2224,NIPS_2021,". 1. The proposed S1DB-ED algorithm is too similar to RMED (Komiyama et al. 2015), so I think the novelty of this part is limited. The paper needs to give a sufficient discussion on the comparison with RMED. 2. The comparison baselines in experiments are not sufficient. The paper only compares the proposed two algorithms, so readers cannot evaluate the empirical performance of the proposed algorithms. While I understand that this is a new problem and there are no other existing algorithms for this problem, the paper can still compare to some ablation variants of proposed algorithms to demonstrate the effectiveness of key algorithmic components, or reduce the setting to conventional dueling bandits and compare with existing dueling bandit algorithms.
After Rebuttal
I read the rebuttal of the authors. Now I agree that the analysis for the S1DB-ED algorithm is non-trivial and the authors correct the errors in prior work [21]. My concerns are well addressed. So I will keep my score.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
1. The authors do not give a comprehensive discussion of previous work on this topic.,PCm1oT8pZI,ICLR_2024,"1. The authors do not give a comprehensive discussion of previous work on this topic.
2. The experimental justification of this work is not sufficient, only compared to the basic backdoor-based strategy.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?",ARR_2022_169_review,ARR_2022,"1. The paper claims that it exploits unlabelled target language data. However, in line 363, it seems that the paper actually uses event-presence labels $e_{i}$ for each target language sample. First, $e_{i}$ is probably extracted directly from labels $y_{i}$; it is when $y_{i}$ says that some word is an event trigger that one can know that $e_{i}=1$. So, for target language data, their labels are actually used in an indirect way. Thus the method is not totally using pure ""unlabelled"" target language data as the paper claims. Second, I think $e_{i}$ provides super crucial information which might be responsible for most of the gain derived. To make fair comparisons with the baselines, I think baseline methods BERT-CRF in section 3.2 and the BERT-CRF+MLM in 3.4 should also see $e_{i}$ labels. 2. Also concerning $e_{i}$ in weakness point 1 above, it is not known how $e_{i}$ and $e_{i}$'s distributions look like at all. I could only guess $e_{i}$ is 0,1 binary variables? Since all SRC and TRG data comes from Cross-Lingual Event Detection datasets, maybe most samples do have an event trigger and thus most $e_{i}$s equal 1. 3. It is confusing in line 339: s$\sim$p(s) and t$\sim$p(t). Do p(s) and p(t) here the ones calculated and updated in equation (6-7) in lines 369-370? Or maybe it is fixed since each sample already has a ground truth $e_{i}$. If it is the former case, I think it might be a little weird to predict the p(s) and p(t) which the paper uses to draw samples, because p(s) and p(t) are already given since $e_{i}$s are known for all samples? 4. The authors did not justify why Optimal Transport (OT) is used and did not elaborate on what are OT's advantages. One simple substitute for OT is average euclidean distance or average cosine similarity, which can be used to replace the paper's equation (8). There are more substitutes to OT, such as the KL divergence or the Jensen-Shannon divergence (which are commonly used to make comparisons with OT). It is worth comparing OT with say, Euclidean distance, KL divergence as a side experiment. All these simple substitutes are probably super-efficient and quicker to compute than OT. 5. It is not known if the OT sample selection process in 2.4.3 only runs once or runs iteratively as EP module is updated during the training steps. Are optimizing the loss of equation (10), i.e. the training steps, and solving OT in equation (3) conducted by turns iteratively? It will be much easier for readers to know the whole process if more details and a flow chart can be added. Furthermore, what is the runtime for solving the entropic regularized discrete OT problem, and the runtime for OT sample selection?
6. It is claimed in lines 128-132 that ""it would be beneficial for the LD to be trained with examples containing events"". The statement in lines 137-148 also focuses only on LD. Why only LD benefits from seeing examples containing events? Do the text encoders also benefit from seeing these examples? A clue that the encoder might benefit from unlabelled data is in 3.4's result where simply MLM fine-tuning can derive considerable gains.
7. In section 3.4, the result shows that simple MLM fine-tuning on unlabelled target language data derives considerable gains against BERT-CRF baseline. I was curious if the authors could do BERT-CRF + MLM + EP like in equation (10), can the performance be better than ALA? If true, it might show that a simple MLM is better than adversarial training.
1. The writing is not fluent enough. Some typos and awkward/redundant/unnatural sentences such as lines 019, 041-043.
2. Using Optimal Transport (OT), or more specifically leveraging the Wasserstein Distance, in GAN is first seen in the Wasserstein GAN paper, i.e. WGAN (Arjovsky et al. ICML 2017). It might be beneficial to discuss WGAN a bit or even add WGAN as a baseline method.
3. The paper should elaborate on OT in both the introduction and the methodology parts and should provide more details and justifications for OT.
4. In equation (4), L2 distance is used. In OT, earth mover's distance is more common. What is the benefit of L2 distance?
5. I hope to see the authors' response in resubmission (if rejected) or clarifications in the camera-ready (if accepted) to remove my concern.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors take time to discuss how KG handles the continuous task setting, but there are no experiments with continuous tasks - It’s great that entropy methods for conditional optimization are derived in Section 7 in the appendix, but why are these not included in the experiments? How does the empirical performance of these methods compare to ConBO?",NIPS_2021_2418,NIPS_2021,"- The class of problems is not very well motivated. The CIFAR example is contrived and built for demonstration purposes. It is not clear what application would warrant sequentially (or in batches) and jointly selecting tasks and parameters to simultaneously optimize multiple objective functions. Although one could achieve lower regret in terms of total task-function evaluations by selecting the specific task(s) to evaluate rather than evaluating all tasks simultaneously, the regret may not be better with respect to timesteps. For example, in the assemble-to-order, even if no parameters are evaluated for task function (warehouse s) at timestep t, that warehouse is going to use some (default) set of parameters at timestep t (assuming it is in operation---if this is all on a simulator then the importance of choosing s seems even less well motivated). There are contextual BO methods (e.g. Feng et al 2020) that address the case of simultaneously tuning parameters for multiple different contexts (tasks), where all tasks are evaluated at every timestep. Compelling motivating examples would help drive home the significance of this paper. - The authors take time to discuss how KG handles the continuous task setting, but there are no experiments with continuous tasks - It’s great that entropy methods for conditional optimization are derived in Section 7 in the appendix, but why are these not included in the experiments? How does the empirical performance of these methods compare to ConBO? - The empirical performance is not that strong. EI is extremely competitive and better in low-budget regimes on ambulance and ATO - The performance evaluation procedure is bizarre: “We measure convergence of each benchmark by sampling a set of test tasks S_test ∼ P[s] ∝ W(s) which are never used during optimization”. Why are the methods evaluated on test tasks not used during the optimization since all benchmark problems have discrete (and relatively small) sets of tasks? Why not evaluate performance on the expected objective (i.e. true, weighted) across tasks? - The asymptotic convergence result for Hybrid KG is not terribly compelling - It is really buried in the appendix that approximate gradients are used to optimize KG using Adam. I would feature this more prominently. - For the global optimization study on hybrid KG, it would be interesting to see performance compared to other recent kg work (e.g. one-shot KG, since that estimator formulation can be optimized with exact gradients)
Writing: - L120: this is a run-on sentence - Figure 2: left title “poster mean” -> “posterior mean” - Figure 4: mislabeled plots. The title says validation error, but many subplots appear to show validation accuracy. Also, “hyperaparameters” -> hyperparameters - L286: “best validation error (max y)” is contradictory - L293: “We apply this trick to all algorithms in this experiment”: what is “this experiment”? - The appendix is not using NeurIPS 2021 style files - I recommend giving the appendix a proofread:
Some things that jump out
P6: “poster mean”, “peicewise-linear”
P9: “sugggest”
Limitations and societal impacts are discussed, but the potential negative societal impacts could be expounded upon.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. In GCG, authors showed that their approach could be transferred to other LLMs. Thus, GCG could craft adversarial prompts and transfer them to other LLMs. It would be good if such a comparison could be included. A minor point: The jailbreaking percentage is low for certain LLMs.",hkjcdmz8Ro,ICLR_2024,"1. The technique contribution is week. The proposed method utilizes the LLM to refine the prompt. Thus, the performance of the proposed method heavily relies on the designed system prompt and LLMs. Moreover, the proposed method is based on heuristics, i.e., there is no insight for the proposed approach. But I understand those two points could be very challenging for LLM research.
2. The evaluation is not systematic. For instance, only 50 questions are used in the evaluation. Thus, it is unclear whether the proposed approach is generalizable. More importantly, is the judge model the same for the proposed algorithm and evaluation? If this is the case, it is hard to see whether the reported results are reliable as LLMs could be inaccurate in their predictions. It would be better if other metrics could be used for cross-validation, e.g., manually check and the word list used by Zou et al. 2023. The proposed method is only compared with GCG. There are also many other baselines, e.g., handcrafted methods (https://www.jailbreakchat.com/).
3. In GCG, authors showed that their approach could be transferred to other LLMs. Thus, GCG could craft adversarial prompts and transfer them to other LLMs. It would be good if such a comparison could be included.
A minor point: The jailbreaking percentage is low for certain LLMs.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2.Authors introduce the importance of unsupervised feature selection from a diffusion perspective and i think this is a very novel thing for feature selection, but i can't understand what is the difference between similarity and exit times in nature. I hope the author can give me a more detailed explanation to understand the difference.",NIPS_2021_835,NIPS_2021,"The authors addressed the limitations and potential negative societal impact of their work. However, there are some concerns as follows: 1.The main concern is the innovation of this paper. Firstly, laplacian score is proposed by Ref.13 for feature selection as an unsupervised measure. Secondly, i think that the main contribution of this paper is stochastic gates, but in Ref.36, the technology of stochastic gates is already used in supervised feature selection. Finally, authors focus on the traditional unsupervised feature selection problem. Thus i think that the core contribution of this paper is that authors extend the supervised problem in Ref.36 to the unsupervised problem without theoretical guarantees. Even authors introduce the importance of unsupervised feature selection from a diffusion perspective, but i don't think this is the core contribution of this article. For this question, if the authors can persuade me , I will change my score. 2.Authors introduce the importance of unsupervised feature selection from a diffusion perspective and i think this is a very novel thing for feature selection, but i can't understand what is the difference between similarity and exit times in nature. I hope the author can give me a more detailed explanation to understand the difference. 3.Authors sample a stochastic gate (STG) vector in algorithm 1 and thus i think that the proposed method should have randomness. But in the main experiment of this paper, i don't see this randomness analyzed by authors. 4.It would be better if the authors add some future work.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- As the unified framework can now obtain provably efficient learning for most POMDP formalisms. Is there any limitations of its, e.g. can it do the same for any general POMDP formulations (continuous or infinite spaces)?",NIPS_2022_1250,NIPS_2022,"Lacking of discussions or motivations for the importance of the proposed idea
Empirical results: Can be on toy tasks
The paper pursues an interesting research direction, which tries to unify existing POMDP formalisms. The approach looks very promising. The proposed design of the critic is very interesting. It would become very interesting if the paper can provides some basic empirical results on toy tasks to show all important claim in practice. - As the unified framework can now obtain provably efficient learning for most POMDP formalisms. Is there any limitations of its, e.g. can it do the same for any general POMDP formulations (continuous or infinite spaces)? - How can one understand agnostic learning? In Algorithm, is z just defined as historical observations? Or is it in the form of belief?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"• How did you calculate precision/recall/F1-score for 4-class classification of breast density? Also, for breast cancer detection, researchers usually report AUC with sensitivity and specificity at different operating points to compare model performance. It might be more informative to provide AUC results for comparisons.",ICLR_2023_4411,ICLR_2023,"Weakness • The reviewer thinks the authors need to elaborate how the output labels are defined for density assessment. In section 3. Datasets, it seems the authors gives confusing definitions of density and BIRADS findings like “we categorized BI-RADS density scores into two separate categories: BI-RADS 2 and 3 as benign and BI-RADS 5 and 6 as malignant”. There is no description about what “Density A”, “Density B”, “Density C”, and “Density D” mean. Also, as the reviewer knows, benign or malignant classification can be confirmed with biopsy results not BIRADS scores. Even though the reviewer is not familiar with the two public datasets, the reviewer thinks the datasets should have biopsy information to annotate lesions whether malignant or benign. • As a preprocessing step, the authors segmented and removed the region of the pectoral muscle from MLO views. However, the authors did not explain how the segmentation model was developed (they just mentioned employed the prior work) and the review has a concern that important features can be removed from this preprocessing step. It might be useful to compare model performance using MLO views with and without this preprocessing step to confirm the benefit of this pectoral muscle removal. • How did you calculate precision/recall/F1-score for 4-class classification of breast density? Also, for breast cancer detection, researchers usually report AUC with sensitivity and specificity at different operating points to compare model performance. It might be more informative to provide AUC results for comparisons. • The reviewer thinks comparison of their proposed approach with the single-view result is unfair. This is because information that multi views contain is 4x larger than the one that the single view has. So, to demonstrate the benefit of using the proposed fusion strategy, they need to report performance of multi-view results with simple fusion approach like the average/maximum of 4 view scores, or max over mean values of each breast. • Are the results reported in this study based on patient/study level? How did you calculate performance when using single views? Did you assume that each study has only one view? • What fusion strategy was used for results in Table 2? Are these results based on image level?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- I found that the creation of the dataset is optional. The Kialo dataset, well-studied in the community, provides exactly what the authors need, pairs of short claims and their counters. It is even cleaner than the dataset the authors created since no automatic processes exist to construct it. Still, what has been created in this paper can be extra data to learn from.",bpArUWbkUF,EMNLP_2023,"- There are some minor issues with the papers, but still, no strong reasons to reject them:
- I found that the creation of the dataset is optional. The Kialo dataset, well-studied in the community, provides exactly what the authors need, pairs of short claims and their counters. It is even cleaner than the dataset the authors created since no automatic processes exist to construct it. Still, what has been created in this paper can be extra data to learn from.
- The related work, especially regarding counter-argument generation, was shortly laid out with little elaboration about how previous works addressed that task and the implications.
- In the abstract, the authors claim they trained Arg-Judge with human preference. However, looking at the details of the data that the model is trained on, it turns out that the data is automatically created and does not precisely reflect human preferences.
- The procedure of creating the seed instructions, expanding them, and mapping them to inputs needed to be clarified. Providing examples here would be very helpful.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Transformer has been adopted for lots of NLP and vision tasks, and it is no longer novel in this field. Although the authors made a modification on the transformer, i.e. cross-layer, it does not bring much insight in aspect of machine learning. Besides, in ablation study (table4 and 5), the self-cross attention brings limited improvement (<1%). I don’t think this should be considered as significant improvement. It seems that the main improvements over other methods come from using a naïve transformer instead of adding the proposed modification.",NIPS_2021_2050,NIPS_2021,"1. Transformer has been adopted for lots of NLP and vision tasks, and it is no longer novel in this field. Although the authors made a modification on the transformer, i.e. cross-layer, it does not bring much insight in aspect of machine learning. Besides, in ablation study (table4 and 5), the self-cross attention brings limited improvement (<1%). I don’t think this should be considered as significant improvement. It seems that the main improvements over other methods come from using a naïve transformer instead of adding the proposed modification. 2. This work only focuses on a niche task, which is more suitable for CV conference like CVPR rather than machine learning conference. The audience should be more interested in techniques that can work for general tasks, like general image retrieval. 3. The proposed method uses AdamW with cosine lr for training, while comparing methods only use adam with fixed lr. Directly comparing with their numbers in paper is unfair. It would be better to reproduce their results using the same setting, since most of the recent methods have their code released.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- In terms of the experiments, I consider the number of tasks quite limited. To be convinced I would like to see several tasks (at least 10) and sequential results in terms of tasks learned rather than epochs. Questions for authors: Please address my comments on the weaknesses above.",ICLR_2021_1849,ICLR_2021,"I see in this paper are: - Although there is a clear and formal explanation of why it is not possible to discriminate among classes from different task when there is no access to data from those previous classes, I am not fully convinced that the set of parameters kept from previous classes, and used in regularization-based approaches, do not represent to some extent this data. In particular, there is no clear argument for the claim on page 5: “However, by hypothesis, \omega_{t-1} does not model the data distribution from C_{t-1} and therefore it does not model data distribution from C_{t-1} classes.”. I would like to see some discussion regarding how fairly a set of parameters \theta_{t-1} would represent the S’ set. - In terms of the experiments, I consider the number of tasks quite limited. To be convinced I would like to see several tasks (at least 10) and sequential results in terms of tasks learned rather than epochs.
Questions for authors:
Please address my comments on the weaknesses above.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.",ICLR_2022_212,ICLR_2022,"Weakness: 1. The introduction of the motivation (the concept of in-context bias) is not easy to understand at the very beginning. The paper said: “the pretrained NLM can model much stronger dependencies between text segments that appeared in the same training example, than it can between text segments that appeared in different training examples.” Acutally it seems quite natural for me and I did not realize it is a problem until I saw more explanations in section 1.1. 2. The theory is a bit complicated and not easy to follow. 3. The experiments are limited. The authors only conduct the evaluation on sentence similarity tasks and open domain QA tasks. However, there are many other tasks that involve sentence pairs. For example, sentence inference tasks such as MNLI and RTE are common tasks in NLP field. The authors should conduct experiments on more types of sentence pair tasks.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"6. It's important to have the prompt included in the appendix or supplement. Was it possibly in a supplement that I cannot access? ### **Minor comments** (These did not affect my score) - Abstract: Lines 016-019 are a bit difficult to understand; consider rephrasing - Figure 2: It’s unclear what this Figure is meant to convey, and the Figure lacks labeled y-axes.",o3V7OuPxu4,ICLR_2025,"Overall, the paper lacks clarity and depth in describing both the technical implementation and practical contributions.
### **Major comments**
1. Unclear contribution: The paper does not effectively justify why this benchmark must exist as a standalone contribution rather than an addition to existing Starcraft II resources. The contribution seems limited to a collection of scripts and metrics, which could likely be integrated into the existing environment without creating a separate benchmark.
2. Lack of implementation details: Key technical aspects of the implementation are insufficiently described, making it hard to understand the benchmark's novelty and how it's technically realized. Several things are not clear, such as:
- Integration: How are LLM agents integrated with StarCraft II? How can users use the benchmark? Does the benchmark use a custom API or an interface for this?
- Decision Tracking: How is decision-making tracked and analyzed? While Table 3 provides a decision trajectory, details of how this is analyzed and used are missing.
- Computational Requirements: What hardware/software is necessary to run this benchmark effectively? This information is critical for usability but is absent.
- Opponents: Are the LLMs evaluated with built-in agents or newly introduced opponents? The fact that agents are evaluated against built-in agents in Starcraft II is mentioned as a limitation, but it is unclear whether the authors change this in their benchmark.
3. Incomplete metric information: The metrics lack context. For instance, while Appendix A.1 outlines the metrics, there are no defined ranges, leaving the reader unsure of how to interpret scores. For example, how should a Real-Time Decision score of 21.12 versus 37.51 in Table 4 be interpreted? Similarly, terms such as “effective” actions in EPM or “collected vespene” are not unexplained, reducing the metrics’ interpretability (how do we know that these are the right metrics to assess decision-making and planning?).
4. Missing benchmark discussion and limitations: A discussion about future development and limitations of the benchmark is missing, which limits the reader's understanding of the benchmark's intended scope and future extensions.
5. Figure 2 indicates a large variance. Why are there no error bars in the tables?
6. It's important to have the prompt included in the appendix or supplement. Was it possibly in a supplement that I cannot access?
### **Minor comments** (These did not affect my score)
- Abstract: Lines 016-019 are a bit difficult to understand; consider rephrasing
- Figure 2: It’s unclear what this Figure is meant to convey, and the Figure lacks labeled y-axes.
- In Section 4.3, line 367 states ""Definitions and methods for these metrics will be further detailed in the figure 4.3."" This seems to refer to a table, possibly Table 3, rather than a figure.
- In Table 3, ""OBSERVERtgreater"" should probably be ""OBSERVER.""
- Lines 323 + 350 state that screenshots illustrating decision traces will be provided in the appendix, but these are not included
- I don't understand what is meant when the authors state that civilization and the other games are not ""strategic and tactical"" in Table 1. Additionally, Werewolf is clearly an imperfect information game. The authors should reconsider this table because I believe many of the entries are inaccurate.
- Why is the score in Table 4 unnormalized? It's an incomprehensible number as it stands.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
* The motivation for analyzing only the last convolutional layer is not clear. Why would numerosity not appear in earlier layers?,NIPS_2020_153,NIPS_2020,"* Both this paper and the Nasr et al paper use number detectors (number selective units) as an indicator of number sense. However, the presence of number selective units is not a necessary condition for number sense. There are potentially other distributed coding schemes (other than tuning curves) that could be employed. It seems like the question that you really want to ask is whether the representation in the last convolutional layer is capable of distinguishing images of varying numerosity. In which case, why not just train a linear probe? Number sense is a cognitive ability, not a property of individual neurons. We don't really care what proportion of units are number selective as long as the network is able to perceive numerosity (which might not require very many units). A larger proportion of number selective units doesn't necessarily imply a better number sense. As such, I question the reliance on the analysis of individual units and would rather see population decoding results. * The motivation for analyzing only the last convolutional layer is not clear. Why would numerosity not appear in earlier layers? * The motivation for using classification rather than regression when training explicitly for numerosity is not well justified. The justification, ""numerosity is a raw perception rather than resulting from arithmetic"", is not clear. Humans clearly perceive numbers on a scale not as unrelated categories. That the subjective experience of numerosity does not involve arithmetic does not constrain the neural mechanisms that could underly that perception. * No effect sizes are reported for number selectivity. Since you did ANOVAs there should be an eta squared for the main effect of numerosity. How number selective are these units?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
* How to set the parameter S remains a problem.,NIPS_2018_761,NIPS_2018,"Weakness] * How to set the parameter S remains a problem. * Algorithm SMILE is interesting but their theoretical results on its performance is not easy to interpret. * No performance comparison with existing algorithms [Recommendation] I recommend this paper to be evaluated as ""a good submission; an accept"". Their problem formalization is clear, and SMILE algorithm and its theoretical results are interesting. All their analyses are asymptotically evaluated, so I worry about how large the constant factors are. It would make this manuscript more valuable if how good their algorithms (OOMM & SMILE) would be shown theoretically and empirically compared to other existing algorithms. [Detailed Comments] p.7 Th 3 & Cor 1: C^G and C^B look random variables. If it is true, then they should not be used as parameters of T's order. Maybe the authors want to use their upper bounds shown above instead of them. p.7 Sec. 5 : Write the values of parameter S. [Comments to Authorsâ Feedback] Setting parameter S: Asymptotic relation shown in Th 3 is a relation between two functions. It is impossible to estimate S from estimated M for a specific n using such a asymptotic functional relation.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
2. A human evaluation for caption generation would be more convincing as the automatic evaluation metrics can be misleading.,NIPS_2016_265,NIPS_2016,"1. For the captioning experiment, the paper compares to related work only on some not official test set or dev set, however the final results should be compared on the official COOC leader board on the blind test set: https://competitions.codalab.org/competitions/3221#results e.g. [5,17] have won this challenge and have been evaluated on the blind challenge set. Also, several other approaches have been proposed since then and significantly improved (see leaderboard, the paper should at least compare to the once where an corresponding publication is available). 2. A human evaluation for caption generation would be more convincing as the automatic evaluation metrics can be misleading. 3. It is not clear from Section 4.2 how the supervision is injected for the source code caption experiment. While it is over interesting work, for acceptance at least points 1 and 3 of the weaknesses have to be addressed. ==== post author response === The author promised to include the results from 1. in the final For 3. it would be good to state it explicitly in Section section 4.2. I encourage the authors to include the additional results they provided in the rebuttal, e.g. T_r in the final version, as it provides more insight in the approach. Mine and, as far as I can see, the other reviewers concerns have been largely addressed, I thus recommend to accept the paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned.",NIPS_2018_630,NIPS_2018,"- While there is not much related work, I am wondering whether more experimental comparisons would be appropriate, e.g. with min-max networks, or Dugas et al., at least on some dataset where such models can express the desired constraints. - The technical delta from monotonic models (existing) to monotonic and convex/concave seems rather small, but sufficient and valuable, in my opinion. - The explanation of lattice models (S4) is fairly opaque for readers unfamiliar with such models. - The SCNN architecture is pretty much given as-is and is pretty terse; I would appreciate a bit more explanation, comparison to ICNN, and maybe a figure. It is not obvious for me to see that it leads to a convex and monotonic model, so it would be great if the paper would guide the reader a bit more there. Questions: - Lattice models expect the input to be scaled in [0, 1]. If this is done at training time using the min/max from the training set, then some test set samples might be clipped, right? Are the constraints affected in such situations? Does convexity hold? - I know the author's motivation (unlike ICNN) is not to learn easy-to-minimize functions; but would convex lattice models be easy to minimize? - Why is this paper categorized under Fairness/Accountability/Transparency, am I missing something? - The SCNN getting ""lucky"" on domain pricing is suspicious given your hyperparameter tuning. Are the chosen hyperparameters ever at the end of the searched range? The distance to the next best model is suspiciously large there. Presentation suggestions: - The introduction claims that ""these shape constraints do not require tuning a free parameter"". While technically true, the *choice* of employing a convex or concave constraint, and an increasing/decreasing constraint, can be seen as a hyperparameter that needs to be chosen or tuned. - ""We have found it easier to be confident about applying ceterus paribus convexity;"" -- the word ""confident"" threw me off a little here, as I was not sure if this is about model confidence or human interpretability. I suspect the latter, but some slight rephrasing would be great. - Unless I missed something, unconstrained neural nets are still often the best model on half of the tasks. After thinking about it, this is not surprising. It would be nice to guide the readers toward acknowledging this. - Notation: the x[d] notation is used in eqn 1 before being defined on line 133. - line 176: ""corresponds"" should be ""corresponding"" (or alternatively, replace ""GAMs, with the"" -> ""GAMs; the"") - line 216: ""was not separately run"" -> ""it was not separately run"" - line 217: ""a human can summarize the machine learned as"": not sure what this means, possibly ""a human can summarize what the machine (has) learned as""? or ""a human can summarize the machine-learned model as""? Consider rephrasing. - line 274, 279: write out ""standard deviation"" instead of ""std dev"" - line 281: write out ""diminishing returns"" - ""Result Scoring"" strikes me as a bit too vague for a section heading, it could be perceived to be about your experiment result. Is there a more specific name for this task, maybe ""query relevance scoring"" or something? === I have read your feedback. Thank you for addressing my observations; moving appendix D to the main seems like a good idea. I am not changing my score.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. **Triviality of Convergence Proof**: The theoretical proof for convergence appears trivial. Although the paper claims that $Z$ is non-i.i.d., Assumption 4.1 indicates that $X$ is i.i.d., leading to a clear covariance matrix for $Z$ as $A^\top A / np$. Following Modification 1 in Appendix C, previous theorems can be trivially adapted with straightforward modifications. Thus, the convergence proof lacks substantial novelty and rigor.",wRbSdbGyfj,ICLR_2025,"1. **Triviality of Convergence Proof**: The theoretical proof for convergence appears trivial. Although the paper claims that $Z$ is non-i.i.d., Assumption 4.1 indicates that $X$ is i.i.d., leading to a clear covariance matrix for $Z$ as $A^\top A / np$. Following Modification 1 in Appendix C, previous theorems can be trivially adapted with straightforward modifications. Thus, the convergence proof lacks substantial novelty and rigor.
2. **Limited Parameterization**: According to Equation 2.2, it seems that only the final logits layer contains parameters $\beta$, while the preceding $S^M X$ lacks parameters. The absence of parameters in these earlier layers raises concerns about why only the last layer is parameterized, which could lead to over-smoothing due to unparameterized iterations of $S X$ and consequently limit the model’s expressiveness.
3. **Basic Transfer Learning Approach**: The transfer learning method employed, a simple $\delta$ fine-tuning, appears overly basic. There is little exploration of alternative, established methods in transfer learning or meta-learning that could potentially enhance the model’s adaptability and robustness.
4. **Issues in Hyperparameter Sensitivity Testing**: The sensitivity experiments on hyperparameters are limited. For instance, in the $\lambda$ experiment, the model fails to achieve the optimal solution seen at $M=5$. Additionally, the range of $\lambda$ tested is narrow; a broader, exponential scale (e.g., 0.01, 0.001, 0.0001) would provide a more comprehensive understanding of the model’s sensitivity.
5. **Lack of Notational Clarity**: The notation lacks clarity and could benefit from a dedicated section outlining all definitions. Many symbols, such as $X_j$, are undefined in Appendix A. A coherent notation guide would improve readability and help readers follow the technical details more effectively.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"5) The experimental setup borrowed from [2] is only semi-real, as multi-node seed cascades are artificially created by merging single-node seed cascades. This should be mentioned clearly.",NIPS_2016_232,NIPS_2016,"weakness of the suggested method. 5) The literature contains other improper methods for influence estimation, e.g. 'Discriminative Learning of Infection Models' [WSDM 16], which can probably be modified to handle noisy observations. 6) The authors discuss the misestimation of mu, but as it is the proportion of missing observations - it is not wholly clear how it can be estimated at all. 5) The experimental setup borrowed from [2] is only semi-real, as multi-node seed cascades are artificially created by merging single-node seed cascades. This should be mentioned clearly. 7) As noted, the assumption of random missing entries is not very realistic. It would seem worthwhile to run an experiment to see how this assumption effects performance when the data is missing due to more realistic mechanisms.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Limited datasets and models. The bias benchmarks only assess gender, race, and religion. Other important biases and datasets are not measured. Also missing are assessments on state-of-the-art generative models like GPT.",pO7YD7PADN,EMNLP_2023,"1. Limited technical contributions. The compression techniques evaluated are standard existing methods like quantization and distillation. The debiasing baselines are also from prior work. There is little technical innovation.
2. Limited datasets and models. The bias benchmarks only assess gender, race, and religion. Other important biases and datasets are not measured. Also missing are assessments on state-of-the-art generative models like GPT.
3. Writing logic needs improvement. Some parts, like introducing debiasing baselines in the results, make the flow confusing.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'boda'], 'labels': ['1', '0']}",,
2. It is stated both that the multi-env model has an inevitable performance loss and that the multi-env model outperforms the single-env model due to knowledge sharing. These two statements seem to be conflicting. Please clarify.,NIPS_2020_342,NIPS_2020,"1. The primary motivation for the work is not well supported. Certainly, cities do manage thousands of intersections. While unquantified, it is not clear that the cost of training individually would surpass that of the degradation seen in the multi-env setting. 2. It is stated both that the multi-env model has an inevitable performance loss and that the multi-env model outperforms the single-env model due to knowledge sharing. These two statements seem to be conflicting. Please clarify. 3. In section 5.1, the single-env results, it is not clear that FRAP is only applicable in 37 of the 112 cases. As there is quite a lot of recent work on the single-env TSCP. It would have been better to compare to a less restrictive baseline. Such methods can be found in the following: a. Shabestary, Soheil Mohamad Alizadeh, and Baher Abdulhai. ""Deep learning vs. discrete reinforcement learning for adaptive traffic signal control."" International Conference on Intelligent Transportation Systems (ITSC). IEEE, 2018. b. Ault, James, et al. ""Learning an Interpretable Traffic Signal Control Policy."" International Conference on Autonomous Agents and MultiAgent Systems. AAMAS, 2020. c. Liang, Xiaoyuan, et al. ""Deep reinforcement learning for traffic light control in vehicular networks."" IEEE Transactions on Vehicular Technology. IEEE, 2019. 4. In the supplementary material it is stated: “AttendLight in single-env regime outperforms other algorithms in 107 cases out of 112 cases“ and that AttendLight reduces ATT by 10% on average over FRAP. While it is clear how attention is useful in the multi-env setting, could you please add some analysis as to why it is expected to outperform an algorithm designed for single intersections? 5. As it is proposed in the paper that the method is suitable for city-wide control, it is important to provide an analysis of the worst-case results of the method. If on average traffic is alleviated, but certain intersections become nearly impassable this would not be a viable solution. A glance at the numbers in the supplement shows this method may result in some intersections experiencing a 78% increase in average travel time. Please provide such a worse case analysis.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The method seems to be quite sensitive to hyperparameters, so in order to apply it method for a new problem, one has to perform some careful hyperparameter search to find a proper $\alpha$.",oqDoAMYbgA,ICLR_2024,"1. The experimental study is limited: the comparisons with other methods are provided only on a single Wiki-small dataset. From that, it’s not enough to judge on the comparison with other baselines.
2. The training time seems to be the main bottleneck of the method, its training is slower than for almost any other tree method (as reported in the paper). Probably because of that, applying the method on bigger datasets becomes infeasible. (Fair to say, that the same shortcoming applies for the original Softmax Tree, and the presented method seems to double the training time).
3. The method seems to be quite sensitive to hyperparameters, so in order to apply it method for a new problem, one has to perform some careful hyperparameter search to find a proper $\alpha$.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
1. The description of the metrics is limited. it would be desirable to have an explanation of the metrics used in the paper. Or at least a citation to the metrics would have been good.,haPIkA8aOk,EMNLP_2023,"1. The description of the metrics is limited. it would be desirable to have an explanation of the metrics used in the paper. Or at least a citation to the metrics would have been good.
2. The training objective in Equation 7 would increase the likelihood of negative cases as well resulting in unwanted behavior. Should the objective be: \mathcal{L}_{c} - \mathcal{L}_{w}?
3. The paper needs a bit of polishing as at times equations are clubbed together. The equations in Sections 4 and 5 can be clubbed together while introducing them.
4. The paper motivates by the fact that we need to generate multiple sequences during the test time and progress to get rid of them. However, ASPIRE generates multiple answers during the training phase. This should be explicitly mentioned in the paper as it directly conflicts with the claim of not generating multiple sequences.
5. A bit more analysis on the impact of the number of model parameters is warranted.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Figure 3 is challenging to understand. The workflow and captions are unclear, and the representation of communication modes on the left side is confusing.",30kbnyD9hF,EMNLP_2023,"- Lack of reference explaining communication in this context.
- The paper introduces four communication modes (debate, report, relay, and memory) without sufficient support from literature, despite existing relevant work in argumentation theory. Section 4.2 provides inadequate details and lacks illustrative examples.
- Figure 3 is challenging to understand. The workflow and captions are unclear, and the representation of communication modes on the left side is confusing.
- Figure 4's tabular representation of node agent interactions is not intuitive.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- It is unclear what is the ""learned [MASK] embedding"" mean in the SSL pre-training stage of the proposed method.",ICLR_2023_2630,ICLR_2023,"- The technical novelty and contributions are a bit limited. The overall idea of using a transformer to process time series data is not new, as also acknowledged by the authors. The masked prediction was also used in prior works e.g. MAE (He et al., 2022). The main contribution, in this case, is the data pre-processing approach that was based on the bins. The continuous value embedding (CVE) was also from a prior work (Tipirneni & Reddy 2022), and also the early fusion instead of late fusion (Tipirneni & Reddy, 2022; Zhang et al., 2022). It would be better to clearly clarify the key novelty compared to previous works, especially the contribution (or performance gain) from the data pre-processing scheme.
- It is unclear if there are masks applied to all the bins, or only to one bin as shown in Fig. 1.
- It is unclear how the static data (age, gender etc.) were encoded to input to the MLP. The time-series data was also not clearly presented.
- It is unclear what is the ""learned [MASK] embedding"" mean in the SSL pre-training stage of the proposed method.
- The proposed ""masked event dropout scheme"" was not clearly presented. Was this dropout applied to the ground truth or the prediction? If it was applied to the prediction or the training input data, will this be considered for the loss function?
- The proposed method was only evaluated on EHR data but claimed to be a method designed for ""time series data"" as in both the title and throughout the paper. Suggest either tone-down the claim or providing justification on more other time series data.
- The experimental comparison with other methods seems to be a bit unfair. As the proposed method was pre-trained before the fine-tuning stage, it is unclear if the compared methods were also initialised with the same (or similar scale) pre-trained model. If not, as shown in Table 1, the proposed method without SSL performs inferior to most of the compared methods.
- Missing reference to the two used EHR datasets at the beginning of Sec. 4.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- The reported results seem to be partially derivative: extension to hyper-networks of results already presented in the literature for standard networks.,NIPS_2020_1108,NIPS_2020,"- The reported results seem to be partially derivative: extension to hyper-networks of results already presented in the literature for standard networks. - The case with finite width for f and infinite width for g is not discussed: it would have provided a complete treatment of the topic. - Presentation could be improved, first of all by removing typos (see additional comments), and then by providing more background on NTK and GP.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The paper starts with the objective of designing fast label aggregation algorithms for a streaming setting. But it doesnât spend any time motivating the applications in which such algorithms are needed. All the datasets used in the empirical analysis are static datasets. For the paper to be useful, the problem considered should be well motivated.",NIPS_2019_1145,NIPS_2019,"The paper has the following main weaknesses: 1. The paper starts with the objective of designing fast label aggregation algorithms for a streaming setting. But it doesnât spend any time motivating the applications in which such algorithms are needed. All the datasets used in the empirical analysis are static datasets. For the paper to be useful, the problem considered should be well motivated. 2. It appears that the output from the algorithm depends on the order in which the data are processed. This should be clarified. 3. The theoretical results are presented under the assumption that the predictions of FBI converge to the ground truth. Why should this assumption be true? It is not clear to me how this assumption is valid for finite R. This needs to clarified/justified. 3. The takeaways from the empirical analysis are not fully clear. It appears that the big advantage of the proposed methods is their speed. However, the experiments donât seem to be explicitly making this point (the running times are reported in the appendix; perhaps they should be moved to the main body). Plus, the paper is lacking the key EM benchmark. Also, perhaps the authors should use a different dataset in which speed is most important to showcase the benefits of this approach. Update after the author response: I read the author rebuttal. I suggest the authors to add the clarifications they detailed in the rebuttal to the final paper. Update after the author response: I read the author rebuttal. I suggest the authors to add the clarifications they detailed in the rebuttal to the final paper. Also, the motivating crowdsourcing application where speed is really important is not completely clear to me from the rebuttal. I suggest the authors clarify this properly in the final paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The scope of the study is under-specified. It seems that the work focuses on injecting CoT- based approach to small-scale Language Models. If that is not the case, additional relevant CoT baselines for in-context learning of Large Language Models (for text-003 and ChatGPT) are missing in Table 2 and 3 (See Question A).",43SOcneD8W,EMNLP_2023,"1. The reported performance gain of the proposed framework is marginal when compared to the improvements introduced by simple Prompt Tuning approaches. For instance,for Table 3, out of 2.7% gain over Roberta backbone on ReTACRED, prompting tuning (i.e. HardPrompt) already achieves the gain of 1.7%.
2. The scope of the study is under-specified. It seems that the work focuses on injecting CoT- based approach to small-scale Language Models. If that is not the case, additional relevant CoT baselines for in-context learning of Large Language Models (for text-003 and ChatGPT) are missing in Table 2 and 3 (See Question A).
3. The major components of the proposed frameworks are CCL and PR. Both of them are incremental over the previous methods with minor adaptation for CoT-based prompting proposal.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- Figure 3 is very hard to read anything on the figure.,NIPS_2016_117,NIPS_2016,"weakness of this work is impact. The idea of ""direct feedback alignment"" follows fairly straightforwardly from the original FA alignment work. Its notable that it is useful in training very deep networks (e.g. 100 layers) but its not clear that this results in an advantage for function approximation (the error rate is higher for these deep networks). If the authors could demonstrate that DFA allows one to train and make use of such deep networks where BP and FA struggle on a larger dataset this would significantly enhance the impact of the paper. In terms of biological understanding, FA seems more supported by biological observations (which typically show reciprocal forward and backward connections between hierarchical brain areas, not direct connections back from one region to all others as might be expected in DFA). The paper doesn't provide support for their claim, in the final paragraph, that DFA is more biologically plausible than FA. Minor issues: - A few typos, there is no line numbers in the draft so I haven't itemized them. - Table 1, 2, 3 the legends should be longer and clarify whether the numbers are % errors, or % correct (MNIST and CIFAR respectively presumably). - Figure 2 right. I found it difficult to distinguish between the different curves. Maybe make use of styles (e.g. dashed lines) or add color. - Figure 3 is very hard to read anything on the figure. - I think this manuscript is not following the NIPS style. The citations are not by number and there are no line numbers or an ""Anonymous Author"" placeholder. - I might be helpful to quantify and clarify the claim ""ReLU does not work very well in very deep or in convolutional networks."" ReLUs were used in the AlexNet paper which, at the time, was considered deep and makes use of convolution (with pooling rather than ReLUs for the convolutional layers).","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- In the introduction the authors mention the fact that tensor decomposition is in general harder in the symmetric than in the non-symmetric case. How is this connected with recent findings about the `nice' landscape of the objective function associated with the decomposition of symmetric (orthogonal) order-4 tensors [1]?,NIPS_2018_756,NIPS_2018,"It looks complicated to assess the practical impact of the paper. On the one hand, the thermodynamic limit and the Gaussianity assumption may be hard to check in practice and it is not straightforward to extrapolate what happens in the finite dimensional case. The idea of identifying the problem's phase transitions is conceptually clear but it is not explicitly specified in the paper how this can help the practitioner. The paper only compares the AMP approach to alternate least squares without mention, for example, positive results obtained in the spectral method literature. Finally, it is not easy to understand if the obtained results only regard the AMP method or generalize to any inference method. Questions: - Is the analysis restricted to the AMP inference? In other words, could a tensor that is hard to infer via AMP approach be easily identifiable by other methods (or the other way round)? - Are the easy-hard-impossible phases be related with conditions on the rank of the tensor? - In the introduction the authors mention the fact that tensor decomposition is in general harder in the symmetric than in the non-symmetric case. How is this connected with recent findings about the `nice' landscape of the objective function associated with the decomposition of symmetric (orthogonal) order-4 tensors [1]? - The Gaussian assumption looks crucial for the analysis and seems to be guaranteed in the limit r << N. Is this a typical situation in practice? Is always possible to compute the `effective' variance for non-gaussian outputs? Is there a finite-N expansion that characterize the departure from Gaussianity in the non-ideal case? - For the themodynamic limit to hold, should one require N_alpha / N = O(1) for all alpha? - Given an observed tensor, is it possible to determine the particular phase it belongs to? [1] Rong Ge and Tengyu Ma, 2017, On the Optimization Landscape of Tensor Decompositions","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
245 The GAT is trained with the whole model? Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity.,ARR_2022_358_review,ARR_2022,"- Some definitions and statements are not clear or well justified.
- Lack of clarity in the definition of the input/outputs for each subtask
063-065 Though most of the existing studies consider the expansion a regression problem ... -> Missing a reference to support this statement 081-082 TEAM that performs both the Attach and Merge operations together -> Performs attach and merge together or is trained together?
092 Missing reference for wordnet definition 112-114 ""The taxonomy T is arranged in a hierarchical manner directed edges in E as shown in Figure 1."" - > It doesn't seem clear.
115 query concept q: Is this a concept or a candidate word? Your initial examples (Page 1) mention ""mango"" and ""nutrient"" and do not seem to be concepts according to your definition.
188 the query synset sq -> Is this a query synset or a word that belongs to the synset (concept) X. I am not sure if I understand correctly but in the case of attach, the query concept is a (d, ss): definition, synonymns included in the synset. However, it is not clear to me if it is the same for merge as it seems like the query concept is (d, ss) but ss is just the word that you are removing.
214 and the synset is represented by the pre-trained embedding of the synonym word itself. - > It applies for merge in most cases but it is not case for attach, right? Because attach considers candidate concept (that can be composed by a synonym set) 224 comprising of the node -> that comprises the node ... 245 The GAT is trained with the whole model?
Needs to be reviewed by a English native speaker and some sentences need to be rewriting for improving the clarity.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Replacing `n^2/(2*s^2)` with an arbitrary parameter `lambda` (lines 119-121) 2. Taking SGD learning rate ~ 0.1 (line 164) — unlike the Adam default value, it is unclear what the justification behind this value is.",NIPS_2020_1524,NIPS_2020,"* The paper makes several “hand-wavy” arguments, which are suitable for supporting the claims in the paper; but it is unclear if they would generalize for analyzing / developing other algorithms. For instance: 1. Replacing `n^2/(2*s^2)` with an arbitrary parameter `lambda` (lines 119-121) 2. Taking SGD learning rate ~ 0.1 (line 164) — unlike the Adam default value, it is unclear what the justification behind this value is.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- Error analysis plays a crucial role in evaluating model performance and identifying potential issues. We encourage the authors to conduct error analysis in the paper and provide detailed explanations of the model's performance under different scenarios. Error analysis will aid in guiding subsequent improvements and expansions of the ERC research.,2z9o8bMQNd,EMNLP_2023,"-	So difficult to follow the contribution of this paper. And it looks like an incremental engineering paper. The proposed method has been introduced in many papers, such as [1] Joshi, A., Bhat, A., Jain, A., Singh, A., & Modi, A. (2022, July). COGMEN: COntextualized GNN-based Multimodal Emotion Recognition. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 4148-4164).
-	The related work should be updated with more recent related works.
-	The experimental section needs some significance tests to further verify the effectiveness of the method put forward in the paper.
-	For the first time appearing in the text, the full name must be written, and abbreviations must be written in parentheses. When it appears in the abstract, it needs to be written once, and when it appears in the main text, it needs to be repeated again, that is, the full name+parentheses (abbreviations) should appear again.
-	Error analysis plays a crucial role in evaluating model performance and identifying potential issues. We encourage the authors to conduct error analysis in the paper and provide detailed explanations of the model's performance under different scenarios. Error analysis will aid in guiding subsequent improvements and expansions of the ERC research.
-	Writing mistakes are common across the overall paper, which could be found in “Typos, Grammar, Style, and Presentation Improvements”.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2) analyze the domain gap. It would be nice to add some discussions about the gap between datasets. Some datasets are closer to each other thus the adaption may not be a big issue. Also, if the method is able to finetune a pre-trained model on synthetic data, then the value of the approach would be much higher.",NIPS_2022_742,NIPS_2022,"It seems that the 6dof camera poses of panoramas are required to do the projection. Hence, precisely speaking, the method is not fully self-supervised but requires camera pose ground truth. This is usually accessible, easier compared to the ground truth layout, but may also cause error for the layout projection and thus hurts the overall finetuning performance.
The experiment could be stronger to demonstrate the effectiveness of the method from two aspects: 1) a stronger baseline. It seems SSLayout360 is in general outperforming HorizonNet. It would be convincing to show that this method is able to improve powerful backbones. 2) analyze the domain gap. It would be nice to add some discussions about the gap between datasets. Some datasets are closer to each other thus the adaption may not be a big issue. Also, if the method is able to finetune a pre-trained model on synthetic data, then the value of the approach would be much higher.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1) Scalability. As shown in Table 3 (a), the performance is getting worse with growth of the maximum number of identities. It means that the capacity should be preset to some small number (e.g., 10). In real-world scenario, we can have more than 10 objects and most of the time we don't know how many objects we will need to handle in the future. Have the authors thought about how to scale up without compromising performance?",NIPS_2021_2163,NIPS_2021,"Weakness:
I have some concerns on identification mechanism based on identity bank. 1) Scalability. As shown in Table 3 (a), the performance is getting worse with growth of the maximum number of identities. It means that the capacity should be preset to some small number (e.g., 10). In real-world scenario, we can have more than 10 objects and most of the time we don't know how many objects we will need to handle in the future. Have the authors thought about how to scale up without compromising performance? 2) Randomness. Identities are randomly assigned one embedding from the identity bank. How the results are robust against this randomness? It would be undesirable for the result to change with each inference. It would be great to have some analysis on this aspect.
Overall Evaluation:
The paper present a novel approach for multi-object video object segmentation and the proposed method outperfrom previous state-of-the-arts on several benchmarks.
Now, I would recommend to accept this paper. I will finalize the score after seeing how authors address my concerns in Weakness.
While future works are discussed in Supplementary Materials, I encourage the authors to include more discussions on limitations and societal impacts.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The authors claim it to be one of the preliminary works discussing the application of LLP to NLP tasks. However, I don't see anything NLP-specific in their approach.",EtNebdSBpe,EMNLP_2023,"- The paper is hard to read and somewhat difficult to follow.
- The motivation is unclear. The authors argue that the LLP setup is relevant for (1) privacy and (2) weak supervision. (1) Privacy: the authors claim that the LLP paradigm is relevant for training on sensitive data as the labels for such datasets are not publicly available. However, the setting proposed in this paper does require gold (and publicly available) labels to formulate the ground truth proportion. If this proportion can be formulated without gold labels, it should be discussed. (2) Weak Supervision: in lines 136-137, the authors mention that the associated label proportions ""...provides the weak supervision for training the model"". However, weak supervision is a paradigm in which data is automatically labeled with noisy labels using some heuristics and labeling functions. It remains unclear to me in what way this setting is related to the proportion parameter authors use in their work.
- The authors claim it to be one of the preliminary works discussing the application of LLP to NLP tasks. However, I don't see anything NLP-specific in their approach.
- Not all theoretical groundings seem to be relevant to the main topic (e.g., some of the L_dppl irregularities). Additional clarification of their relevance is needed.
- Section 3.3 says the results are provided for binary classifiers only, and the multi-class setting remains for future work. However, one of the datasets used for experiments is multi-label.
- The experimental setting is unclear: does Table 1 contain the test results of the best model? If so, how was the best model selected (given that there is no validation set)? Also, if the proposed method is of special relevance to the sensitive data, why not select a sensitive dataset to demonstrate the method's performance on it? Or LLP data?
- The authors claim the results to be significant. However, no results of significance testing are provided.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- For comparison, at least one NCE-based method should be included. [1] shows that with a strong noise distribution, this line of work is possible to learn EBM on natural images.",NIPS_2020_1519,NIPS_2020,"- The proposed gradient unrolling method requires N steps to unrolling the gradient, which is slow and perhaps difficult to scale up to learning large and complicated EBLVMs. Although corollary 3 indicates that the estimation accuracy can be asymptoticly arbitrarily small, that requires N to be sufficiently large that may exceed the computing limit. - For comparison, at least one NCE-based method should be included. [1] shows that with a strong noise distribution, this line of work is possible to learn EBM on natural images. - In Table 2, it seems that higher dimension of h leads to worse result. Possible reason needs to be discussed. - Figure 4 shows that the learning can be unstable. [1] Flow Contrastive Estimation of Energy-Based Models","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. The experiment section could be improved. For example, it is better to carry significance test on the human evaluation results. It is also beneficial to compare the proposed method with some most recent LLM.",Akk5ep2gQx,EMNLP_2023,"1. The experiment section could be improved. For example, it is better to carry significance test on the human evaluation results. It is also beneficial to compare the proposed method with some most recent LLM.
2. The classifier of determining attributes using only parts of the sentence may not perform well. Specifically, I am wondering what is the performance of the attribute classifer obtained using Eq.2 and Eq.7.
3. Some of the experiment results could be explained in more details. For example, the author observes that ""Compared to CTRL, DASC has lower Sensibleness but higher Interestingness"", but why? Is that because DASC is bad for exhibiting Sensibleness? Similar results are also observed in Table1.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The authors claim that there is still no research focusing on the joint error for UDA. But this problem of arbitrarily increased joint error has already been studied in previous works like “Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment”, in ICML2019. The authors should discuss on that work and directly illustrate the relationship between that work and the proposed one, and why the proposed method is better.",ICLR_2021_2802,ICLR_2021,"of this paper include the following aspects: 1. This paper is not well written and some parts are hard to follow. It lacks necessary logical transition and important figures. For example, it lacks explanations to support the connection between the proposed training objective and the Cross Margin Discrepancy. Also, it should at least contain one figure to explain the overall architecture or training pipeline. 2. The authors claim that there is still no research focusing on the joint error for UDA. But this problem of arbitrarily increased joint error has already been studied in previous works like “Domain Adaptation with Asymmetrically-Relaxed Distribution Alignment”, in ICML2019. The authors should discuss on that work and directly illustrate the relationship between that work and the proposed one, and why the proposed method is better. 3. Although the joint error is indeed included in the proposed upper bound, in practice the authors have to use Source-driven Hypothesis Space and Target-driven Hypothesis Space to obtain approximation of f_{S} and f_{T}. To me, in practice the use of three classifiers h, f_{1}, f_{2} is just like an improvement over MCD. Hence, I doubt whether the proposed method can still simultaneously minimize the domain discrepancy and the joint error. For example, as shown in the Digit experiments, the performance is highly sensitive to the choice of \gamma in SHS, and sometimes the optimal \gamma value is conflicting for different domains in the same dataset, which is strange since according to the paper’s theorem, smaller \gamma only means more relaxed constraint on hypothesis space. Also, as shown in the VisDA experiments, the optimal value of \eta is close to 1, which means classification error from the approximate target domain is basically useless. 4. The benchmark results are inferior to the state-of-the-art methods. For instance, the Contrastive Adaptation Network achieves an average of 87.2 on VisDA2017, which is much higher than 79.7 achieved by the proposed method. And the same goes with Digit, Office31, and Office-Home dataset.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The comparison with the SOTA methods may be unfair. The performance of the paper is based on the newly collected 209M dataset. However, the existing methods use smaller datasets. For example, GEM employs only 20M unlabeled data. Because the scale of datasets has a significant impact on the accuracy, the superior of the proposed method may be from the new large-scale datasets.",ICLR_2023_1093,ICLR_2023,"1. I thought the novelty is questionable. The authors claimed that the proposed Uni-Mol is the first pure 3D molecular pretraining framework. However, there have been already a few similar works. For example,
a. The Graph Multi-View Pre-training (GraphMVP) framework leverages the correspondence and consistency between 2D topological structures and 3D geometric views.
Liu et al., Pre-training Molecular Graph Representation with 3D Geometry, ICLR 2021.
b. The geometry-enhanced molecular representation learning method (GEM) proposes includes several dedicated geometry-level self-supervised learning strategies to learn molecular geometry knowledge.
Fang et al., Geometry-enhanced molecular representation learning for property prediction, nature machine intelligence, 2022.
c. Guo et al. proposed a self-supervised pre-training model for learning structure embeddings from protein 3D structures.
Guo et al., Self-Supervised Pre-training for Protein Embeddings Using Tertiary Structures, AAAI 2022.
d. The GeomEtry-Aware Relational Graph Neural Network (GearNet) framework uses type prediction, distance prediction and angle prediction of masked parts for pretaining.
Zhang et al., Protein Representation Learning by Geometric Structure Pretraining, ICML 2022 workshop.
2. The comparison with the SOTA methods may be unfair. The performance of the paper is based on the newly collected 209M dataset. However, the existing methods use smaller datasets. For example, GEM employs only 20M unlabeled data. Because the scale of datasets has a significant impact on the accuracy, the superior of the proposed method may be from the new large-scale datasets.
3. The authors claimed one of the contributions is that the proposed Uni-Mol contains a simple and efficient SE(3)-equivariant Transformer backbone. However, I thought this contribution is too weak.
4. The improvement is not very impressive or convincing. Although with a larger dataset for pretraining, the improvement is a bit limited, e.g., in Table 1.
5. It is not clear which part causes the main improvement: Transformer, pretraining or the larger dataset?
6. It could be better to show the 3D position recovery and masked atom prediction accuracy and visualize the results.
7. The visualization of the self-attention map and pair distance map in Appendix H is interesting. However, according to the visualization, the self-attention map is very similar to the pair distance map, as the author explained. In this case, why not directly use pair distance as attention? Or what does self-attention actually learn besides distance in the task? As self-attention is computationally expensive, is it really needed?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2. Several curriculum learning methods have been discussed in Section 1. However, the need for designing a new curriculum learning method for text graphs is not justified. The research gap, e.g., why existing methods can’t be applied, is not discussed.",rs78DlnUB8,EMNLP_2023,"1.	The paper lacks a clear motivation for considering text graphs. Except for the choice of complexity indices, which can easily be changed according to the domain, the proposed method is general and can be applied to other graphs or even other types of data. Moreover, formal formulations of text graphs and the research question are missing in the paper.
2.	Several curriculum learning methods have been discussed in Section 1. However, the need for designing a new curriculum learning method for text graphs is not justified. The research gap, e.g., why existing methods can’t be applied, is not discussed.
3.	Equations 7-11 provide several choices of function f. However, there is no theoretical analysis or empirical experiments to advise on the choice of function f.
4.	In the overall performance comparison (Table 2), other curriculum learning methods do not improve performance compared to No-CL. These results are not consistent with the results reported in the papers of the competitors. At least some discussions about the reason should be included. In addition, it is unclear how many independent runs have been conducted to get the accuracy and F1. What are the standard deviations?
5.	Although experimental results in Table 3 and Table 4 show that the performance remains unchanged, it is unclear how the transfer of knowledge is done in the proposed method. An in-depth discussion of this property should strengthen the soundness of the paper.
6.	In line 118, the authors said the learned curricula are model-dependent, but they also said the curricula are transferrable across models. These two statements seem to be contradictory.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.",ICLR_2021_1181,ICLR_2021,"1.For domain adaptation in the NLP field, powerful pre-trained language models, e.g., BERT, XLNet, can overcome the domain-shift problem to some extent. Thus, the authors should be used as the base encoder for all methods and then compare the efficacy of the transfer parts instead of the simplest n-gram features.
2.The whole procedure is slightly complex. The author formulates the prototypical distribution as a GMM, which has high algorithm complexity. However, formal complexity analysis is absent. The author should provide an analysis of the time complexity and training time of the proposed SAUM method compared with other baselines. Besides, a statistically significant test is absent for performance improvements.
3.The motivation of learning a large margin between different classes is exactly discriminative learning, which is not novel when combined with domain adaptation methods and already proposed in the existing literature, e.g., Unified Deep Supervised Domain Adaptation and Generalization, Saeid et al., ICCV 2017. Contrastive Adaptation Network for Unsupervised Domain Adaptation, Kang et al., CVPR 2019 Joint Domain Alignment and Discriminative Feature Learning for Unsupervised Deep Domain Adaptation, Chen et al., AAAI 2019.
However, this paper lacks detailed discussions and comparisons with existing discriminative feature learning methods for domain adaptation.
4.The unlabeled data (2000) from the preprocessed Amazon review dataset (Blitzer version) is perfectly balanced, which is impractical in real-world applications. Since we cannot control the label distribution of unlabeled data during training, the author should also use a more convinced setting as did in Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018, which directly samples unlabeled data from millions of reviews.
5.The paper lacks some related work about cross-domain sentiment analysis, e.g., End-to-end adversarial memory network for cross-domain sentiment classification, Li et al., IJCAI 2017 Adaptive Semi-supervised Learning for Cross-domain Sentiment Classification, He et al., EMNLP 2018 Hierarchical attention transfer network for cross-domain sentiment classification, Li et al., AAAI 18 Questions:
1.Have the authors conducted the significance tests for the improvements?
2.How fast does this algorithm run or train compared with other baselines?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
2.The proposed method comprises several complicated modules and has more parameters than the baselines. It remains unclear whether the main performance gain originates from a particular module or if the improvement is merely due to having more parameters. The current version of the ablation study does not provide definitive answers to these questions.,zWGDn1AmRH,EMNLP_2023,"1.This paper is challenging to follow, and the proposed method is highly complex, making it difficult to reproduce.
2.The proposed method comprises several complicated modules and has more parameters than the baselines. It remains unclear whether the main performance gain originates from a particular module or if the improvement is merely due to having more parameters. The current version of the ablation study does not provide definitive answers to these questions.
3.The authors claim that one of their main contributions is the use of a Mahalanobis contrastive learning method to narrow the distribution gap between retrieved examples and current samples. However, there are no experiments to verify whether Mahalanobis yields better results than standard contrastive learning.
4.The proposed method involves multiple modules, which could impact training and inference speed. There should be experiments conducted to study and analyze these effects.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"8) l 196-7: this requires more explanation. Why exactly are the two quantities different, and why does this capture the difference in learning settings? ---- I still lean toward acceptance. I think NIPS should have room for a few ""pure theory"" papers.",NIPS_2016_537,NIPS_2016,"weakness of the paper is the lack of clarity in some of the presentation. Here are some examples of what I mean. 1) l 63, refers to a ""joint distribution on D x C"". But C is a collection of classifiers, so this framework where the decision functions are random is unfamiliar. 2) In the first three paragraphs of section 2, the setting needs to be spelled out more clearly. It seems like the authors want to receive credit for doing something in greater generality than what they actually present, and this muddles the exposition. 3) l 123, this is not the definition of ""dominated"" 4) for the third point of definition one, is there some connection to properties of universal kernels? See in particular chapter 4 of Steinwart and Christmann which discusses the ability of universal kernels two separate an arbitrary finite data set with margin arbitrarily close to one. 5) an example and perhaps a figure would be quite helpful in explaining the definition of uniform shattering. 6) in section 2.1 the phrase ""group action"" is used repeatedly, but it is not clear what this means. 7) in the same section, the notation {\cal P} with a subscript is used several times without being defined. 8) l 196-7: this requires more explanation. Why exactly are the two quantities different, and why does this capture the difference in learning settings? ---- I still lean toward acceptance. I think NIPS should have room for a few ""pure theory"" papers.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)?,NIPS_2017_130,NIPS_2017,"weakness)?
4.] Can the authors discuss the sensitivity of any fixed tuning parameters in the model (both strengths and weakness)?
5.] What is the scalability of the model proposed and computational complexity? Will the authors be making the code publicly available with the data? Are all results reproducible using the code and data?
6.] What conclusion should a user learn and drawn? The applications section was a bit disappointing given the motivation of the paper. A longer discussion is important to the impact and success of this paper. Please discuss.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- It would be interesting to see if the proposed framework works with different policy gradient approaches. *experiment results* - How many random seeds are used for learning the policies (DDPO and IPPG)?,NIPS_2019_1411,NIPS_2019,"] *assumption* - I am not sure if it is safe to assume any programmatic policy can be parameterized by a vector \theta and is differentiable in \theta. (for Theorem 4.2) *initial policy* - In all the experiments (TORCS, MountainCar, and Pendulum), the IPPG polices improve upon the PRIOR. It is not clear if IPPG can learn from scratch. Showing the performance of IPPG learning from scratch would be important to verify this. - Can IPPG be initialized with a neural policy? It seems that it is possible based on Algorithm 1. If so, it would be interesting to see how well IPPG work using a neural policy learned with DDPG instead of PRIOR. Can IIPG improve upon DDPG? *experiment setup* - It is mentioned that ""both NDPS and VIPER rely on imitating a fixed neural policy oracle"" (L244). What is this policy oracle? Is this the policy learned using DDPG shown in the tables? If not, what's the performance of using NDPS and VIPER to distill the DDPG policies? - It would be interesting to see if the proposed framework works with different policy gradient approaches. *experiment results* - How many random seeds are used for learning the policies (DDPO and IPPG)? - What are the standard deviation or confidence intervals for all performance values? Are all the tracks deterministic? Are the DDPG policies deterministic during testing? - It would be better if the authors provided some videos showing different policies controlling cars on different tracks so that we can have a better idea of how different methods work. *reproducibility* - Some implementation details are lacking from the main paper, which makes reproducing the results difficult. It is not clear to me what policy gradient approach is used. - The provided dropbox link leads to an empty folder (I checked it on July 5th). *related work* - I believe it would be better if some prior works [1-5] exploring learning-based program synthesis frameworks were mentioned in the paper. *reference* [1] ""Neuro-symbolic program synthesis"" in ICLR 2017 [2] ""Robustfill: Neural program learning under noisy I/O"" in ICML 2017 [3] ""Leveraging Grammar and Reinforcement Learning for Neural Program Synthesis"" in ICLR 2018 [4] ""Neural program synthesis from diverse demonstration videos"" in ICML 2018 [5] ""Execution-Guided Neural Program Synthesis"" in ICLR 2019 ----- final review ----- After reading the other reviews and the author response, I have mixed feelings about this paper. On one hand, I do recognize the importance of this problem and appreciate the proposed framework (IPPG). On the other hand, many of my concerns (e.g. the choices of initial policy, experiment setup, and experiment results) are not addressed, which makes me worried about the empirical performance of the proposed framework. To be more specific, I believe the following questions are important for understanding the performance of IPPG, which remain unanswered: (1) Can IPPG learn from scratch (i.e. where no neural policy could solve the task that we are interested in)? The authors stated that ""IPPG can be initialized with a neural policy, learned for example via DDPG, and thus can be made to learn"" in the rebuttal, which does not answer my question, but it is probably because my original question was confusing. (2) Can IPPG be initialized with a neural policy? If so, can IPPG be initialized with a policy learned using DDPG and improve it? As DDPG achieves great performance on different tracks, I am just interested in if IPPG can even improve it. (3) How many random seeds are used for learning the policies (DDPO and IPPG)? What are the standard deviation or confidence intervals for all performance values? I believe this is important for understanding the performance of RL algorithms. (4) What is the oracle policy that NDPS and VIPER learn from? If they do not learn from the DDPG policy, what is the performance if they distill the DDPG policy. (5) Can IPPG learn from a TPRO/PPO policy? While the authors mentioned that TRPO and PPO can't solve TORCS tasks, I believe this can be verified using the CartPole or other simpler environment. In sum, I decided to keep my score as 5. I am ok if this paper gets accepted (which is likely to happen given positive reviews from other reviewers) but I do hope this paper gets improved from the above points. Also, it would be good to discuss learning-based program synthesis frameworks as they are highly-related.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
-W1) The paper evaluates only on one dataset and on one task. Results and conclusions would be stronger if the analysis were applied to more datasets and more tasks.,mERmlOPxPY,EMNLP_2023,"- W1) The paper evaluates only on one dataset and on one task. Results and conclusions would be stronger if the analysis were applied to more datasets and more tasks.
- W2) Similarly, only one LLM model (GPT-3) is examined.
- W3) Some terms like ""co-prediction"" (line 278) and ""in-context"" (line 285) are not defined or explained.
- W4) A potential weakness is that the paper overlooks the presence of multi-label tweets. For instance, do multi-label tweets impact (i.e., harm) the effectiveness of the EG approach? I imagine that multi-label tweets would confuse the model insofar as the model would have a harder time generating a clear definition of a single label concept. See Q5 below.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. Writtings could be improved in some places. For two examples, * In definition 2.1, what are the ""relevant"" auxiliary model weights? The current definition is a bit difficult for me to interpret.",VmqTuFMk68,ICLR_2024,"1. Writtings could be improved in some places. For two examples,
* In definition 2.1, what are the ""relevant"" auxiliary model weights? The current definition is a bit difficult for me to interpret.
* In definition 2.3, are $p_t$'s referring to positional embedding? Could you explain why there aren't positional embeddings in definition 2.10.
2. Theorem 2.5 shows linear attention could be approximated by softmax attention. Can softmax attention also be approximated by linear attention? If not, I feel Theorem 2.5 alone does not suffice to justify the claim that ""Thus, we often use linear attention in TINT"". Let me know if I have misunderstood anything. In addition, is the claimed parameter saving based on linear attention or self-attention?
3. Definition 2.8 uses finite difference to approximate gradient. I am wondering if we can do this from end to end. That is, can we simulate a backward pass by doing finite-difference and two forward-pass? What's the disadvantage of doing so?
4. This work provides experiments on language tasks, while prior works provide experiments on simulated tasks (e.g., Akyurek et al 2022 did ICL for linear regression). So the empirical results are not directly comparable with prior works.
5. I feel an important prior work [1] is missed. Specifically, [1] also did approximation theory for ICL using transformers. How would the required number of parameters in the construction in this work compare to theirs?
[1] Bai, Yu, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. ""Transformers as Statisticians: Provable In-Context Learning with In-Context Algorithm Selection."" NeurIPS 2023","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"• Dependence on MIA (Membership Inference Attack) Testing via Ulira: While the paper uses MIA testing as a metric for unlearning effectiveness, the effectiveness of MIA testing itself is not sufficiently robust for privacy guarantees. Additionally the use of U-LiRA [1] is recommended.",7tpMhoPXrL,ICLR_2025,"•	GDPR Compliance Concerns: The paper’s reliance on approximate unlearning without theoretical guarantees presents a significant shortfall. While approximate unlearning may be practical, it falls short in scenarios where data privacy and regulatory compliance are non-negotiable. Without provable guarantees, it is questionable whether this method can satisfy GDPR requirements for data erasure. This gap undermines the core purpose of Model Unlearning in privacy-centered contexts, where the ""right to be forgotten"" demands more than a probabilistic assurance.
•	Scalability to Other Domains: The Forget Vector approach is developed and validated primarily for image classification tasks, potentially limiting its application in NLP or other non-visual domains where input perturbations may be less effective.
•	Dependence on MIA (Membership Inference Attack) Testing via Ulira: While the paper uses MIA testing as a metric for unlearning effectiveness, the effectiveness of MIA testing itself is not sufficiently robust for privacy guarantees. Additionally the use of U-LiRA [1] is recommended.
•	Sensitivity to Data Shifts: From the paper the effectiveness of unlearning decreases under certain data shifts, which may hinder the reliability of Forget Vectors in dynamic data environments or adversarial settings.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- I am not familiar with the literature: all the considerations in this paper should also be applicable to kernel (ridge) regression, no? Maybe this could also be presented in the 'language of kernel interpolation/smoothing' as well?",NIPS_2018_197,NIPS_2018,"weakness of the paper: its clarity. From the presentation, it seems evident that the author is an expert in the field of computer algebra/algebraic geometry. It is my assumption that most members of the NIPS community will not have a strong background on this subject, me including. As a consequence, I found it very hard to follow Sect. 3. My impression was that the closer the manuscript comes to the core of algebraic geometry results, the less background was provided. In particular, I would have loved to see at least a proof idea or some more details/background on Thm. 3.1 and Cor. 3.2. Or maybe, the author could include one less example in the main text but show the entire derivation how to get from one concrete instance of A to right kernel B by manual computation? Also, for me the description in Sect. 2.4 was insufficient. As a constructive instruction, maybe drop one of the examples (R(del_t) / R[sigma_x]), but give some more background on the other? This problem of insufficient clarity cannot be explained by different backgrounds alone. In Sect. 3.2, the sentence ""They are implemented in various computer algebra systems, 174 e.g., Singular [8] and Macaulay2 [16] are two well-known open source systems."" appears twice (and also needs grammar checking). If the author could find a minimal non-trivial example (to me, this would be an example not including the previously considered linear differential operator examples) for which the author can show the entire computation in Sect. 3.2 or maybe show pseudo-code for some algorithms involving the Groebner basis, this would probably go a long way in the community. That being said, the paper's strengths are (to the best of this reviewer's knowledge) its originality and potential significance. The insight that Groebner bases can be used as a rich language to encode algebraic constraints and highlighting the connection to this vast background theory opens an entirely new approach in modelling capacities for Gaussian processes. I can easily imagine this work being the foundation for many physical/empirical-hybrid models in many engineering applications. I fully agree and applaud the rationale in lines 43-54! Crucially, the significance of this work will depend on whether this view will be adopted fast enough by the rest of the community which in turn depends on the clarity of the presentation. In conclusion: if I understood the paper correctly, I think the theory presented therein is highly original and significant, but in my opinion, the clarity should be improved significantly before acceptance, if this work should reach its full potential. However, if other reviewers have a different opinion on the level of necessary background material, I would even consider this work for oral presentation. Minor suggestions for improvements: - In line 75, the author writes that the ""mean function is used as regression model"" and this is how the author uses GPs throughout. However, in practice the (posterior) covariance is also considered as ""measure of uncertainty"". It would be insightful, if the author could find a way to visualize this for one or two of the examples the author considers, e.g., by drawing from the posterior process. - I am not familiar with the literature: all the considerations in this paper should also be applicable to kernel (ridge) regression, no? Maybe this could also be presented in the 'language of kernel interpolation/smoothing' as well? - I am uncertain about the author's reasoning on line 103. Does the author want to express that the mean is a sample from the GP? But the mean is not a sample from the GP with probability 1. Generally, there seems to be some inconsistency with the (algebraic) GP object and samples from said object. - The comment on line 158 ""This did not lead to practical problems, yet."" is very ominous. Would we even expect any problem? If not, I would argue you can drop it entirely. - I am not sure whether I understood Fig. 2 correctly. Am I correct that u(t) is either given by data or as one draw from the GP and then, x(t) is the corresponding resulting state function for this specified u? I'm assuming that Fig. 3 is done the other way around, right? --- Post-rebuttal update: Thank you for your rebuttal. I think that adding computer-algebra code sounds like a good idea. Maybe presenting the work more in the context of kernel ridge regression would eliminate the discussion about interpreting the uncertainty. Alternatively, if the author opts to present it as GP, maybe a video could be used to represent the uncertainty by sampling a random walk through the distribution. Finally, it might help to not use differential equations as expository material. I assume the author's rationale for using this was that reader might already a bit familiar with it and thus help its understanding. I agree, but for me it made it harder to understand the generality with respect to Groebner bases. My first intuition was that ""this has been done"". Maybe make they Weyl algebra and Figure 4 the basic piece? But I expect this suggestion to have high variance.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Quantitive evaluation results in Figure 3 only reflect middle outputs rather than the final outputs. Figure 4 illustrates the comparison of final results with a single data sample. Thereby, current evaluations are not convincing enough to confirm ModelAngelo’s superiority to competitors. Is it possible for a quantitive comparison on the final outputs?",ICLR_2023_1500,ICLR_2023,"1. A mathematical formulation for the entire problem is missed. Though the problem is complex and difficult to be solved with an end-to-end framework, the original formulation is still needed at the beginning of the Methods section, followed by a brief introduction of the entire framework, i.e. how to split the entire task into several components and what specific role does each component play. I also suggest authors present a figure to illustrate the overall computing procedure of ModelAngelo. 2. Quantitive evaluation results in Figure 3 only reflect middle outputs rather than the final outputs. Figure 4 illustrates the comparison of final results with a single data sample. Thereby, current evaluations are not convincing enough to confirm ModelAngelo’s superiority to competitors. Is it possible for a quantitive comparison on the final outputs? 3. Many losses (e.g. (RMSD) loss, backbone RMSD loss, amino-acid classification loss, local confidence score loss, torsion angles loss, and full atom loss) are involved in the learning of GNN. What are the definition of these losses? How do you get the ground truth labels required (if so) for these losses?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.",NIPS_2017_28,NIPS_2017,"- Most importantly, the explanations are very qualitative and whenever simulation or experiment-based evidence is given, the procedures are described very minimally or not at all, and some figures are confusing, e.g. what is ""sample count"" in fig. 2? It would really help adding more details to the paper and/or supplementary information in order to appreciate what exactly was done in each simulation. Whenever statistical inferences are made, there should be error bars and/or p-values.
- Although in principle the argument that in case of recognition lists are recalled based on items makes sense, in the most common case of recognition, old vs new judgments, new items comprise the list of all items available in memory (minus the ones seen), and it's hard to see how such an exhaustive list could be effectively implemented and concrete predictions tested with simulations.
- Model implementation should be better justified: for example, the stopping rule with n consecutive identical samples seems a bit arbitrary (at least it's hard to imagine neural/behavioral parallels for that) and sensitivity with regard to n is not discussed.
- Finally it's unclear how perceptual modifications apply for the case of recall: in my understanding the items are freely recalled from memory and hence can't be perceptually modified. Also what are speeded/unspeeded conditions?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. Some claims may be inspired from existing studies; thus, it is critical to add the supportive references. For example, Lines 55-64: ""we identify four critical factors that affect the performance of chain-of-thought prompting and require large human effort to deal with: (1) order sensitivity: the order combination of the exemplars; (2) complexity: the number of reasoning steps of the rationale chains; (3) diversity: the combination of different complex-level exemplars; (4) style sensitivity: the writing/linguistic style of the rationale chains."" --- Most of the above factors have been discussed in existing studies.",FGBEoz9WzI,EMNLP_2023,"1. Some claims may be inspired from existing studies; thus, it is critical to add the supportive references. For example, Lines 55-64: ""we identify four critical factors that affect the performance of chain-of-thought prompting and require large human effort to deal with: (1) order sensitivity: the order combination of the exemplars; (2) complexity: the number of reasoning steps of the rationale chains; (3) diversity: the combination of different complex-level exemplars; (4) style sensitivity: the writing/linguistic style of the rationale chains."" --- Most of the above factors have been discussed in existing studies.
2. This approach requires extensive queries to optimize and organize the demonstration exemplars, which would costly behind the paywalls. It also relies on a training-based pipeline, which further increases the complexity of the whole framework.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Impact: This paper could be improved by explicitly showing the settings for the various knobs of this algorithm to mimic prior work: Dagger, searn, etc...it would help the community by providing a single review of the various advances in this area.",NIPS_2016_192,NIPS_2016,"Weakness: (e.g., why I am recommending poster, and not oral) - Impact: This paper makes it easier to train models using learning to search, but it doesn't really advance state-of-the-art in terms of the kind of models we can build. - Impact: This paper could be improved by explicitly showing the settings for the various knobs of this algorithm to mimic prior work: Dagger, searn, etc...it would help the community by providing a single review of the various advances in this area. - (Minor issue) What's up with Figure 3? ""OAA"" is never referenced in the body text. It looks like there's more content in the appendix that is missing here, or the caption is out of date.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Another unclear point is the paper presents specific examples of biases of target statistics (section 3.2) and prediction shift of gradient values (Theorem 1), and we can know that the bias can happen, but on the other hand, we are not sure how general these situations are.",NIPS_2018_612,NIPS_2018,"Weakness: - Two types of methods are mixed into a single package (CatBoost) and evaluation experiments, and the contribution of each trick would be a bit unclear. In particular, it would be unclear whether CatBoost is basically for categorical data or it would also work with the numerical data only. - The bias under discussion is basically the ones occurred at each step, and their impact to the total ensemble is unclear. For example, randomization as seen in Friedman's stochastic gradient boosting can work for debiasing/stabilizing this type of overfitting biases. - The examples of Theorem 1 and the biases of TS are too specific and it is not convincing how these statement can be practical issues in general. Comment: - The main unclear point to me is whether CatBoost is mainly for categorical features or not. If the section 3 and 4 are independent, then it would be informative to separately evaluate the contribution of each trick. - Another unclear point is the paper presents specific examples of biases of target statistics (section 3.2) and prediction shift of gradient values (Theorem 1), and we can know that the bias can happen, but on the other hand, we are not sure how general these situations are. - One important thing I'm also interested in is that the latter bias 'prediction shift' is caused at each step, and its effect on the entire ensemble is not clear. For example, I guess the effect of the presented 'ordered boosting' could be related to Friedman's stochastic gradient boosting cited as [13]. This simple trick is just apply bagging to each gradient-computing step of gradient boosting, which would randomly perturb the exact computation of gradient. Each step would be just randomly biased, but the entire ensemble would be expected to be stabilized as a whole. Both XGBoost and LightGBM have this stochastic/bagging option, we can use it when we need it. Comment After Author Response: Thank you for the response. I appreciate the great engineering effort to realize a nice & high-performance implementations of CatBoost. But I'm still not sure that how 'ordering boosting', one of two main ideas of the paper, gives the performance improvement in general. As I mentioned in the previous comment, the bias occurs at each base learner h_t. But it is unclear that how this affects the entire ensemble F_t that we actually use. Since each h_t is a ""weak"" learner anyway, any small biases can be corrected to some extent through the entire boosting process. I couldn't find any comments for this point in the response. I understand the nice empirical results of Tab. 3 (Ordered vs. Plain gradient values) and Tab. 4 (Ordered TS vs. alternative TS methods). But I'm still unsure whether this improvement comes only from the 'ordering' ideas to address two types of target leakages. Because the comparing models have many different hyper parameters and (some of?) these are tuned by Hyperopt, so the improvement can come not only from addressing the two types of leakage. For example, it would be nice to have something like the following comparisons o focus only on two ideas of ordered TS and ordered boosting in addition: 1) Hyperopt-best-tuned comparisons of CatBoost (plain) vs LightGBM vs XGboost (to make sure no advantages exists for CatBoost (plain) ) 2) Hyperopt-best-tuned comparisons of CatBoost without column sampling + row sampling vs LightGBM/XGBoost without column sampling + row sampling 3) Hyperopt-best-tuned comparisons of CatBoost(plain) + ordered TS without ordered boosting vs CatBoost(plain) (any other randomization options, column sampling and row sampling, should be off) 4) Hyperopt-best-tuned comparisons of CatBoost(plain) + ordered boosting without ordered TS vs CatBoost(plain) (any other randomization options, column sampling and row sampling, should be off)","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- In the related work section, the class of binary networks is missing. These networks are also efficient and compact. Example papers are:",NIPS_2017_71,NIPS_2017,"- The paper is a bit incremental. Basically, knowledge distillation is applied to object detection (as opposed to classification as in the original paper).
- Table 4 is incomplete. It should include the results for all four datasets.
- In the related work section, the class of binary networks is missing. These networks are also efficient and compact. Example papers are:
* XNOR-Net: ImageNet Classification Using Binary Convolutional Neural Networks, ECCV 2016
* Binaryconnect: Training deep neural networks with binary weights during propagations, NIPS 2015
Overall assessment: The idea of the paper is interesting. The experiment section is solid. Hence, I recommend acceptance of the paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"- a few more datasets would've been appreciated, especially concerning the cross-task transferability",NIPS_2020_1211,NIPS_2020,"I have only a few remarks on this paper, even though they shouldn't be considered as weaknesses. They are listed below in no particular order. - in eq.1 | is used both as the absolute value operator and the cardinality one, which can lead to confusion - in eq.2, \tau and v have not been previously defined (unless I'm missing something) - I find it regrettable that no theoretical analysis of Mesa (e.g. convergence speed, generalization error, etc) is proposed aside from the complexity one, especially since it is built upon frameworks with strong theoretical properties - line 155 ""is thus can be"" typo - line 173 reference error ""Haarnoja et al."" - line 233 compares > compared - table 2, what does k correspond to? Is it the parameter of Algorithm 2? - a few more datasets would've been appreciated, especially concerning the cross-task transferability","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The tasks are somewhat standard - Figure captioning, and matching figures/sub-figures to appropriate captions. It would have been nice to see some unique tasks created from this nice dataset showcasing the diversity of images/plots. e.g. some variety of interleaved image-text tasks such as Question Answering from images could have been considered.",DEOV74Idsg,ICLR_2025,"The main weaknesses are that considering the diversity in the data, the tasks do not seem to go beyond standard tasks; and even on the figure captioning tasks, the analysis is lacking, particularly in terms of representation of strong evaluations from domain experts.
1. The tasks are somewhat standard - Figure captioning, and matching figures/sub-figures to appropriate captions. It would have been nice to see some unique tasks created from this nice dataset showcasing the diversity of images/plots. e.g. some variety of interleaved image-text tasks such as Question Answering from images could have been considered.
2. It would have been nicer to have more detailed analysis of model responses for a few images (3-5) in about 10 domains. Where experts weigh-in on model responses even for just the figure captioning task to evaluate the strengths and weaknesses of models in the different domains. Especially on the variety showcased in Figure-2, e.g. 10 examples from each category in figure-2 analyzed by domain experts.
3. Some metrics for figure captioning are missing e.g. BLEU, CIDEr, SPICE (https://github.com/tylin/coco-caption) are metrics often used in figure captioning evaluations, and it would be good to include these. ROUGE is primarily a recall based metric, while it’s relevant, in itself it’s not a sufficient signal particularly for captioning.
* Other LLM based metrics to consider using: LAVE (https://arxiv.org/pdf/2310.02567), L3Score (https://github.com/google/spiqa), PrometheusVision (https://github.com/prometheus-eval/prometheus-vision). L3Score is particularly interesting because you get the confidence from GPT-4o in addition to the generated response.
4. The results for the materials science case study is hard to interpret.
4.1 What is the baseline LLAMA-2-7B performance? (without any tuning?) Many numbers in Table 5 and Figure 6 already seem quite high so it is hard to understand what baseline you are starting from and how much room for improvement there was (and from the presented results, it doesn’t look like that much, which perhaps may not be correct)
4.2 How well do proprietary models perform on this task? Are there any proprietary models that generate reasonable responses worth evaluating?
4.3 In Table 5, the “Stable DFT” column numbers for LLAMA models (from Gruver et. al.) appear to not be consistent with numbers reported in Gruver et. al. Why is that? Minor
5. Related to point-2 Figure 4 is extremely difficult to follow. Perhaps reduce the materials science case study and include more detailed analysis of model responses and a discussion.
6. Figure 3 can be improved to clearly highlight the tasks and also the ground truth caption.
7. Other papers to consider citing in related works:
* the papers proposing different relevant metrics noted in Weakness-3.
* https://openaccess.thecvf.com/content/WACV2024/papers/Tarsi_SciOL_and_MuLMS-Img_Introducing_a_Large-Scale_Multimodal_Scientific_Dataset_and_WACV_2024_paper.pdf
Initial rationale for rating of 5 is primarily due to weakness 1 and 2.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The Sec. 3.1 for 3D Gaussians generation seems to just follow the previous work, Luciddreamer. Please correct me there is any additional novel effort for this part.",IcYDRzcccP,ICLR_2025,"1. The Sec. 3.1 for 3D Gaussians generation seems to just follow the previous work, Luciddreamer. Please correct me there is any additional novel effort for this part.
2. Reprojecting the point cloud from the single view image to different views always leads to the holes, distortion and some other artifacts. A common idea to incorporate the generative model to fulfill the missing information. But it seems the author does not include anything related to this point. Please include more discussion if it is not necessary.
3. Minimizing the reprojection error for the predicted motions seems to be feasible in Sec. 3.2, However, this optimization is still based on the 2D information and can hardly achieve the 3D consistency. For example, in Structure from Motions, such method is always used refine the estimated camera poses and the position for the point clouds. But it does not consider the inter relationship between each 3D positions, so I am not sure that the 3D consistency can be achieved with this method.
4. Please provide more implementation details of the 3D Motion Optimization Module. It seems to be vague now.
5. The Sec. 3.3 for 4D Gaussians generation seems to just follow the previous work. Please correct me there is any additional novel effort for this part.
6. The experimental results will be more solid and comprehensive if more dataset and baselines can be included. For example, the author mentions that simply using animation based method cannot achieve satisfying results, so similar baselines can be included.
7. The paper presentation can be more compact if the part of just following the previous work can be shortened while emphasizing the novel part.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
* (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods.,NIPS_2019_524,NIPS_2019,"weakness of the paper are as follows (from my perspective): * (strength) the authors introduce a generative approach for applying Hindsight Experience Replay (HER) in visual domains: the idea is simple and has the potential to improve our current Deep RL methods. * (weakness) currently, the paper does not seem to have a detailed discussion on how their generative model was trained to produce images containing the goal information. The authors do clarify this on their feedback and it would be useful if they also add this discussion on their next version of the paper. More importantly, including this discussion is useful for the Deep RL community. * (weakness) their current approach of training the generative model relies on manually annotating the goal images, which may prevent scalability of the algorithm. Addressing this could make their approach be more impactful.","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"2. Unlike phi-divergence or Wasserstein uncertainty sets, MMD DRO seems not enjoy a tractable exact equivalent reformulation, which seems to be a severe drawback to me. The upper bound provided in Theorem 3.1 is crude especially because it drops the nonnegative constraint on the distribution, and further approximation is still needed even applied to a simple kernel ridge regression problem. Moreover, it seems restrictive to assume the loss \ell_f belongs to the RKHS as already pointed out by the authors.",NIPS_2019_819,NIPS_2019,"Weakness: Due to the intractbility of the MMD DRO problem, the submission did not find an exact reformulation as much other literature in DRO did for other probability metrics. Instead, the author provides several layers of approximation. The reason why I emphasize the importance of a tight bound, if not an exact reformulation, is that one of the major criticism about (distributionally) robust optimization is that it is sometimes too conservative, and thus a loose upper bound might not be sufficient to mitigate the over-conservativeness and demonstrate the power of distributionally robust optimization. When a new distance is introduced into the DRO framework, a natural question is why it should be used compared with other existing approaches. I hope there will be a more fair comparision in the camera-ready version. =============== 1. The study of MMD DRO is mostly motivated by the poor out-of-sample performance of existing phi-divergence and Wasserstein uncertainty sets. However, I don't believe this is indeed the case. For example, Namkoong and Duchi (2016), and Blanchet, Kang, and Murthy (2016) show the dimension-independent bound 1/\sqrt{n} for a broad class of objective functions in the case of phi-divergence and Wasserstein metric respectively. They didn't require the population distribution to be within the uncertainty set, and in fact, such a requirement is way too conservative and it is exactly what they wanted to avoid. 2. Unlike phi-divergence or Wasserstein uncertainty sets, MMD DRO seems not enjoy a tractable exact equivalent reformulation, which seems to be a severe drawback to me. The upper bound provided in Theorem 3.1 is crude especially because it drops the nonnegative constraint on the distribution, and further approximation is still needed even applied to a simple kernel ridge regression problem. Moreover, it seems restrictive to assume the loss \ell_f belongs to the RKHS as already pointed out by the authors. 3. I am confused about the statement in Theorem 5.1, as it might indicate some disadvantage of MMD DRO, as it provides a more conservative upper bound than the variance regularized problem. 4. Given the intractability of the MMD DRO and several layers of approximation, the numerical experiment in Section 6 is insufficient to demonstrate the usefulness of the new framework. References: Namkoong, H. and Duchi, J.C., 2017. Variance-based regularization with convex objectives. In Advances in Neural Information Processing Systems (pp. 2971-2980). Blanchet, J., Kang, Y. and Murthy, K., 2016. Robust wasserstein profile inference and applications to machine learning. arXiv preprint arXiv:1610.05627.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2. Secondly, what relevance does the framework have with problems of non-convex losses and/or non-norm type defenses? Would the non-vanishing duality gap and the difficulty of maximization over non-norm type constraints make the algorithm irrelevant? Or would it still give some intuitions on the risk upperbound? p.3, binary classification: If the true mean is known through an oracle, can one use the covariance or any other statistics to design a better defense?",NIPS_2017_337,NIPS_2017,"Not many, but there are several approximations made in the derivation. Also the strong assumptions of convexity and norm-based defense were made.
Qualitative evaluation Quality:
The paper is technically sound for the most part, except for severals approximation steps that perhaps needs more precise descriptions. Experiments seem to support the claims well. Literature review is broad and relevant. Clarity:
The paper is written and organized very well. Perhaps the sections on data-dependent defenses are a bit complex to follow. Originality:
While the components such as online learning by regret minimization and minimax duality may be well-known, the paper uses them in the context of poisoning attack to find a (approximate) solution/certificate for norm-based defense, which is original as far as I know. Significance:
The problem addressed in the paper has been gaining importance, and the paper presents a clean analysis and experiments on computing an approximate upperbound on the risk, assuming a convex loss and/or a norm-based defense.
Detailed comments
I didn't find too many things to complain. To nitpick, there are two concerns about the paper.
1. The authors introduce several approximations (i -iii) which leaves loose ends. It is understandable that approximations are necessary to derive clean results, but the possible vulnerability (e.g., due to the assumption of attacks being in the feasible set only in lines 107-110) needs to be expanded to reassure the readers that it is not a real concern.
2. Secondly, what relevance does the framework have with problems of non-convex losses and/or non-norm type defenses? Would the non-vanishing duality gap and the difficulty of maximization over non-norm type constraints make the algorithm irrelevant? Or would it still give some intuitions on the risk upperbound?
p.3, binary classification: If the true mean is known through an oracle, can one use the covariance or any other statistics to design a better defense?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. In Figure 3, for the baselines on the left hand side, what if we sparsify the trained models to reduce the number of selected features and compare accuracy to the proposed model?",NIPS_2018_840,NIPS_2018,"1. It is confusing to me what the exact goal of this paper is. Are we claiming the multi-prototype model is superior to other binary classification models (such as linear SVM, kNN, etc.) in terms of interpretability? Why do we have two sets of baselines for higher-dimensional and lower-dimensional data? 2. In Figure 3, for the baselines on the left hand side, what if we sparsify the trained models to reduce the number of selected features and compare accuracy to the proposed model? 3. Since the parameter for sparsity constraint has to be manually picked, can the authors provide any experimental results on the sensitivity of this parameter? Similar issue arises when picking the number of prototypes. Update after Author's Feedback: All my concerns are addressed by the authors's additional results. I'm changing my score based on that.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc.",NIPS_2016_313,NIPS_2016,"Weakness: 1. The proposed method consists of two major components: generative shape model and the word parsing model. It is unclear which component contributes to the performance gain. Since the proposed approach follows detection-parsing paradigm, it is better to evaluate on baseline detection or parsing techniques sperately to better support the claim. 2. Lacks in detail about the techniques and make it hard to reproduce the result. For example, it is unclear about the sparsification process since it is important to extract the landmark features for following steps. And how to generate the landmark on the edge? How to decide the number of landmark used? What kind of images features? What is the fixed radius with different scales? How to achieve shape invariance, etc. 3. The authors claim to achieve state-of-the-art results on challenging scene text recognition tasks, even outperforms the deep-learning based approaches, which is not convincing. As claimed, the performance majorly come from the first step which makes it reasonable to conduct comparisons experiments with existing detection methods. 4. It is time-consuming since the shape model is trained in pixel level(though sparsity by landmark) and the model is trained independently on all font images and characters. In addition, parsing model is a high-order factor graph with four types of factors. The processing efficiency of training and testing should be described and compared with existing work. 5. For the shape model invariance study, evaluation on transformations of training images cannot fully prove the point. Are there any quantitative results on testing images?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. For the Appendix H section, it should be reorganized which is difficult to follow.",ARR_2022_186_review,ARR_2022,"There are two main concerns from me . The first one is that how to generate the adversarial training set with code is not clearly discussed and the detail of adversarial training set is not be provided. The second concern is that the reason of excluding 25% of the word phrases said in the last paragraph in 4.1 and the reason of using first 50 examples from the OntoNotes test set is not be fully discussed.
1. For the Appendix H section, it should be reorganized which is difficult to follow.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Lacking awareness of related work (see Comment 1) - Missing relevant baselines and runtime experimental results (Comments 2, 3 and 4) Major Comments/Questions:",NIPS_2019_1207,NIPS_2019,"- Moderate novelty. This paper combines various components proposed in previous work (some of it, it seems, unbeknownst to the authors - see Comment 1): hierarchical/structured optimal transport distances, Wasserstein-Procrustes methods, sample complexity results for Wasserstein/Sinkhorn objectives. Thus, I see the contributions of this paper being essentially: putting together these pieces and solving them cleverly via ADMM. - Lacking awareness of related work (see Comment 1) - Missing relevant baselines and runtime experimental results (Comments 2, 3 and 4) Major Comments/Questions: 1 Related Work. My main concern with this paper is its apparent lack of awareness of two very related lines of work. On the one hand, the idea of defining hierarchical OT distances has been explored before in various contexts (e.g., [5], [6] and [7]), and so has leveraging cluster information for structured losses, e.g. [9] and [10] (note that latter of these relies on an ADMM approach too). On the other hand, combining OT with Procrustes alignment has a long history too (e.g, [1]), with recent successful application in high-dimensional problems ([2], [3], [4]). All of these papers solve some version of Eq (4) with orthogonality (or more general constraints), leading to algorithms whose core is identical to Algorithm 1. Given that this paper sits at the intersection of two rich lines of work in the OT literature, I would have expected some effort to contrast their approach, both theoretically and empirically, with all these related methods. 2. Baselines. Related to the point above, any method that does not account for rotations across data domains (e.g., classic Wasserstein distance) is inadequate as a baseline. Comparing to any of the methods [1]-[4] would have been much more informative. In addition, none of the baselines models group structure, which again, would have been easy to remedy by including at least one alternative that does (e.g., [10] or the method of Courty et al, which is cited and mentioned in passing, but not compared against). As for the neuron application, I am not familiar with the DAD method, but the same applies about the lack of comparison to OT-based methods with structure/Procrustes invariance. 3. Conflation of geometric invariance and hierarchical components. Given that this approach combines two independent extensions on the classic OT problem (namely, the hierarchical formulation and the aligment over the stiefel manifold), I would like to understand how important these two are for the applications explored in this work. Yet, no ablation results are provided. A starting point would be to solve the same problem but fixing the transformation T to be the identity, which would provide a lower bound that, when compared against the classic WA, would neatly show the advantage of the hierarchical vs a ""flat"" classic OT versions of the problem. 4. No runtime results. Since computational efficiency is one of the major contributions touted in the abstract and introduction, I was expecting to see at least empirical and/or a formal convergence/runtime complexity analysis, but neither of these was provided. Since the toy example is relatively small, and no details about the neural population task are provided, the reader is left to wonder about the practical applicability of this framework for real applications. Minor Comments/Typos: - L53. *the* data. - L147. It's not clear to me why (1) is referred to as an update step here. Wrong eqref? - Please provide details (size, dimensionality, interpretation) about the neural population datasets, at least on the supplement. Many readers will not be familiar with it. References: * OT-based methods to align in the presence of unitary transformations: [1] Rangarajan et al, ""The Softassign Procrustes Matching Algorithm"", 1997. [2] Zhang et al, ""Earth Moverâs Distance Minimization for Unsupervised Bilingual Lexicon Induction"", 2017. [3] Alvarez-Melis et al, ""Towards Optimal Transport with Global Invariances"", 2019. [4] Grave et al, ""Unsupervised Alignment of Embeddings with Wasserstein Procrustes"", 2019. *Hierarchical OT methods: [5] Yuorochkin et al, ""Hierarhical Optimal Transport for Document Representation"". [6] Shmitzer and Schnorr, ""A Hierarchical Approach to Optimal Transport"", 2013 [7] Dukler et al, ""Wasserstein of Wasserstein Loss for Learning Generative Models"", 2019 [9] Alvarez-Melis et al, ""Structured Optimal Transport"", 2018 [10] Das and Lee, ""Unsupervised Domain Adaptation Using Regularized Hyper-Graph Matching"", 2018","{'annotators': ['DFou9r7M'], 'labels': ['4']}",,,7,"{'annotators': ['DFou9r7M'], 'labels': ['3']}",,,"{'annotators': ['DFou9r7M'], 'labels': ['2']}",,,"{'annotators': ['DFou9r7M'], 'labels': ['3']}",,,"{'annotators': ['DFou9r7M'], 'labels': ['1']}",,,"{'annotators': ['DFou9r7M'], 'labels': ['1']}",,,"{'annotators': ['DFou9r7M'], 'labels': ['1']}",,
"- even with the pseudocode given in the supplementary material I don't get the feeling the paper is written to be reproduced. It is written to provide an intuitive understanding of the work, but to actually reproduce it, more details are required that are neither provided in the paper nor in the supplementary material. This includes, for example, details about the RNN implementation (like number of units etc), and many other technical details.",NIPS_2019_494,NIPS_2019,"of the approach, it may be interesting to do that. Clarity: The paper is well written but clarity could be improved in several cases: - I found the notation / the explicit split between ""static"" and temporal features into two variables confusing, at least initially. In my view this requires more information than is provided in the paper (what is S and Xt). - even with the pseudocode given in the supplementary material I don't get the feeling the paper is written to be reproduced. It is written to provide an intuitive understanding of the work, but to actually reproduce it, more details are required that are neither provided in the paper nor in the supplementary material. This includes, for example, details about the RNN implementation (like number of units etc), and many other technical details. - the paper is presented well, e.g., quality of graphs is good (though labels on the graphs in Fig 3 could be slightly bigger) Significance: - from just the paper: the results would be more interesting (and significant) if there was a way to reproduce the work more easily. At present I cannot see this work easily taken up by many other researchers mainly due to lack of detail in the description. The work is interesting, and I like the idea, but with a relatively high-level description of it in the paper it would need a little more than the peudocode in the materials to convince me using it (but see next). - In the supplementary material it is stated the source code will be made available, and in combination with paper and information in the supplementary material, the level of detail may be just right (but it's hard to say without seeing the code). Given the promising results, I can imagine this approach being useful at least for more research in a similar direction.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
- Figure 1 would be much stronger if there were error bars and/or if there were more random trials that would (potentially) get rid of some of the (most likely) random fluctuations in the results.,NIPS_2018_605,NIPS_2018,"in the related work and the experiments. If some of these concerns would be addressed in the rebuttal, I would be willing to upgrade my recommended score. Strengths: - The results seem to be correct. - In contrast to Huggins et al. (2016) and Tolochinsky & Feldman (2018), the coreset guarantee applies to the standard loss function of logistic regression and not to variations. - The (theoretical) algorithm (without the sketching algorithm for the QR decomposition) seems simple and practical. If space permits, the authors might consider explicitly specifying the algorithm in pseudo-code (so that practitioners do not have to extract it from the Theorems). - The authors include in the Supplementary Materials an example where uniform sampling fails even if the complexity parameter mu is bounded. - The authors show that the proposed approach obtains a better trade-off between error and absolute running time than uniform sampling and the approach by Huggins et al. (2016). Weaknesses: - Novelty: The sensitivity bound in this paper seems very similar to the one presented in [1] which is not cited in the manuscript. The paper [1] also uses a mix between sampling according to the data point weights and the l2-sampling with regards to the mean of the data to bound the sensitivity and then do importance sampling. Clearly, this paper treats a different problem (logistic regression vs k-means clustering) and has differences. However, this submission would be strengthened if the proposed approach would be compared to the one in [1]. In particular, I wonder if the idea of both additive and multiplicative errors in [1] could be applied in this paper (instead of restricting mu-complexity) to arrive at a coreset construction that does not require any assumptions on the data data set. [1] Scalable k-Means Clustering via Lightweight Coresets Olivier Bachem, Mario Lucic and Andreas Krause To Appear In International Conference on Knowledge Discovery and Data Mining (KDD), 2018. - Practical significance: The paper only contains a limited set of experiments, i.e., few data sets and no comparison to Tolochinsky & Feldman (2018). Furthermore, the paper does not compare against any non-coreset based approaches, e.g., SGD, SDCA, SAGA, and friends. It is not clear whether the proposed approach is useful in practice compared to these approaches. - Figure 1 would be much stronger if there were error bars and/or if there were more random trials that would (potentially) get rid of some of the (most likely) random fluctuations in the results.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. It is suggested that the authors provide a brief introduction to energy models in the related work section. In Figure 1, it is not mentioned which points different learning rates in the left graph and different steps in the right graph correspond to. [1] Context-aware robust fine-tuning. [2] Fine-tuning can cripple your foundation model; preserving features may be the solution. [3] Robust fine-tuning of zero-shot models.",2JF8mJRJ7M,ICLR_2024,"1. Utilizing energy models to explain the fine-tuning of pre-trained models seems not to be essential. As per my understanding, the objective of the method in this paper as well as related methods ([1,2,3], etc.) is to reduce the difference in features extracted by the models before and after fine-tuning.
2. The authors claim that the text used is randomly generated, but it appears from the code in the supplementary material that tokens are sampled from the openai_imagenet_template. According to CAR-FT, using all templates as text input also yields good performance. What then is the significance of random token sampling in this scenario?
3. It is suggested that the authors provide a brief introduction to energy models in the related work section.
In Figure 1, it is not mentioned which points different learning rates in the left graph and different steps in the right graph correspond to.
[1] Context-aware robust fine-tuning.
[2] Fine-tuning can cripple your foundation model; preserving features may be the solution.
[3] Robust fine-tuning of zero-shot models.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. At the heart of FIITED is the utility-based approach to determine chunk significance. However, basing eviction decisions purely on utility scores might introduce biases. For instance, recent chunks might gain a temporary high utility, leading to potentially premature evictions of other valuable chunks.",gDDW5zMKFe,ICLR_2024,"1.	At the heart of FIITED is the utility-based approach to determine chunk significance. However, basing eviction decisions purely on utility scores might introduce biases. For instance, recent chunks might gain a temporary high utility, leading to potentially premature evictions of other valuable chunks.
2.	This approach does not consider the individual significance of dimensions within a chunk, leading to potential information loss.
3.	While the chunk address manager maintains a free address stack, this design assumes that the most recently evicted space is optimal for the next allocation. This might not always be the case, especially when considering the locality of data and frequent access patterns.
4.	The system heavily depends on the hash table to fetch and manage embeddings. This approach, while efficient in accessing chunks, might lead to hashing collisions even though the design ensures a low collision rate. Any collision, however rare, can introduce latency in access times or even potential overwrites.
5.	The methodology leans heavily on access frequency to decide on embedding significance. However, frequency doesn't always equate to importance. There could be rarely accessed but critically important embeddings, and the method might be prone to undervaluing them.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The first paragraph of the Introduction is entirely devoted to a general introduction of DNNs, without any mention of drift. Given that the paper's core focus is on detecting drift types and drift magnitude, I believe the DNN-related introduction is not central to this paper, and this entire paragraph provides little valuable information to readers.",W6fIyuK8Lk,ICLR_2025,"1. The first paragraph of the Introduction is entirely devoted to a general introduction of DNNs, without any mention of drift. Given that the paper's core focus is on detecting drift types and drift magnitude, I believe the DNN-related introduction is not central to this paper, and this entire paragraph provides little valuable information to readers.
2. The paper is poorly written. A paper should highlight its key points and quickly convey its innovations to readers. In the introduction, three paragraphs are spent on related work, while only one paragraph describes the paper's contribution, and even this paragraph fails to intuitively explain why the proposed method would work.
3. The first paragraph of Preliminaries is entirely about concept drift. So I assume the paper aims to address concept drift issues. If this is the case, there is a serious misuse of terminology. In concept drift, drift types include abrupt drift, gradual drift, recurrent drift, etc. While this paper uses the term ""drift type"" 86 times, it never explains or strictly defines what drift type means. According to Table 1 in the paper, the authors treat ""gaussian noise, poisson noise, salt noise, snow fog rain, etc"" as drift types. I find this inappropriate as these are more like different types of concepts. In summary, drift type is a specialized term in concept drift [1].
4. Line 163 states: ""Pi,j denote the prediction probability distribution of the images belonging to the class j predicted as class i"", but according to equation 1, I believe p_ij is a scalar, not a distribution. This appears to be an expression issue. I believe the paper consistently misuses the term ""distribution"".
5. Line 183, ""per each drift type"" should be changed to ""for each effect type"".
6. Lines 117-119 state: ""To understand the impact of data drifts on image classification neural networks, let us consider the impact of Gaussian noise on a classification network trained on the MNIST handwritten digit image dataset, detailed in Section 4, under the effect of Gaussian noise."" This sentence is highly redundant.
7. The experimental evaluation is inadequate as it only compares against a single baseline. For a paper proposing a new framework, comparing with multiple state-of-the-art methods is essential to demonstrate the effectiveness and advantages of the proposed approach. The limited comparison significantly weakens the paper's experimental validation.
Overall, I find the paper poorly written, with issues including misuse of terminology, redundant expressions, unclear logical flow, and lack of focus. Moreover, the paper seems to compare against only one baseline, which makes the experimental results unconvincing to me.
[1] Agrahari, S. and Singh, A.K., 2022. Concept drift detection in data stream mining: A literature review. Journal of King Saud University-Computer and Information Sciences, 34(10), pp.9523-9540.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"3. Besides the methodology, it's also not clear how the different part of this framework performs and contribute to the final result from the experimental aspect. While in the result section, we can see that the framework can yield promising visual stimuli result, it lacks either quantitative experiments and comparison between selection of algorithms, or a more detailed explanations on the presented ones. (See questions.) Therefore, it's unclear for me what the exact performance of the whole framework and individual parts compared to other solutions.",4ltiMYgJo9,ICLR_2025,"1. One of the main claims by the authors is the adaptation of the whole close-loop framework. While the authors claim it can be simply replaced by recording EEG data from human participants, there are actually no more concrete demonstrations on how. For example, what is the ""specific neural activity in the brain"" in this paper and in a possible real scenario? What's the difference? And how difficult is it and how much effort will it take to apply the framework to the real world? It's always easy to just claim a methodology ""generalizable"", but without more justification that doesn't actually help strengthen the contribution of the paper.
2. Based on 1, I feel it is not sufficiently demonstrated in the paper what role the EEG plays in the whole framework. As far as I can understand from the current paper, it seems to be related to the reward $R$ in the MDP design, because it should provide signal based on the desired neural activities. However, we know neither how the reward is exactly calculated nor what kinds of the neural signal the authors are caring about (e.g., a specific frequency bank? a specific shape of waveforms? a specific activation from some brain area?).
3. Besides the methodology, it's also not clear how the different part of this framework performs and contribute to the final result from the experimental aspect. While in the result section, we can see that the framework can yield promising visual stimuli result, it lacks either quantitative experiments and comparison between selection of algorithms, or a more detailed explanations on the presented ones. (See questions.) Therefore, it's unclear for me what the exact performance of the whole framework and individual parts compared to other solutions.
4. Overall, the presentation of this paper is unsatisfying (and that's probably why I have the concerns in 2 and 3). On the one hand, the author is presenting more well-known details in the main content but didn't make their own claims clear. For example, the algorithm 1 and algorithm 2 is a direct adaptation from previous work. Instead of using space to present them, I wish to see more on how the MDP is constructed. On the other hand, mixing citations with sentences (please use \citep instead \cite) and a few typos (in line 222, algorithm 1, the bracket is not matched) give me the feeling that the paper is not yet ready to be published.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
2. It is not clear to me whether such a model could generate novel knowledge or testable hypothesis about neuron data.,ICLR_2023_149,ICLR_2023,"Weakness: 1. Some recent RNN-based latent models eg. LFADs and Oerich 2020, were overlooked in the current manuscript. It would be great to discuss those. 2. It is not clear to me whether such a model could generate novel knowledge or testable hypothesis about neuron data.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- Akin to theorem 1, is it possible to present a simplified version of theorem 2 for the general audience? As it is, definition 2 and theorem 2 are hard to digest just on their own.",et5l9qPUhm,ICLR_2025,"- Model Assumptions: I am unsure of the modelling of synthetic data and how well it translates into practice. Specifically, the authors model synthetic data using a label shift, assuming that the data (X) marginal remains the same. However, it seems unrealistic for autoregressive training (a key experiment in the paper), where the input tokens for next token generation come from the synthetic distribution.
- Experimental Details: The theoretical results establish a strong dependence on the quality of synthetic data. However, the experiments with real data (MNIST/GPT-2) do not provide quantitative metrics to measure the degradation in the synthetic data source (either accuracy of MNIST classifier or perplexity/goodness scores of the trained GPT-2 generator), which makes it hard to ascertain which paradigm (in fig.1) the experiments align best with or what level of degradation in practice results in the observed trend.
- Minor Issues
- Typos: Line 225 (v \in R^{m}), line 299 (synthetic data P2), line 398 (represented by stars)
- Suggestions for Clarity:
- Akin to theorem 1, is it possible to present a simplified version of theorem 2 for the general audience? As it is, definition 2 and theorem 2 are hard to digest just on their own.
- Line 481, before stating the result, can the authors explain in words the process of iterative mixing proposed in Ferbach et al., 2024? It would make the manuscript more self-contained.
- Missing Citation: Line 424, for the MNIST dataset, please include the citation for ""Gradient Based Learning Applied to Document
Recognition"", LeCun et al, 1998
- Visualization: For fig.1, please consider changing the y-axes test error to the same scale. Right now, it is hard to compare the error values or the slope in subplot 1 to those in 2 and 3.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1, All the experiments are conducted using images under 224*224 resolution, it would be interesting to see how the performance will be if we use a larger resolution.",NIPS_2021_2306,NIPS_2021,"1, All the experiments are conducted using images under 224*224 resolution, it would be interesting to see how the performance will be if we use a larger resolution. 2. The accuracy with lower resolutions for some examples is even better than the model with full resolution. Is there any underlying reason for this phenomenon? 3, It seems the improvement over the flops does align well with that over the real latency as shown in fig.3 and tab.3. It would be good to provide the performance and speed trade-off for real acceleration. 4, For the training process, the base models will be first trained and then combined with the resolution selector network for fine-tuning. I’m wondering if it is possible to train the whole model from scratch?
Some minor issues: Line121: “The first is the large classifier network with both high performance and expensive computational costs is first trained”, is the “is first trained” redundant?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '4', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
* In the KeyQN section What is the keypoint mask averaged feature vector? just multiply each feature map element wise by H_psi?,NIPS_2019_962,NIPS_2019,"** * Clarity. There are parts of this paper that are a bit unclear. The diagram and caption for KeyQN section are very helpful, but the actual text section could be fleshed out more. It would nice if the text could have a little more detail on how the outputs from the transporter are input to the KeyQN architecture and how the whole thing is trained. The exploration section was well explained for most part, but it took a bit of time to understand. Maybe would help to have an algorithm box. Also, the explanation of training process a bit confusing. Maybe a diagram of the architecture and how the transporter feeds into this would help. Also, I am confused a bit about whether the transporter is pretrained and frozen or fine-tuned. One quote from the paper in this regard confused me: âOur transporter model and all control policies simultaneuosly â so the weights of the Transporter network are not frozen during the downstream task like in KeyQN? * Experiments: They only show these results on a few games (and no error bars), so it would have been nice (but not a dealbreaker) to see results from more Atari games. They do partially justify this by saying they couldnât use a random policy on other games, but Iâd be curious just to see what happens when they try a couple more games. Would be nice to see comparisons to other exploration methods (they only show results compared to random exploration) Nitpicks/Questions * Makes sense to just refer the reader to the PointNet paper instead of re-explaining it, but a short explanation if possible of PointNet (couple sentences) might be helpful, so that one doesnât have to skim that paper to understand this paper * The diagram in figure 5 (h_psi) should show a heat map not keypoints superimposed on raw frame right? * In the appendix âK is handpicked for each game?â How? Validation loss? * The tracking experiments but the section is a bit unclear. I have a few questions on that front: * why is there a need to separating precision and recall? * why not just report overall mean average precision or F1 score? Might be a bit easier for reader to digest one number * Why bucket into diff sequence lengths? what do the different sequence lengths mean? There is no prediction-in-keypoint space model right? So there is no concept of the performance worsening as the trajectory gets longer. Arenât the keypoint guesses just the output of the PointNet at each frame, so why would the results from a 200 frame sequence be much different than 100 or something? Why not just report overall precision and recall on the test set? * In the KeyQN section What is the keypoint mask averaged feature vector? just multiply each feature map element wise by H_psi?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- Figure 2 right. I found it difficult to distinguish between the different curves. Maybe make use of styles (e.g. dashed lines) or add color.,NIPS_2016_117,NIPS_2016,"weakness of this work is impact. The idea of ""direct feedback alignment"" follows fairly straightforwardly from the original FA alignment work. Its notable that it is useful in training very deep networks (e.g. 100 layers) but its not clear that this results in an advantage for function approximation (the error rate is higher for these deep networks). If the authors could demonstrate that DFA allows one to train and make use of such deep networks where BP and FA struggle on a larger dataset this would significantly enhance the impact of the paper. In terms of biological understanding, FA seems more supported by biological observations (which typically show reciprocal forward and backward connections between hierarchical brain areas, not direct connections back from one region to all others as might be expected in DFA). The paper doesn't provide support for their claim, in the final paragraph, that DFA is more biologically plausible than FA. Minor issues: - A few typos, there is no line numbers in the draft so I haven't itemized them. - Table 1, 2, 3 the legends should be longer and clarify whether the numbers are % errors, or % correct (MNIST and CIFAR respectively presumably). - Figure 2 right. I found it difficult to distinguish between the different curves. Maybe make use of styles (e.g. dashed lines) or add color. - Figure 3 is very hard to read anything on the figure. - I think this manuscript is not following the NIPS style. The citations are not by number and there are no line numbers or an ""Anonymous Author"" placeholder. - I might be helpful to quantify and clarify the claim ""ReLU does not work very well in very deep or in convolutional networks."" ReLUs were used in the AlexNet paper which, at the time, was considered deep and makes use of convolution (with pooling rather than ReLUs for the convolutional layers).","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The claims made in the introduction are far from what has been achieved by the tasks and the models. The authors call this task language learning, but evaluate on question answering. I recommend the authors tone-down the intro and not call this language learning. It is rather a feedback driven QA in the form of a dialog.",NIPS_2016_93,NIPS_2016,"- The claims made in the introduction are far from what has been achieved by the tasks and the models. The authors call this task language learning, but evaluate on question answering. I recommend the authors tone-down the intro and not call this language learning. It is rather a feedback driven QA in the form of a dialog. - With a fixed policy, this setting is a subset of reinforcement learning. Can tasks get more complicated (like what explained in the last paragraph of the paper) so that the policy is not fixed. Then, the authors can compare with a reinforcement learning algorithm baseline. - The details of the forward-prediction model is not well explained. In particular, Figure 2(b) does not really show the schematic representation of the forward prediction model; the figure should be redrawn. It was hard to connect the pieces of the text with the figure as well as the equations. - Overall, the writing quality of the paper should be improved; e.g., the authors spend the same space on explaining basic memory networks and then the forward model. The related work has missing pieces on more reinforcement learning tasks in the literature. - The 10 sub-tasks are rather simplistic for bAbi. They could solve all the sub-tasks with their final model. More discussions are required here. - The error analysis on the movie dataset is missing. In order for other researchers to continue on this task, they need to know what are the cases that such model fails.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
1. The central contribution of modeling weight evolution using ODEs hinges on the mentioned problem of neural ODEs exhibiting inaccuracy while recomputing activations. It appears a previous paper first reported this issue. The reviewer is not convinced about this problem. The current paper doesn't provide a convincing analytical argument or empirical evidence about this issue.,NIPS_2019_463,NIPS_2019,"1. The central contribution of modeling weight evolution using ODEs hinges on the mentioned problem of neural ODEs exhibiting inaccuracy while recomputing activations. It appears a previous paper first reported this issue. The reviewer is not convinced about this problem. The current paper doesn't provide a convincing analytical argument or empirical evidence about this issue. 2. Leaving aside the claimed weakness of neuralODE, the idea of modeling weight evolution as ODE is itself very intellectually interesting and worthy of pursuit. But the empirical improvement reported in Table 1 over AlexNet, ResNet-4 and ResNet-10 is <= 1.75 % for both configurations. The improvement of decoupling weight evolution is in fact even small and not consistent - the improvement in ResNet for configuration 2 is smaller than keeping the evolution of parameters and activations aligned. The improvement for ablation study over neuralODE is also minimal. So, the empirical case for the proposed approach is not convincing. 3. The derivation of optimality conditions for the coupled formulation is interesting because of connections to a machine learning application (backpropagation) but a pretty standard textbook derivation from dynamical systems / controls point of view.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '0']}",,
* L37: Might want to mention that these algorithms follow the sampled policy for awhile.,NIPS_2016_450,NIPS_2016,". First of all, the experimental results are quite interesting, especially that the algorithm outperforms DQN on Atari. The results on the synthetic experiment are also interesting. I have three main concerns about the paper. 1. There is significant difficulty in reconstructing what is precisely going on. For example, in Figure 1, what exactly is a head? How many layers would it have? What is the ""Frame""? I wish the paper would spend a lot more space explaining how exactly bootstrapped DQN operates (Appendix B cleared up a lot of my queries and I suggest this be moved into the main body). 2. The general approach involves partitioning (with some duplication) the samples between the heads with the idea that some heads will be optimistic and encouraging exploration. I think that's an interesting idea, but the setting where it is used is complicated. It would be useful if this was reduced to (say) a bandit setting without the neural network. The resulting algorithm will partition the data for each arm into K (possibly overlapping) sub-samples and use the empirical estimate from each partition at random in each step. This seems like it could be interesting, but I am worried that the partitioning will mean that a lot of data is essentially discarded when it comes to eliminating arms. Any thoughts on how much data efficiency is lost in simple settings? Can you prove regret guarantees in this setting? 3. The paper does an OK job at describing the experimental setup, but still it is complicated with a lot of engineering going on in the background. This presents two issues. First, it would take months to re-produce these experiments (besides the hardware requirements). Second, with such complicated algorithms it's hard to know what exactly is leading to the improvement. For this reason I find this kind of paper a little unscientific, but maybe this is how things have to be. I wonder, do the authors plan to release their code? Overall I think this is an interesting idea, but the authors have not convinced me that this is a principled approach. The experimental results do look promising, however, and I'm sure there would be interest in this paper at NIPS. I wish the paper was more concrete, and also that code/data/network initialisation can be released. For me it is borderline. Minor comments: * L156-166: I can barely understand this paragraph, although I think I know what you want to say. First of all, there /are/ bandit algorithms that plan to explore. Notably the Gittins strategy, which treats the evolution of the posterior for each arm as a Markov chain. Besides this, the figure is hard to understand. ""Dashed lines indicate that the agent can plan ahead..."" is too vague to be understood concretely. * L176: What is $x$? * L37: Might want to mention that these algorithms follow the sampled policy for awhile. * L81: Please give more details. The state-space is finite? Continuous? What about the actions? In what space does theta lie? I can guess the answers to all these questions, but why not be precise? * Can you say something about the computation required to implement the experiments? How long did the experiments take and on what kind of hardware? * Just before Appendix D.2. ""For training we used an epsilon-greedy ..."" What does this mean exactly? You have epsilon-greedy exploration on top of the proposed strategy?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Also, proving lower bounds for round complexity is the major chuck of work involved in proving results for batched ranking problems. However, this paper exploits an easy reduction from the problem of collaborative ranking, and hence, the lower bound results follow as an easy corollary of these collaborative ranking results.",NIPS_2020_1344,NIPS_2020,"1. There have been several results on the problems of batched top-k ranking and fully adaptive coarse ranking in recent years. From that point of view the results in this paper are not particularly surprising. Even the idea that one can reduce the size of active arm set by a factor of n^{1/R} has appeared in [37] for the problem of collaborative top-1 ranking. However, the main novelty in this paper seems to be the application of this idea for the problem of coarse ranking using a successive-accepts-and-rejects type algorithm. 2. Also, proving lower bounds for round complexity is the major chuck of work involved in proving results for batched ranking problems. However, this paper exploits an easy reduction from the problem of collaborative ranking, and hence, the lower bound results follow as an easy corollary of these collaborative ranking results.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
3. The prompting technique used in this study is very basic and fail to leverage the full potentials of LLMs. Carefully curated prompts can gain better results in generating better systematic reviews.,Nk2vfZa4lX,EMNLP_2023,"1. In this study, three distinct LLMs named Galactica, BioMedLM, and ChatGPT have been selected by the authors. The differences between these LLMs are outlined in the related work section of the paper. Despite their notable distinctions in training data and size, the evaluation of all these LLMs follows a uniform approach. A more effective approach would involve assessing the outputs of each LLM separately, as this could provide valuable insights into their relative performance in generating medical systematic reviews. It is plausible that certain LLMs might exhibit higher risk factors, while others could excel in generating coherent systematic reviews. In essence, this study would benefit significantly from a comprehensive comparative analysis between the LLMs, allowing for a more nuanced understanding of their respective capabilities, limitations, and potential benefits.
2. The number of samples presented to each domain expert appears to be relative inadequate to draw definitive conclusions about the abilities and constraints of LLMs in generating systematic reviews. Additionally, during the expert interviews, the inclusion of human-written systematic reviews, without indicating their human origin, could offer valuable insights. This approach would allow observation of how domain experts react to these reviews, shedding light on the deficiencies of LLM-generated systematic reviews and thereby allowing a more comprehensive understanding of the lacking of the LLM generated review.
3. The prompting technique used in this study is very basic and fail to leverage the full potentials of LLMs. Carefully curated prompts can gain better results in generating better systematic reviews.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- Additional experiments on larger data sets would be nice (but I understand that compute might be an issue). --- Thanks for the author response. I still think maintaining the probabilities might become an issue, in particular at large batch size, but I don't think this aspect is critical. Generally, the response addressed my concerns well.",NIPS_2020_1623,NIPS_2020,"- If I understand correctly, the method requires maintaining a probability vector for each data point. This is not an issue for small data sets with few classes, but can become a problem at ImageNet scale. I did not find any comment regarding this issue in the main paper or in the supplement. Could the authors please elaborate on this? - From Table 2 b) it seems that for 40% label noise on CIFAR10 the method is reasonably robust to the hyper parameter values. Does this observation transfer to other corruption percentages and data sets? - Additional experiments on larger data sets would be nice (but I understand that compute might be an issue). --- Thanks for the author response. I still think maintaining the probabilities might become an issue, in particular at large batch size, but I don't think this aspect is critical. Generally, the response addressed my concerns well.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '1']}",,
"- Table 4: The performance on REC and RES are clearly behind more recent models. For example, GLaMM [ref1] achieves 83.2 RES cIoU on RefCOCO TestA, and UNINEXT [ref2] achieves 89.4 REC accuracy (IoU>0.5) on RefCOCOg Test.",a4PBF1YInZ,ICLR_2025,"1. Although the model is able to perform various tasks, the performance in each domain is questionable. In the quantitative results, this paper seems to be deliberately comparing with weaker models. Examples include but are not limited to:
- Table 2: Only earlier grounded MLLMs are compared. An early baseline, LLaVA-1.5-7B (the ""fundamental MLLM"" module in the proposed method should be comparable to it) can achieve 78.5 accuracy on VQAv2.
- Table 3: On POPE, ""F1 score"" should be the main metric to compare with, as one may achieve a very high precision by making the model predicting conservatively. Previous models like LLaVA-1.5 also mainly reports the F1 score.
- Table 4: The performance on REC and RES are clearly behind more recent models. For example, GLaMM [ref1] achieves 83.2 RES cIoU on RefCOCO TestA, and UNINEXT [ref2] achieves 89.4 REC accuracy (IoU>0.5) on RefCOCOg Test.
2. Although this work claims keypoints as one major decoding modality, there is no quantitative evaluation of keypoint detection.
3. The ablation study does not discuss the most important module designs, e.g., decoding with CoT vs. without CoT, how data mixture in VT-Instruct affects the final results.
4. The presentation is not suitable for an academic research paper. In sections 3 and 4, only a high-level overview of the method is introduced, with a few short paragraphs and 5 large-sized figures. After reading these sections, readers may get a glimpse of the model and data, but too many details are missing or hidden in the appendix.
5. The qualitative results mainly show simple cases with few objects clearly visible. Even so, there seems to be many artifacts in the visualization (e.g., duplicate ""person"" detections in Figure 8 top right, low-quality masks in Figure 9 top right). It is unclear how the model perform in more challenging natural scene images.
[ref1] Hanoona Rasheed, Muhammad Maaz, Sahal Shaji, Abdelrahman Shaker, Salman Khan, Hisham Cholakkal, Rao M. Anwer, Eric Xing, Ming-Hsuan Yang, Fahad S. Khan. GLaMM: Pixel Grounding Large Multimodal Model. In CVPR, 2024.
[ref2] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, Huchuan Lu. Universal Instance Perception as Object Discovery and Retrieval. In CVPR, 2023.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"5 shows evidence that some information is learned before the model is able to use the concepts."" --> I think ""evidence"" may be too strong here, and would say something more like ""Fig.",NIPS_2022_331,NIPS_2022,"I believe that one small (but important) part of the paper could use some clarifications in the writing: Section 3.2 (on Representational Probing). I will elaborate below.
I think that a couple of claims in the paper may be slightly too strong and need a bit more nuance. I will elaborate below.
A lot of the details described in Section 3.3 (Behavioral Tests) seem quite specific to the game of Hex. For the specific case of Hex, we can indeed know how to create such states that (i) contain a concept, (ii) contain that concept in only exactly one place, (iii) make sure that the agent must play according to the concept immediately, because otherwise it would lose. I imagine that setting up such specific situations may be much more difficult in many other games or RL environments, and would certainly require highly game-specific knowledge again for such tasks. This seems like a potential limitation (which doesn't seem to be discussed yet).
On weakness (1):
The first sentence that I personally found confusing was ""To form the random control, for each board ( H ( 0 ) , y )
in the probing dataset, we consistently map each cell in that board to a random cell, forming $H_s^{(0)}."" I guess that ""map each cell in that board to a random cell"" means creating a random mapping from all cells in the original board to all cells in the control board, in a way such that every original cell maps to exactly one randomly-selected control cell, and every control cell is also mapped to by exactly one original cell. And then, the value of each original cell (black/white/empty) is assigned to the control cell that it maps to. I guess this is what is done, and it makes sense, but it's not 100% explicit. I'm afraid that many readers could misunderstand it as simply saying that every cell gets a random value directly.
Then, a bit further down under Implementation Details, it is described how the boards in the probing dataset get constructed. I suspect it would make more sense to actually describe this before describing how the matching controls are created.
On weakness (2):
(a) The behavioral tests involve states created specifically such that they (i) contain the concept, but also (ii) demand that the agent immediately plays according to the concept, because it will lose otherwise. In the game of Hex, this means that all of these board states, for all these different concepts, actually include one more new ""concept"" that is shared across all the tests; a concept that recognises a long chain of enemy pieces that is about to become a winning connection if not interrupted by playing in what is usually just one or two remaining blank cells in between. So, I do not believe that we can say with 100% certainty that all these behavior tests are actually testing for the concept that you intend them to test for. Some or all of them may simply be testing more generally if the agent can recognise when it needs to interrupt the opponent's soon-to-be-winning chain.
(b) ""Fig. 5 shows evidence that some information is learned before the model is able to use the concepts."" --> I think ""evidence"" may be too strong here, and would say something more like ""Fig. 5 suggests that some information may be learned [...]"". Technically, Fig. 5 just shows that there is generally a long period with no progress on the tests, and after a long time suddenly rapid progress on the tests. To me this indeed suggests that it is likely that it is learning something else first, but it is not hard evidence. It could also be that it's just randomly wandering about the parameter space and suddenly gets lucky and makes quick progress then, having learned nothing at all before.
(c) ""Behavioral tests can also expose heuristics the model may be using."" --> yes, but only if we actually already know that the heuristics exist, and know how to explicitly encode them and create probes for them. They can't teach us any new heuristics that we didn't already know about. So maybe, better phrasing could be something like ""Behavioral tests can also confirm whether or not the model may be using certain heuristics.""
It may be useful to discuss the apparent limitation that quite a bit of Hex-specific knowledge is used for setting up the probes (discussed in more detail as a weakness above).
It may be useful to discuss the potential limitation I discussed in more detail above that the behavioral tests may simply all be testing for an agent's ability to recognise when it needs to interrupt an opponent's immediate winning threat.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"1. **Limited Novelty in Video Storyboarding**: The innovation of the proposed video storyboarding approach is limited. The primary method relies on frame-wise SDSA, which largely mirrors the approach used in ConsiStory. The only notable difference lies in the mask source, utilizing CLIPseg and OTSU segmentation rather than cross-attention.",0zRuk3QdiH,ICLR_2025,"1. **Limited Novelty in Video Storyboarding**: The innovation of the proposed video storyboarding approach is limited. The primary method relies on frame-wise SDSA, which largely mirrors the approach used in ConsiStory. The only notable difference lies in the mask source, utilizing CLIPseg and OTSU segmentation rather than cross-attention.
2. **Poor Writing and Project Organization**: The paper's writing and the project page's layout hinder comprehension, making it difficult for readers to follow the key contributions the authors intend to convey.
3. **Minimal Improvement over Baseline Models**: The generated video storyboarding results appear similar to those produced by existing video generation baselines like Videocrafter2 or TokenFlow encoder, with little noticeable difference in output quality.
4. **Lack of Motion Dynamics**: The method demonstrates limited motion dynamics. In most video segments, the objects remain static, and in every case, the object consistently occupies the center of the frame, resulting in rigid, uninspired visuals.
5. **Overclaiming the Benchmark**: The authors’ claim of establishing a benchmark based on a dataset of only 30 videos, each containing 5 video shots, is unsubstantiated. This dataset is insufficiently sized and lacks diversity, with evaluations limited to character consistency and dynamic degree, providing a narrow view that does not comprehensively assess the model's capabilities.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"4) It seems that the weakness that this method is addressing would be more prominent in images comprised of multiple objects, or cluttered scenes. It would be very interesting to compare this approach to previous ones on few-shot classification on such a dataset!",NIPS_2019_360,NIPS_2019,"weakness (since unfortunately there are other confounding factors). Further, orthogonally to the accuracy results, it is an interesting finding if standard approaches indeed suffer from this and the proposed method provides a remedy. I would therefore focus on these qualitative results more, and explain in the main text (not just the appendix) exactly how those visualization are created, and show those results for various models. 2) Somewhat related to the previous point: Pure metric-based models like Prototypical Networks lack an explicit mechanism for adaptation to each task at hand and it therefore seems plausible that they indeed suffer from the identified issue. However, it is less clear whether (or to what extent) models that do perform task-specific adaptation run the same danger. Intuitively, it seems that task adaptation also constitutes a mechanism for modifying the embedding function so that it favours the identification of objects that are targets of the associated classification task. By task adaptation here Iâm referring either to gradient-based adaptation (as in MAML and variants) or amortized conditioning-based adaptation (as in TADAM for example). Therefore, it would be very interesting to empirically compare the proposed method to these other ones not only in terms of classification accuracy but also qualitatively via visualizations as in Figure 1 that show the areas of the image that a model focuses more for making classification decisions. 3) Suggestion for the transductive framework: In Equation 8, it might be useful to incorporate the unlabeled examples in a weighted fashion instead of trusting that every example whose confidence surpasses a manually-set threshold can safely contribute to the prototype of the class that it is predicted to belong to. Specifically, the contribution of an unlabeled example to the updated class prototype can be weighted by the cosine similarity between that unlabeled example and that prototype (normalized across classes) and maybe additionally by the confidence c_b^q. This might slightly relieve the need to find the perfect threshold, since even if it is not conservative enough, a query example will be prohibited by modifying a prototype too much. An example of this is in Ren et al. [1] when computing refined prototypes by including unlabeled examples. 4) It seems that the weakness that this method is addressing would be more prominent in images comprised of multiple objects, or cluttered scenes. It would be very interesting to compare this approach to previous ones on few-shot classification on such a dataset! 5) For more easily assessing the degree of apples-to-applesness of the various comparisons in the tables, it would be useful to note which of the listed methods use data augmentations (as until recently this was not common practice for few-shot classification), what architecture they use, and what objective (most are episodic only but I think TADAM also performs joint training as the proposed method). 6) Another difference between the proposed approach and previous Prototypical Network-like methods is that the distance comparisons that inform the classification decisions are done in a feature-wise manner in this work. Specifically, when comparing embeddings a and b, for each spatial location, the distance between the feature vectors of a and b at that location is computed. The final estimate of the distance between a and b is obtained by aggregating those feature-wise distance estimates over all spatial locations. In contrast, usually the output of the last embedding layer is reshaped into a single vector (of shape channels x height x width) and distance comparisons of examples are made by directly comparing these vectors. It would therefore be useful to perform another ablation where a standard Prototypical Network is modified to perform the same type of distance comparison as their method. 7) Similarly to how the proposed transductive method was applied to other models, it would be nice to see results where the proposed joint training is also applied to other models, since this is orthogonal to the choice of the meta-learner too. References [1] Meta-Learning for Semi-Supervised Few-shot Classification. Ren et al. ICLR 2018.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- While I understand the space limitations, I think the paper could greatly benefit from more explanation of the meaning of the bounds (perhaps in the appendix).",NIPS_2020_547,NIPS_2020,"- While I understand the space limitations, I think the paper could greatly benefit from more explanation of the meaning of the bounds (perhaps in the appendix). - Line 122: it's not obvious to me that the smoothed bound is more stable, since the \gamma factor in the numerator is also larger. Some calculations here, or a very simple experiment, would greatly help the reader understand when smoothing would be desirable. - The above also applies for the discussion on overestimation starting on line 181, especially in the trade-off of reducing overestimation error and converging to a suboptimal value function. - The above applies for the combined smoothness + regularization algorithm","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets.",NIPS_2018_232,NIPS_2018,"- Strengths: the paper is well-written and well-organized. It clearly positions the main idea and proposed approach related to existing work and experimentally demonstrates the effectiveness of the proposed approach in comparison with the state-of-the-art. - Weaknesses: the research method is not very clearly described in the paper or in the abstract. The paper lacks a clear assessment of the validity of the experimental approach, the analysis, and the conclusions. Quality - Your definition of interpretable (human simulatable) focuses on to what extent a human can perform and describe the model calculations. This definition does not take into account our ability to make inferences or predictions about something as an indicator of our understanding of or our ability to interpret that something. Yet, regarding your approach, you state that you are ânot trying to find causal structure in the data, but in the modelâs responseâ and that âwe can freely manipulate the input and observe how the model response changesâ. Is your chosen definition of interpretability too narrow for the proposed approach? Clarity - Overall, the writing is well-organized, clear, and concise. - The abstract does a good job explaining the proposed idea but lacks description of how the idea was evaluated and what was the outcome. Minor language issues p. 95: âfrom fromâ -> âfromâ p. 110: âto toâ -> âhow toâ p. 126: âas wayâ -> âas a wayâ p. 182 âcan sortedâ -> âcan be sortedâ p. 197: âon directly onâ -> âdirectly onâ p. 222: âwhere wantâ -> âwhere we wantâ p. 245: âas accurateâ -> âas accurate asâ Tab. 1: âsquareâ -> âsquared errorâ p. 323: âthis are featuresâ -> âthis is featuresâ Originality - the paper builds on recent work in IML and combines two separate lines of existing work; the work by Bloniarz et al. (2016) on supervised neighborhood selection for local linear modeling (denoted SILO) and the work by Kazemitabar et al. (2017) on feature selection (denoted DStump). The framing of the problem, combination of existing work, and empirical evaluation and analysis appear to be original contributions. Significance - the proposed method is compared to a suitable state-of-the-art IML approach (LIME) and outperforms it on seven out of eight data sets. - some concrete illustrations on how the proposed method makes explanations, from a user perspective, would likely make the paper more accessible for researchers and practitioners at the intersection between human-computer interaction and IML. You propose a âcausal metricâ and use it to demonstrate that your approach achieves âgood local explanationsâ but from a user or human perspective it might be difficult to get convinced about the interpretability in this way only. - the experiments conducted demonstrate that the proposed method is indeed effective with respect to both accuracy and interpretability, at least for a significant majority of the studied datasets. - the paper points out two interesting directions for future work, which are likely to seed future research.","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"- Kernels are implemented with OpenAI's Triton, not CUDA; a full-page explanation is unnecessary due to well-known engineering improvements.",SjgfWbamtN,ICLR_2024,"- The prediction process is faster, but final performance significantly decreases.
- Removing IPA is disadvantageous, as the structure module is less costly than Evoformer.
- Kernels are implemented with OpenAI's Triton, not CUDA; a full-page explanation is unnecessary due to well-known engineering improvements.
- The analysis of kernels is wrong. For example, ""This reduces the number of reads from 5 to 1, and the number of writes from 4 to 1"". The Wx and Vx are matrix multiplication operators, which will call GEMM kernels, thus these read/write cannot be merged. We usually can only save the read/write times for element-wise operators.
- The method relies on a computationally demanding pretrained protein language model; simplification would be beneficial.
- Coordinate recovery omits chirality consideration, potentially negatively impacting performance.
- In-depth analysis of uncertainty estimation technique is needed for better understanding of robustness.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Why is this zero-shot? In addition, I guess the transferability might be limited by the real difficulty of the source task / target task. Walker-run is clearly harder than walker-walk, so the policy transfer here is possible. Also, for the manipulation scenario, 3-prong task with both clockwise / counter clockwise rotations, together with one 4-prong task, actually provides sufficient information about the target task. I guess it might be difficult to transfer a policy from simpler tasks to more complex tasks. This has to be made clear in the paper. Otherwise, it is quite misleading.",NIPS_2021_1072,NIPS_2021,"I have certain concerns about the paper.
1/ I think the contribution of the paper is a bit limited. V-MAIL combines several existing ideas, e.g., latent imagination, GAIL, variational state space model, etc., and achieves a good performance. However, how these components affect each other and how they contribute to the final performance are not clear. An ablation study is also missing from the paper. In this case, it would be hard to get inspiration from reading it.
2/ Although the paper has provided theoretical analysis about the model-based adversarial imitation learning (sect. 4.1 and sect. 4.2), they are disconnected from the practical implementations (sect. 4.3). In particular, Theorem 1 shows that the divergence of the visitation distributions in a MDP can be upper bounded by a divergence of the visitation distributions in a POMDP. However, in the practical implementation, a variational state space model captures the belief, rather than the visitation distribution of the belief. In addition, it feels difficult to compute the visitation frequency of a belief, whose size is exponential to the size of the history and the state space. I believe the proposed algorithm indeed has its merit, but I don’t think Theorem 1 provides a correct justification of the optimization objective used in this paper.
3/ I feel the author should be careful when making certain claims. For example, from line 39 to line 48, the authors are analyzing the limitations of the existing IRL methods and adversarial imitation learning methods. “These approaches explicitly train a GAN-based classifier [17] to distinguish the visitation distribution of the agent from the expert, and use it as a reward signal for training the agent with RL….” However, not all IRL methods are adversarial imitation learning. In fact, most of them don’t train a GAN-based classifier and do RL afterwards. Instead, a lot of them recover the reward and do planning instead.
4/ The authors claimed that V-MAIL achieves zero-shot transfer to novel environments. However, the policy is fine-tuned with additional expert demonstrations, as shown in Alg. 2. Why is this zero-shot? In addition, I guess the transferability might be limited by the real difficulty of the source task / target task. Walker-run is clearly harder than walker-walk, so the policy transfer here is possible. Also, for the manipulation scenario, 3-prong task with both clockwise / counter clockwise rotations, together with one 4-prong task, actually provides sufficient information about the target task. I guess it might be difficult to transfer a policy from simpler tasks to more complex tasks. This has to be made clear in the paper. Otherwise, it is quite misleading.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. Though the improvement is consistent for different frameworks and tasks, the relative gains are not very strong. For most of the baselines, the proposed methods can only achieve just about 1% gain on a relative small backbone ResNet-50. As the proposed method introduces global pooling into its structure, it might be easy to improve a relatively small backbone since it is with a smaller receptive field. I suspect whether the proposed method still works well on large backbone models like Swin-B or Swin-L.",ICLR_2023_3203,ICLR_2023,"1. The novelty is limited. The proposed method is too similar to other attentional modules proposed in previous works [1, 2, 3]. The group attention design seems to be related to ResNeSt [4] but it is not discussed in the paper. Although these works did not evaluate their performance on object detection and instance segmentation, the overall structures between these modules and the one that this paper proposed are pretty similar.
2. Though the improvement is consistent for different frameworks and tasks, the relative gains are not very strong. For most of the baselines, the proposed methods can only achieve just about 1% gain on a relative small backbone ResNet-50. As the proposed method introduces global pooling into its structure, it might be easy to improve a relatively small backbone since it is with a smaller receptive field. I suspect whether the proposed method still works well on large backbone models like Swin-B or Swin-L.
3. Some of the baseline results do not matched with their original paper. I roughly checked the original Mask2former paper but the performance reported in this paper is much lower than the one reported in the original Mask2former paper. For example, for panoptic segmentation, Mask2former reported 51.9 but in this paper it's 50.4, and the AP for instance segmentation reported in the original paper is 43.7 but here what reported is 42.4.
Meanwhile, there are some missing references about panoptic segmentation that should be included in this paper [5, 6]. Reference
[1] Chen, Yunpeng, et al. ""A^ 2-nets: Double attention networks."" NeurIPS 2018.
[2] Cao, Yue, et al. ""Gcnet: Non-local networks meet squeeze-excitation networks and beyond."" T-PAMI 2020
[3] Yinpeng Chen, et al. Dynamic convolution: Attention over convolution kernels. CVPR 2020.
[4] Zhang, Hang, et al. ""Resnest: Split-attention networks."" CVPR workshop 2022.
[5] Zhang, Wenwei, et al. ""K-net: Towards unified image segmentation."" Advances in Neural Information Processing Systems 34 (2021): 10326-10338.
[6] Wang, Huiyu, et al. ""Max-deeplab: End-to-end panoptic segmentation with mask transformers."" CVPR 2021","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2. The analysis of neural networks contributes less. With the existing NTK theorem, the extension from linear models to wide fully-connected neural networks is trivial (Section 3.2, 3.3). The work bypasses the core problem of overparametrized neural networks and only considers the easy wide fully-connected neural networks.",ICLR_2022_2081,ICLR_2022,"1. My feeling is that the conclusion is somewhat overclaimed. In both abstract and conclusion, it is emphasized that this work proves the pessimistic result that reweighting algorithms always overfit. However, the paper only proves that this conclusion might be true for some specific situations. For example, the reweighting algorithms need to satisfy Assumption 1 and Assumption 2, which means not all reweighting algorithms are considered. The overparameterized models need to be linear models, linearized neural networks or wide fully-connected neural networks, which are not commonly used in practice. Besides, the squared loss needs to be used to confirm the update rule is linear. All those assumptions are not quite mild for me. 2. The analysis of neural networks contributes less. With the existing NTK theorem, the extension from linear models to wide fully-connected neural networks is trivial (Section 3.2, 3.3). The work bypasses the core problem of overparametrized neural networks and only considers the easy wide fully-connected neural networks. 3. The theoretical results and experiments do not match. The theoretical proof considers wide fully-connected neural networks, while the experiments utilize a ResNet18 as the model, which is quite different. 4. Some key steps are empirical, although the paper claims that it provides a theoretical backing in the abstract. For example, this paper only proves that reweighting algorithms will converge to the same level as ERM, but the conclusion that ERM has a poor worst-group test performance is summarized through observation in practice. Besides, the paper can only empirically demonstrate that commonly used algorithms satisfy Assumption 2.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- Given that many methods aim at one of the three tasks, having 5, 6 and 4 datasets for the tasks respectively, might not be enough for a very rigorous evaluation, in particular if some of the datasets are so large that not all algorithms can be used on them. Addendum: Thank you to the authors for their detailed reply. A repository and online platform for reproducing the experiments was provided, and it was clarified that the datasets are substantially novel. Motivations for the number and choice of datasets were given and I updated my assessment to reflect that.",NIPS_2020_1854,NIPS_2020,"- The contribution goes in several directions which makes the paper hard to evaluate; is the main contribution the selection of existing datasets, introducing new datasets or new versions of datasets, the empirical evaluation or the software tooling? - The dataset does not describe existing datasets and benchmarks, and so it is hard to judge the exact differences between the proposed datasets and currently used datasets. A more direct comparison might be useful, and it's not clear why existing, smaller datasets are not included in the collection. - For some of the datasets, it's unclear if or how they have been used or published before. In particular, the datasets from Moleculenet seem to be mostly reproduced, using the splitting strategy that was suggested in their paper, with the modification potentially being addition of new features. - If the selection of the datasets is a main contribution, the selection process should be made more clear. What was the pool of datasets that was drawn from, and how were datasets selected? An example of such a work is the OpenML100 and OpenML CC-18 for classification, see Bischl et. al. ""OpenML Benchmarking Suites"". or Gijsbers et al ""An Open Source AutoML Benchmark"" In addition to selection of the datasets, the selection of the splitting procedure and split ratios also seems ad-hoc and is not detailed. - Neither the software package, nor the datasets, nor the code for the experiments has been submitted as supplementary material, and the details in the paper are unlikely to be enough to reproduce the creation of the datasets or the experiments given the datasets. - Given that many methods aim at one of the three tasks, having 5, 6 and 4 datasets for the tasks respectively, might not be enough for a very rigorous evaluation, in particular if some of the datasets are so large that not all algorithms can be used on them. Addendum: Thank you to the authors for their detailed reply. A repository and online platform for reproducing the experiments was provided, and it was clarified that the datasets are substantially novel. Motivations for the number and choice of datasets were given and I updated my assessment to reflect that.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152.",NIPS_2020_125,NIPS_2020,"- In section 3.1, the logic of extending HOGA from second order is not consistent with the extension from first order to second order; i.e., second order attention creates one more intermediate state U compared to the first order attention module. However, from the second order to higher order attention module, although intermediate states U0, U1 … are created, they are only part of the intermediate feature (Concatenating them will form U with full channel resolution). In this way, it seems we could regard the higher order attention module as a special form of second order attention module. - The paper does not clearly explain the intuition as to why different channel groups should have different attention mechanisms; i.e., in what specific way the network can benefit from the proposed channel group specific attention module. - Experiments are not solid enough: 1. There are no ablation studies on the effect of parameter numbers, so it is not clear whether the performance gain is due to the proposed approach or additional parameters. 2. Although there is good performance on imageNet classification with ResNet50/34/18, there are no results with larger models like ResNet101/152. 3. There are no results using strong object detection frameworks; the current SSD framework is relatively weak (e.g. Faster RCNN would be a stronger, more standard approach); it is not clear whether the improvements would be retained with a stronger base framework. - The proposed approach requires larger FLOPS compared to baselines; i.e., any performance gain requires large computation overhead (this is particularly pronounced in Table 3). - In Table 3 shows ResNet32/56 but L222 refers to ResNet34/50, which is confusing.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '4', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"* Coming to the multi-fidelity framework and sequential design for learning quantities (e.g. probabilities of threshold exceedance) of interest, see notably Assessing Fire Safety using Complex Numerical Models with a Bayesian Multi-fidelity Approach (Stroh et al. 2017) https://www.sciencedirect.com/science/article/pii/S0379711217301297?via%3Dihub Some further points * Somehow confusing to read ""relatively inexpensive"" in the abstract and then ""expensive to evaluate"" in the first line of the introduction!",NIPS_2018_482,NIPS_2018,", and while I recommend acceptance, I think that it deserves to be substantially improved before publication. One of the main concerns is that part of the relevant literature has been ignored, and also importantly that the proposed approach has not really been extensively compared to potential competitors (that might need to be adapted to the multi-source framework; not e also that single-fidelity experiments could be run in order to better understand how the proposed acquisition function compares to others from the literature). Another main concern in connection with the previous one is that the presented examples remain relatively simple, one testcase being an analytical function and the other one a one-dimensional mototonic function. While I am not necessarily requesting a gigantic benchmark or a list of complicated high-dimensional real-world test cases, the paper would significantly benefit from a more informative application section. Ideally, the two aspects of improving the representativity of numerical test cases and of better benchmarking against competitor strategies could be combined. As of missing approaches from the literature, some entry points follow: * Adaptive Designs of Experiments for Accurate Approximation of a Target Region (Picheny et al. 2010) http://mechanicaldesign.asmedigitalcollection.asme.org/article.aspx?articleid=1450081 *Fast kriging-based stepwise uncertainty reduction with application to the identification of an excursion set (Chevalier et al. 2014a) http://amstat.tandfonline.com/doi/full/10.1080/00401706.2013.860918 * NB: approaches from the two articles above and more (some cited in the submitted paper but not benchmarked against) are coded for instance in the R package ""KrigInv"". The following article gives an overview of some of the package's functionalities: KrigInv: An efficient and user-friendly implementation of batch-sequential inversion strategies based on kriging (Chevalier et al. 2014b) * In the following paper, an entropy-based approach (but not in the same fashion as the one proposed in the submitted paper) is used, in a closely related reliability framework: Gaussian process surrogates for failure detection: A Bayesian experimental design approach (Wang et al. 2016) https://www.sciencedirect.com/science/article/pii/S002199911600125X * For an overall discussion on estimating and quantifying uncertainty on sets under GP priors (with an example in contour line estimation), see Quantifying Uncertainties on Excursion Sets Under a Gaussian Random Field Prior (Azzimonti et al. 2016) https://epubs.siam.org/doi/abs/10.1137/141000749 NB: the presented approaches to quantify uncertainties on sets under GP priors could also be useful here to return a more complete output (than just the contour line of the predictive GP mean) in the CLoVER algorithm. * Coming to the multi-fidelity framework and sequential design for learning quantities (e.g. probabilities of threshold exceedance) of interest, see notably Assessing Fire Safety using Complex Numerical Models with a Bayesian Multi-fidelity Approach (Stroh et al. 2017) https://www.sciencedirect.com/science/article/pii/S0379711217301297?via%3Dihub Some further points * Somehow confusing to read ""relatively inexpensive"" in the abstract and then ""expensive to evaluate"" in the first line of the introduction! * L 55 ""A contour"" should be ""A contour line""? * L88: what does ""f(l,x) being the normal distribution..."" mean? * It would be nice to have a point-by-point derivation of equation (10) in the supplementary material (that would among others help readers including referees proofchecking the calculation). * About the integral appearing in the criterion, some more detail on how its computation is dealt with could be worth. ##### added after the rebuttal I updated my overall grade from 7 to 8 as I found the response to the point and it made me confident that the final paper would be improved (by suitably accounting for my remarks and those of the other referees) upon acceptance. Let me add a comment about related work by RÃ©mi Stroh. The authors are right, the Fire Safety paper is of relevance but does actually not address the design of GP-based multi-fidelity acquisition functions (in the BO fashion). However, this point has been further developed by Stroh et al; see ""Sequential design of experiments to estimate a probability of exceeding a threshold in a multi-fidelity stochastic simulator"" in conference contributions listed in http://www.l2s.centralesupelec.fr/en/perso/remi.stroh/publications","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '1']}",,
"• How does this method address sparse reward problems in a better way? From the experiments, this does not support well. in practice, the proposed method requires sub-task-specific rewards to be specified, which would be similar to providing a dense reward signal that includes rewards for reaching sub-goals. If given the sum of low-level reward as the global reward, will the other methods (Qmix) solve the sparse-reward tasks as well? minor comments:",ICLR_2023_3317,ICLR_2023,"Weakness main comments: • what is the advantage of using a differentiable LP layer (GNN and a LP solver) as a high-level policy, shown in Eq. 10?
– compare it to [1] that considers the LP optimization layer as a meta-environment?
– compare it to an explicit task assignment protocol (e.g. not implicit).
E.g. a high-level policy that directly outputs task weightings instead of the intermediary C matrix?
• How does this method address sparse reward problems in a better way? From the experiments, this does not support well. in practice, the proposed method requires sub-task-specific rewards to be specified, which would be similar to providing a dense reward signal that includes rewards for reaching sub-goals. If given the sum of low-level reward as the global reward, will the other methods (Qmix) solve the sparse-reward tasks as well?
minor comments: • It is hard to determine whether the solution to the matching problem (learned agent-task score matrix C) optimized by LP is achieving global perspective over the learning process.
• When the lower-level policies are also trained online, the learning could be unstable. Details on how to solve the instability in hierarchical learning are missing.
• What is the effect of the use of hand-defined tasks on performance? what is the effect of the algorithm itself? maybe do an ablation study.
• Section 5.2 ”training low-level actor-critic” should be put in the main text.
[1] Carion N, Usunier N, Synnaeve G, et al. A structured prediction approach for generalization in cooperative multi-agent reinforcement learning[J]. Advances in neural information processing systems, 2019, 32: 8130-8140.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- It is not clearly indicated whether the curated AH36M dataset is used for training. If so, did other methods eg. HMR, SPIN have access to AH36M data during training for a fair comparison?",NIPS_2020_1720,NIPS_2020,"- In L104-112 several prior arts are listed. I understand that the task authors tackle is predicting full mesh, but why proposed method is better than [21] or [6]? What makes the proposed approach better than previous methods? From the experiments, the performance difference is clear. However, I am missing the core insights/motivations behind the approach. - In L230, it is indicated that ""we allow it (3D pose regressor) to output M possible predictions and, out of this set, we select the one that minimizes the MPJPE/RE metric"". Comparison here seems a bit unfair. Instead of using oracle poses, the authors would compute the MPJPE/RE for all of the M or maybe n out of M poses, then report the median error. - It is not clearly indicated whether the curated AH36M dataset is used for training. If so, did other methods eg. HMR, SPIN have access to AH36M data during training for a fair comparison? - There is no promise to release the code and the data. Even though the method is explained clearly, a standard implementation would be quite helpful for the research community. - There is no failure cases/limitations sections. It would be insightful to include such information for researchers who would like to build on this work.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
2. There are some confusing mistake in the proof of the main results. This paper lacks a detailed discussion and comparison with the previous work. This paper seemed not to give any new insight on this field.,NIPS_2022_815,NIPS_2022,"Weakness: 1. This paper requires a more detailed discussion and comparison with the previous related work. 2. There are some confusing mistake in the proof of the main results.
This paper lacks a detailed discussion and comparison with the previous work.
This paper seemed not to give any new insight on this field.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2.The motivation is unclear. Why the adversarial network is needed in this model? the comparison of experimental results is unfair. The proposed model is equipped with newly-added CAT and GAN, which is a bigger model than others. Even the pre-trained model is compared with other models.",NIPS_2022_1725,NIPS_2022,"Weakness:
the novelty and contribution are limited. The key contribution is the class-aware transformer module, a revised transformer, to learn the class aware context. Pyramid structure and adversarial training are common approaches used in medical image analysis. 2.The motivation is unclear. Why the adversarial network is needed in this model?
the comparison of experimental results is unfair. The proposed model is equipped with newly-added CAT and GAN, which is a bigger model than others. Even the pre-trained model is compared with other models.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. For the captioning experiment, the paper compares to related work only on some not official test set or dev set, however the final results should be compared on the official COOC leader board on the blind test set: https://competitions.codalab.org/competitions/3221#results e.g. [5,17] have won this challenge and have been evaluated on the blind challenge set. Also, several other approaches have been proposed since then and significantly improved (see leaderboard, the paper should at least compare to the once where an corresponding publication is available).",NIPS_2016_265,NIPS_2016,"1. For the captioning experiment, the paper compares to related work only on some not official test set or dev set, however the final results should be compared on the official COOC leader board on the blind test set: https://competitions.codalab.org/competitions/3221#results e.g. [5,17] have won this challenge and have been evaluated on the blind challenge set. Also, several other approaches have been proposed since then and significantly improved (see leaderboard, the paper should at least compare to the once where an corresponding publication is available). 2. A human evaluation for caption generation would be more convincing as the automatic evaluation metrics can be misleading. 3. It is not clear from Section 4.2 how the supervision is injected for the source code caption experiment. While it is over interesting work, for acceptance at least points 1 and 3 of the weaknesses have to be addressed. ==== post author response === The author promised to include the results from 1. in the final For 3. it would be good to state it explicitly in Section section 4.2. I encourage the authors to include the additional results they provided in the rebuttal, e.g. T_r in the final version, as it provides more insight in the approach. Mine and, as far as I can see, the other reviewers concerns have been largely addressed, I thus recommend to accept the paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"4. The experimental results are unreliable, especially in Table 1, where the MSE is significantly smaller than the MAE, which raises concerns about their validity.",USGY5t7fwG,ICLR_2025,"1. The proposed method lacks novelty, as it simply splits the image into background and foreground before processing them separately.
2. In Table 1, the best MAE for SD→SR is not achieved by the proposed method, yet it is marked in bold. This should be corrected.
3. In Table 1, the best performance for SN→FH is also incorrectly highlighted, and the proposed method performs worse than other methods by a large margin. The authors should explain this discrepancy.
4. The experimental results are unreliable, especially in Table 1, where the MSE is significantly smaller than the MAE, which raises concerns about their validity.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.,NIPS_2017_401,NIPS_2017,"Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.
Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility:
Appears to be reproducible.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
2. No adversarial loss to guarantee the perturbed data being similar to the authentic data.,ICLR_2021_343,ICLR_2021,"1. This paper is not well-organized and many parts are misleading. For example, above Eq.3, the author assumes P_{G_{0}} = P_{D}. Does the author take the samples generated by the root generator as the authentic dataset? However, in Section 2 above Eq. 4, the author claims that the authentic data does not belong to any generator. 2. In Eq.4, the key-dependent generator is obtained via adding perturbation to the output of the root model. This setting may be troublesome as :1. These generators are not actually trained. This is different from the problem which this paper tempt to solve. 2. No adversarial loss to guarantee the perturbed data being similar to the authentic data. 2. How to distinguish the samples from different generators. 3. Since Eq.4 is closely related to adversarial attack, the authors are supposed to discuss their connections in the related works. 4. The name of ‘decentralized attribution’ is misleading. Decentralized models are something like federated learning, where a ‘center’ model grasps information from ‘decentralized models’. However, the presented work is not related to such decentralization. 5. Typos: regarding the adversarial generative models ->regarding to the adversarial generative models; along the keys->along with the keys.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '1', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"* L248: What does ""wrong"" mean here? The paper gets into some of the nuance of this position at L255, but it would be helpful to clarify what is meant by a good/bad/wrong explanation before using those concepts.",NIPS_2019_263,NIPS_2019,"--- Weaknesses of the evaluation in general: * 4th loss (active fooling): The concatenation of 4 images into one and the choice of only one pair of classes makes me doubt whether the motivation aligns well with the implementation, so 1) the presentation should be clearer or 2) it should be more clearly shown that it does generalize to the initial intuition about any two objects in the same image. The 2nd option might be accomplished by filtering an existing dataset to create a new one that only contains images with pairs of classes and trying to swap those classes (in the same non-composite image). * I understand how LRP_T works and why it might be a good idea in general, but it seems new. Is it new? How does it relate to prior work? Does the original LRP would work as the basis or target of adversarial attacks? What can we say about the succeptibility of LRP to these attacks based on the LRP_T results? * How hard is it to find examples that illustrate the loss principles clearly like those presented in the paper and the supplement? Weaknesses of the proposed FSR metric specifically: * L195: Why does the norm need to be changed for the center mass version of FSR? * The metric should measure how different the explanations are before and after adversarial manipulation. It does this indirectly by measuring losses that capture similar but more specific intuitions. It would be better to measure the difference in heatmaps before and after explicitly. This could be done using something like the rank correlation metric used in Grad-CAM. I think this would be a clearly superior metric because it would be more direct. * Which 10k images were used to compute FSR? Will the set be released? Philosohpical and presentation weaknesses: * L248: What does ""wrong"" mean here? The paper gets into some of the nuance of this position at L255, but it would be helpful to clarify what is meant by a good/bad/wrong explanation before using those concepts. * L255: Even though this is an interesting argument that forwards the discussion, I'm not sure I really buy it. If this was an attention layer that acted as a bottleneck in the CNN architecuture then I think I'd be forced to buy this argument. As it is, I'm not convinced one way or the other. It seems plausible, but how do you know that the final representation fed to the classifier has no information outside the highlighted area. Furthermore, even if there is a very small amount of attention on relevant parts that might be enough. * The random parameterization sanity check from [25] also changes the model parameters to evaluation visualizations. This particular experiment should be emphasized more because it is the only other case I can think of which considers how explanations change as a function of model parameters (other than considering completely different models). To be clear, the experiment in [25] is different from what is proposed here, I just think it provides interesting contrast to these experiments. The claim here is that the explanations change too much while the claim there is that they don't change enough. Final Justification --- Quality - There are a number of minor weaknesses in the evaluation that together make me unsure about how easy it is to perform this kind of attack and how generalizable the attack is. I think the experiments do clearly establish that the attack is possible. Clarity - The presentation is pretty clear. I didn't have to work hard to understand any of it. Originality - I haven't seen an attack on interpreters via model manipulation before. Significance - This is interesting because it establishes a new way to evaluate models and/or interpreters. The paper is a bit lacking in scientific quality in a number of minor ways, but the other factors clearly make up for that defect.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- There are lots of hyperparameters that need to be tuned (step size, N, $\delta t$) and as the optimization needs to be solved on a sample-by-sample basis it is not clear how much variation in optimal hyperparameters can occur.",ZnmofqLWMQ,ICLR_2024,"- The proposed method is computationally very costly. The outer optimization needs to differentiate through several (n = 10+) chained calls of a large score model in each iteration, with an overall N (N=100+) outer loop iterations, per image. Thus, it requires at least 1000 NFEs plus the heavy cost of N backpropagations through a large, chained model.
- There are lots of hyperparameters that need to be tuned (step size, N, $\delta t$) and as the optimization needs to be solved on a sample-by-sample basis it is not clear how much variation in optimal hyperparameters can occur.
- I have serious doubts about the experiments:
1) There is no comparison with DPS which is a well-known diffusion-based solver that has better performance than the included competing techniques and source code is made public. For instance, for 4x SR *with noise* DPS reports far better LPIPS than any techniques in this paper.
2) Distortion metrics such as PSNR, NMSE and SSIM are completely missing, which are crucial in evaluating inverse problem solvers.
3) A very small (100) number of samples is used for evaluation. In other competing methods it is standard to use a 1000-sample validation split. Thus the results are not necessarily reliable and it is very difficult to compare to existing published results (DDRM, DDNM, DPS). Furthermore, since FID heavily depends on the number of samples used in the generated distribution, the reposrted FIDs are not compatible with the ones reported in competing method's original papers.
4) I have doubts about the reported timing results in Table 3. SHRED is reported as approx. 5x slower than DDRM. According to the DDRM paper, they use 20 NFEs. How is the reported timing possible, when SHRED uses 100 outer loop iterations with 10 NFEs in each outer loop (total 1000 NFEs) plus the additional cost of 100 backpropagation?
5) The robustness experiments could be more rigorous. Instead of showing some good looking samples, it would be more meaningful to quantify the variation of image quality metrics for the validation dataset over 5-10 samples.
6) The framework is developed for noisy inverse problems, however there are no experiments for the noisy case. Reconstruction performance under measurement noise is crucial in evaluating the utility of the algorithm.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- The performance is only compared with few methods. And the proposed is not consistently better than other methods. For those inferior results, some analysis should be provided since the results violate the motivation. I am willing to change my rating according to the feedback from authors and the comments from other reviewers.",ICLR_2021_1014,ICLR_2021,"- I am not an expert in the area of pruning. I think this motivation is quite good but the results seem to be less impressive. Moreover, I believe the results should be evaluated from more aspects, e.g., the actual latency on target device, the memory consumption during the inference time and the actual network size. - The performance is only compared with few methods. And the proposed is not consistently better than other methods. For those inferior results, some analysis should be provided since the results violate the motivation.
I am willing to change my rating according to the feedback from authors and the comments from other reviewers.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '1']}",,
"* quality: The technical content of the paper is sound and rigorous * clarity: The paper is in general very well-written, and should be easy to follow for expert readers.",NIPS_2019_168,NIPS_2019,"of the submission. * originality: This is a highly specialized contribution building up novel results on two main fronts: The derivation of the lower bound on the competitive ratio of any online algorithm and the introduction of two variants of an existing algorithm so as to meet this lower bound. Most of the proofs and techniques are natural and not surprising. In my view the main contribution is the introduction of the regularized version which brings a different, and arguably more modern interpretation, about the conditions under which these online algorithms perform well in these adversarial settings. * quality: The technical content of the paper is sound and rigorous * clarity: The paper is in general very well-written, and should be easy to follow for expert readers. * significance: As mentioned above this is a very specialized paper likely to interest some experts in the online convex optimization communities. Although narrow in scope, it contains interesting theoretical results advancing the state of the art in dealing with these specific problems. * minor details/comments: - p.1, line 6-7: I would rewrite the sentence to simply express that the lower bound is $\Omega(m^{-1/2})$ \- p.3, line 141: cost an algorithm => cost of an algorithm \- p.4, Algorithm 1, step 3: mention somewhere that this is the projection operator (not every reader will be familiar with this notation \- p.5, Theorem 2: remind the reader that the $\gamma$ in the statement is the parameter of OBD as defined in Algorithm 1 \- p.8, line 314: why surprisingly?","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,
"2. The paper does not have sufficient experimental demonstration of the contribution points. There is only an experimental comparison between ELF (the author's method) and the baseline without Mid Vision Feedback (MVF), but no comparison with the image classification result of Mid Vision Feedback (MVF). This does not prove that the schema searched by ELF (the author's method) is better than the schema in Mid Vision Feedback (MVF).",WDO5hfLZvN,ICLR_2025,"1. The paper combines EvoPrompt and Mid Vision Feedback (MVF), but does not explain the principles and detailed processes of the two in the intrduction or related work section. In addition, the method section is a bit casual, without strict mathematical definitions and rigorous process expressions, making the method not specific and clear enough.
2. The paper does not have sufficient experimental demonstration of the contribution points. There is only an experimental comparison between ELF (the author's method) and the baseline without Mid Vision Feedback (MVF), but no comparison with the image classification result of Mid Vision Feedback (MVF). This does not prove that the schema searched by ELF (the author's method) is better than the schema in Mid Vision Feedback (MVF).
3. The description of the experimental section is not rigorous enough (potentially, it may lead to an imprecise experimental setting). For example, in the comparison of Stage1 and ELF in Table 1, the total training generations of the two do not seem to be consistent. Whether Stage1 has reached sufficient convergence may need to be explained. In lines #346-347, the author mentions using a 32x32 input size neural network for CIFAR100 experiments, but in lines #383-384, the experiment continues on ImageNet, switching to a larger ViT-B/16 and ResNet50, and the resolution setting is not explained at this time.
4. The analysis in the experimental part is not sufficient. The authors can show the difference between the schema optimized by EvoPrompt and the original schema (and MVF), and explain clearly and more deeply the growth points brought by using EvoPrompt to optimize the schema.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1. The authors have not covered more on the types of activities captured in the datasets, and their importance in smart homes, particularly from the perspective of occupant comfort and energy efficiency.",AqXzHRU2cs,ICLR_2024,"1. The authors have not covered more on the types of activities captured in the datasets, and their importance in smart homes, particularly from the perspective of occupant comfort and energy efficiency.
2. The number of sensors used to collect data seems a lot. In practice, its not practical to have so many sensors in a home collecting information. The authors should try some benchmarking on a subset of sensors if the dataset permits.
3. How will a sensor fusion approach work in this scenario?
4. What are the motivations behind hierarchical approach?
5. For Milan and Cairo, the temporal method might not be effective since the number of days in the experiment is less.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- ""D"" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion. Review summary:",NIPS_2018_76,NIPS_2018,"- A main weakness of this work is its technical novelty with respect to spatial transformer networks (STN) and also the missing comparison to the same. The proposed X-transformation seems quite similar to STN, but applied locally in a neighborhood. There are also existing works that propose to apply STN in a local pixel neighborhood. Also, PointNet uses a variant of STN in their network architecture. In this regard, the technical novelty seems limited in this work. Also, there are no empirical or conceptual comparisons to STN in this work, which is important. - There are no ablation studies on network architectures and also no ablation experiments on how the representative points are selected. - The runtime of the proposed network seems slow compared to several recent techniques. Even for just 1K-2K points, the network seem to be taking 0.2-0.3 seconds. How does the runtime scales with more points (say 100K to 1M points)? It would be good if authors also report relative runtime comparisons with existing techniques. Minor corrections: - Line 88: ""lose"" -> ""loss"". - line 135: ""where K"" -> ""and K"". Minor suggestions: - ""PointCNN"" is a very short non-informative title. It would be good to have a more informative title that represents the proposed technique. - In several places: ""firstly"" -> ""first"". - ""D"" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion. Review summary: - The proposed technique is sensible and the performance on different benchmarks is impressive. Missing comparisons to established STN technique (with both local and global transformations) makes this short of being a very good paper. After rebuttal and reviewer discussion: - I have the following minor concerns and reviewers only partially addressed them. 1. Explicit comparison with STN: Authors didn't explicitly compare their technique with STN. They compared with PointNet which uses STN. 2. No ablation studies on network architecture. 3. Runtimes are only reported for small point clouds (1024 points) but with bigger batch sizes. How does runtime scale with bigger point clouds? Authors did not provide new experiments to address the above concerns. They promised that a more comprehensive runtime comparison will be provided in the revision. Overall, the author response is not that satisfactory, but the positive aspects of this work make me recommend acceptance assuming that authors would update the paper with the changes promised in the rebuttal. Authors also agreed to change the tile to better reflect this work.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the âelementsâ equivalent to âstatesâ? or âactionsâ? More should be elaborated.",NIPS_2018_821,NIPS_2018,"1. A few parts of this paper are not very clear or not sufficiently provided, such as the model details. Section 4.3 should be addressed more to make it clearer since some concepts/statements are misleading or confusing. 2. The trace to code model is complex, which requires as many LSTMs as the number of input/output pairs, and it may be hard to be applied to other program synthesis scenarios. 3. Apart from the DSL, more experiments on dominant PL (e.g., Python) would be appreciated by people in this research field. Details are described as below: 1. For a given program, how to generate diverse execution traces that can be captured by the I/O -> Trace model? Since execution traces are generated by running the program on N I/O pairs, it is possible that some execution traces have a large overlap. For example, in the extreme case, two execution traces may be the same (or very similar) given different I/O pairs. 2. The authors involve the program interpreter in their approach, which is a good trial and it should help enhance the performance. However, I am curious about is it easy to be integrated with the neural network during training and testing? 3. The concept of state is not very clear, from my understanding, it represents the grid status (e.g., agent position) and it is obtained after applying an action of the trace. Line 186-line 187, is the âelementsâ equivalent to âstatesâ? or âactionsâ? More should be elaborated. 4. Line 183-line 184, is it necessary to use embedding for only four conditionals (of Boolean type)? only 16 possible combinations. 5. As depicted in Figure 3, if more I/O pairs are provided, the Trace->Code should be very complex since each i/o example requires such an LSTM model. How to solve this issue? 6. In essence, the Trace->Code structure is a Sequence to Sequence model with attention, the only differences are the employment of I/O pair embedding and the max pooling on multiple LSTM. How are the I/O pair embeddings integrated into the computation? Some supplementary information should be provided. 7. It is interesting to find that model trained on gold traces perform poorly on inferred traces, the authors do not give a convincing explanation. More exploration should be conducted for this part. 8. It would be better if some synthesized program samples are introduced in an appendix or other supplementary documents.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- It would also be interesting to compare the support of the solution obtained by the proposed scheme with that obtained by the baseline methods (e.g., using a Jaccard index).",FAYIlGDBa1,ICLR_2025,"W1) Some of the technical contents, including the problem formulation, are not properly formalized, with some important aspects completely omitted. Notably, it should be stated that matrix $A$ in (SPCA) is necessarily symmetric positive semidefinite, implying the same for the blocks in the block-diagonal approximation. The absence of this information and the fact that throughout the paper $A$ is simply referred to as an ""input matrix"" rather than a covariance matrix may mislead the reader into thinking that the problem is more general than it actually is.
W2) The presentation of the simulation results is somewhat superficial, focusing only on presenting and briefly commenting the two quantitative criteria used for comparison, without much discussion or insight into what is going on. Specifically:
- Separate values for the different used $k$ should be reported (see point W2 below).
- It would be interesting to report the threshold value $\varepsilon$ effectively chosen by the algorithm (relatively to the infinity norm of the input matrix), as well as the proportion of zero entries after the thresholding.
- It would also be interesting to compare the support of the solution obtained by the proposed scheme with that obtained by the baseline methods (e.g., using a Jaccard index).
W3) Reporting average results with respect to $k$ is not a reasonable choice in my opinion, as the statistics of the chosen metrics probably behave very differently for different values of $k$.
W4) As it is, I don't see the utility of Section 4.1. First, this model is not applied to any of the datasets used in the experiments. This leads one to suspect that in practice it is quite hard to come up with estimates of the parameters required by Algorithm 4. Second, the authors do not even generate synthetic data following such a model (which is one typical use of a model) in order to illustrate the obtained results. In my view results of this kind should be added, or else the contents of 4.1 should be moved to the appendices as they're not really important (or both).
W5) Though generally well-written, the paper lacks some clarity at times, and the notation is not always consistent/clear. In particular:
- The sentence ""This result is non-trivial: while the support of an optimal solution could span multiple blocks, we theoretically show that there must exist a block that contains the support of an optimal solution, which guarantees the efficiency of our framework."" seems to be contradicatory. I believe that the authors mean the following: *one could think* that the solution could span multiple blocks, but they show this is not true. The same goes for Remark 1.
- What is the difference between $A^\varepsilon$ and $\tilde{A}$ in Theorem 1? It seems that two different symbols are used to denote the same object.
- The constraint $\|x\|_0 \le k$ in the last problem that appears in the proof of Theorem 2 is inocuous, since the size of each block $\tilde{A}_i'$ is at most $k$ anyway. This should be commented.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2) I noticed that in Sec 5.3, a generator equipped with a standard R-GCN as discriminator tends to collapse after several (around 20), while the proposed module will not. The reason behind this fact can be essential to show the mechanism how the proposed method differs from previous one. However, this part is missing in this version of submission. I would like to see why the proposed module can prevent a generator from collapsing.",NIPS_2020_0,NIPS_2020,"I mainly have the following concerns. 1) In general, this paper is incremental to GIN [1], which limits the contribution of this paper. While GIN is well motivated by WL test with solid theoretical background, this paper lacks deeper analysis and new motivation behind the algorithm design. I suggest the authors to give more insightful analysis and motivation. 2) I noticed that in Sec 5.3, a generator equipped with a standard R-GCN as discriminator tends to collapse after several (around 20), while the proposed module will not. The reason behind this fact can be essential to show the mechanism how the proposed method differs from previous one. However, this part is missing in this version of submission. I would like to see why the proposed module can prevent a generator from collapsing. 3) I understand that stochastic/random projection is with high probability to preserve the metric before mapping . My concern is that when stacking multiple layers of WLS units, the probability of the failure case of stochastic/random projection also increases (since projection is performed at each layer). This may greatly hinder the scenario of the proposed method from forming deeper GNN. In this case, authors should justify the stability of the proposed method. How stable is the proposed method? And what happens when stacking more layers?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"4. **Originality Concerns**: The article's reasoning and writing logic bear similarities to those found in ""How Do Nonlinear Transformers Learn and Generalize in In-Context Learning."" It raises the question of whether this work is merely an extension of the previous study or if it introduces novel contributions.",n7n8McETXw,ICLR_2025,"1. **Limitations in Model Complexity**: The paper primarily analyzes a single-head attention Transformer, which may not encapsulate the full complexity and performance characteristics of multi-layer and multi-head attention models. The validation was confined to binary classification tasks, thereby restricting the generalizability of the theoretical findings.
2. **Formula Accuracy**: The paper requires a meticulous review of its mathematical expressions. For instance, in Example 1, the function should be denoted as \( f_2(\mu_2') = \mu_1' \) instead of \( f_1(\mu_2') = \mu_1' \), and the second matrix \( A_1^f \) should be corrected to \( A_2^f \). It is essential to verify all formulas throughout the text to ensure accuracy.
3. **Theorem Validity and Clarification**: The theorems presented in the article should be scrutinized for their validity, particularly the sections that substantiate reasoning, as they may induce some ambiguity. Reading the appendix is mandatory for a comprehensive understanding of the article; otherwise, it might lead to misconceptions.
4. **Originality Concerns**: The article's reasoning and writing logic bear similarities to those found in ""How Do Nonlinear Transformers Learn and Generalize in In-Context Learning."" It raises the question of whether this work is merely an extension of the previous study or if it introduces novel contributions.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '0']}",,
1.) Theoretical comparisons to adaptive learning of GPRGNN is not clear.,NIPS_2022_1572,NIPS_2022,1.) Theoretical comparisons to adaptive learning of GPRGNN is not clear. 2.) Incremental work though the contribution is useful.,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2. Is it sufficient to measure object hallucination through only yes/no responses? Yes response does not necessarily indicate that the model comprehends the presence of the object in the image, as it may still produce incorrect objects when undertaking other tasks.",xozJw0kZXF,EMNLP_2023,"1. Is object hallucination the most important problem of multimodal LLM? Others include knowledge, object spatial relationships, fine-grained attributes, etc.
2. Is it sufficient to measure object hallucination through only yes/no responses? Yes response does not necessarily indicate that the model comprehends the presence of the object in the image, as it may still produce incorrect objects when undertaking other tasks.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '1', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The very-long-term forecasting task is of limited practical significance. Despite that, the discussion requires improvement, e.g., by conducting experiments on more datasets and training the baseline models with the ""correct"" forecast horizon to put the results in a proper context.",4NhMhElWqP,ICLR_2024,"- The main weakness of this paper is that it overclaims and underdelivers. In its current state, the study is not strong enough to claim the title of a ""foundational model"".
- The authors mention that they use datasets from diverse domains. However, out of the 12 datasets studied, 6 come from a single domain. The distribution of sampling frequencies of these datasets are also not diverse with 6 hourly datasets and a limited representation of other frequencies (and some popular frequencies completely missing).
- Another aspect that could have justified the term ""foundational"" is a diversity of tasks. However, the paper mostly focuses on the long-term forecasting tasks with limited discussion of other tasks. Importantly, the practically relevant task of short-term forecasting (e.g., Monash time series forecasting archive) gets very less attention.
- The claim _Most existing forecasting models were designed to process regularly sampled data of fixed length. We argue that this restriction is the central reason for poor generalisation in time series forecasting_ has not been justified convincingly.
- The visualizations are poorly done and confusing for a serious academic paper. Please consider using cleaner figures. It is unclear how exactly inference on a new dataset is performed. It would improve the clarity of the paper if a specific paragraph on inference is added. Please see specific questions in the questions section.
- The results on the long-term forecasting benchmarks, while reasonable, are not impressive for a ""foundational model"" that has been trained on a larger corpus of datasets.
- The very-long-term forecasting task is of limited practical significance. Despite that, the discussion requires improvement, e.g., by conducting experiments on more datasets and training the baseline models with the ""correct"" forecast horizon to put the results in a proper context.
- The zero-shot analysis (Sec. 4.3) has only been conducted on two datasets. Moreover, since prior works such as PatchTST and NHITS do not claim to be foundational models, a proper comparison would be with baselines trained specifically on these held-out datasets. DAM would most likely be worse in that case but it would be a better gauge for the zero-shot performance.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"7. Experiments - Ablation - missing components: There should be experiments and explanation regarding the different queries used in spatio-temporal representation, i.e., spatial, temporal and summary. That is the key difference to VideoChatGPT and other works. What if only have spatial one, or temporal and summary one?",R6sIi9Kbxv,ICLR_2025,"1. The approach of decomposing video representation into spatial and temporal representation for efficient and effective spatio-temporal modelling is a general idea in video understanding. I'm not going to blame using this in large video language models, however, I think proper credit and literature reviews should be included. For example, TimeSFormer[1], Uniformer[2], Dual-AI[3] and others using transformer for video recognition.
2. Training specialized and efficient video QFormer has been explored and utilized by UMT [4] and VideoChat2 [5]. Please clarify the difference and include them in literature reviews.
2. Comfusing attentive pooling module architecture. It seems the temporal representation $v_{t}$ is derived from spatial queries $Q_{s}$ attending to a set of frame features (with T as batch dimension). It means the spatial queries can only attend to in-frame content. This is doubtable why the representation is called temporal representation.
3. Training data: what specific data are used from training? Please provide details of how many videos from what dataset and how you make sampling. This is critical for reproduction and measure the method effectiveness.
4. Experiments - Comparison fairness: More latest methods should be included in comparison, especially those with similar motivations, e.g., VideoChat2.
5. Experiments - Image benchmark: As image dataset is used, it would be great to show the performance variance after such ST QFormer tuning. Also compared to normal QFormer.
6. Experiments - Video summarization: there are some new good benchmarks, like Shot2Story ranging from different topics and using only text and frames modalities. This is not mandatory, but it should be good to include.
7. Experiments - Ablation - missing components: There should be experiments and explanation regarding the different queries used in spatio-temporal representation, i.e., spatial, temporal and summary. That is the key difference to VideoChatGPT and other works. What if only have spatial one, or temporal and summary one?
8. Experiments - Ablation - metric: for abaltions, I suggest to use QA benchmarks for experiments rather than captioning benchmark. When things come to LLM, the current captioning metrics such as B@4 and CIDEr might not be ideal to reflect model ability.
[1] Is Space-Time Attention All You Need for Video Understanding?
[2] UniFormer: Unified Transformer for Efficient Spatiotemporal Representation Learning
[3] Dual-AI: Dual-Path Actor Interaction Learning for Group Activity Recognition
[4] MVBench","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
3. The proposed FRM is a simple combination of channel attention and spatial attention. The innovative should be given in detail.,ICLR_2022_3068,ICLR_2022,"1. The innovation in the paper is limited, more like an assembly of existing work, such as DeepLabv3+, attention and spatial attention。 2. The proposed method was not evaluated on other datasets, such as MS COCO dataset. 3. The main focus of this paper is tiny object detection, but the analysis of small object is limited in the experimental results.
Question: 1. What’s the ‘CEM’ and ‘FPM’ mean in Figure 1? 2. The novelty of CAM is limited, A similar structure has been proposed in DeepLabv3+. 3. The proposed FRM is a simple combination of channel attention and spatial attention. The innovative should be given in detail.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"6 Societal impact The authors state that they foresee no negative social impacts of their work (line 379). While I do not believe this work has the potential for significant negative social impact (and I'm not quite sure if/how I'm meant to review this aspect of their work), the authors could always mention the social impact of increased automation, or the risks from the dual use of their method, etc.",NIPS_2021_780,NIPS_2021,"5 Limitations
a. The authors briefly talk about the limitations of the approach in section 5. The main limitation they draw attention to is the challenge of moving closer to the local maxima of the reward function in the latter stages of optimization. To resolve this they discuss combining their method with local optimization techniques; however, I wonder whether the temperature approach they discuss in the earlier part of their paper (combined with some annealing scheme) could also be used here?
b. One limitation the authors do not mention is how the method scales in terms of the size of the state and action space. The loss function requires for every current state the sum over all previous states and actions that may have led to the current state (see term 1 of Eq.9). I assume this may become intractable for very large state-action spaces (and the flows one is trying to model become very small). Can one approximate the sum using a subset? Also what about continuous state/action spaces?
6 Societal impact
The authors state that they foresee no negative social impacts of their work (line 379). While I do not believe this work has the potential for significant negative social impact (and I'm not quite sure if/how I'm meant to review this aspect of their work), the authors could always mention the social impact of increased automation, or the risks from the dual use of their method, etc.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
- Section 3 and Section 4 are slightly redundant: maybe putting the first paragraph of sec 4 in sec 3 and putting the remainder of sec 4 before section 3 would help.,NIPS_2018_606,NIPS_2018,", I tend to vote in favor of this paper. * Detailed remarks: - The analysis in Figure 4 is very interesting. What is a possible explanation for the behaviour in Figure 4(d), showing that the number of function evaluations automatically increases with the epochs? Consequently, how is it possible to control the tradeoff between accuracy and computation time if, automatically, the computation time increases along the training? In this direction, I think it would be nice to see how classification accuracy evolves (e.g. on MNIST) with the precision required. - In Figure 6, an interpolation experiment shows that the probability distribution evolves smoothly along time, which is an indirect way to interpret it. Since this is a low (2D) dimensional case, wouldn't it be possible to directly analyze the learnt ODE function, by looking at its fixed points and their stability? - For the continuous-time time-series model, subsec 6.1 clarity should be improved. Regarding the autoencoder formulation, why is an RNN used for the encoder, and not an ODE-like layer? Indeed, the authors argue that RNNs have trouble coping with such time-series, so this might also be the case in the encoding part. - Do the authors plan to share the code of the experiments (not only of the main module)? - I think it would be better if notations in appendix A followed the notations of the main paper. - Section 3 and Section 4 are slightly redundant: maybe putting the first paragraph of sec 4 in sec 3 and putting the remainder of sec 4 before section 3 would help.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. At a high-level, I don’t see a clear connection between “improved variance control of prediction y^ or the smoothness of loss landscape” and “zero-shot learning effectiveness.” Details below. This is in part due to poor clarity.",ICLR_2021_458,ICLR_2021,"W1. Clarity
The organization of the paper is such that the reader has to refer to the appendix a lot. My biggest concern on clarity is on the “theoretical” results which are not rigorous and at times unsupported. Further, some statements/claims are not precise or clear enough for me to be convinced that the method is well-motivated and is doing what it is claimed to be doing.
W2. Soundness
I have a lot of concerns and questions here as I read through Sect. 3. At a high-level, I don’t see a clear connection between “improved variance control of prediction y^ or the smoothness of loss landscape” and “zero-shot learning effectiveness.” Details below. This is in part due to poor clarity.
W3. Experiments
IMO, if the main claim is really about the effectiveness of the two tricks and the proposed class normalization, then the experiments should go beyond one zero-shot learning starting point --- 3-layer MLP (Table 2).
If baseline methods already adopt some of these tricks, it should be made clear and see if removing these tricks lead to inferior performance.
If baseline methods do not adopt some of these tricks, these tricks, especially class normalization, could be applied to show improved performance. If it is difficult to apply these tricks, further explanation should be given (generally, also mention applicability of these tricks.)
This is done to some degree in the continual setting.
W4. Related work
As I mentioned in W3, it is unclear which methods are linear/deep, and which methods have already benefited from existing/proposed tricks.
Detailed comments (mainly to clarify my points about weaknesses)
Statement 1
The main claim for this part is that this statement provides “a theoretical understanding of the trick” and “allows to speed up the search [of the optimal value fo \gamma].”
However, I feel that we need further justifications on the correlation between Statement 1 (variance of y^_c, “better stability” and “the training would not stale”) and the zero-shot learning accuracy for this to be the “why normalization + scaling works.” My understanding is that the Appendix simply validates that Eq. (4) seems to hold in practice.
Moreover, is the usual search region [5,10] actually effective? Do we have stronger supporting empirical evidence than the three groups of practitioners (Li et al 2019, Zhang et al. 2019, Guo et al. 2020), who may have influenced each other, used it?
Finally, can the authors comment on the validity of multiple assumptions in Appendix A? To which degrees does each of them hold in practice?
Statement 2 and 3
Why wouldn’t the following statement in Sect. 3.3 invalidate Statement 1? “This may create an impression that it does not matter how we initialize the weights — normalization would undo any fluctuations. However it is not true, because it is still important how the signal flows, i.e. for an unnormalized and unscaled logit value”
It is unclear (at least not from the beginning) why understanding attribute normalization has to do with initialization of the weights.
Similar to my comments to Statement 1, why should we believe that the explanation in Sect. 3.3 and Sect. 3.4 is the reason for zero-shot learning effectiveness? In particular, the authors again claim that the main bottleneck in improving zero-shot learning is “variance control” (the end of Sect. 3.3).
I also have a hard time understanding some statements in Appendix H, which is needed to motivate the following statement in Sect. 3.3: “And these assumptions are safe to assume only for z but not for a_c, because they do not hold for the standard datasets (see Appendix H).” H1: Would this statement still be true after we transform a_c with an MLP? H2: Why is it not “a sensible thing to do” if we just want zero mean and unit variance? H3: Why is “such an approach far from being scalable”? H4: What if these are things like word embeddings? H5: Fig. 12 and Fig. 13 are not explained. H6: Histograms in Fig. 13 look quite normal.
How useful is Statement 2? Why is the connection with Xavier initialization important?
Why is “preserving the variance between z and y~” in Statement 3 important for zero-shot learning?
Improved smoothness
The claim “improved smoothness” at the end of Sect. 3 and Appendix F is really hard to understand. F1: How do the authors define “irregular loss surface”? F2: “Santurkar et al. (2018) showed that batch-wise standardization procedure decreases the Lipschitz constant of a model, which suggests that our class-wise standardization will provide the same impact.” This is not very precise and seems unsupported. Please make it clear how. If this is a hypothesis, please make it clear.
Similarly to my comments to Statement 1-3, how is improved smoothness related to zero-shot learning effectiveness?
Other more minor comments
Abstract: Are the authors the one to “generalize ZSL to a broader problem”? Please tone down the claim if not.
After Eq. (2): Why does attribute normalization look “inconsiderable” (possibly this is not the right word?) or why is it “surprising” that this is preferred in practice? Don’t most zero-shot learning methods use this (see for example Table 4 in [A])?
Suggestions for references for attribute normalization. This can be improved; I can trace this back to much earlier work such as [A] and [B] (though I think this fact is stated more explicitly in [A]).
Under Table 1 “These two tricks work well and normalize the variance to a unit value when the underlying ZSL model is linear (see Figure 1), but they fail when we use a multi-layer architecture.”: Could the authors provide a reference to evidence to support this? I think it is also important to provide a clear statement of what separates a “linear” or “multi-layer” model.
The first paragraph of Sect. 3: Could you provide references for motivations for different activation functions? Further, It is unclear that all of them perform normalization.
The second paragraph of Sect. 3: What exactly limits “the tools” for zero-shot learning vs. supervised learning? Further, it would also be nice to separate traditional supervised learning where classes are balanced and imbalanced; see, e.g., [C].
What is the closest existing zero-shot model to the one the authors describe in Sect. 3.1? Why is the described model considered/selected?
[A] Synthesized Classifiers for Zero-Shot Learning
[B] Zero-Shot Learning by Convex Combination of Semantic Embeddings
[C] Class-Balanced Loss Based on Effective Number of Samples","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"* L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else.",NIPS_2016_386,NIPS_2016,", however. For of all, there is a lot of sloppy writing, typos and undefined notation. See the long list of minor comments below. A larger concern is that some parts of the proof I could not understand, despite trying quite hard. The authors should focus their response to this review on these technical concerns, which I mark with ** in the minor comments below. Hopefully I am missing something silly. One also has to wonder about the practicality of such algorithms. The main algorithm relies on an estimate of the payoff for the optimal policy, which can be learnt with sufficient precision in a ""short"" initialisation period. Some synthetic experiments might shed some light on how long the horizon needs to be before any real learning occurs. A final note. The paper is over length. Up to the two pages of references it is 10 pages, but only 9 are allowed. The appendix should have been submitted as supplementary material and the reference list cut down. Despite the weaknesses I am quite positive about this paper, although it could certainly use quite a lot of polishing. I will raise my score once the ** points are addressed in the rebuttal. Minor comments: * L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0? * L177: ""(OCO )"" -> ""(OCO)"" and similar things elsewhere * L176: You might want to mention that the learner observes the whole concave function (full information setting) * L223: I would prefer to see a constant here. What does the O(.) really mean here? * L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards. * L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes? * The algorithm only stops /after/ it has exhausted its budget. Don't you need to stop just before? (the regret is only trivially affected, so this isn't too important). * L213: \tilde \mu is undefined. I guess you mean \tilde \mu_t, but that is also not defined except in Corollary 1, where it just given as some point in the confidence ellipsoid in round t. The result holds for all points in the ellipsoid uniformly with time, so maybe just write that, or at least clarify somehow. ** L435: I do not see how this follows from Corollary 2 (I guess you meant part 1, please say so). So first of all mu_t(a_t) is not defined. Did you mean tilde mu_t(a_t)? But still I don't understand. pi^*(X_t) is (possibly random) optimal static strategy while \tilde \mu_t(a_t) is the optimistic mu for action a_t, which may not be optimistic for pi^*(X_t)? I have similar concerns about the claim on the use of budget as well. * L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else. * L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee? * L200: ""for every arm a"" implies there is a single optimistic parameter, but of course it depends on a ** L303: Why not choose T_0 = m Sqrt(T)? Then the condition becomes B > Sqrt(m) T^(3/4), which improves slightly on what you give. * It would be nice to have more interpretation of theta (I hope I got it right), since this is the most novel component of the proof/algorithm.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper. Detailed Comments:",NIPS_2021_537,NIPS_2021,"Weakness: The main weakness of the approach is the lack of novelty. 1. The key contribution of the paper is to propose a framework which gradually fits the high-performing sub-space in the NAS search space using a set of weak predictors rather than fitting the whole space using one strong predictor. However, this high-level idea, though not explicitly highlighted, has been adopted in almost all query-based NAS approaches where the promising architectures are predicted and selected at each iteration and used to update the predictor model for next iteration. As the authors acknowledged in Section 2.3, their approach is exactly a simplified version of BO which has been extensively used for NAS [1,2,3,4]. However, unlike BO, the predictor doesn’t output uncertainty and thus the authors use a heuristic to trade-off exploitation and exploration rather than using more principled acquisition functions.
2. If we look at the specific components of the approach, they are not novel as well. The weak predictor used are MLP, Regression Tree or Random Forest, all of which have been used for NAS performance prediction before [2,3,7]. The sampling strategy is similar to epsilon-greedy and exactly the same as that in BRP-NAS[5]. In fact the results of the proposed WeakNAS is almost the same as BRP-NAS as shown in Table 2 in Appendix C. 3. Given the strong empirical results of the proposed method, a potentially more novel and interesting contribution would be to find out through theorical analyses or extensive experiments the reasons why simple greedy selection approach outperforms more principled acquisition functions (if that’s true) on NAS and why deterministic MLP predictors, which is often overconfident when extrapolate, outperform more robust probabilistic predictors like GPs, deep ensemble or Bayesian neural networks. However, such rigorous analyses are missing in the paper.
Detailed Comments: 1. The authors conduct some ablation studies in Section 3.2. However, a more important ablation would be to modify the proposed predictor model to get some uncertainty (by deep-ensemble or add a BLR final output layer) and then use BO acquisition functions (e.g. EI) to do the sampling. The proposed greedy sampling strategy works because the search space for NAS-Bench-201 and 101 are relatively small and as demonstrated in [6], local search even gives the SOTA performance on these benchmark search spaces. For a more realistic search space like NAS-Bench-301[7], the greedy sampling strategy which lacks a principled exploitation-exploration trade-off might not work well. 2. Following the above comment, I’ll suggest the authors to evaluate their methods on NAS-Bench-301 and compare with more recent BO methods like BANANAS[2] and NAS-BOWL[4] or predictor-based method like BRP-NAS [5] which is almost the same as the proposed approach. I’m aware that the authors have compared to BONAS and shows better performance. However, BONAS uses a different surrogate which might be worse than the options proposed in this paper. More importantly, BONAS use weight-sharing to evaluate architectures queried which may significantly underestimate the true architecture performance. This trades off its performance for time efficiency. 3. For results on open-domain search, the authors perform search based on a pre-trained super-net. Thus, the good final performance of WeakNAS on MobileNet space and NASNet space might be due to the use of a good/well-trained supernet; as shown in Table 6, OFA with evalutinary algorithm can give near top performance already. More importantly, if a super-net has been well-trained and is good, the cost of finding the good subnetwork from it is rather low as each query via weight-sharing is super cheap. Thus, the cost gain in query efficiency by WeakNAS on these open-domain experiments is rather insignificant. The query efficiency improvement is likely due to the use of a predictor to guide the subnetwork selection in contrast to the naïve model-free selection methods like evolutionary algorithm or random search. A more convincing result would be to perform the proposed method on DARTS space (I acknowledge that doing it on ImageNet would be too expensive) without using the supernet (i.e. evaluate the sampled architectures from scratch) and compare its performance with BANANAS[2] or NAS-BOWL[4]. 4. If the advantage of the proposed method is query-efficiency, I’d love to see Table 2, 3 (at least the BO baselines) in plots like Fig. 4 and 5, which help better visualise the faster convergence of the proposed method. 5. Some intuitions are provided in the paper on what I commented in Point 3 in Weakness above. However, more thorough experiments or theoretical justifications are needed to convince potential users to use the proposed heuristic (a simplified version of BO) rather than the original BO for NAS. 6. I might misunderstand something here but the results in Table 3 seem to contradicts with the results in Table 4. As in Table 4, WeakNAS takes 195 queries on average to find the best architecture on NAS-Bench-101 but in Table 3, WeakNAS cannot reach the best architecture after even 2000 queries.
7. The results in Table 2 which show linear-/exponential-decay sampling clearly underperforms uniform sampling confuse me a bit. If the predictor is accurate on the good subregion, as argued by the authors, increasing the sampling probability for top-performing predicted architectures should lead to better performance than uniform sampling, especially when the performance of architectures in the good subregion are rather close. 8. In Table 1, what does the number of predictors mean? To me, they are simply the number of search iterations. Do the authors reuse the weak predictors from previous iterations in later iterations like an ensemble?
I understand that given the time constraint, the authors are unlikely to respond to my comments. Hope those comments can help the authors for future improvement of the paper.
References: [1] Kandasamy, Kirthevasan, et al. ""Neural architecture search with Bayesian optimisation and optimal transport."" NeurIPS. 2018. [2] White, Colin, et al. ""BANANAS: Bayesian Optimization with Neural Architectures for Neural Architecture Search."" AAAI. 2021. [3] Shi, Han, et al. ""Bridging the Gap between Sample-based and One-shot Neural Architecture Search with BONAS."" NeurIPS. 2020. [4] Ru, Binxin, et al. ""Interpretable Neural Architecture Search via Bayesian Optimisation with Weisfeiler-Lehman Kernels."" ICLR. 2020. [5] Dudziak, Lukasz, et al. ""BRP-NAS: Prediction-based NAS using GCNs."" NeurIPS. 2020. [6] White, Colin, et al. ""Local search is state of the art for nas benchmarks."" arXiv. 2020. [7] Siems, Julien, et al. ""NAS-Bench-301 and the case for surrogate benchmarks for neural architecture search."" arXiv. 2020.
The limitation and social impacts are briefly discussed in the conclusion.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- Where else was the k-max problem discussed? Please provide a citation for this.,NIPS_2016_186,NIPS_2016,"weakness in the algorithm is the handling of the discretization. It seems that two improvements are somewhat easily achievable: First, there should probably be a way to obtain instance dependent bounds for the continuous setting. It seems that by taking a confidence bound of size \sqrt{log(st)/T_{i,t}} rather than \sqrt{log(t)/T_{i,t}}, one can get a logarithmic dependence on s, rather than polynomial, which may solve this issue. If that doesnât work, the paper should benefit from an explanation for why that doesnât work. Second, it seems that the discretization should be adaptive to the data. Otherwise, the running time and memory are dependent of the time horizon in cases where they do not have to. Overall, the paper is well written and motivated. Its results, though having room for improvement are non-trivial and deserve publication. Minor comments: - Where else was the k-max problem discussed? Please provide a citation for this.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- There is no information on how the function for the optimal sequence length was estimated (Equation 1) and how reliable we expect this model to be.,UEx5dZqXvr,EMNLP_2023,"Based on the analyses presented in the paper, the scaling behavior of document-level machine translation models (in terms of number of parameters and number of data points) is not crucially different from the scaling behavior of sentence-level machine translation systems, which has already been studied by [Ghorbani et al. (2021)](https://arxiv.org/pdf/2109.07740.pdf).
In my opinion, the experiments (or their reporting) could be more thorough in many places to back up the claims made by the authors, e.g.
- Figure 1 indicates a causal relationship between the number of parameters and translation quality by affecting sample efficiency and optimal sequence length. While the graphic is confusing to me (e.g. it looks to me as if the number of parameters is affecting the corpus size), I don’t think that the intended claim (as reiterated at the end of the “Introduction” section) is supported by the results. None of the experiments actually show causation but rather association between the variables.
- There is no information on how the function for the optimal sequence length was estimated (Equation 1) and how reliable we expect this model to be.
- In Section 5.2, the authors should define what they mean by “error accumulation” explicitly. Is the problem that the models base the translation of later sentences made on erroneous translation of earlier sentences? If that is what the authors meant by “error accumulation”, it would need more analysis to verify such a phenomenon. Personally, I’d be interested in details like the maximum sequence length used for these experiments and the number of training examples per bin (which are not reported yet), as the observations made by the authors could be related to tendencies of the Transformer to overfit to length statistics of the training data, see [Varis and Bojar (2021)](https://aclanthology.org/2021.emnlp-main.650.pdf).
- The results presented in Section 5.3 look rather noisy to me as the accuracy on ContraPro decreases from context length 60 to 120 and from 120 to 250 while it improves considerably for larger context lengths. Intuitively, a substantial context length increase (e.g. from 60 to 250 tokens) should not hurt the accuracy. The authors do not make an attempt to explain this trend.
- It would be helpful to also provide the confidence intervals to strengthen the conclusions from the experiments with one or multiple factors.
- The authors mention in Section 5.1 that the cross entropy loss “fails to fully depict the translation quality”. I don’t think that this conclusion is valid based on the authors' experiments. The authors are measuring translation quality in terms of (d-)BLEU, i.e. in terms of a metric that has many limitations, especially for document-level MT, see [Kocmi et al. (2021)](https://aclanthology.org/2021.wmt-1.57.pdf); [Post and Junczys-Dowmunt (2023)](https://arxiv.org/pdf/2304.12959.pdf). If general translation quality is not properly reflected by the metric, then it can’t really be determined whether the loss is a good indicator of general translation quality.
Some minor comments are:
- For most of the experiments in Section 4, the factor that is held constant is not reported, e.g. for the experiment of the joint effect of maximum sequence length and data scale (in Section 4.2), I couldn’t find the model size.
- The authors mention that previous work, in particular, Beltagy et al. (2020) and Press et al. (2021) demonstrate that model performance improves with a larger context. Let me note here that their methods are different from the ones presented in this paper and thus conclusions can be different for a number of reasons.
- In my opinion, a lot of the details on training and inference configurations can be moved to the appendix.
- The abbreviation “MAC” is used in line 194 already but only explained later in the paper.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
- The applicability of the methods to real world problems is rather limited as strong assumptions are made about the availability of camera parameters (extrinsics and intrinsics are known) and object segmentation.,NIPS_2017_35,NIPS_2017,"- The applicability of the methods to real world problems is rather limited as strong assumptions are made about the availability of camera parameters (extrinsics and intrinsics are known) and object segmentation.
- The numerical evaluation is not fully convincing as the method is only evaluated on synthetic data. The comparison with [5] is not completely fair as [5] is designed for a more complex problem, i.e., no knowledge of the camera pose parameters.
- Some explanations are a little vague. For example, the last paragraph of Section 3 (lines 207-210) on the single image case. Questions/comments:
- In the Recurrent Grid Fusion, have you tried ordering the views sequentially with respect to the camera viewing sphere?
- The main weakness to me is the numerical evaluation. I understand that the hypothesis of clean segmentation of the object and known camera pose limit the evaluation to purely synthetic settings. However, it would be interesting to see how the architecture performs when the camera pose is not perfect and/or when the segmentation is noisy. Per category results could also be useful.
- Many typos (e.g., lines 14, 102, 161, 239 ), please run a spell-check.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- L006, as later written in the main text, ""thousands"" is not accurate here. Maybe add ""on the subword level""?",ARR_2022_286_review,ARR_2022,"While there exist many papers discussing the softmax bottleneck or the stolen probability problem, similar to what the authors found, I personally have not found enough evidence in my work that the problem is really severe.
After all, there are intrinsic uncertainties in the empirical distributions of the training data, and it is quite natural for us to use a smaller hidden dimension size than the vocabulary size, because after all, we call them ""word embeddings"" for a reason.
I guess what I mean to say here is that the problem is of limited interest to me (which says nothing about a broader audience) because the results agree very well with my expectations.
This is definitely not against the authors because they did a good job in showing this via their algorithm and the concrete experiments.
I feel like the authors could mention and expand on the implications when beam search is used.
Because in reality, especially that many MT models are considered in the paper, greedy search is seldomly used.
In other words, ""even if greedy search is used, SPP is not a big deal, let alone that in reality we use beam search"", something like that.
Compared to the main text, I am personally more interested in the point brought up at L595.
What implications are there for the training of our models?
How does the gradient search algorithm decide on where to put the word vectors and hidden state vector?
Is there anything we, i.e. the trainers of the NNs, can do to make it easier for the NNs?
Small issues: - L006, as later written in the main text, ""thousands"" is not accurate here. Maybe add ""on the subword level""?
- L010, be predicted - L034, personally, I think ""expressiveness"" is more commonly used, this happens elsewhere in the paper as well.
- L082, autoencoder - L104, greater than or equal to","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- A number of hyperparameters (e.g. regularization) are not given - For all the latent path figures (eg Fig 3) why is the y value at x= 0 always 0? Is it normalized to this? Be clear in your description (or maybe I missed it) - I would be interested in seeing some further analysis on this model, perhaps using the interpolations themselves",ICLR_2021_634,ICLR_2021,"+ Clarifications: - The question of the latent variable model seems relevant and interesting. It seems that the mixup method is only as good as the model, and also the trained model might add its own biases to the classification task. It would be nice to see some discussion of this in the paper - I am surprised that mixup improves precision on the adult task. It would be good to see some exploration of this - For experiments, are all runs shown? Or just the Pareto fronts. - A number of hyperparameters (e.g. regularization) are not given - For all the latent path figures (eg Fig 3) why is the y value at x= 0 always 0? Is it normalized to this? Be clear in your description (or maybe I missed it) - I would be interested in seeing some further analysis on this model, perhaps using the interpolations themselves","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- There seems to be forward referencing in the paper. Material is introduced without proper explanation, and is explained in later sections e.g. Figure1 - The exact contribution(s) need to be written more clearly in the Introduction. Moreover, the material supporting the main contributions seems to be in the appendix and not the main sections e.g. deep-rag algorithm or discussion on the high concurrency.",kNvwWXp6xD,ICLR_2025,"- I found the paper's flow to be quite confusing. It seems the author's had a lot of material to cover, most of which is placed in the appendix. Perhaps because of this, the actual paper lacks clarity and the required detail. It would be helpful if the authors present the material in the main sections, and refer to the appropriate appendix in case the reader wants further detail.
- There seems to be forward referencing in the paper. Material is introduced without proper explanation, and is explained in later sections e.g. Figure1
- The exact contribution(s) need to be written more clearly in the Introduction. Moreover, the material supporting the main contributions seems to be in the appendix and not the main sections e.g. deep-rag algorithm or discussion on the high concurrency.
- The experiments section seems to be defining the evaluation measures rather than focusing on an explanation of the experiments and results
- The authors mention that the superior performance of their approach can be attributed to several factors. However, it is not clear which factor is actually contributing towards the better results
- Some sentences are confusing e.g. in the first para of the Introduction: HumanEval() first proposed to let LLM generating code based on .......","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- In the paper it is mentioned that the obtained core tensors can be rounded to smaller ranks with a given accuracy by clustering the values of the domain sets or imposing some error decision epsilon if the values are not discrete. It is not clear what is, in theory, the effect on the approximation in the full tensor error. Is there any error bound in terms of epsilon?",ICLR_2023_1195,ICLR_2023,"- The assumption that a set of analytical derivative functions is available is a very strong hypothesis so the number of cases where this method can be applied seems limited. - The high dimensional tensor can be also compactly represented by the set of derivative functions avoiding the curse of dimensionality, so it is not clear what is the advantage of replacing the original compact representation by the TT representation. Maybe the reason is that in TT-format many operations can be implemented more efficiently. The paper gives not a clear explanation about the necessity of the TT representation in this case. - It is not clear in which cases the minimum rank is achieved by the proposed method. Is there a way to check it? - In the paper it is mentioned that the obtained core tensors can be rounded to smaller ranks with a given accuracy by clustering the values of the domain sets or imposing some error decision epsilon if the values are not discrete. It is not clear what is, in theory, the effect on the approximation in the full tensor error. Is there any error bound in terms of epsilon? - The last two bullets in the list of main contributions and advantages of the proposed approach are not clear to me (Page 2). - The method is introduced by an application example using the P_step function (section 2.2). I found this example difficult to follow and maybe not relevant from the point of view of an application. I think, a better option would be to use some problem easier to understand, for example, one application to game theory as it is done later in the paper. - Very relevant ideas and results are not included in the main paper and referred instead to the Appendix, which makes the paper not well self-contained. - The obtained performance in terms of complexity for the calculation of the permanent of a matrix is not better than standard algorithms as commented by the authors (Hamilton walks obtained the result with half of the complexity). It is not clear what is the advantage of the proposed new method for this application. - The comparison with the TT-cross method is not clear enough. What is the number of samples taken in the TT-cross method? What is the effect to increase the number of samples in the TT-cross method. I wonder if the accuracy of the TT-cross method can be improved by sampling more entries of the tensor.
Minor issues: - Page 2: “an unified approach” -> “a unified approach” - Page 2: “and in several examples is Appendix” -> “and in several examples in the Appendix” - In page 3, “basic vector e” is not defined. I think the authors refers to different elements of the canonical base, i.e., vectors containing all zeros except one “1” in a different location. This should be formally introduced somewhere in the paper. - Page 9: “as an contraction” -> “as a contraction”","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2: There are two test settings in visual dialog, while the Table 1 only shows the result on discriminative setting. It's known that discriminative setting can not apply on real applications, what is the result on generative setting?",NIPS_2017_356,NIPS_2017,"]
My major concerns about this paper is the experiment on visual dialog dataset. The authors only show the proposed model's performance on discriminative setting without any ablation studies. There is not enough experiment result to show how the proposed model works on the real dataset. If possible, please answer my following questions in the rebuttal.
1: The authors claim their model can achieve superior performance having significantly fewer parameters than baseline [1]. This is mainly achieved by using a much smaller word embedding size and LSTM size. To me, it could be authors in [1] just test model with standard parameter setting. To backup this claim, is there any improvements when the proposed model use larger word embedding, and LSTM parameters?
2: There are two test settings in visual dialog, while the Table 1 only shows the result on discriminative setting. It's known that discriminative setting can not apply on real applications, what is the result on generative setting?
3: To further backup the proposed visual reference resolution model works in real dataset, please also conduct ablation study on visDial dataset. One experiment I'm really interested is the performance of ATT(+H) (in figure 4 left). What is the result if the proposed model didn't consider the relevant attention retrieval from the attention memory.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- Perhaps something more should be done to convince the reader that a query of the type SEARCH is feasible in some realistic scenario. One other little thing:,NIPS_2016_374,NIPS_2016,"weakness is the presentation: - From my understanding of the submission instructions, the main part of the paper should include all that one needs to understand the paper (even if proofs may be in supplementary material). I thus found it awkward to have a huge algorithm listing as in Alg. 2, without any accompanying text explaining it, and to have algorithms in the supplementary material without giving at least a brief idea of the algorithms in the main body of the paper. This makes it hard to read the paper, and I think it is not appropriate for publication. - Perhaps something more should be done to convince the reader that a query of the type SEARCH is feasible in some realistic scenario. One other little thing: - what is meant by ""and quantities that do appear"" in line 115?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
2) The effectiveness of the proposed approach for other language families remains unknown.,e4dXIBRQ9u,EMNLP_2023,"- **Flexibility**: One of the limitations of this approach lies in the requirement for a homologous teacher model in terms of paradigm and vocabulary table with the student model. This can hinder the method's flexibility and may pose challenges during the preparation of the teacher model.
- **Scope**: The pre-trained teacher model tends to be already a strong model; do we truly need to train a student model from scratch using weighted training? Is it possible to simply fine-tune the teacher model with weighted training? Weighted training would be meaningful if our goal is to obtain a small yet strong student model. However, the authors have not clearly indicated the specific targeted scenarios in this paper.
- **Experiments**:
1) The authors did not explore how different teacher models affect the student's learning effectiveness, which could have provided valuable insights into the impact of varying teacher models on the proposed method's performance. The choice of teacher model may lack flexibility; however, it is worth noting that there are numerous robust GEC systems that share the same PLM architecture.
2) The effectiveness of the proposed approach for other language families remains unknown.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.",ICLR_2022_2677,ICLR_2022,"1 The authors do not analyze the security (i.e., protection of the privacy) of the proposed framework.
2 The authors do not analyze the communication cost between each client (i.e., domain) and the server. In a typical federated learning system, the communication cost is a very important issue.
3 The way of using an encoder and a decoder, or a domain-specific part and a domain-independent part are well known in existing cross-domain or transfer learning works.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"* It would help if the form of p was described somewhere near line 135. As per my above comment, I assume it is a Gaussian distribution, but it's not explicitly stated.",NIPS_2019_663,NIPS_2019,"of their work?""] The submission is overall reasonably sound, although I have some comments and questions: * Regarding the model itself, I am confused by the GRU-Bayes component. I must be missing something, but why is it not possible to ingest observed data using the GRU itself, as in equation 2? This confusion would perhaps be clarified by an explanation in line 89 of why continuous observations are required. As it is written, I am not sure why it you couldn't just forecast (by solving the ODE defined by equation 3) the hidden state until the next measurement arrives, at which point g(t) and z(t) can be updated to define a new evolution equation for the hidden state. I am guessing the issue here is that this update only changes the derivative of the hidden state and not its value itself, but since the absolute value of the hidden state is not necessarily meaningful, the problem with this approach isn't very clear to me. I imagine the authors have considered such a model, so I would like to understand why it wouldn't be feasible here. * In lines 143-156, it is mentioned that the KL term of the loss can be computed empirically for binomial and Gaussian distributions. I understand that in the case of an Ornstein-Uhlenbeck SDE, the distribution of the observations are known to be (conditionally) Gaussian, but in the case of arbitrary data (e.g. health data), as far as I'm aware, few assumptions can be made of the underlying process. In this case, how is the KL term managed? Is a Gaussian distribution assumption made? Line 291 indicates this is the case, but it should be made clear that this is an assumption imposed on the data. For example, in the case of lab test results as in MIMIC, these values are rarely Gaussian-distributed and may not have Gaussian-distributed observation noise. On a similar note, it's mentioned in line 154 that many real-world cases have very little observation noise relative to the predicted distribution - I assume this is because the predicted distribution has high variance, but this statement could be better qualified (e.g. which real-world cases?). * It is mentioned several times (lines 203, 215) that the GRU (and by extension GRU-ODE-Bayes) excels at long-term forecasting problems, however in both experiments (sections 5.2 and 5.3) only near-term forecasting is explored - in both cases only the next 3 observations are predicted. To support this claim, longer prediction horizons should be considered. * I find it interesting that the experiments on MIMIC do not use any regularly-measured vital signs. I assume this was done to increase the ""sporadicity"" of the data, but it makes the application setting very unrealistic. It would be very unusual for values such as heart rate, respiratory rate, blood pressure and temperature not to be available in a forecasting problem in the ICU. I also think it's a missed opportunity to potentially highlight the ability of the proposed model to use the relationship between the time series to refine the hidden state. I would like to know why these variables were left out, and ideally how the model would perform in their presence. * I think the experiment in Section 5.5 is quite interesting, but I think a more direct test of the ""continuity prior"" would be to explicitly test how the model performs (in the low v. high data cases) on data which is explicitly continuous and *not* continuous (or at least, not 2-Lipschitz). The hypothesis that this continuity prior is useful *because* it encodes prior information about the data would be more directly tested by such a setup. At present, we can see that the model outperforms the discretised version in the low data regime, but I fear this discretisation process may introduce other factors which could explain this difference. It is slightly hard to evaluate because I'm not entirely sure what the discretised version consists of , however - this should be explained (perhaps in the appendix). Furthermore, at present there is no particular reason to believe that the data in MIMIC *is* Lipschitz-2 - indeed, in the case of inputs and outputs (Table 4, Appendix), many of these values can be quite non-smooth (e.g. a patient receiving aspirin). * It is mentioned (lines 240-242, section H.1.3) that this approach can handle ""non-aligned"" time series well. As mentioned, this is quite a challenging problem in the healthcare setting, so I read this with some interest. Do these statements imply that this ability is unique to GRU-ODE-Bayes, and is there a way to experimentally test this claim? My intuition is that any latent-variable model could in theory capture the unobserved ""stage"" of a patient's disease process, but if GRU-ODE-Bayes has some unique advantage in this setting it would be a valuable contribution. At present it is not clearly demonstrated - the superior performance shown in Table 1 could arise from any number of differences between this model and the baselines. 2.c Clarity: [""Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.)""] While I quite like the layout of the paper (specifically placing related work after a description of the methodology, which is somewhat unusual but makes sense here) and think it is overall well written, I have some minor comments: * Section 4 is placed quite far away from the Figure it refers to (Figure 1). I realise this is because Figure 1 is mentioned in the introduction of the paper, but it makes section 4 somewhat hard to follow. A possible solution would be to place section 4 before the related research, since the only related work it draws on is the NeuralODE-VAE, which is already mentioned in the Introduction. * I appreciate the clear description of baseline methods in Section 5.1. * The comprehensive Appendix is appreciated to provide additional detail about parts of the paper. I did not carefully read additional experiments described in the Appendix (e.g. the Brusselator) out of time consideration. * How are negative log-likelihoods computed for non-probabilistic models in this paper? * Typo on line 426 (""me"" instead of ""we""). * It would help if the form of p was described somewhere near line 135. As per my above comment, I assume it is a Gaussian distribution, but it's not explicitly stated. 2.d Significance: [""Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?""] This paper describes quite an interesting approach to the modelling of sporadically-measured time series. I think this will be of interest to the community, and appears to advance state of the art even if it is not explicitly clear where these gains come from.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- Clarity: Related work could be improved. Some related works are mainly named but their differences are not described enough.,NIPS_2019_445,NIPS_2019,"- Quality: Results of Section 2.1, which builds the main motivation of the paper, is demonstrated on a very limited settings and examples. It does not convince the reader that overfitting is the general reason for potential poor performance of the models under study. - Soundness: While expressiveness is useful, it does not mean that the optimal weights are learnable. The paper seem to not pay attention to this issue. - Clarity: Related work could be improved. Some related works are mainly named but their differences are not described enough. - Organization could be improved. Currently the paper is dependent on appendix (eg the algorithms). Also the contents of tables are too small. Overall, I do not think the quality of the paper is high enough and I vote for it to be rejected.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"3. Authors state that reliable PPP metrics are important for understanding PPP effects in different tasks. While this point is surely intriguing, such an explanation or understanding is not explicitly given in the article. Can the authors explicitly explain what type of understanding one reaches by looking at the PPP maps?",NIPS_2022_2814,NIPS_2022,"1. A solution towards removing the position encoding is not discussed. 2. Importance of quantifying the strength of PPP is not clear to me. 3. Authors state that reliable PPP metrics are important for understanding PPP effects in different tasks. While this point is surely intriguing, such an explanation or understanding is not explicitly given in the article. Can the authors explicitly explain what type of understanding one reaches by looking at the PPP maps? 4. The conclusion of the article remains a bit vague. While the proposed metrics have some more desirable attributes, value of these attributes for applications is unclear to me. How will this actually improve the practice or our understanding?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '4', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"5. The authors do not compare their methods with other state-of-the-art methods for span-related tasks, such as SpanBERT, thus lacking some credibility.",JMSkoIYFSn,EMNLP_2023,"1.	This paper shows little novelty, as it is only a marginal improvement of the conventional attention mechanism based on some span-level inductive bias.
2.	The performance improvement over simple baselines like max-pooling is also marginal for probing tasks.
3.	Although 4 patterns are proposed in this paper, it seems hard to find a unified solution or guideline for how to combine them in different tasks. If we have to try all possible combinations every time, the practicality of this method would be significantly reduced.
4. The authors validate the effectiveness of the proposed span-level attention only on the backbone of BERT. The paper lacks experiments on other encoder backbones to further demonstrate the generality of the proposed method.
5. The authors do not compare their methods with other state-of-the-art methods for span-related tasks, such as SpanBERT, thus lacking some credibility.
6. The writing can be improved. There are some typos and unclear descriptions. Please refer to comments for detail.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- lines 32 - 37: You discuss how the regret cannot be sublinear, but proceed to prove that your method achieves T^{1/2} regret. Do you mean that the prediction error over the entire horizon T cannot be sublinear?",NIPS_2018_428,NIPS_2018,"weakness which decreased my score. Some line by line comments: - lines 32 - 37: You discuss how the regret cannot be sublinear, but proceed to prove that your method achieves T^{1/2} regret. Do you mean that the prediction error over the entire horizon T cannot be sublinear? - eq after line 145: typo --- i goes from 1 to n and since M,N are W x k x n x m, the index i should go in the third position. Based on the proof, the summation over u should go from tau to t, not from 1 to T. - line 159: typo -- ""M"" --> Theta_hat - line 188: use Theta_hat for consistency. - line 200: typo -- there should no Pi in the polynomial. - line 212: typo --- ""beta^j"" --> beta_j - line 219: the vector should be indexed - lines 227 - 231: the predictions in hindsight are denoted once by y_t^* and once by hat{y}_t^* - eq after line 255: in the last two terms hat{y}_t --> y_t Comments on the Appendix: - General comment about the Appendix: the references to Theorems and equations are broken. It is not clear if a reference points to the main text or to the appendix. - line 10: Consider a noiseless LDS... - line 19: typo -- N_i ---> P_i - equation (21): same comment about the summation over u as above. - line 41: what is P? - line 46: typo --- M_omega' ---> M_ell' - eq (31): typo -- no parenthesis before N_ell - line 56: the projection formula is broken - eq (56): why did you use Holder in that fashion? By assumption the Euclidean norm of x is bounded, so Cauchy Schwartz would avoid the extra T^{1/2}. ================== In line 40 of the appendix you defined R_x to be a bound on \|x\|_2 so there is no need for the inequality you used in the rebuttal. Maybe there is a typo in line 40, \|x\|_2 maybe should be \|x\|_\infty","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The details of the forward-prediction model is not well explained. In particular, Figure 2(b) does not really show the schematic representation of the forward prediction model; the figure should be redrawn. It was hard to connect the pieces of the text with the figure as well as the equations.",NIPS_2016_93,NIPS_2016,"- The claims made in the introduction are far from what has been achieved by the tasks and the models. The authors call this task language learning, but evaluate on question answering. I recommend the authors tone-down the intro and not call this language learning. It is rather a feedback driven QA in the form of a dialog. - With a fixed policy, this setting is a subset of reinforcement learning. Can tasks get more complicated (like what explained in the last paragraph of the paper) so that the policy is not fixed. Then, the authors can compare with a reinforcement learning algorithm baseline. - The details of the forward-prediction model is not well explained. In particular, Figure 2(b) does not really show the schematic representation of the forward prediction model; the figure should be redrawn. It was hard to connect the pieces of the text with the figure as well as the equations. - Overall, the writing quality of the paper should be improved; e.g., the authors spend the same space on explaining basic memory networks and then the forward model. The related work has missing pieces on more reinforcement learning tasks in the literature. - The 10 sub-tasks are rather simplistic for bAbi. They could solve all the sub-tasks with their final model. More discussions are required here. - The error analysis on the movie dataset is missing. In order for other researchers to continue on this task, they need to know what are the cases that such model fails.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- For RBI, they only train on rewarded actions. Then this means rewardless actions that get useful supervision (such as ""No, the answer is Timothy Dalton."" in Task 3) is ignored as well. I think this could be one significant factor that makes FP + RBI better than RBI alone. If not, I think the authors should provide stronger baseline than RBI (that is supervised by such feedback) to prove the usefulness of FP. Questions / Minor concerns:",NIPS_2016_93,NIPS_2016,"/ Major concerns: - It is difficult to evaluate whether the MovieQA result should be considered significant given that +10% gap exists between MemN2N on dataset with explicit answers (Task 1) and RBI + FP on dataset with other forms of supervision, especially Task 3. If I understood correctly, the different tasks are coming from the same data, but authors provide different forms of supervision. Also, Task 3 gives full supervision of the answers. Then I wonder why RBI + FP on task 3 (69%) is doing much worse than MemN2N on task 1 (80%). Is it because the supervision is presented in a more implicit way (""No, the answer is kitchen"" instead of ""kitchen"")? - For RBI, they only train on rewarded actions. Then this means rewardless actions that get useful supervision (such as ""No, the answer is Timothy Dalton."" in Task 3) is ignored as well. I think this could be one significant factor that makes FP + RBI better than RBI alone. If not, I think the authors should provide stronger baseline than RBI (that is supervised by such feedback) to prove the usefulness of FP. Questions / Minor concerns: - For bAbI, it seems the model was only tested on single supporting fact dataset (Task 1 of bAbI). How about other tasks? - How is dialog dataset obtained from QA datasets? Are you using a few simple rules? - Lack of lexical / syntactic diversity of teacher feedback: assuming the teacher feedback was auto-generated, do you intend to turk the teacher feedback and / or generate a few different kinds of feedback (which is more real-life situation)? - How does other models than MemN2N do on MovieQA?","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN.",NIPS_2017_567,NIPS_2017,"Weakness:
1. I find the first two sections of the paper hard to read. The author stacked a number of previous approaches but failed to explain each method clearly.
Here are some examples:
(1) In line 43, I do not understand why the stacked LSTM in Fig 2(a) is ""trivial"" to convert to the sequential LSTM Fig2(b). Where are the h_{t-1}^{1..5} in Fig2(b)? What is h_{t-1} in Figure2(b)?
(2) In line 96, I do not understand the sentence ""our lower hierarchical layers zoom in time"" and the sentence following that.
2. It seems to me that the multi-scale statement is a bit misleading, because the slow and fast RNN do not operate on different physical time scale, but rather on the logical time scale when the stacks are sequentialized in the graph. Therefore, the only benefit here seems to be the reduce of gradient path by the slow RNN.
3. To reduce the gradient path on stacked RNN, a simpler approach is to use the Residual Units or simply fully connect the stacked cells. However, there is no comparison or mention in the paper.
4. The experimental results do not contain standard deviations and therefore it is hard to judge the significance of the results.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"3) extending the equalized odds results to more datasets (why are Parkinsons and Retired Adult results only reported for demographic parity? it seems like equalized odds should also apply here, and an empirical story built on 2 datasets seems thin). I think",ICLR_2023_1522,ICLR_2023,"The application of FERMI is not obviously a large improvement over its introduction in (Lowy+ 2021): we want to optimize a weird fairness-constrained loss, so we instead optimize an upper bound on it, which admits a stochastic convergence analysis, and also handles non-binary classification. The added contribution here is, in the paper's phrasing, ""a careful analysis of how the Gaussian noises [necessary for DP-SGD] propagate through the optimization trajectories"". I don't have much feel for what constitutes an ""interesting"" convergence analysis, but the conceptual novelty here is unclear, and the introduction is a bit slippery about what is novel and what is borrowed from the FERMI paper.
The paper also struggles to explain its technical contributions in terms between a very high level summary and a long, opaque theorem statement. I suggest changing the focus of the paper to 1) reduce, relegate to the appendix, or eliminate the discussion of demographic parity (an extremely coarse notion of fairness that, IMO, the fairness literature needs to move past, and has only been discussed this long because it's very simple), which takes up over a page of the main body without meaningfully adding to the story told by the equalized odds results alone, 2) extending the discussion of how Theorem 3.1 works and what it accomplishes (the current statement is a blizzard of notation with little explanation -- I still don't know what W is doing), along with Theorem 3.2, and 3) extending the equalized odds results to more datasets (why are Parkinsons and Retired Adult results only reported for demographic parity? it seems like equalized odds should also apply here, and an empirical story built on 2 datasets seems thin). I think 2) would help provide a clearer explanation of the paper's improvement over (Lowy+ 2021) and 3) would make a stronger empirical case separate from the convergence analysis.
Other questions/comments:
I'd appreciate a table in the appendix attempting to concisely explain all of the relevant variables -- by my count, Theorem B.1 has well over a dozen.
Why is Tran 2021b a single point where the other methods have curves? More generally, perhaps I missed the explanation of this in the text, but what is varied to generate the curves?
As far as I can tell, the paper does not discuss the tightness of the upper bound offered by ERMI, nor does it explicitly write out the expression for equalized odds. This makes it hard to contextualize the convergence guarantee in terms of the underlying loss we actually want to optimize.
Figure 4 ""priavte""","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['4', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"4. Baseline methods are weak and not presenting state-of-the-art. There is no discussion of limitation. Along with my questions regarding the difference between this work and reinforcement learning, one possible direction in the conclusion is to talk about the similarity and difference, and to what extent the results are generalizable to RL settings.",NIPS_2022_1317,NIPS_2022,"Weakness: 1. Literature review is not adequate. Even with the content in the appendix, this is no discussion of off-policy evaluation for reinforcement learning or non-stationary multi-armed bandits. 2. The claim and the discussion of “active non-stationarity” is somewhat confusing (more on this later in Questions). 3. The authors seem to overstate their contribution.
4. Baseline methods are weak and not presenting state-of-the-art.
There is no discussion of limitation. Along with my questions regarding the difference between this work and reinforcement learning, one possible direction in the conclusion is to talk about the similarity and difference, and to what extent the results are generalizable to RL settings.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '5', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Typically, expected performance under observation noise is used for evaluation because the decision-maker is interested in the true objective function and the noise is assumed to be noise (misleading, not representative). In the formulation in this paper, the decision maker does care about the noise; rather the objective function of interest is the stochastic noisy function. It would be good to make this distinction clearer upfront.",NIPS_2021_1251,NIPS_2021,"- Typically, expected performance under observation noise is used for evaluation because the decision-maker is interested in the true objective function and the noise is assumed to be noise (misleading, not representative). In the formulation in this paper, the decision maker does care about the noise; rather the objective function of interest is the stochastic noisy function. It would be good to make this distinction clearer upfront. - The RF experiment is not super compelling. It is not nearly as interesting as the FEL problem, and the risk aversion does not make a significant difference in average performance. Overall the empirical evaluation is fairly limited. - It is unclear why the mean-variance model is the best metric to use for evaluating performance - Why not also evaluate performance in terms of the VaR or CVaR? - The MV objective is nice for the proposed UCB-style algorithm and theoretical work, but for evaluation VaR and CVaR also are important considerations
Writing: - Very high quality and easy to follow writing - Grammar: - L164: “that that” - Figure 5 caption: “Simple regret fat the reprted”
Questions: - Figure 2: “RAHBO not only leads to strong results in terms of MV, but also in terms of mean objective”? Why is it better than GP-UCB on this metric? Is this an artifact of the specific toy problem?
Limitations are discussed and potential future directions are interesting. “We are not aware of any societal impacts of our work” – this (as with an optimization algorithm) could be used for nefarious endeavors and could be discussed.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Can we run VGAE with a vamp prior to more accurately match the doubly stochastic construction in this work? That would help inform if the benefits are coming from a better generative model or better inference due to doubly-semi implicit variational inference. Minor Points - Figure 3: It might be nice to keep the generative model fixed and then optimize only the inference part of the model, parameterizing it as either SIG-VAE or VGAE to compare the representations. Its impossible to know / compare representations when the underlying generative models are also potentially different.",NIPS_2019_961,NIPS_2019,"- It would be good to better justify and understand the bernoulli poisson link. Why are the number of layers used in the link in the poisson part? The motivation for the original paper [40] seems to be that one can capture communities and the sum in the exponential is over r_k coefficientst where each coefficient corresponds to a community. In this case the sum is over layers. How do the intuitions from that work transfer here? In what way do the communities correspond to layers in the encoder? It would be nice to beter understand this. Missing Baselines - It would be instructive to vary the number of layers of processing for the representation during inference and analyze how that affects the representations and performance on downstream tasks. - Can we run VGAE with a vamp prior to more accurately match the doubly stochastic construction in this work? That would help inform if the benefits are coming from a better generative model or better inference due to doubly-semi implicit variational inference. Minor Points - Figure 3: It might be nice to keep the generative model fixed and then optimize only the inference part of the model, parameterizing it as either SIG-VAE or VGAE to compare the representations. Its impossible to know / compare representations when the underlying generative models are also potentially different.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The proposed algorithm is well constructed and is a step-forward from the main reference [1]. While not having a major practical impact (has an high regret in stationary regime and in highly non-stationary regime, even if the best action does not change) it resolves and open problem and gives some closure the the analysis using the variation.",NIPS_2016_23,NIPS_2016,"in the bibliography, switching-bandit algorithms with dependencies on the number of switches (EXP3.S is however cited[7]) and gaps between action's mean rewards being not described (Discounted UCB, Sliding Window UCB, EXP3 with reset). The detector of non-stationary has also some similarities with the SAO (Stochastic and Adversarial Optimal) algorithm that starts with a stationary algorithm and then switches to EXP3 if a non-stationarity is detected. - The proposed algorithm is well constructed and is a step-forward from the main reference [1]. While not having a major practical impact (has an high regret in stationary regime and in highly non-stationary regime, even if the best action does not change) it resolves and open problem and gives some closure the the analysis using the variation. - After the post rebuttal discussion, we agreed to rank this paper at poster level (5: 4->3).","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
"- Though the performance of method is pretty good especially in Table 2, the novelty/contribution of the method is somewhat incremental. The main contribution of the work is a new network design drawing inspirations from prior work for the sound source localization task.",NIPS_2020_844,NIPS_2020,"- It is claimed that the proposed method aims to discrminatively localize the sounding objects from their mixed sound without any manual annotations. However, the method aslo aims to do class-aware localization. As shown in Figure 4, the object categories are labeled for the localized regions for the proposed method. It is unclear to this reviewer whether the labels there are only for illustrative purposes? - Even the proposed method doesn't rely on any class labels, it needs the number of categories of potential sound sources in the data to build the object dictionary. - Though the performance of method is pretty good especially in Table 2, the novelty/contribution of the method is somewhat incremental. The main contribution of the work is a new network design drawing inspirations from prior work for the sound source localization task. - The method assumes single source videos are available to train in the first stage, which is also a strong assumption even though class labels are not used. Most in-the-wild videos are noisy and multi-source. It would be desired to have some analysis to show how robust the system is to noise in videos or how the system can learn without clean single source videos to build the object dictionary.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- The paper opens that learning long-range dependencies is important for powerful predictors. In the example of semantic segmentation I can see that this is actually happening, e.g., in the visualisations in table 3; but I am not sure if it is fully required. Probably the truth lies somewhere in between and I miss a discussion about this. If no form of locality with respect to the 2d image space is encoded in the graph structure, I suspect that prediction suddenly depends on the image size.",NIPS_2018_849,NIPS_2018,"- The presented node count for the graphs is quite low. How is performance affected if the count is increased? In the example of semantic segmentation: how does it affect the number of predicted classes? - Ablation study: how much of the learned pixel to node association is responsible for the performance boost. Previous work has also shown in the past that super-pixel based prediction is powerful and fast, I.e. with fixed associations. # Typos - Line 36: and computes *an* adjacency matrix - Line 255: there seems to be *a weak* correlation # Further Questions - Is there an advantage in speed in replacing some of the intermediate layers with this type of convolutional blocks? - Any ideas on how to derive the number of nodes for the graph? Any intuition on how this number regularises the predictor? - As far as I can tell the projection and re-projection is using activations from the previous layer both as feature (the where it will be mapped) and as data (the what will be mapped). Have you thought about deriving different features based on the activations; maybe also changing the dimension of the features through a non-linearity? Also concatenating hand-crafted features (or a learned derived value thereof), e.g., location, might lead to a stronger notion of ""regions"" as pointed out in the discussion about the result of semantic segmentation. - The paper opens that learning long-range dependencies is important for powerful predictors. In the example of semantic segmentation I can see that this is actually happening, e.g., in the visualisations in table 3; but I am not sure if it is fully required. Probably the truth lies somewhere in between and I miss a discussion about this. If no form of locality with respect to the 2d image space is encoded in the graph structure, I suspect that prediction suddenly depends on the image size.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- Eq. (3): What is $e_l$? Corollaries 1, 2 and 3 and Theorem 4: All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).",NIPS_2017_585,NIPS_2017,"weakness of the paper is in the experiments: there should be more complete comparisons in computation time, and comparisons with QMC-based methods of Yang et al (ICML2014). Without this the advantage of the proposed method remains unclear.
- The limitation of the obtained results:
The authors assume that the spectrum of a kernel is sub-gaussian. This is OK, as the popular Gaussian kernels are in this class. However, another popular class of kernels such as Matern kernels are not included, since their spectrum only decay polynomially. In this sense, the results of the paper could be restrictive.
- Eq. (3):
What is $e_l$?
Corollaries 1, 2 and 3 and Theorem 4:
All of these results have exponential dependence on the diameter $M$ of the domain of data: a required feature size increases exponentially as $M$ grows. While this factor does not increase as a required amount of error $\varepsilon$ decreases, the dependence on $M$ affects the constant factor of the required feature size. In fact, Figure 1 shows that the performance is more quickly getting worse than standard random features. This may exhibit the weakness of the proposed approaches (or at least of the theoretical results).
- The equation in Line 170:
What is $e_i$?
- Subsampled dense grid:
This approach is what the authors used in Section 5 on experiments. However, it looks that there is no theoretical guarantee for this method. Those having theoretical guarantees seem not to be practically useful.
- Reweighted grid quadrature:
(i) It looks that there is no theoretical guarantee with this method.
(ii) The approach reminds me of Bayesian quadrature, which essentially obtains the weights by minimizing the worst case error in the unit ball of an RKHS. I would like to look at comparison with this approach.
(iii) Would it be possible to derive a time complexity?
(iv) How do you chose the regularization parameter $\lambda$ in the case of the $\ell_1$ approach?
- Experiments in Section 5:
(i) The authors reported the results of computation time very briefly (320 secs vs. 384 seconds for 28800 features in MNIST and ""The quadrature-based features ... are about twice as fast to generate, compared to random Fourier features ..."" in TIMIT). I do not they are not enough: the authors should report the results in the form of Tables, for example, varying the number of features.
(ii) There should be comparison with the QMC-based methods of Yang et al. (ICML2014, JMLR2016). It is not clear what is the advantage of the proposed method over the QMC-based methods.
(iii) There should be explanation on the settings of the MNIST and TIMIT classification tasks: what classifiers did you use, and how did you determine the hyper-parameters of these methods? At least such explantion should be included in the appendix.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- The poor long-range modelling ability of DGNs is attributed to oversquashing and vanishing/exploding gradients but the poor performance could also be due to oversmoothing, another phenomenon observed in the context of very deep graph networks [Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, In AAAI'18].",ICLR_2023_802,ICLR_2023,"- There are no experiments to support the claim that A-DGN can specifically alleviate/mitigate oversquashing.
- There are no experiments to support the claim that A-DGN can effectively handle long-range dependencies specifically on graph data requiring long-range reasoning.
- The poor long-range modelling ability of DGNs is attributed to oversquashing and vanishing/exploding gradients but the poor performance could also be due to oversmoothing, another phenomenon observed in the context of very deep graph networks [Deeper Insights into Graph Convolutional Networks for Semi-Supervised Learning, In AAAI'18].","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- Do you have a demonstration or result related to your model collapsing less than other methods? In line 159, you mentioned gradients become 0 and collapse; it was a good point, is it commonly encountered, did you observe it in your experiments?",NIPS_2021_2445,NIPS_2021,"and strengths in their analysis with sufficient experimental detail, it is admirable, but they could provide more intuition why other methods do better than theirs.
The claims could be better supported. Some examples and questions(if I did not miss out anything)
Why using normalization is a problem for a network or a task (it can be thought as a part of cosine distance)? How would Barlow Twins perform if their invariance term is replaced with a euclidean distance?
Your method still uses 2048 as the batch size, I would not consider it as small. For example, Simclr uses examples in the same batch and its batch size changes between 256-8192. Most of the methods you mentioned need even much lower batch size.
You mentioned not sharing weights as an advantage, but you have shared weights in your results, except Table 4 in which the results degraded as you mentioned. What stops the other methods from using different weights? It should be possible even though they have covariance term between the embeddings, how much their performance would be affected compared with yours?
My intuition is that a proper design might be sufficient rather than separating variance terms.
- Do you have a demonstration or result related to your model collapsing less than other methods? In line 159, you mentioned gradients become 0 and collapse; it was a good point, is it commonly encountered, did you observe it in your experiments?
- I am also not convinced to the idea that the images and their augmentations need to be treated separately, they can be interchangeable.
- Variances of the results could be included to show the stability of the algorithms since it was another claim in the paper(although ""collapsing"" shows it partly, it is a biased criteria since the other methods are not designed for var/cov terms).
- How hard is it to balance these 3 terms?
- When someone thinks about gathering two batches from two networks and calculate the global batch covariance in this way; it includes both your terms and Barlow Twins terms. Can anything be said based on this observation, about which one is better and why? Significance:
Currently, the paper needs more solid intuition or analysis or better results to make an impact in my opinion. The changes compared with the prior work are minimal. Most of the ideas and problems in the paper are important, but they are already known.
The comparisons with the previous work is valuable to the field, they could maybe extend their experiments to the more of the mentioned methods or other variants.
The authors did a great job in presenting their work's limitations, their results in general not being better than the previous works and their extensive analysis(tables). If they did a better job in explaining the reasons/intuitions in a more solid way, or include some theory if there is any, I would be inclined to give an accept.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
1.The problem formulation is somewhat unclear in the statement and introduction examples.,NIPS_2021_1958,NIPS_2021,"1.The problem formulation is somewhat unclear in the statement and introduction examples. 2. More baselines or self variants should be compared to better prove the effectiveness.
Detailed comments:
The problem definition of keyword search on incomplete graphs is ambiguous and confusing. The KS-GNN mostly optimizes on node similarity and the inference stage tends to select the top-k most similar results towards the query keyword set. However, the problem itself seems more like a combinatorial one, or say, set optimization, node-set selection with minimal distance measurement. Are the targets here equivalent?
The baseline approach seems much inferior to KS-GNN. It would be great to include some variants of the KS-GNN that delete some of the module or training objectives to confirm the contribution of each component.
Table 2 with missing edges is supposed to be more challenging than the task in Table 1. However, a lot of models perform even better (or comparable) which seems strange. Also, the claim of “KS-GNN has no significant effect” does not apply to the Toy and Video datasets for correctness.
It is hard to conclude from Figure 4 on the benefit of keyword frequency regularization. That is, it’s better to show the performance scores along with the visualization.
Figure 3: The notations of the figure (especially function f and g) are confusing.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '2', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '2', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.",5BoXZXTJvL,ICLR_2024,"1. The novelty of this method appears somewhat constrained. Utilizing the first-order gradient for determining parameter importance is a common approach in pruning techniques applied to CNN, BERT, and ViT. This technique is well-established within the realm of model pruning. Considering in some instances this method even falls short of those achieved by SparseGPT (e.g., 2:4 for LLaMA-1 and LLaMA-2), I cannot say the first-order gradient in pruning LLMs might be a major contribution.
2. This paper lacks experiments on different LLM families. Conducting trials with models like OPT, BLOOM, or other alternatives could provide valuable insights into the method's applicability and generalizability across various LLM families.
3. The paper doesn't provide details regarding the latency of the pruned model. In a study centered on LLM compression, including latency metrics is crucial since such information is highly important to the readers to understand the efficiency of the pruned model.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '2', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- How do the video generation results from the network trained on 5000 hours of video look? Summary: While somewhat incremental, the paper seems to have enough novelty for a poster. The visual results encouraging but with many artifacts. The action classification results demonstrate benefits of the learnt representation compared with random weights but are significantly below state-of-the-art results on the considered dataset.",NIPS_2016_69,NIPS_2016,"- The paper is somewhat incremental. The developed model is a fairly straighforward extension of the GAN for static images. - The generated videos have significant artifacts. Only some of the beach videos are kind of convincing. The action recognition performance is much below the current state-of-the-art on the UCF dataset, which uses more complex (deeper, also processing optic flow) architectures. Questions: - What is the size of the beach/golf course/train station/hospital datasets? - How do the video generation results from the network trained on 5000 hours of video look? Summary: While somewhat incremental, the paper seems to have enough novelty for a poster. The visual results encouraging but with many artifacts. The action classification results demonstrate benefits of the learnt representation compared with random weights but are significantly below state-of-the-art results on the considered dataset.","{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '3']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['3', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh'], 'labels': ['1', '1']}",,
- This method seems to only work for generative models that can be fine-tuned as an in/outpainting model.,F61IzZl5jw,ICLR_2025,"- The paper uses 5,000 images as the training set (am I correct?) . I think the training set size is too small, and is easily memorized with sufficient long learning by large models such as SD 2 . What I am concerned about is what proportion of data is memorized when training with a huge set.
- This method seems to only work for generative models that can be fine-tuned as an in/outpainting model.
- Since the model has been fine-tuned, is it still capable of reflecting the memorization of the original model?
- Although the approach is novel and interesting, it lacks strong evidence to support its effectiveness as an evaluation method for memorization.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', '4', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '0']}",,
"- The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding) Comment:",NIPS_2018_809,NIPS_2018,"Weakness: - The uniqueness of connecting curves between two weights would be unclear, and there might be a gap between the curve and FGE. A natural question would be, for example, if we run the curve findings several times, we will see many different curves? Or, those curves would be nearly unique? - The evidences are basically empirical, and it would be nice if we have some supportive explanations on why this curve happens (and whether it always happens). - The connections of the curve finding (the first part) and FGE (the second part) would be rather weak. When I read the first part and the title, I imagined that take random weights, learn curves between weights, and find nice wights to be mixed into the final ensemble, but it was not like that. (this can work, but also computationally demanding) Comment: - Overall I liked the paper even though the evidences are empirical. It was fun to read. The reported phenomena are quite mysterious, and interesting enough to inspire some subsequent research. - To be honest, I'm not sure the first curve-finding part explains well why the FGE work. The cyclical learning rate scheduling would perturb the weight around the initial converged weight, but it cannot guarantee that weight is changing along the curve described in the first part.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '2']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"5. Competitive Baselines: According to the experimental results, the learning-based solvers are much better than the heuristic-based solvers. However, for the single objective TSP, the SOTA heuristic-solver (e.g., Concorde) usually has the best performance. Since the obtained Pareto front is not highly non-convex (as in Figure 2), the results for linear scalarization + Concorde should be included for a better comparison.",ICLR_2022_2618,ICLR_2022,"Weakness: Method:
1. Problem Formulation:
A key step in the proposed method is to convert the multi-objective optimization problem into different constrained single-objective optimization problems. However, many conversion methods have already been proposed [1-3] which can tackle the non-convex Pareto frontier. None of them are discussed in this work. Is the proposed problem formulation new or adopted from other works?
What are its advantages (and disadvantages) over other methods?
What is J(\pi_k) in (7)?
2. Finding the Whole Pareto Front:
Theorem 1 is a crucial motivation of this work to find the whole Pareto front. However, the theoretical property of Theorem 1 is relatively weak. It can only guarantee that the optimal solution of the constrained single-objective problem can dominate other solutions that satisfy the same constraint. Therefore, the obtained solution is not guaranteed to be Pareto optimal (can be dominated by other feasible solutions out of the constrained set). In addition, even assuming all the single-objective problems can be successfully optimized, there is also no guarantee that all Pareto solutions can be found by the proposed method.
It is also confusing why the obtained solutions can exactly lie on the unit vector in Figure 1. In my understanding, the proposed method can find a solution that satisfies the preference-based constraint while minimizing the L2 norm of all objectives. Since the problem has an inequality constraint, the obtained solution should be close to the unit preference vector but not guaranteed to be exactly on the unit vector.
3. The Model Structure:
To incorporate the preference into the TSP-Net model, the proposed method simply adds the preference embedding to each node (city) embedding. According to the ablation study in Appendix, the preference embedding network can be a simple single-layer FC. The rest model is the standard TSP-Net where the whole transformer-based encoder can be totally replaced by a single 1-D convolution layer. Why can this simple structure work so well for MOTSP? Will it significantly hurt the model performance of each individual preference compared to a single objective model?
More concerns on the results are listed below in the experiment part.
4. Other Problems and Related Work:
One important advantage of the learning-based method is its flexibility to solve different problems [4]. Although this work focuses on MOTSP, I believe it could have a larger impact by showing its ability to solve other problems.
Many works on deep multi-task learning and multi-objective RL have been cited and discussed multiple times in the related work section, while they are not the most related work to this paper. I think it is better to shorten this part into a single paragraph, and leave more space to discuss the related work on learning-based solvers (e.g., [4]) and other approaches for MOTSP. Experiments:
5. Competitive Baselines:
According to the experimental results, the learning-based solvers are much better than the heuristic-based solvers. However, for the single objective TSP, the SOTA heuristic-solver (e.g., Concorde) usually has the best performance. Since the obtained Pareto front is not highly non-convex (as in Figure 2), the results for linear scalarization + Concorde should be included for a better comparison.
6. Unfair Comparison:
The comparison to other learning-based solvers is questionable. First of all, one contribution of DRL-MOA is the transfer learning based training method. The required training epoch (and time) is far less than those reported in this paper. Hence the results reported in Table 1 are misleading. It seems that all the PA-Nets are trained on the 120-city instances for 2,3 and 5 objective MOTSP. However, the original DRL-MOA is only trained on the 40-city MOTSP. As reported in this work, ""The trained model of bi-objective TSP for DRL-MOA (Li et al.,2020) is used."", will it lead to unfair comparison?
The TSP-Net is more powerful than the Ptr-Net used in DRL-MOA, so it is expected that it can have better performance. However, it only moderately outperforms DRL-MOA on the 2 and 5 objective problems, and has worse performance on the 3-obj problems. A more reasonable baseline is to use the TSP-Net in DRL-MOA to check whether the preference-based model could lead to worse performance.
To calculate the hypervolume, this work reports the results with {100,500,500} preferences for the 2, 3, and 5 objective MOTSP instances, while it only reports the results of {100, 91, 40} networks for DRL-MOA. Although it is understandable that one advantage of PA-Net is to generate a dense approximation, the results of {91, 40} preferences should also be reported for a clear comparison. Since PA-Net (with 500 preferences) are already outperformed by DRL-MOA (with only 91 models) for the 3-objective MOTSP, will it be significantly outperformed by DRL-MOA with the same number of solutions?
7. Missing Experiment Settings:
How many training samples and epochs are used to train the PA-Net and DRL-MOA? How many test instances are used to measure the performance for different methods? If there is only one instance for each kind of problem, the results are not convincing. What is the run time (inference) for each method to solve the MOTSP instance with different numbers of objectives and cities?
The details for Hypervolume calculation (e.g., the reference points for different instances) are missing.
8. Surprising Results on the Ablation Studies:
It is quite surprising that the transformer-based encoder is not needed for the TSP-Net, and only a simple one-layer FC is needed for preference embedding. With this setting, the whole PA-Net should have a much smaller scale. What are the total numbers of parameters for these ablation settings (1, 2, and 3)? With the 1-D Conv encoder, the ablation 3 model should be even smaller than the Prt-Net. Why it still needs ~12 hrs to train the model? Why can this model still outperform the Ptr-Net model on the 2-objective MOTSP? Reference:
[1] Das, Indraneel, and John E. Dennis. ""Normal-boundary intersection: A new method for generating the Pareto surface in nonlinear multicriteria optimization problems."" SIAM journal on optimization 8, no. 3: 631-657, 1998.
[2] Mavrotas, George. Effective implementation of the ε-constraint method in multi-objective mathematical programming problems. Applied mathematics and computation 213, no. 2: 455-465, 2009.
[3] Miettinen, Kaisa. Nonlinear multiobjective optimization. Vol. 12. Springer Science & Business Media, 2012.
[4] Kool, Wouter, Herke van Hoof, and Max Welling. Attention, Learn to Solve Routing Problems!. ICLR 2019.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"- while it is interesting to see the grounding of the proposed method in neuroscience, some of the general ideas are already present in other methods for exploration, in particular, reasoning topologically is captured by methods that use the generalized Voronoi graph or semantic maps to guide the exploration, and the long-term storage through pose graphs in SLAM, where loop closure is applied (discussed in graph-based slam appendix section), or curiosity-driven exploration. The paper should discuss the proposed method with respect to such methods.",kYXZ4FT2b3,ICLR_2024,"- while it is interesting to see the grounding of the proposed method in neuroscience, some of the general ideas are already present in other methods for exploration, in particular, reasoning topologically is captured by methods that use the generalized Voronoi graph or semantic maps to guide the exploration, and the long-term storage through pose graphs in SLAM, where loop closure is applied (discussed in graph-based slam appendix section), or curiosity-driven exploration. The paper should discuss the proposed method with respect to such methods.
- the paper's comparison is limited in considering only the standard frontier-based exploration, when in fact there are a number of exploration methods showing better performance than the standard one, both in terms of exploration, as well as planning time. Some examples both classic and learning based include:
Cao, C., Zhu, H., Choset, H., & Zhang, J. (2021, July). TARE: A Hierarchical Framework for Efficiently Exploring Complex 3D Environments. In Robotics: Science and Systems (Vol. 5).
Lindqvist, B., Agha-Mohammadi, A. A., & Nikolakopoulos, G. (2021, September). Exploration-RRT: A multi-objective path planning and exploration framework for unknown and unstructured environments. In 2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (pp. 3429-3435). IEEE.
Shrestha, R., Tian, F. P., Feng, W., Tan, P., & Vaughan, R. (2019, May). Learned map prediction for enhanced mobile robot exploration. In 2019 International Conference on Robotics and Automation (ICRA) (pp. 1197-1204). IEEE.
Caley, J. A., Lawrance, N. R., & Hollinger, G. A. (2019). Deep learning of structured environments for robot search. Autonomous Robots, 43, 1695-1714.
- the gain in memory appears to be a major component of the proposed method, however, overall, the trend seems to be fairly close to the frontier-based approach and somewhat surprising given the use of local maps. In fact, for the realistic experiments, in AWS office, memory appears better for Frontier. The size of each local map might depend on the complexity of the environment, but it is worth discussing what affects the determination of the local map in practice.
A couple of minor presentation comments:
- to be more precise in assumptions and corresponding presentation of functions, it is worth mentioning that the robot is non-omnidirectional, as otherwise the indicator function for whether the frontier edge is spatially behind the agent wouldn't apply. In addition, for that function there would be a threshold to determine what ""behind"" means, with respect to the orientation of the robot.
- usually white pixels are used for free space, instead of black.
- ""FRAGMENTAION"" -> ""FRAGMENTATION""
- ""that work did not seriously explore"" -> ""that work did not explore in-depth""
- instead of calling ""wall-clock time"" it is better to characterize it with ""planning time""","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '4', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
"2. The experimental setup, tasks, and other details are also moved to the appendix which makes it hard to interpret this anyway. I would suggest moving some of these details back in and moving some background from Section 2 to the appendix instead.",NIPS_2018_494,NIPS_2018,"1. The biggest weakness is that there is little empirical validation provided for the constructed methods. A single table presents some mixed results where in some cases hyperbolic networks perform better and in others their euclidean counterparts or a mixture of the two work best. It seems that more work is needed to clearly understand how powerful the proposed hyperbolic neural networks are. 2. The experimental setup, tasks, and other details are also moved to the appendix which makes it hard to interpret this anyway. I would suggest moving some of these details back in and moving some background from Section 2 to the appendix instead. 3. The tasks studied in the experiments section (textual entailment, and a constructed prefix detection task) also fail to provide any insight on when / how the hyperbolic layers might be useful. Perhaps more thought could have been given to constructing a synthetic task which can clearly show the benefits of using such layers. In summary, the theoretical contributions of the paper are significant and would foster more exciting research in this nascent field. However, though it is not the central focus of the paper, the experiments carried out are unconvincing.","{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '5']}",,,7,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['3', 'X', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['DFou9r7M', 'BXzyXMPh', 'boda'], 'labels': ['1', '1', '1']}",,
- It would be helpful if you provided glosses in Figure 2.,ACL_2017_433_review,ACL_2017,"- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement. UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.
- General Discussion: I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.
- Questions for the authors: - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.
- Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?
- Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use ""discourse"" for things that are not considered ""discourse"" in other languages in UD?
- Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.
- Low-level comments: - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.
- You use the word ""grammar"" in a slightly strange way. I think replacing ""grammar"" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90) - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.
- Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with ""very high performing model"" or something like that.
- It would be helpful if you provided glosses in Figure 2.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- Memb is apparently the previous state-of-the-art, but there is no mention to any reference.",ACL_2017_67_review,ACL_2017,"The main weaknesses for me are evaluation and overall presentation/writing.
- The list of baselines is hard to understand. Some methods are really old and it doesn't seem justified to show them here (e.g., Mpttern).
- Memb is apparently the previous state-of-the-art, but there is no mention to any reference.
- While it looks like the method outperforms the previous best performing approach, the paper is not convincing enough. Especially, on the first dataset, the difference between the new system and the previous state-of-the-art one is pretty small.
- The paper seriously lacks proofreading, and could not be published until this is fixed – for instance, I noted 11 errors in the first column of page 2.
- The CilinE hierarchy is very shallow (5 levels only). However apparently, it has been used in the past by other authors. I would expect that the deeper the more difficult it is to branch new hyponym-hypernyms. This can explain the very high results obtained (even by previous studies)... - General Discussion: The approach itself is not really original or novel, but it is applied to a problem that has not been addressed with deep learning yet. For this reason, I think this paper is interesting, but there are two main flaws. The first and easiest to fix is the presentation. There are many errors/typos that need to be corrected. I started listing them to help, but there are just too many of them.
The second issue is the evaluation, in my opinion. Technically, the performances are better, but it does not feel convincing as explained above.
What is Memb, is it the method from Shwartz et al 2016, maybe? If not, what performance did this recent approach have? I think the authors need to reorganize the evaluation section, in order to properly list the baseline systems, clearly show the benefit of their approach and where the others fail.
Significance tests also seem necessary given the slight improvement on one dataset.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
2) why not to consider finer grouping for quantization instead of per-tensor and per-channel?,ICLR_2023_1833,ICLR_2023,". Strengths first:
The paper is one of the first to give an empirical study of quantization of MoE networks. It would be a good manual/starting point for practitioners in the field. Weaknessess:
Thoroughness: Despite having good results and having investigated several quantization options, one would still have questions of ""what if?"" style. There are many additional experiments and empirical evaluations that are needed to make it a stronger contribution, and to be certain of presented recommendations. For instance here are additional questions: 1) if inference happens in fp16, why to stick with uniform or log-uniform quantization schemes? how about non-inform quantization akin k-means? 2) why not to consider finer grouping for quantization instead of per-tensor and per-channel? 3) why PTQ calibration techniques are not discussed? are all calibrations work the same? 4) what is the tradeoff between # experts vs bit-width of compression? are there certain recommendation? and many other questions of this format
The paper would benefit from another proof-reading pass: there are many places where it is hard to understand what was exactly meant.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. It would be better to study the impact of the ratio of unseen classes. For example, how the performance varies with different ratios of unseen classes unlabeled examples.",NIPS_2022_1402,NIPS_2022,"1. The representation could be further improved. For example, there are both “unseen classes” and “unseen-classes” in the paper, this should be unified. 2. It would be better to study the impact of the ratio of unseen classes. For example, how the performance varies with different ratios of unseen classes unlabeled examples.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?,ACL_2017_371_review,ACL_2017,"- The description is hard to follow. Proof-reading by an English native speaker would benefit the understanding - The evaluation of the approach has several weaknesses - General discussion - In Equation 1 and 2 the authors mention a phrase representation give a fix-length word embedding vector. But this is not used in the model. The representation is generated based on an RNN. What the propose of this description?
- Why are you using GRU for the Pyramid and LSTM for the sequential part? Is the combination of two architectures a reason for your improvements?
- What is the simplified version of the GRU? Why is it performing better? How is it performing on the large data set?
- What is the difference between RNNsearch (groundhog) and RNNsearch(baseline) in Table 4?
- What is the motivation for only using the ending phrases and e.g. not using the starting phrases?
- Did you use only the pyramid encoder? How is it performing? That would be a more fair comparison since it normally helps to make the model more complex.
- Why did you run RNNsearch several times, but PBNMT only once?
- Section 5.2: What is the intent of this section","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"6) In line 135, the author says ""Initially the network only has a few active vertices, due to sparsity."" How is ""active vertices"" defined here?",NIPS_2018_700,NIPS_2018,"Weakness: The major quality problem of this paper is clarity. In terms of clarity, there are several confusing places in the paper, especially in equation 9, 10, 11, 12. 1) What is s_{i,j} in these equations? In definition 1, the author mentions that s_{i,j} denotes edge weights in the graph, but what are their values exactly in the experiments? Are they 0/1 or continuous values? 2) How is the diffusion map computed for structural embedding in 10 and 12? Is it using equation 1 only with the learned structural embedding and without text embedding? 3) Why is the diffusion convolution operator only applied to text embedding? Can it also be applied to structural embedding? On the other hand, if the author wants to capture global information in the graph as claimed between line 191 and line 194, why not directly use the diffusion map in equation (1) on text embedding instead of applying the diffusion convolution operator in 4.2? It's confusing to me the relationship between equation (1) and equation (5), (6) and (7) in section 4.2. In other words, there are two methods that could capture the global information: equation (1), and equation (5)(6)(7). Equation (1) is applied to structural embedding in equation (10) and (12); equation (5)(6)(7) are applied to textual embeddings. The author should explain why they do so. 4) I wonder whether the structural embedding is really necessary in this case, since in the learning process, the structural embedding just involves a embedding table lookup. The author does not explain the point of using a structural embedding, especially in such a way. What if just use diffusion text embedding? I don't see any experimental results proving the effectiveness of structural embedding in Table 1. 5) What's the motivation of each part in equation (8)? For example, what's the motivation of maximizing the probability of textual embedding of vertex i conditioned on the diffusion map of structural embedding of vertex j in equation (12)? 6) In line 135, the author says ""Initially the network only has a few active vertices, due to sparsity."" How is ""active vertices"" defined here? 7) In 5.2, when the author trains the SVM classifier, do they also fine-tune the embeddings or just freeze them? There are many existing CNN and RNN based neural classifier for text classification. What if just use any of those off-the-shelf methods on the text embedding without the diffusion process and fine-tune those embedding? This is typically a strong baseline for text classification, but there is no corresponding experimental results in Figure 5. 8) In line 234, how those objective function weights obtained? Are they tuned on any development set? Has the author tried only using a subset of those objectives? It's not clear how important each of those four objectives is. 9) In line 315, the author attributes the result of Table 3 to ""both structure and text information"". However, the fact that the method picks vertex 3 is due to diffusion convolution operator, as explained in line 313. Does the ""structure"" here mean the diffusion convolution operator or the structural embedding?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"4. I read the paper in detail. The fact that their theory does not seem to be applicable to the used model, is not honestly mentioned in the limitations. To the contrary, the vagueness of unspecified 'structural assumptions', that are only given in the appendix, makes this theoretical limitation hard to find. I think the authors underestimate the current use of graph neural networks in industry. They are used widely. As such, some more elaboration on potential negative societal impact of graph neural networks in general could be given.",NIPS_2021_1731,NIPS_2021,"I am not quite convinced by the motivation of the proposed method as a discrete analogue of the continuous Beltrami flow. The “structural assumptions on the diffusivity” a
seem to not be satisfied scaled dot product attention in BLEND. What is the point of all the theoretical motivation if the actual construction violates the assumptions of the theory?
Currently, I find it unclear which aspect of the proposed method makes it perform well. Is it the position that is added as a pre-processing step, the continuousness of the flow, the particular integrator, or the particular flow equation (6)? The ablation analysis in the appendix only partially answers this question. Further experiments could include: augmenting GAT with positional encodings; using BLEND with Euler steps; using GAT with continuous integration.
The method does not seem to get state-of-the-art results on larger sized data sets. The GAT baseline uses more parameters, but would BLEND improve if it used as many? I expect that BLEND has much higher training and inference time than GAT, even with the smaller model, because of the continuous integration. Concrete run-times are not given, so I can’t say for sure.
Further suggestions for improvement:
I’d be very interested in the performance of including channel mixing in the flow, referred to as Onsager diffusion, as currently, the fact that the channels only interact via the attention seems limiting. The same holds for time-dependent diffusivity.
Include (some idea of) the structural assumptions in thm 1 in the main paper.
Include results on BLEND-kNN on ogb-arxiv or explain why this result is missing.
Include some actual run-times of BLEND(-kNN) vs other methods.
In the appendix clarify why (9, suppl mat) is the obvious discrete analogue of (6, suppl mat). I see some notational similarity between (10, suppl mat) and (6, suppl mat), but that looks rather superficial, besides it being one possible generalization of the classic Dirichlet energy. In fact, the paper doesn’t seem to be using this additional generality and only shows the classic example in which ψ ~
is constant. What does this additional ψ
generality add?
Could the authors clarify the step from (8, suppl mat) to (1)? Where does the time come from? Typos:
Eqn 4, x ( x , 0 )
should be x ( v , 0 )
In Table 3, the score for GCN on CiteSeer is bold.
The colouring in Table 3 seems incorrect. The BLEND-kNN performs on par with CGNN.
The eqn under line 48 in the suppl mat would be clearer if parenthesis would be added to indicate that the partial derivative only applies to Z.
Line 153, missing reference Conclusion:
Originality: The work is original. It tries to connect differential geometry to continuous flows in graphs in a way I hadn’t seen before.
Quality: I have doubts about the correctness, as I question whether the presented theory applies to the proposed model. Additionally, important ablations are missing.
Clarity: The paper is clearly written.
Significance: The paper can be significant to all people researching graph neural nets and open exploration into continuous flows on this domain.
Score: 5, marginally below acceptance threshold. If the authors can convince me the theory does apply to their model, I will increase my score.
Confidence: 4. I read the paper in detail.
The fact that their theory does not seem to be applicable to the used model, is not honestly mentioned in the limitations. To the contrary, the vagueness of unspecified 'structural assumptions', that are only given in the appendix, makes this theoretical limitation hard to find.
I think the authors underestimate the current use of graph neural networks in industry. They are used widely. As such, some more elaboration on potential negative societal impact of graph neural networks in general could be given.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
* How hard is it to find examples that illustrate the loss principles clearly like those presented in the paper and the supplement? Weaknesses of the proposed FSR metric specifically:,NIPS_2019_263,NIPS_2019,"--- Weaknesses of the evaluation in general: * 4th loss (active fooling): The concatenation of 4 images into one and the choice of only one pair of classes makes me doubt whether the motivation aligns well with the implementation, so 1) the presentation should be clearer or 2) it should be more clearly shown that it does generalize to the initial intuition about any two objects in the same image. The 2nd option might be accomplished by filtering an existing dataset to create a new one that only contains images with pairs of classes and trying to swap those classes (in the same non-composite image). * I understand how LRP_T works and why it might be a good idea in general, but it seems new. Is it new? How does it relate to prior work? Does the original LRP would work as the basis or target of adversarial attacks? What can we say about the succeptibility of LRP to these attacks based on the LRP_T results? * How hard is it to find examples that illustrate the loss principles clearly like those presented in the paper and the supplement? Weaknesses of the proposed FSR metric specifically: * L195: Why does the norm need to be changed for the center mass version of FSR? * The metric should measure how different the explanations are before and after adversarial manipulation. It does this indirectly by measuring losses that capture similar but more specific intuitions. It would be better to measure the difference in heatmaps before and after explicitly. This could be done using something like the rank correlation metric used in Grad-CAM. I think this would be a clearly superior metric because it would be more direct. * Which 10k images were used to compute FSR? Will the set be released? Philosohpical and presentation weaknesses: * L248: What does ""wrong"" mean here? The paper gets into some of the nuance of this position at L255, but it would be helpful to clarify what is meant by a good/bad/wrong explanation before using those concepts. * L255: Even though this is an interesting argument that forwards the discussion, I'm not sure I really buy it. If this was an attention layer that acted as a bottleneck in the CNN architecuture then I think I'd be forced to buy this argument. As it is, I'm not convinced one way or the other. It seems plausible, but how do you know that the final representation fed to the classifier has no information outside the highlighted area. Furthermore, even if there is a very small amount of attention on relevant parts that might be enough. * The random parameterization sanity check from [25] also changes the model parameters to evaluation visualizations. This particular experiment should be emphasized more because it is the only other case I can think of which considers how explanations change as a function of model parameters (other than considering completely different models). To be clear, the experiment in [25] is different from what is proposed here, I just think it provides interesting contrast to these experiments. The claim here is that the explanations change too much while the claim there is that they don't change enough. Final Justification --- Quality - There are a number of minor weaknesses in the evaluation that together make me unsure about how easy it is to perform this kind of attack and how generalizable the attack is. I think the experiments do clearly establish that the attack is possible. Clarity - The presentation is pretty clear. I didn't have to work hard to understand any of it. Originality - I haven't seen an attack on interpreters via model manipulation before. Significance - This is interesting because it establishes a new way to evaluate models and/or interpreters. The paper is a bit lacking in scientific quality in a number of minor ways, but the other factors clearly make up for that defect.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['1', '1']}",,
"* Just before Appendix D.2. ""For training we used an epsilon-greedy ..."" What does this mean exactly? You have epsilon-greedy exploration on top of the proposed strategy?",NIPS_2016_450,NIPS_2016,". First of all, the experimental results are quite interesting, especially that the algorithm outperforms DQN on Atari. The results on the synthetic experiment are also interesting. I have three main concerns about the paper. 1. There is significant difficulty in reconstructing what is precisely going on. For example, in Figure 1, what exactly is a head? How many layers would it have? What is the ""Frame""? I wish the paper would spend a lot more space explaining how exactly bootstrapped DQN operates (Appendix B cleared up a lot of my queries and I suggest this be moved into the main body). 2. The general approach involves partitioning (with some duplication) the samples between the heads with the idea that some heads will be optimistic and encouraging exploration. I think that's an interesting idea, but the setting where it is used is complicated. It would be useful if this was reduced to (say) a bandit setting without the neural network. The resulting algorithm will partition the data for each arm into K (possibly overlapping) sub-samples and use the empirical estimate from each partition at random in each step. This seems like it could be interesting, but I am worried that the partitioning will mean that a lot of data is essentially discarded when it comes to eliminating arms. Any thoughts on how much data efficiency is lost in simple settings? Can you prove regret guarantees in this setting? 3. The paper does an OK job at describing the experimental setup, but still it is complicated with a lot of engineering going on in the background. This presents two issues. First, it would take months to re-produce these experiments (besides the hardware requirements). Second, with such complicated algorithms it's hard to know what exactly is leading to the improvement. For this reason I find this kind of paper a little unscientific, but maybe this is how things have to be. I wonder, do the authors plan to release their code? Overall I think this is an interesting idea, but the authors have not convinced me that this is a principled approach. The experimental results do look promising, however, and I'm sure there would be interest in this paper at NIPS. I wish the paper was more concrete, and also that code/data/network initialisation can be released. For me it is borderline. Minor comments: * L156-166: I can barely understand this paragraph, although I think I know what you want to say. First of all, there /are/ bandit algorithms that plan to explore. Notably the Gittins strategy, which treats the evolution of the posterior for each arm as a Markov chain. Besides this, the figure is hard to understand. ""Dashed lines indicate that the agent can plan ahead..."" is too vague to be understood concretely. * L176: What is $x$? * L37: Might want to mention that these algorithms follow the sampled policy for awhile. * L81: Please give more details. The state-space is finite? Continuous? What about the actions? In what space does theta lie? I can guess the answers to all these questions, but why not be precise? * Can you say something about the computation required to implement the experiments? How long did the experiments take and on what kind of hardware? * Just before Appendix D.2. ""For training we used an epsilon-greedy ..."" What does this mean exactly? You have epsilon-greedy exploration on top of the proposed strategy?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1) The proposed method can be viewed as a direct combination of GCN and normalizing flow, with the ultimate transformed distribution, which is Gaussian in conventional NF, replaced by Gaussian mixture distribution, encouraging the latent representation to be more clustered. Technically, there is no enough new stuffs here.",ICLR_2023_2698,ICLR_2023,"1) The proposed method can be viewed as a direct combination of GCN and normalizing flow, with the ultimate transformed distribution, which is Gaussian in conventional NF, replaced by Gaussian mixture distribution, encouraging the latent representation to be more clustered. Technically, there is no enough new stuffs here. 2) More Seriously, to ensure the intractability of the normalizing flow after absorbing the graph neural network, the proposed model has to replace the basic operation \sigma(AXW) with the operation \sigma(AX) in the graph neural networks, abandon the feature affine transformation operation, i.e., XW, before passing the intermediate representations to neighboring nodes. Since the W is the main parameters to be learned in GNN, abandoning it means the representation ability of GNN is restricted significantly. The experimental results also show that the proposed model brings very little gains over the old models like GCN and GAT on classification tasks. 3) Without using the feature affine transformation AXW, then the dimension of intermediate hidden representations will always be kept the same as that of input feature since the NF have to maintain the dimension unchanged. Then, if the dimension of input feature is very high, in addition to the complexity issue, the learned feature will be also be very high, which may not be very useful as nowadays we often expect the learned features to be compact. 4) For the experiments, since the paper want to demonstrate the proposed model is able to learn clustering-friendly representations, we expect to directly see how the model performs on clustering task on the clustering performance metric, like accuracy ACC, normalized mutual information NMI etc, rather than the indirect Silhouette criteria, which is not meaningful at all.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
1). Only projection head (CNN layers) are affected but not classification head (FCN layer);,ICLR_2022_2470,ICLR_2022,"Weakness:
The idea is a bit simple -- which in of itself is not a true weakness. ResNet as an idea is not complicated at all. I find it disheartening that the paper did not really tell readers how to construct a white paper in section 3 (if I simply missed it, please let me know). However, the code in the supplementary materials helped. White paper is constructed as follow:
white_paper_gen = torch.ones(args.train_batch, 3, 32, 32)
It offers another way of constructing white paper, which is
white_paper_gen = 255 * np.ones((32, 32, 3), dtype=np.uint8)
white_paper_gen = Image.fromarray(white_paper_gen)
white_paper_gen = transforms.ToTensor()(white_paper_gen)
white_paper_gen = transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))(white_paper_gen)
The code states that either version works similarly and does not affect the performance. I wonder if there are other white papers as well, for example np.zeros((32, 32, 3)) -- most CNN models add explicit bias terms in their CNN kernel. Would a different white paper reveal different bias in the model? I don't think the paper answers this question or discusses it. 2. Section 4 ""Is white paper training harmful to the model?"" -- the evidences do not seem to support the claim. The evidences are 1). Only projection head (CNN layers) are affected but not classification head (FCN layer); 2). Parameter changes are small. None of these constitute as a direct support that the training is not ""harmful"" to the model. This point can simply be illustrated by the experimental results 3. Section 5.1 and 5.2 mainly build the narrative that WPA improves the test performance (generalization performance), but they are indirect evidence to support that WPA does in fact alleviate shortcut learning. Only Section 5.3 and Table 6 directly show whether WPA does what it's designed to do. A suggestion is to discuss the result of Section 5.3 more. 4. It would be interesting to try to explain why WPA works -- with np.ones input, what is the model predicting? Would any input serve as white paper? Figure 2 seems to suggest that Gaussian noise input does not work as well as WPA. Why? Authors spend lot of time showing WPA improves the test performance of the original model, but fails to provide useful insights on how WPA works -- this is particularly important because it can spark future research directions.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1. Regarding the whole framework, which part is vital for using CLIP to guide weakly supervised learning? I think the discussion is necessary (but I didn’t find clear answer in the discussion) and help this paper to be distinguished from the other related work.",ICLR_2023_1418,ICLR_2023,"Weakness: 1. Regarding the whole framework, which part is vital for using CLIP to guide weakly supervised learning? I think the discussion is necessary (but I didn’t find clear answer in the discussion) and help this paper to be distinguished from the other related work. 2. The knowledge bank is based on classes appearing in the full dataset and is defined by the text. Can you explain how the size of the knowledge bank affects performance? After all, when there exists a brunch of interaction classes, I am not sure about the training efficiency and workload. 3. SRC only does not help much with the detection, according to Table 2. It is not very effective and is kind of counterintuitive. I would recommend providing a more detailed explanation or removing the SRC part. 4. When a CLIP model is used, it is always necessary to explain the potential issue of fair comparison. After all, CLIP has seen quite a lot of training data during pretraining, and there is a risk of potential data leakage. As such, explanation is necessary.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The analogy between HOI analysis and Harmonic analysis is interesting at first glance, but the link is quite weak. In the problem contexts, there is only two “basis” (human and object) to form an HOI. The decomposition/integration steps introduced in this paper also do not have a close connection with the Fourier analysis as claimed.",NIPS_2020_420,NIPS_2020,"**Exposition** - I think the paper contains interesting ideas with good empirical results. However, the exposition of the method is not easy to follow and require significant revision. Here are a couple of examples that were unclear. - L6: “coherent HOI.” What does it mean to have “coherent HOI”? What are the incoherent ones? - L8: “transformations between human-object pairs.” The “transformation” is vague. Later in the paper, it turns that this is merely replacing instance-level (human or object) from similar HOI examples. The exposition is unnecessarily complicated. - The analogy between HOI analysis and Harmonic analysis is interesting at first glance, but the link is quite weak. In the problem contexts, there is only two “basis” (human and object) to form an HOI. The decomposition/integration steps introduced in this paper also do not have a close connection with the Fourier analysis as claimed. - On L33, what does the “eigen” structure of HOI mean? - On L51, “IDN can learn to represent the interaction/verb with T_I and T_D.” What does this mean? - On L205, I was not able to follow the concept of Interactive validity. There is no definition of these loss terms and no figures to illustrate this part. - Figure 2: o What does “X” mean? o g_h and g_o are not discussed. Later I found that this is just identity (swapping instance features) - Figure 3: o (a) Please specify the loss terms here. o (b) I know that the f_u^{v_i} is predicted from the concatenated feature f_h and f_o (the integration step). However, for the decomposition step, why not use f_u as input (as discussed in Eqn 1) and predict f_h and f_o? - When using the autoencoder for compressing the features f_h + f_o, isn’t that the encoded features already are “integrated”? How can we “slice” the features to get individual features? **Novelty** - The inter-pair transformation idea has been exploited in [A]. The paper should cite and discuss the differences with respect to [A] (as it was published before the NeurIPS submission). [A] Detecting Human-Object Interactions via Functional Generalization. AAAI 2020 **Method** - The proposed approach seems to require a much larger model size. For example, the method needs two (T_I and T_D) two-layer fully connected networks for *each* verb interaction. This is certainly not scalable and can be slow at test time. For example, for HICO-DET, this requires evaluating the T_I and T_D 117 times. Unfortunately, the paper did not discuss the model size and runtime performance. At least this should be discussed as a limitation. **Evaluation** - In Table 4, which dataset is this conducted on? It seems to me that this is done on the *testing set* of the HICO-DET dataset. The ablation should be done in the validation set without seeing the testing set. This may suggest that all the model tuning may also be conducted on the testing datasets, which may lead to overfitting.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"3. Dynamic precision control during training might only show meaningful performance gains on bit-serial accelerators. However, most existing ML accelerators tend to use bit-parallel fixed-point numbers, this might restrict the implications of the proposed methodology.",NIPS_2020_1016,NIPS_2020,"1. The PFQ algorithm introduced many hyperparameters, and I am curious how the authors chose the parameters \epsilon and \alpha. The authors simply claimed these parameters are determined from the four-stage manual PFQ from Figure 1, and then claim that FracTrain is insensitive to hyperparameters. First, the precision choices of the four stage PFQ in Figure 1 is already arbitrary. Second, I do not think the empirical results can support the claim that FracTrain is insensitive to hyperparameters. I would encourage the authors to have an ablation study of \epsilon and \alpha. I do understand an ablation study of various precision combinations is shown in the appendix, but this might not provide enough insights for users of FracTrain that simply want to know what is the best hyperparameter combination to use. 2. I found the MACs and Energy results reported in the paper needs further explanation. For instance, in Table2, it seems to me MACs cannot be a useful measurement since SBM and FracTrain might use different precisions. Even if the MACs numbers are the same, low-precision operations will surely be more energy efficient. A more useful measurement metric might be bitwise operations. In terms of the Energy reported in this paper, the authors claim it is calculated from an RTL design. However, BitFusion is an inference accelerator, what modifications have you done to the BitFusion RTL to support this training energy estimation? What is the reuse pattern for gradients/activations? 3. Dynamic precision control during training might only show meaningful performance gains on bit-serial accelerators. However, most existing ML accelerators tend to use bit-parallel fixed-point numbers, this might restrict the implications of the proposed methodology. 4. I think this paper has missed a number of citations in the recent advances of dynamic inference methods. Dynamic channel control [1,2] and dynamic precisions [3] have recently been widely explored and these citations are not seen in this paper. [1] Gao, Xitong, et al. ""Dynamic channel pruning: Feature boosting and suppression."" ICLR 2018. [2] Hua, Weizhe, et al. ""Channel gating neural networks."" Advances in Neural Information Processing Systems. 2019. [3] Song, Zhuoran, et al. ""DRQ: Dynamic Region-based Quantization for Deep Neural Network Acceleration."" ISCA 2020","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?",ICLR_2021_2674,ICLR_2021,"Though the training procedure is novel, a part of the algorithm is not well-justified to follow the physics and optics nature of this problem. A few key challenges in depth from defocus are missing, and the results lack a full analysis. See details below:
- the authors leverage multiple datasets, including building their own to train the model. However, different dataset is captured by different cameras, and thus the focusing distance, aperture settings, and native image resolution all affect the circle of confusion, how are those ambiguities taken into consideration during training?
- related to the point above, the paper doesn't describe the pre-processing stage, neither did it mention how the image is passed into the network. Is the native resolution preserved, or is it downsampled?
- According to Held et al ""Using Blur to Affect Perceived Distance and Size"", disparity and defocus can be approximated by a scalar that is related to the aperture and the focus plane distance. In the focal stack synthesis stage, how is the estimated depth map converted to a defocus map to synthesize the blur?
- the paper doesn't describe how is the focal stack synthesized, what's the forward model of using a defocus map and an image to synthesize defocused image? how do you handle the edges where depth discontinuities happen?
- in 3.4, what does “Make the original in-focus region to be more clear” mean? in-focus is defined to be sharpest region an optical system can resolve, how can it be more clear?
- the paper doesn't address handling textureless regions, which is a challenging scenario in depth from defocus. Related to this point, how are the ArUco markers placed? is it random?
- fig 8 shows images with different focusing distance, but it only shows 1m and 5m, which both exist in the training data. How about focusing distance other than those appeared in training? does it generalize well?
- what is the limit of the amount of blur presented in the input that the proposed models would fail? Are there any efforts in testing on smartphone images where the defocus is *just* noticeable by human eyes? how do the model performances differ for different defocus levels?
Minor suggestions
- figure text should be rasterized, and figures should maintain its aspect ratio.
- figure 3 is confusing as if the two nets are drawn to be independent from each other -- CNN layers are represented differently, one has output labeled while the other doesn't. It's not labeled as the notation written in the text so it's hard to reference the figure from the text, or vice versa.
- the results shown in the paper are low-resolution, it'd be helpful to have zoomed in regions of the rendered focal stack or all-in-focus images to inspect the quality.
- the sensor plane notation 's' introduced in 3.1 should be consistent in format with the other notations.
- calling 'hyper-spectral' is confusing. Hyperspectral imaging is defined as the imaging technique that obtains the spectrum for each pixel in the image of a scene.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The authors should also consider defining content and style more broadly as it relates to their specific neural application (e.g., as in Gabbay &Hosehn (2018)) where style is instance-specific(?) and content includes information that can be transferred among groups. More specifically, since their model is not sequential and does not capture the temporal dynamic structure, what do they really mean by ‘style’ represents the ‘movement dynamic’?",NIPS_2021_28,NIPS_2021,"The paper is overall interesting, well-written and makes a valuable contribution. I do, however, have some comments for the authors to consider (which in my mind, are potential limitations of the study):
- Comparison of the proposed unsupervised method with the supervised baseline is not suggestive because of the absence of augmentations in the supervised baseline. The authors should consider reporting performance on the decoding task when the supervised method employed data augmentations as well. - For completeness, the authors should also report how the hyperparameters for the linear decoder were determined. Ideally, I would’ve liked to see error bars for decoding accuracies as well (maybe by bootstrapping training set for the decoder?) - In future, the authors could also consider replacing the acc metric for decoding with better evaluation metrics for circular data, like circular correlation. This would treat the reach direction as a continuous variable (which it is) rather than as a discrete unordered variable (which it theoretically isn’t).
- The authors consider swapping only the block of variables belonging to the `content’ group. What would happen if the reconstruction term in the BlockSwap method swapped both the content and style of the augmented views? Does swapping only the content block necessarily facilitate disentanglement? If the BlockSwap was essential, does the proposed method required knowing the number of latent factors in advance. The authors could discuss these aspects in their conclusion/discussion. - When comparing the proposed SwapVAE against vanilla VAE, the authors should also consider reporting other metrics more commonly employed in VAE evaluation (likelihood etc.) and not just the reconstruction error (which can be trivially minimized). This is important, since the authors mention ‘generating realistic neural activity’ as a significant contribution of their paper. - The authors should also consider defining content and style more broadly as it relates to their specific neural application (e.g., as in Gabbay &Hosehn (2018)) where style is instance-specific(?) and content includes information that can be transferred among groups. More specifically, since their model is not sequential and does not capture the temporal dynamic structure, what do they really mean by ‘style’ represents the ‘movement dynamic’?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. The analysis of vit quantification could be explained in depth: (a) this paper argues that `a direct quantization method leads to the information distortion’ in Line 45. The approach proposed in this paper does not improve this phenomenon either (e.g. 1.2268 in Fig1(b) v.s. 1.3672 in Fig5(b) for Block.3. The variance difference is even larger with the proposed approach). (b) The quantization of MHSA introduces a large loss of precision, which has been found in transformer quantization in the NLP (such as Q-BERT, Q8BERT, BinaryBERT, FullyBinaryBert, etc.) and is not unique to the ViT model.",NIPS_2022_2572,NIPS_2022,"1. The analysis of vit quantification could be explained in depth: (a) this paper argues that `a direct quantization method leads to the information distortion’ in Line 45. The approach proposed in this paper does not improve this phenomenon either (e.g. 1.2268 in Fig1(b) v.s. 1.3672 in Fig5(b) for Block.3. The variance difference is even larger with the proposed approach). (b) The quantization of MHSA introduces a large loss of precision, which has been found in transformer quantization in the NLP (such as Q-BERT, Q8BERT, BinaryBERT, FullyBinaryBert, etc.) and is not unique to the ViT model. 2. Some minor problems: (a) In Fig2, the tilde hat of k is too small. It should be inconsistent with q’s hat. (b) In Equation 9, Q k
might be Q ( k )
to be consistent with Q ( q ) .","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- A main weakness of this work is its technical novelty with respect to spatial transformer networks (STN) and also the missing comparison to the same. The proposed X-transformation seems quite similar to STN, but applied locally in a neighborhood. There are also existing works that propose to apply STN in a local pixel neighborhood. Also, PointNet uses a variant of STN in their network architecture. In this regard, the technical novelty seems limited in this work. Also, there are no empirical or conceptual comparisons to STN in this work, which is important.",NIPS_2018_76,NIPS_2018,"- A main weakness of this work is its technical novelty with respect to spatial transformer networks (STN) and also the missing comparison to the same. The proposed X-transformation seems quite similar to STN, but applied locally in a neighborhood. There are also existing works that propose to apply STN in a local pixel neighborhood. Also, PointNet uses a variant of STN in their network architecture. In this regard, the technical novelty seems limited in this work. Also, there are no empirical or conceptual comparisons to STN in this work, which is important. - There are no ablation studies on network architectures and also no ablation experiments on how the representative points are selected. - The runtime of the proposed network seems slow compared to several recent techniques. Even for just 1K-2K points, the network seem to be taking 0.2-0.3 seconds. How does the runtime scales with more points (say 100K to 1M points)? It would be good if authors also report relative runtime comparisons with existing techniques. Minor corrections: - Line 88: ""lose"" -> ""loss"". - line 135: ""where K"" -> ""and K"". Minor suggestions: - ""PointCNN"" is a very short non-informative title. It would be good to have a more informative title that represents the proposed technique. - In several places: ""firstly"" -> ""first"". - ""D"" is used to represent both dimensionality of points and dilation factor. Better to use different notation to avoid confusion. Review summary: - The proposed technique is sensible and the performance on different benchmarks is impressive. Missing comparisons to established STN technique (with both local and global transformations) makes this short of being a very good paper. After rebuttal and reviewer discussion: - I have the following minor concerns and reviewers only partially addressed them. 1. Explicit comparison with STN: Authors didn't explicitly compare their technique with STN. They compared with PointNet which uses STN. 2. No ablation studies on network architecture. 3. Runtimes are only reported for small point clouds (1024 points) but with bigger batch sizes. How does runtime scale with bigger point clouds? Authors did not provide new experiments to address the above concerns. They promised that a more comprehensive runtime comparison will be provided in the revision. Overall, the author response is not that satisfactory, but the positive aspects of this work make me recommend acceptance assuming that authors would update the paper with the changes promised in the rebuttal. Authors also agreed to change the tile to better reflect this work.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"* L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes?",NIPS_2016_386,NIPS_2016,", however. For of all, there is a lot of sloppy writing, typos and undefined notation. See the long list of minor comments below. A larger concern is that some parts of the proof I could not understand, despite trying quite hard. The authors should focus their response to this review on these technical concerns, which I mark with ** in the minor comments below. Hopefully I am missing something silly. One also has to wonder about the practicality of such algorithms. The main algorithm relies on an estimate of the payoff for the optimal policy, which can be learnt with sufficient precision in a ""short"" initialisation period. Some synthetic experiments might shed some light on how long the horizon needs to be before any real learning occurs. A final note. The paper is over length. Up to the two pages of references it is 10 pages, but only 9 are allowed. The appendix should have been submitted as supplementary material and the reference list cut down. Despite the weaknesses I am quite positive about this paper, although it could certainly use quite a lot of polishing. I will raise my score once the ** points are addressed in the rebuttal. Minor comments: * L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0? * L177: ""(OCO )"" -> ""(OCO)"" and similar things elsewhere * L176: You might want to mention that the learner observes the whole concave function (full information setting) * L223: I would prefer to see a constant here. What does the O(.) really mean here? * L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards. * L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes? * The algorithm only stops /after/ it has exhausted its budget. Don't you need to stop just before? (the regret is only trivially affected, so this isn't too important). * L213: \tilde \mu is undefined. I guess you mean \tilde \mu_t, but that is also not defined except in Corollary 1, where it just given as some point in the confidence ellipsoid in round t. The result holds for all points in the ellipsoid uniformly with time, so maybe just write that, or at least clarify somehow. ** L435: I do not see how this follows from Corollary 2 (I guess you meant part 1, please say so). So first of all mu_t(a_t) is not defined. Did you mean tilde mu_t(a_t)? But still I don't understand. pi^*(X_t) is (possibly random) optimal static strategy while \tilde \mu_t(a_t) is the optimistic mu for action a_t, which may not be optimistic for pi^*(X_t)? I have similar concerns about the claim on the use of budget as well. * L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else. * L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee? * L200: ""for every arm a"" implies there is a single optimistic parameter, but of course it depends on a ** L303: Why not choose T_0 = m Sqrt(T)? Then the condition becomes B > Sqrt(m) T^(3/4), which improves slightly on what you give. * It would be nice to have more interpretation of theta (I hope I got it right), since this is the most novel component of the proof/algorithm.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
8. Eq. 12 is confusing. Where does the reward come from at each trial? Is one of the r_i taken from Eq. 11? Explaining the network model in Sec. 4.2 with equations would greatly improve clarity. [1] https://www.sciencedirect.com/science/article/pii/S0893608019301741 [2] https://www.frontiersin.org/articles/10.3389/fnins.2018.00608/full [3] https://proceedings.neurips.cc/paper/2020/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html,ICLR_2022_1824,ICLR_2022,".
However, I struggle to see the novelty in the author’s approach: spikes and local connections alone have been tried many times (Tab.3 and also [1]). Training the output layer (rather than the whole network) with an RL-based rule is somewhat new, but I find this approach unreasonable for the following reasons:
The last layer is usually trained with SGD + cross-entropy to assess the quality of representations built by previous layers. So the performance of R-STDP in any case would be limited by the representations it gets from earlier layers, which are arguably more important for training networks. (This paper tries to do that too with SVM, however.)
There’s no reason for this approach to scale beyond MNIST, as the hardest part of training is done by a simple STDP rule. Maybe some layer-wise R-STDP can be a valid approach (akin to [2]), or a backprop-like RL error [3].
As a side point, I couldn’t run the code in colab (with PyTorch 1.8 and Bindsnet installed). Running image_classification_experiment.py gives record() got an unexpected keyword argument 'n_labels'. Disabling recording makes it go away, but then there’s a shape mismatch. And if you make the running time 256 instead of 256*3 to fix it, the accuracy doesn't improve at all.
The main result of the paper -- MNIST accuracy (Tab. 3) -- is very weak. It’s pretty straightforward to achieve 95%+ test accuracy with spiking networks, local connections and unsupervised pre-training (using SGD for predictions) [1] (Tab. 2 there). Therefore, even ignoring the potential weakness of the R-STDP in the final layer and concentrating on the STDP + SVM result (87.5%), it is clear that the network does not learn useful representations. There are multiple potential reasons for that:
The STDP in the first layer is at fault, which would be a bit surprising given the clarity of filters in Fig.3A. As a sanity check, you can train an SVM on the hidden layer without any pre-training, and see if it improves the results.
The decoding scheme is ill-fitted for SVM. I’d suggest using SGD with cross-entropy like in [1] and probably many papers in Tab. 3 of your paper. If you see a large improvement, then R-STDP needs some rethinking to properly make use of the pre-trained layer.
Local connections make it harder. Some works in Tab. 2 of [1] successfully use LC layers, however. I would test performance with the same architecture, but using convolutions. Another thing I noticed is really large filters -- 15x15 filters for a 28x28 image are not too far from a fully connected layer.
When the winner-take-all in the LC layer makes a mistake by activating the wrong “digit” (and the filter weights do look like digits in Fig.3A), the readout layer can’t fix it.
Finding the root of poor performance would improve the paper, but the overall approach (hidden layer STDP + last layer R-STDP) is still unlikely to scale to harder problems and deeper networks. Recommendation
Due to limited novelty and unsatisfying results, I would recommend rejecting the paper.
Minor comments
It is noteworthy that in many learning problems, we do not have direct access to the explicit label of the data. Consequently, we may need to abandon gradient-based methods, and utilize reinforcement and reward-modulated learning rules
Gradient-based doesn’t mean it uses labels. See VAEs, self-supervised learning, etc. that all use backprop.
The proposed network is … the first locally-connected SNN with a hidden layer
That’s not true. See [1] and references therein.
Various problems:
Eq. 3 has extra underscores. It has to be dg/dt.
P_ij^+- in Eqs. 7-8 only need one index, j for Eq. 7 and i for Eq. 8.
Eq. 12 is confusing. Where does the reward come from at each trial? Is one of the r_i taken from Eq. 11?
Explaining the network model in Sec. 4.2 with equations would greatly improve clarity.
[1] https://www.sciencedirect.com/science/article/pii/S0893608019301741
[2] https://www.frontiersin.org/articles/10.3389/fnins.2018.00608/full
[3] https://proceedings.neurips.cc/paper/2020/hash/1abb1e1ea5f481b589da52303b091cbb-Abstract.html","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"4. Row 821 in Supp. Page 31: “Fig.7” should be “Fig.12”. Last but not least, each theorem and corollary appearing in the main paper should be attached to its corresponding proof link to make it easy for the reader to follow. The primary concerns are motivation, methodology soundness, and experiment persuasion. I believe this is a qualified paper with good novelty, clear theoretical guarantees, and convincing empirical results.",NIPS_2022_2635,NIPS_2022,"Weakness: The writing of this paper is roughly good but could be further improved. For example, there are a few typos and mistakes in grammar: 1. Row 236 in Page 4, “…show its superiority.”: I think this sentence should be polished.
2. Row 495 in Supp. Page 15: “Hard” should be “hard”. 3. Row 757 in Supp. Page 29: “…training/validation/test” should be “…training/validation/test sets”. 4. Row 821 in Supp. Page 31: “Fig.7” should be “Fig.12”. Last but not least, each theorem and corollary appearing in the main paper should be attached to its corresponding proof link to make it easy for the reader to follow.
The primary concerns are motivation, methodology soundness, and experiment persuasion. I believe this is a qualified paper with good novelty, clear theoretical guarantees, and convincing empirical results.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?",ACL_2017_818_review,ACL_2017,"1) Many aspects of the approach need to be clarified (see detailed comments below). What worries me the most is that I did not understand how the approach makes knowledge about objects interact with knowledge about verbs such that it allows us to overcome reporting bias. The paper gets very quickly into highly technical details, without clearly explaining the overall approach and why it is a good idea.
2) The experiments and the discussion need to be finished. In particular, there is no discussion of the results of one of the two tasks tackled (lower half of Table 2), and there is one obvious experiment missing: Variant B of the authors' model gives much better results on the first task than Variant A, but for the second task only Variant A is tested -- and indeed it doesn't improve over the baseline. - General Discussion: The paper needs quite a bit of work before it is ready for publication. - Detailed comments: 026 five dimensions, not six Figure 1, caption: ""implies physical relations"": how do you know which physical relations it implies?
Figure 1 and 113-114: what you are trying to do, it looks to me, is essentially to extract lexical entailments (as defined in formal semantics; see e.g. Dowty 1991) for verbs. Could you please explicit link to that literature?
Dowty, David. "" Thematic proto-roles and argument selection."" Language (1991): 547-619.
135 around here you should explain the key insight of your approach: why and how does doing joint inference over these two pieces of information help overcome reporting bias?
141 ""values"" ==> ""value""?
143 please also consider work on multimodal distributional semantics, here and/or in the related work section. The following two papers are particularly related to your goals: Bruni, Elia, et al. ""Distributional semantics in technicolor."" Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-Volume 1. Association for Computational Linguistics, 2012.
Silberer, Carina, Vittorio Ferrari, and Mirella Lapata. "" Models of Semantic Representation with Visual Attributes."" ACL (1). 2013.
146 please clarify that your contribution is the specific task and approach -- commonsense knowledge extraction from language is long-standing task.
152 it is not clear what ""grounded"" means at this point Section 2.1: why these dimensions, and how did you choose them?
177 explain terms ""pre-condition"" and ""post-condition"", and how they are relevant here 197-198 an example of the full distribution for an item (obtained by the model, or crowd-sourced, or ""ideal"") would help.
Figure 2. I don't really see the ""x is slower than y"" part: it seems to me like this is related to the distinction, in formal semantics, between stage-level vs. individual-level predicates: when a person throws a ball, the ball is faster than the person (stage-level) but it's not true in general that balls are faster than people (individual-level).
I guess this is related to the pre-condition vs. post-condition issue. Please spell out the type of information that you want to extract.
248 ""Above definition"": determiner missing Section 3 ""Action verbs"": Which 50 classes do you pick, and you do you choose them? Are the verbs that you pick all explicitly tagged as action verbs by Levin? 306ff What are ""action frames""? How do you pick them?
326 How do you know whether the frame is under- or over-generating?
Table 1: are the partitions made by frame, by verb, or how? That is, do you reuse verbs or frames across partitions? Also, proportions are given for 2 cases (2/3 and 3/3 agreement), whereas counts are only given for one case; which?
336 ""with... PMI"": something missing (threshold?)
371 did you do this partitions randomly?
376 ""rate *the* general relationship"" 378 ""knowledge dimension we choose"": ? ( how do you choose which dimensions you will annotate for each frame?)
Section 4 What is a factor graph? Please give enough background on factor graphs for a CL audience to be able to follow your approach. What are substrates, and what is the role of factors? How is the factor graph different from a standard graph?
More generally, at the beginning of section 4 you should give a higher level description of how your model works and why it is a good idea.
420 ""both classes of knowledge"": antecedent missing.
421 ""object first type"" 445 so far you have been only talking about object pairs and verbs, and suddenly selectional preference factors pop in. They seem to be a crucial part of your model -- introduce earlier? In any case, I didn't understand their role.
461 ""also""?
471 where do you get verb-level similarities from?
Figure 3: I find the figure totally unintelligible. Maybe if the text was clearer it would be interpretable, but maybe you can think whether you can find a way to convey your model a bit more intuitively. Also, make sure that it is readable in black-and-white, as per ACL submission instructions.
598 define term ""message"" and its role in the factor graph.
621 why do you need a ""soft 1"" instead of a hard 1?
647ff you need to provide more details about the EMB-MAXENT classifier (how did you train it, what was the input data, how was it encoded), and also explain why it is an appropriate baseline.
654 ""more skimp seed knowledge"": ?
659 here and in 681, problem with table reference (should be Table 2). 664ff I like the thought but I'm not sure the example is the right one: in what sense is the entity larger than the revolution? Also, ""larger"" is not the same as ""stronger"".
681 as mentioned above, you should discuss the results for the task of inferring knowledge on objects, and also include results for model (B) (incidentally, it would be better if you used the same terminology for the model in Tables 1 and 2) 778 ""latent in verbs"": why don't you mention objects here?
781 ""both tasks"": antecedent missing The references should be checked for format, e.g. Grice, Sorower et al for capitalization, the verbnet reference for bibliographic details.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3 Minor Issues: Ln 32 on Page 1, ‘Empiically’ should be ‘Empirically’",mhCNUP4Udw,ICLR_2025,"1 The motivation for incorporating vision modality into MPNNs for link prediction should be better clarified and discussed. Why is this design effective? Any theoretical evidence? Maybe a dedicated section for this discussion could be valuable.
2 The counterpart methods used for experimental comparison seem not SOTA enough. The authors should compare some 2024 SOTAs.
3 Minor Issues: Ln 32 on Page 1, ‘Empiically’ should be ‘Empirically’","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. The proposed invariant learning module (Sec. 4.2) focuses on mask selection and raw-level features. The former framework (Line 167-174, Sec. 4) seems not limited to raw-level selection. There is also a discussion about representation learning in the appendix. I think the feature selection, presented in Section 4.2, could be further improved, with consideration of representation learning.",NIPS_2022_1667,NIPS_2022,"1. The proposed invariant learning module (Sec. 4.2) focuses on mask selection and raw-level features. The former framework (Line 167-174, Sec. 4) seems not limited to raw-level selection. There is also a discussion about representation learning in the appendix. I think the feature selection, presented in Section 4.2, could be further improved, with consideration of representation learning. 2. There are two interactive modules in the proposed RA2. Compared to previous active adaptation methods, which are designed on a specific metric, it introduces more computation processes. How about the complexity compared with previous methods? 3. Illustration: The text in Figures 2, and 4 is too small. It should be adjusted to the same size as Figures 1, and 3.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- some details are missing. For example, how to design the rewards is not fully understandable.",NIPS_2018_591,NIPS_2018,"Weakness: - some details are missing. For example, how to design the rewards is not fully understandable. - some model settings are arbitrarily set and are not well tested. For example, what is the sensitivity of the model performance w.r.t. the number of layers used in GCN for both the generator and discriminator?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '5', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '3', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1). Second, there are other parameters that can affect the performance (e.g., L and L_max) of the proposed approach.",ICLR_2023_4599,ICLR_2023,"Lack of clarity. The paper lacks important information to reproduce the results:
Overall, the paper lacks a clear high-level explanation of the proposed method. In particular, I think Fig. 2 is very hard to parse and fails to communicate the intuition or high-level idea of the proposed method. The section b) of Fig. 2 is quite convoluted and the text lacks details about how to parse the image. It is unclear to me how the features shown in Fig. 2 are extracted; what the positional embedding used is; and a justification of the architecture used for the “Surface Extractor” in the Figure. From the “Network Architecture” paragraph it seems like the proposed approach is just putting existing components together; this in my opinion decreases the novelty of the approach.
What is the loss used in Eq. 7? I could not find any no discussion about it.
The estimation refinement process presented in Sec. 3.2 lacks details:
The update step shown in Eq. 9 does not preserve the properties of a rotation matrix. It is unclear from Eq. 8 how the optimization problem ensures that the rotation estimates still belong to the SO(3) group. This is a crucial aspect since the paper states that it aims at refining a pose, and the formulation does not seem to be that solid.
The method lacks robustness. This is because the formulation assumes that the pre-trained SDF function is perfect. However, this may not be the case and the estimates can be severely affected.
The proposed method requires a pre-trained SDF function for every object. I think this is not scalable as it requires training several networks, increasing time and computational resources.
Insufficient experiments:
The paper mainly claims that the proposed method is faster than ICP in the abstract and Table 1. Unfortunately, I don’t see a more complete experiment backing this up besides Table 1. In principle, comparing both methods is not a fair comparison because the proposed method uses GPUs and requires an SDF network for each object. Thus, the training time of each SDF network for each object is discounted in Table 1.
The ablation study is quite limited. It is unclear an optimal number of iterations (or the value of n in Algo. 1). Second, there are other parameters that can affect the performance (e.g., L and L_max) of the proposed approach.","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,
"* The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).",NIPS_2017_434,NIPS_2017,"---
This paper is very clean, so I mainly have nits to pick and suggestions for material that would be interesting to see. In roughly decreasing order of importance:
1. A seemingly important novel feature of the model is the use of multiple INs at different speeds in the dynamics predictor. This design choice is not
ablated. How important is the added complexity? Will one IN do?
2. Section 4.2: To what extent should long term rollouts be predictable? After a certain amount of time it seems MSE becomes meaningless because too many small errors have accumulated. This is a subtle point that could mislead readers who see relatively large MSEs in figure 4, so perhaps a discussion should be added in section 4.2.
3. The images used in this paper sample have randomly sampled CIFAR images as backgrounds to make the task harder.
While more difficult tasks are more interesting modulo all other factors of interest, this choice is not well motivated.
Why is this particular dimension of difficulty interesting?
4. line 232: This hypothesis could be specified a bit more clearly. How do noisy rollouts contribute to lower rollout error?
5. Are the learned object state embeddings interpretable in any way before decoding?
6. It may be beneficial to spend more time discussing model limitations and other dimensions of generalization. Some suggestions:
* The number of entities is fixed and it's not clear how to generalize a model to different numbers of entities (e.g., as shown in figure 3 of INs).
* How many different kinds of physical interaction can be in one simulation?
* How sensitive is the visual encoder to shorter/longer sequence lengths? Does the model deal well with different frame rates?
Preliminary Evaluation ---
Clear accept. The only thing which I feel is really missing is the first point in the weaknesses section, but its lack would not merit rejection.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.",ARR_2022_143_review,ARR_2022,"1. [ Double edge point] It's an incremental improvement to K-NN based MT approach, little novelty but large engineering and execution effort, backed by good experimental design. This weakness is a little nitpicking esp when I personally execution (replicable) beats idea (novelty); but if there's no code release is produced after the revision process, then this weakness stands given the next.
2. Replicability of method is not clear, there's no indication that the code will be released.
- I might have missed the specific in the paper; hopefully, I get CKMT = ""Compact-network K-nearest-neighbor MT"" correct. If it is, it'll be good to have the the abbreviation explicit in its full form somewhere in the paper. Otherwise some clarification on the abbreviation would be good. Same for PCKMT = ""Pruned CKMT"", hope I get that right too.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- The selling point for ML-based emulators of climate model parametrizations is often their computational cheapness. Thus, the runtime of Prithvi WxC should be discussed. Given the large parameter count of Prithvi WxC it might be important to note its runtime as a limitation for these kinds of applications.",xCFdAN5DY3,ICLR_2025,"1. The paper falls short of establishing a compelling case for Prithvi WxC as a foundation model for weather or climate. The practical significance and advantages of this approach remain inadequately demonstrated:
a.) While foundation models typically excel at zero-shot performance and data-efficient fine-tuning across diverse tasks, the evidence presented for Prithvi WxC's capabilities in these areas is not convincing. Baselines for the non-forecasting experiments are either very weak (interpolation-based downscaling) or non-existent (gravity wave experiments). Some highly relevant and simple baselines are:
- How much worse(?) does Prithvi WxC perform on these tasks if you omit the pre-training stage (i.e. initialize with random weights instead of the frozen pre-trained ones, and train all parameters jointly from scratch on the tasks)?
- How about completely removing the pre-trained transformer backbone (i.e. removing the Prithvi WxC block from Figures 12 & 13)?
- For the latter, it would be also good to run an experiment where you replace the pre-trained Prithvi WxC backbone with some ""lightweight"" blocks (e.g. a (deeper) U-Net), trained in a task-specific way from scratch, to account for the huge difference in parameter counts if you completely remove Prithvi WxC.
These ablations would immensely help in understanding how useful the pre-training stage is for these downstream applications (e.g. does using pre-trained Prithvi WxC improve performance over such simple baselines? Is it more data-efficient?). Besides, otherwise, it is hard to see evidence for the claim in the conclusion that *""Instead of building task-specific ML-models from scratch, these pretrained encoders can be used to develop more precise data-driven models of atmospheric processes""*.
b.) No ablations are included. I understand that training such a huge model is expensive but having a few ablations would have been very appreciated (perhaps, with a smaller-scale version of the model). For example:
- How crucial is it to predict climatology-normalized targets as opposed to normal per-variable means/stds?
- What's the forecasting performance of Prithvi WxC after the first pre-training phase?
- How important is local vs global masking? What about the masking rates?
- What's the line of thought behind randomizing the distance between input timesteps? Can the model only use one input timestep? I presume this is possible by masking the corresponding snapshot by 100%, but no experiments with this setting are shown.
c.) The weather forecasting results seem lukewarm, albeit it is hard to judge because the comparison is not apples-to-apples.
- Prithvi WxC is trained and evaluated on Merra-2. The baselines are evaluated on ERA5. These reanalysis datasets have different spatial resolutions. The evaluation years seem to be different too (correct me if I'm wrong). It would help to fix this mismatch. For example, given the foundational nature of Prithvi WxC... why not fine-tune it on ERA5 directly? Showing that it can be competitive to these baselines in an apples-to-apples comparison would be a very strong result.
- Based on the mismatched comparison, Prithvi WxC seems to be competitive on 6h to 12h forecasts but it's quite notable that its performance implodes compared to the baselines for longer lead times. It is very unclear why. I wouldn't necessarily expect this version of Prithvi WxC to be state-of-the-art, but the performance does seem underwhelming. Especially given that the authors did ""several things"" to tune these results (i.e. a second forecasting-specific pre-training stage and autoregressive rollout fine-tuning).
- The hurricane evaluation includes hurricanes from 2017 to 2023. This seems to overlap with the training data period (up to 2019).
- Either Figure 6 or its analysis in the main body of the text (lines 251-253) is wrong because I see all of the three models do best on exactly one of the three RMSE figures.
- For the hurricane forecasting experiments, I would appreciate a comparison to the state-of-the-art models included in the weather forecasting experiments (e.g. GraphCast) which have shown to be better than FourcastNet.
d.) The downscaling problem setup is artificial. Downscaling coarsened of existing reanalysis/model outputs is not of much use in practice. A realistic and important downscaling application, as discussed in the Appendix, would be to downscale coarse-resolution model outputs to high-resolution outputs (either of a different model, observations, or the same model run at higher resolution).
e.) The climate model parameterization experiments should be more carefully interpreted.
- The model predicts outputs that are normalized by the 1980-2019 climatology. Unfortunately, decadal or centennial simulations of the future under a changing climate are inherently a non-stationary problem. It is highly unclear if Prithvi WxC would remain stable, let alone effective, under this highly relevant use case. This is particularly so as the in-the-loop (coupled to a running climate model) stability of ML-based climate model parameterizations is a well-known issue.
- The selling point for ML-based emulators of climate model parametrizations is often their computational cheapness. Thus, the runtime of Prithvi WxC should be discussed. Given the large parameter count of Prithvi WxC it might be important to note its runtime as a limitation for these kinds of applications.
- Line 461 claims that Prithvi WxC ""outperforms"" task-specific baselines but no baselines whatsoever are included in the manuscript for this experiment.
- Are the inputs a global map? I am not familiar with gravity waves, but I believe that most physics parameterizations in climate models are modeled column-wise (i.e. across atmospheric height but ignoring lat/lon interactions). This is surely a simplification of these parameterizations, but it seems to indicate that they're highly local problems. What's the motivation for using global context then?
- The end of the section should be worded more carefully, clearly stating the aforementioned limitations.
f.) No scaling experiments are included. Thus, it is unclear how important its 2.3 billion parameter size is, how well the model scales, and how its size impacts performance on the downstream applications. Besides, vision and language models are usually released with multiple model sizes that cover different use cases (e.g. balancing inference speed with accuracy). It would be really useful to get these (and carefully compare them) for Prithvi WxC.
2. Related work is insufficiently discussed. Please include an explicit section discussing it, focusing on:
- Carefully comparing similarities/differences to existing weather foundation models (e.g. architectures, pre-training objectives, downstream applications etc.). Besides, ClimaX is not properly discussed in the paper. Given that it's also a transformer-based foundation model, validated on forecasting, downscaling, and climate emulation, it is very important to include it in the comparison.
- Similarly, please discuss how exactly the masking technique in this paper relates to the ones proposed in Vandal et al. and McNally et al..
- Carefully discuss how the architecture is derived from Hiera and/or MaxViT (and other papers of which components were derived, if any).
3. While the authors transparently discuss some issues/limitations with their experiments (e.g. the evaluation data mismatches), it would be nice to also include an explicit paragraph or section on this (and include aforementioned things like the issues with the climate model parameterization experiments). Minor:
- Can you properly discuss, and include a reference to, what a Swin-shift is?
- Similarly, for the ""pixel shuffle layers""
- Line 39: Pangu -> Pangu-Weather
- Line 48: Nowcasting should be lower-case
- Equation 1: Consider reformulating this as an objective/loss function.
- Also Eq. 1: What is $\hat{X}_t$? What is $\sigma_C$?
- Line 93: $\sigma^2_C = \sigma^2_C(X_t - C_t)$ doesn't make sense to me.
- Line 104: *"" same 20 year period that we used for pretraining.""* .... Do you mean 40 year period? If not, which 20-year period from the 40-year training period did you use?
- Line 157: Multiple symbols are undefined (e.g. $V_S$).
- Line 169: It's not entirely clear what ""alternates"" means in this context.
- Line 429: ""baseline""... do you mean Prithvi WxC?
- Line 507: ""improved""... improved compared to what?
- Figure 12: Do you mean 'downscale' on the right ""upscale"" block?
- Sections D. 2.3 and D.2.4 in the appendix are literal copies of the corresponding paragraphs on pages 8 and 9. Please remove.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. The novelty of the idea is not enough. In addition to the limitations pointed out above, both new metric and method are relatively straightforward.",NIPS_2022_2182,NIPS_2022,"Weakness: 1. Contribution is not convincing. They argue that the traditional adaptive filterbank uses a scalar weight shared by all nodes, and their proposed method learns different weights for different nodes. However, in my opinion, FAGCN can do the same thing. 2. There is a gap between the proposed metric and method. Based on post-aggregation node similarity, they propose an aggregation similarity metric. However, the final 3-channel filterbank has nothing to do with the above metric. 3. The novelty of the idea is not enough. In addition to the limitations pointed out above, both new metric and method are relatively straightforward. 4. The improvement in Table 4 does not seem statistically significant because of high variance. 5. There is a problem with the typesetting of the paper.
In addition to the limitations mentioned in the paper, the intrinsic relationship between the proposed metric and method should be taken into consideration. No potential negative societal impact.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
2) The framing of the paper seems to oversell the method in a way that makes the contribution less clear.,ICLR_2021_394,ICLR_2021,"which lead me to recommend against acceptance. In no particular order:
The crucial ""unseen is forbidden"" hypothesis is vague and seems to be a bit of a strawman. 2) The framing of the paper seems to oversell the method in a way that makes the contribution less clear. 3) The writing is not very clear. 4) The experiments seem to be only proof-of-concept in scenarios where the method is designed to work.
The method seems to incur an exponential cost, but this is not discussed. Elaborating:
The authors claim that, because DNN behavior is undefined on unseen datapoints, the ""unseen-is-forbidden learning hypothesis is currently preventing neural networks from assuming symmetric extrapolations without evidence."" This claim is stated in various forms several times, but never made very precise, and it is crucial in motivating the authors' approach. Roughly, I take the authors to be claiming that (i) the correct way to ""extrapolate"" is to assume that: transformations that were not observed to change the target distribution should be assumed to NOT change the target distribution, (ii) DNNs will not extrapolate in this way by default, and must be explicitly designed to do so. These claims (or whatever the authors actually mean) need(s) to be stated explicitly, and with appropriate modesty. After all, both (i) and (ii) seem contentious. The claim about an ""economical data generating process"" supports (i), but is itself somewhat vague and dubious, and should be discussed in the introduction as motivation for (i).
The authors claim that their method can discover invariances without any data supporting them. And their abstract claims: ""Any invariance to transformation groups is mandatory even without evidence, unless the learner deems it inconsistent with the training data."" But in reality, the authors specify a small number of possible invariances which the method selects among (in a soft way). And the data is used to guide this selection process. So in reality, the designer is in charge of specifying a (restricted) set of (possible) invariances. So like previous works on enforcing invariances, it places a burden on the designer to identify plausible invariances. Overall, I found the framing in the work to be ""the model discovers invariances by itself without any data!"" whereas a more neutral version would be ""instead of enforcing a set of invariances, we propose a set of possible invariances, and assume that any input transformations that are not observed to affect the label should be enforced""
Besides the above issues (vagueness of ""unseen-is-forbidden"" and related discussion (1), overselling (2)), there were several other issues of clarity. The paper is not poorly written overall, but is much harder to read and understand than it needs to be. Some specific issues are:
The results in Section 4 are presented with insufficient context or intuition. Theorems are stated without any proof intuition and should reference proofs in the appendix. The intuition for the penalty arrived at (eqn13) is unclear.
The flow is sometimes unclear. For instance, ""Learning CG-invariant representations without knowledge of G_I. "" should be a subsection, not a (latex) paragraph, and should explain what the point of the subsection is before diving in. The authors seem to be using (latex) paragraphs (i.e. beginning with bolded phrases) as subsections and paragraphs beginning with italicized phrases as (latex) paragraphs. I suspect the paper was edited to fit into 8 pages without removing sufficient content. This impedes the flow and sacrifices clarity.
I think a graph showing the data generating process would be much clearer than the current explanations (e.g. eqn4/5) - it is unclear what equation 7 is saying... the text above makes it seem like a definition of a goal, but the following paragraph treats it as an assertion that the goal is possible to achieve. ...Overall, I recommend stripping out some of the mathematical details and using more words and diagrams in the main text to describe the underlying issues/motivations/methods. The overall story should be made clearer (e.g. by addressing (1) and (2)), and more space should be devoted to linking each part of the paper into the overall story.
The experiments are synthetic tasks where the correct invariance group is included in the set of invariances being searched over. I don't think that showing that this method can bring some benefits on a real task is an absolute requirement, given the novelty of the approach. But without more meaningful results, the paper is held to a much higher standard. Even for synthetic experiments, these are rather weak; for instance, it would be interesting to see whether/how the method degrades when we consider much larger sets of possible invariances.
It seems like the method might require including a set of parameters for each of the possible 2^m invariances. Is this in fact the case? If not, why not? If so, it should be discussed as a limitation. Suggestions/Questions:
In Section 4 paragraph 1, are G-invariance and G_I-invariance used interchangeably? This was confusing.
say what I and D are as soon as they are introduced (top of page 4).
Typo: ""a somewhat a""
Why a ""nonpolynomial"" activation function?
The definition of ""almost surely"" at the bottom of page 4 is not correct (it is possible to sample probability 0 events), and also it should say that samples of Gamma(X^(obs)/(cf)) (not X^(obs)/(cf)) are equal with probability 1 (these are not the same statement!).
""level of invariance"" and ""non-extrapolated validation accuracy"", and several other phrases are not defined and should probably be replaced by something more clear and explicit.
It seems like you might need to assume that that different x^(hid) can't be used to generate the same x^(obs) or x^(cf). If so, this should be explicit.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"3. The model description could be improved, e.g., the generative process is in detail but presenting such process in separate steps should be better for understanding, too many symbols and a notation table could be better.",NIPS_2020_309,NIPS_2020,"1. The motivation is conceptually described, and an example could help reader understand how the hierarchical structure benefits the document representation. 2. A standalone literature review part could be better. 3. The model description could be improved, e.g., the generative process is in detail but presenting such process in separate steps should be better for understanding, too many symbols and a notation table could be better. 4. The evaluation task only contains text classification and more tasks should be included. Besides, the paper does not provide enough details to reproduce the results (the demo code is not enough, some suggestions/guidance about the model setting for different types of documents could be more helpful).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.",ARR_2022_18_review,ARR_2022,"1. The exposition becomes very dense at times leading to reduced clarity of explanation. This could be improved.
2. No details on the. multi-task learning mentioned in Section 4.4 are available.
3. When generating paraphrases for the training data, it is unclear how different the paraphrases are from the original sentences. This crucially impacts the subsequent steps because the model will greatly rely on the quality of these paraphrases. If the difference between the paraphrases and the original sentence is not large enough, the quality of the final training data will be low and as a result of the discarding process very few pairs will be added into the new training data.
4. Again, using style vector differences for control also relies heavily on the style diversity of paraphrases. If the style of the paraphrases is similar to or the same as the original sentences, it will be very difficulty for the model to learn a good style extractor and the whole model will default to a paraphrase model. Examples of the generated paraphrases in the training data could have been presented in addition to some intermediate evaluations to confirm the quality of the intermediate stages.
5. The method of addressing the issue of the lack of translation data doesn't contribute to the technical novelty and should not be considered as a modeling fix.
6. Again, a quantitative evaluation of the degree of word overlap between the input and output could will strengthen the results showing the extent of the copying issue.
7. The combination of the individual metrics into one score (AGG; section 5.5) seems to conflate different scales of the different components. This can result in differences that are not comparable. Thus, it is unclear how the differences in AGG compare across systems. For example, comparing two instances, suppose instance 1 has A= 1, S = 0.8 and L =1, and instance 2 has A=0.9, S = 0.7 and L = 1. Clearly the instances seem alike with small changes in A and S. However, taking their composites, instance 1 has AGG=1 and instance 2 has AGG = 0.63 exaggerating the differences. Seeing in this light, the results in table 1 do not convey anything significant.
8. Table 4 shows human evaluation on code-mixing addition and explains that DIFFUR-MLT+BT performs best (AGG), giving high style accuracy (ACC) without loss in similarity (SIM). However, we do see that SIM values are very low for DIFFUR- ML, BT. What are we missing here?
9. In Figure 4, the analysis on formality transfer seems limited without showing how it is applicable to the other languages studied. Even in Hindi, to what extent is the degree of formality and the use of Persian/Sanskrit forms maintained for Hindi? What does it look like for the other languages?
See comments/questions in the summary of weaknesses for ways to improve the paper.
A few typos to be corrected: Line 491 ""help improve"" Line 495: ""performance of across"" Line 496: ""model fail ...since they"" Figure 1, example for \lambda = 1.5 nA -> na (short vowel)","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '4', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation. The paper is well organized and easy to follow. Figure 2 is a bit cluttered and the ""bold"" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better.",ARR_2022_252_review,ARR_2022,"- The proposed approach is not fully automatic, and still requires human annotations for identifying rationales and correcting errors from the static semi-factual generation phase. While this annotation effort could be less significant that other data augmentation methods, it still presents a significant cost overhead.
- It is not clear how to generalize this approach to other NLP tasks aside from sentiment analysis and text classification. For instance, it is not clear how to generalize this approach to other sequence-to-sequence tasks like machine translation.
- Identifying rationales is not a simple problem, specifically for more complicated NLP tasks like machine translation.
The paper is well organized and easy to follow. Figure 2 is a bit cluttered and the ""bold"" text is hard to see, perhaps another color or a bigger font could help in highlighting the human identified rationales better.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. I believe the Flipped-QA is a general framework for various generative VideoQA models. However, the authors only apply this framework to LLM-based models. It would be better to further verify the effectiveness and universality to non-LLM-based models like HiTeA and InternVideo.",jhdVt7rC8k,EMNLP_2023,"I don’t find significant flaws in this paper. There are some minor suggestions:
1. The VideoQA benchmarks in the paper are all choice-based. It would be better to choose some generation-based VideoQA datasets like ActivityNet-QA to increase the diversity.
2. I believe the Flipped-QA is a general framework for various generative VideoQA models. However, the authors only apply this framework to LLM-based models. It would be better to further verify the effectiveness and universality to non-LLM-based models like HiTeA and InternVideo.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
• The writing could be improved. It took me quite a lot of effort to go back and forth to understand the main idea and the theoretical analysis of the paper.,NIPS_2022_1440,NIPS_2022,"• The writing could be improved. It took me quite a lot of effort to go back and forth to understand the main idea and the theoretical analysis of the paper.
• Using neural networks as surrogate models will certainly help to improve the model's accuracy, however, I'm wondering how the hyper-parameters of these NN surrogate models (e.g., the number of layers, the width in each layer, the learning rate) could be efficiently set. From the experimental results, it shows that the performance of BO depends on a lot in the hyperparameters of the NN surrogate models. Finding an optimal set of hyperparameters seem to create another AutoML problem to be solved.
• I would also like to understand more about the time cost of the proposed algorithms, and how they are compared to the time cost of existing baselines. Training an NN surrogate model for every single input query in the batch probably takes a lot of time.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. The proposed method primarily builds upon a combination of existing methods (i.e., Clopper-Pearson intervals [1], Gaussian elimination [2]) and it doesn't present significant theoretical novelty. I am willing to improve my score, if the authors can well address these concerns. [1] Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case of the binomial. Biometrika, 26(4):404–413, 1934. [2] Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.",4vPVBh3fhz,ICLR_2024,"1. Theorem 3.2 lacks a detailed proof procedure, although the authors provide an interesting discussion on the confusion matrix in section 3.3. Please let me know where the proof is if I missed.
2. All experiments are conducted on small-scale datasets where the number of classes is small, but it is always desired to include large-scale experiments. Can you share any experimental results (e.g., on CIFAR100) compared with other methods?
3. The proposed method primarily builds upon a combination of existing methods (i.e., Clopper-Pearson intervals [1], Gaussian elimination [2]) and it doesn't present significant theoretical novelty.
I am willing to improve my score, if the authors can well address these concerns.
[1] Charles J Clopper and Egon S Pearson. The use of confidence or fiducial limits illustrated in the case of the binomial. Biometrika, 26(4):404–413, 1934.
[2] Gene H Golub and Charles F Van Loan. Matrix computations. JHU press, 2013.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
① Can the text input is concatenated by the four text elements of an object?,ICLR_2023_516,ICLR_2023,"Weakness:
Although the motivation is innovative in constructing the pretraining model for the UI modeling field, the overall pretraining pipeline may lack appropriate innovation and some aspects are similar to Flamingo, e.g., the vision-language model architecture, the evaluation method on the multi-task and few-shot learning, and the computational pipeline of Region Summarizer is similar to the Perceiver Resampler of Flamingo.
The illustration of the pretraining dataset is too brief. It is recommended to add more statistical analysis and construction details, e.g., The text and content description are human-annotated or not?
Considering the readability, it is recommended to add the introduction of test datasets about downstream tasks. Perhaps the authors can add these in the supplementary material rather than suggesting the readers read the previous literature.
The ablation study is limited and maybe authors can consider the following ablation to make the paper clear and complete. (1) Region Summarizer: ① Why choose the bbox coordinates as q rather than the region feature of bbox? ② What is the result when kv is just the vit_outputs rather than the concatenation of vit-outputs and bbox. (2) Pretraining: ① Can the text input is concatenated by the four text elements of an object? ② The ViT is freezing as Flamingo or not? If not, The much screenshots of examples will bring the burden of GPU memory or not?
The 2.69M pre-training data as extra knowledge may obscure the fairness of the method compared with the baseline method. Maybe author can try to further finetune the comparison methods combined with single-modal encoders of the pretraining model? This is just a suggestion out of curiosity and not influence the final judgment.
In the Discussion section, these “early exploration” are supposed to illustrate hypotheses by some experimental data.
The paper of the comparison method “Widget caption” is cited repeatedly.
Maybe the authors can consider making the dataset and code public to promote the development of this field. It is just advice and not influence the final judgment.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The MV objective is nice for the proposed UCB-style algorithm and theoretical work, but for evaluation VaR and CVaR also are important considerations Writing:",NIPS_2021_1251,NIPS_2021,"- Typically, expected performance under observation noise is used for evaluation because the decision-maker is interested in the true objective function and the noise is assumed to be noise (misleading, not representative). In the formulation in this paper, the decision maker does care about the noise; rather the objective function of interest is the stochastic noisy function. It would be good to make this distinction clearer upfront. - The RF experiment is not super compelling. It is not nearly as interesting as the FEL problem, and the risk aversion does not make a significant difference in average performance. Overall the empirical evaluation is fairly limited. - It is unclear why the mean-variance model is the best metric to use for evaluating performance - Why not also evaluate performance in terms of the VaR or CVaR? - The MV objective is nice for the proposed UCB-style algorithm and theoretical work, but for evaluation VaR and CVaR also are important considerations
Writing: - Very high quality and easy to follow writing - Grammar: - L164: “that that” - Figure 5 caption: “Simple regret fat the reprted”
Questions: - Figure 2: “RAHBO not only leads to strong results in terms of MV, but also in terms of mean objective”? Why is it better than GP-UCB on this metric? Is this an artifact of the specific toy problem?
Limitations are discussed and potential future directions are interesting. “We are not aware of any societal impacts of our work” – this (as with an optimization algorithm) could be used for nefarious endeavors and could be discussed.","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,
"- The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented).",ICLR_2023_1511,ICLR_2023,"Weakness_ - The paper could do better to first motivate the ""Why"" (why do we care about what we are going to be presented). - Similarly, it is lacking a ""So What"" on the bounds provided, which are often just left there as final statements, without an analysis that explains whether 1) they are (likely to be) tight and 2) what this implies for practitioners. - Although well-written, the paper felt quite dense, even compared to other pure-math ML papers. More examples such as Figure 2 would help. - As far as I understood, the assumption on the non-linearities discards the sigmoid and the softmax, which are popular non-linearities. It would be good to acknowledge this directly by name.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Abstract: The sentence in lines 12-17 (""After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version"") is cumbersome and can be made clearer.",ARR_2022_130_review,ARR_2022,"1. Section 4 (Models) misses some details about the proposed model. For example, what is the exact inference procedure over $O_p$ (personally I prefer equations over textual descriptions) 2. Evaluation metrics: This subsection is difficult to read and not rigorous.
Comments & questions: - Abstract: The sentence in lines 12-17 (""After multi-span re-annotation, MultiSpanQA consists of over a total of 6,0000 multi-span questions in the basic version, and over 19,000 examples with unanswerable questions, and questions with single-, and multi-span answers in the expanded version"") is cumbersome and can be made clearer. - Do you perform re-annotation for the expanded dataset as well? The text now says ""..and applying the same preprocessing"" (line 355) - this point can be made more clear.
- What is the inference procedure for single-span (v1)? Is the prediction a long span like in training?
Typos: - Line 47: ""constitinga"" -> ""consisting"" - Line 216: ""classifies"" -> ""classify""","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
3. The domain-specific model is trained on Pix3D. And the experiments are conducted on Pix3D. Such comparisons to those zero-shot single-image 3D reconstruction models are even more unfair.,hkWHdI8ss5,ICLR_2024,"1. Spending 1 hour to optimize a coarse mesh from a domain-specific model for furniture is not necessary. For domain-specific single-image 3D reconstruction, there are many existing fast and robust models—for example, Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction.
2. No technical novelty. It is a simple combination of two off-the-shelf models: Total3DUnderstanding and Magic3D-style DMTet finetuning.
3. The domain-specific model is trained on Pix3D. And the experiments are conducted on Pix3D. Such comparisons to those zero-shot single-image 3D reconstruction models are even more unfair.
4. No two-stage ablation studies. The paper does not ablate either stage to show the significance of design choices.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission.",ICLR_2021_2824,ICLR_2021,"Weakness:
While the authors claimed that they challenged the hypothesis by Kang et al. that the learning of feature representation and classifier should be completely decoupled in long-tail classification, from my perspective this paper is a nature extension of Kang et al. Similar to Kang et al., this paper further demonstrates the importance of progressive learning to address long-tailed distribution, which first mainly focuses on head classes (first stage) and then on tail classes (second stage). Different from Kang et al., a) this paper uses more aggressive sampling strategy at the second stage, by replacing class-balanced sampler with class-reversed sampler, and b) this paper shows that the feature can be further fine-tuned at the second stage.
This paper mainly reported the overall performance. It would be more convincing to provide the per-class performance to show the improvement on the tail classes.
This paper mainly focused on the comparison with Kang et al. Since Kang et al. extensively conducted experiments on the long-tailed ImageNet and Places datasets, how does the proposed approach perform on these two datasets?
While the time to switch from instance-balanced sampling to class-reversed sampling is a hyper-parameter as the authors mentioned, I was wondering if there is any principle to guide this design choice. Also, if this is dataset/distribution dependent or sensitive.
Consider an even smoother transition, from instance-balanced sampling to class-balanced sampling and finally to class-reversed sampling. Will this combination outperform the strategy in the paper?
In the discussion section, the authors claimed that the near-optimality of their method and that little room left for the improvement of the successive resampling strategy. This seems a very strong argument. I was wondering if there is any formal theoretical guarantee on this.
Post Rebuttal:
I do appreciate the efforts and additional experiments and theoretical analysis that the authors made in the rebuttal. While this paper proposed an interesting approach to long-tail recognition, some connections, distinctions, and comparisons with related work and thorough experimental analysis were missing in the original manuscript, as mentioned by other reviewers as well. Some of these concerns were addressed in the rebuttal, but not fully clarified. For example, the new comparison on the more challenging ImageNet-LT dataset (Table 3) shows that the proposed approach does not outperform or is even worse than Decouple [Kang et al.] for the overall performance. Also, Table 5 shows the trade-off between head and tail categories. But similar trade-off has not been fully investigated for the baselines; for example, by changing the hyper-parameters in Decouple [Kang et al.], Decouple [Kang et al.] could also significantly improve the tail accuracy while slightly decreasing the head accuracy. This makes the paper not ready for this ICLR. I encourage the authors to continue this line of work for future submission.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
2. Additional experiments on realistic noisy datasets like WebVision would have provided more support for C2D.,ICLR_2021_2926,ICLR_2021,"and suggestions: 1. It is not clear to me if the warm-up phase makes a difference in performance on larger, more realistic datasets like Clothing1M. More careful analysis of how the warm-up phase affects the sample separation in SSL versus a fully supervised setting would have been useful, including experiments on CIFAR-10. 2. Additional experiments on realistic noisy datasets like WebVision would have provided more support for C2D. 3. The paper is not clearly written. Important components like MixMatch are not explained. For instance, the Method section contains discussion on various design decisions, rather than a step-by-step description of the method itself. An algorithm figure detailing C2D method would be useful for exposition. In sum, the paper definitely has a good idea and interesting results, but it is not well-structured, which makes it harder to parse the method and results.
Questions and suggestions: 1. Do you have any additional insights into modest performance gains on Clothing1M 2. How does the algorithm perform on other real-world datasets like WebVision, evaluated by DivideMix?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- In the proposed E2W algorithm, what is the intuition behind the very specific choice of $\lambda_t$ for encouraging exploration? What if the exploration parameter $\epsilon$ is not included? Also, why is $\sum_a N(s, a)$ (but not $N(s, a)$) used for $\lambda_s$ in Equation (7)?",NIPS_2019_854,NIPS_2019,"weakness I found in the paper is that the experimental results for Atari games are not significant enough. Here are my questions: - In the proposed E2W algorithm, what is the intuition behind the very specific choice of $\lambda_t$ for encouraging exploration? What if the exploration parameter $\epsilon$ is not included? Also, why is $\sum_a N(s, a)$ (but not $N(s, a)$) used for $\lambda_s$ in Equation (7)? - In Figure 3, when $d=5$, MENTS performs slightly worse than UCT at the beginning (for about 20 simulation steps) and then suddenly performs much better than UCT. Any hypothesis about this? It makes me wonder whether the algorithm scales with larger tree depth $d$. - In Table 1, what are the standard errors? Is it just one run for each algorithm? There is no learning curve showing whether each algorithm converges. What about the final performance? Itâs hard for me to justify the significance of the results without these details. - In Appendix A (experimental details), there are sentences like ``The exploration parameters for both algorithms are tuned from {}.ââ What are the exact values of all the hyperparameters used for generating the figures and tables? What hyperparameters is the algorithm sensitive to? Please make it more clear to help researchers replicate the results. To summarize based on the four review criteria: - Originality: To the best of my knowledge, the algorithm presented is original: it builds on previous work (a combination of MCTS and maximum entropy policy optimization), but comes up with a new idea for selecting actions in the tree based on the softmax value estimate. - Quality: The contribution is technically sound. The proposed method is shown to achieve an exponential convergence rate to the optimal solution, which is much faster than the polynomial convergence rate of UCT. It is also evaluated on two test domains with some good results. The experimental results for Atari games are not significant enough though. - Clarity: The paper is clear and well-written. - Significance: I think the paper is likely to be useful to those working on developing more sample efficient online planning algorithms. UPDATE: Thanks for the author's response! It addresses some of my concerns about the significance of the results. But it is still not strong enough to cause me to increase my score as it is already relatively high.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. The evaluation is limited, mostly relying on 4 OCR QA datasets. As the authors admit in Fig 4(5), this evaluation may be unreliable. More scenarios like the LLaVA benchmark would be expected, especially in ablation studies.",tj4a1JY03u,ICLR_2024,"1. The conclusions are a bit obvious - that higher resolution inputs and more specialized training data improve LLaVA's OCR performance.
2. The most important contribution of the paper is the collected dataset. It succeeds in showing the data improves LLaVA's OCR capabilities, but does not demonstrate it is superior to other visual instruction datasets. For example, mPLUG-Owl has comparable OCR performance to LLaVAR under the same resolution in Table 2. This raises the question of whether OCR-specific data is needed, or if the scale of data in the paper is insufficient.
3. The evaluation is limited, mostly relying on 4 OCR QA datasets. As the authors admit in Fig 4(5), this evaluation may be unreliable. More scenarios like the LLaVA benchmark would be expected, especially in ablation studies.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"* L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0?",NIPS_2016_386,NIPS_2016,", however. For of all, there is a lot of sloppy writing, typos and undefined notation. See the long list of minor comments below. A larger concern is that some parts of the proof I could not understand, despite trying quite hard. The authors should focus their response to this review on these technical concerns, which I mark with ** in the minor comments below. Hopefully I am missing something silly. One also has to wonder about the practicality of such algorithms. The main algorithm relies on an estimate of the payoff for the optimal policy, which can be learnt with sufficient precision in a ""short"" initialisation period. Some synthetic experiments might shed some light on how long the horizon needs to be before any real learning occurs. A final note. The paper is over length. Up to the two pages of references it is 10 pages, but only 9 are allowed. The appendix should have been submitted as supplementary material and the reference list cut down. Despite the weaknesses I am quite positive about this paper, although it could certainly use quite a lot of polishing. I will raise my score once the ** points are addressed in the rebuttal. Minor comments: * L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0? * L177: ""(OCO )"" -> ""(OCO)"" and similar things elsewhere * L176: You might want to mention that the learner observes the whole concave function (full information setting) * L223: I would prefer to see a constant here. What does the O(.) really mean here? * L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards. * L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes? * The algorithm only stops /after/ it has exhausted its budget. Don't you need to stop just before? (the regret is only trivially affected, so this isn't too important). * L213: \tilde \mu is undefined. I guess you mean \tilde \mu_t, but that is also not defined except in Corollary 1, where it just given as some point in the confidence ellipsoid in round t. The result holds for all points in the ellipsoid uniformly with time, so maybe just write that, or at least clarify somehow. ** L435: I do not see how this follows from Corollary 2 (I guess you meant part 1, please say so). So first of all mu_t(a_t) is not defined. Did you mean tilde mu_t(a_t)? But still I don't understand. pi^*(X_t) is (possibly random) optimal static strategy while \tilde \mu_t(a_t) is the optimistic mu for action a_t, which may not be optimistic for pi^*(X_t)? I have similar concerns about the claim on the use of budget as well. * L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else. * L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee? * L200: ""for every arm a"" implies there is a single optimistic parameter, but of course it depends on a ** L303: Why not choose T_0 = m Sqrt(T)? Then the condition becomes B > Sqrt(m) T^(3/4), which improves slightly on what you give. * It would be nice to have more interpretation of theta (I hope I got it right), since this is the most novel component of the proof/algorithm.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
* I have a lot of problems with these abstract visual reasoning tasks. They seem a bit unintuitive and overly difficult (I have a lot of trouble solving them). Having multiple rows and having multiple and different factors changing between each frame is very confusing and it seems like it would be hard to interpret how much these models actually learn the pattern or just exploit some artifacts. Do we have any proof that more simpler visual reasoning tasks wouldnât do and this formulation in the paper is the way to go?,NIPS_2019_1276,NIPS_2019,"* Really only one real takeaway/useful experiment from the paper, which is that disentangling is sample efficient for this strange set of upstream tasks. * I have a lot of problems with these abstract visual reasoning tasks. They seem a bit unintuitive and overly difficult (I have a lot of trouble solving them). Having multiple rows and having multiple and different factors changing between each frame is very confusing and it seems like it would be hard to interpret how much these models actually learn the pattern or just exploit some artifacts. Do we have any proof that more simpler visual reasoning tasks wouldnât do and this formulation in the paper is the way to go? * It seems weird the authors didnât just consider a task with one row and one panel missing and the same one factor changing between panels. Is there any empirical evidence that this is too easy or uninformative? Why not a row where there are a few panels of the ellipse getting bigger and then for the missing frame the model chooses between a smaller ellipse, same size ellipse, *bigger ellipse*, bigger ellipse but at the wrong angle, bigger ellipse, but translated, bigger ellipse but different color, etc. or at least some progression of difficulty starting from the easiest and working up to the tasks in the paper?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- Weak supervision could be better evaluated - eg, how realistic are the evaluated tweets? The prompt requires ""all of the structured elements for perspectives to be present in the generated tweets"", which doesn't see the most realistic. The generation of authors is also not realistic (""[author] embeddings are initialized by averaging the corresponding artificial tweets"").",MMrqu8SD6y,EMNLP_2023,"- Weak supervision could be better evaluated - eg, how realistic are the evaluated tweets? The prompt requires ""all of the structured elements for perspectives to be present in the generated tweets"", which doesn't see the most realistic. The generation of authors is also not realistic (""[author] embeddings are initialized by averaging the corresponding artificial tweets"").
- The authors also claim that weak supervision achieves ""comparable"" performance - this feels like a bit of an overstatement. For author stance, performance drops 5 points from direct to weak (same gap as from their model to baseline); also substantial drops for ambiguous (30 points) and entity mapping (8 points).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
3. There is a lack of essential visualization of intermediate processes and comparisons.,KUpUO7aSSg,ICLR_2025,"1. The experiment appears to be somewhat limited. While the proposed method is tailored for agricultural settings, I would recommend the authors to transfer it to natural environments, such as cityscapes, to compare its effectiveness.
2. The method proposed in this paper does not seem to specifically address issues present in agricultural settings.
3. There is a lack of essential visualization of intermediate processes and comparisons.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', 'X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
3. The expected counterfactual violates $\mathcal{P}_2$ stated in Definition 1.,9TpgFnRJ1y,ICLR_2025,"1. Similar to other generator-based explanation frameworks, the transparency of the explanation process itself is limited due to the black-box nature of the neural-network-implemented generator.
2. The flexibility of the proposed method is another concern, as the delivered explanations appear to be model-specific.
3. The expected counterfactual violates $\mathcal{P}_2$ stated in Definition 1.
4. The benefit of the rotation for accelerating expectation computation is unclear.
Questions 1 and 2 detail the concerns mentioned in points 3 and 4 respectively.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1. I am concerned about the importance of this result. Since [15] says that perturbed gradient descent is able to find second-order stationary with almost-dimension free (with polylog factors of dimension) polynomial iteration complexity, it is not surprising to me that the decentralized algorithm with occasionally added noise is able to escape saddle point in polynomial time. In addition, the iteration complexity is no longer dimension-free anymore in Theorem 3 (there is a $d$ dependency instead of $log d$).",NIPS_2020_1776,NIPS_2020,"1. I am concerned about the importance of this result. Since [15] says that perturbed gradient descent is able to find second-order stationary with almost-dimension free (with polylog factors of dimension) polynomial iteration complexity, it is not surprising to me that the decentralized algorithm with occasionally added noise is able to escape saddle point in polynomial time. In addition, the iteration complexity is no longer dimension-free anymore in Theorem 3 (there is a $d$ dependency instead of $log d$). 2. There is no empirical study in this paper. The authors should have constructed some synthetic examples where we know the exact location of saddle points and tried to verify the theoretical claims of the proposed algorithm. Furthermore, PGD [15] should also be compared. I know this is a theory paper, but given the presence of [15], the theoretical contribution is not strong enough from my perspective.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
- Keypoint detection results should be included in the experiments section.,NIPS_2017_217,NIPS_2017,"- The paper is incremental and does not have much technical substance. It just adds a new loss to [31].
- ""Embedding"" is an overloaded word for a scalar value that represents object ID.
- The model of [31] is used in a post-processing stage to refine the detection. Ideally, the proposed model should be end-to-end without any post-processing.
- Keypoint detection results should be included in the experiments section.
- Sometimes the predicted tag value might be in the range of tag values for two or more nearby people, how is it determined to which person the keypoint belongs?
- Line 168: It is mentioned that the anchor point changes if the neck is occluded. This makes training noisy since the distances for most examples are computed with respect to the neck.
Overall assessment: I am on the fence for this paper. The paper achieves state-of-the-art performance, but it is incremental and does not have much technical substance. Furthermore, the main improvement comes from running [31] in a post-processing stage.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).",NIPS_2017_351,NIPS_2017,"1.	The approach mentions attention over 3 modalities â image, question and answer. However, it is not clear what attention over answers mean because most of the answers are single words and even if they are multiword, they are treated as single word. The paper does not present any visualizations for attention over answers. So, I would like the authors to clarify this.
2.	From the results in table 1, it seems that the main improvement in the proposed model is coming from the ternary potential. Without the ternary potential, the proposed model is not outperforming the existing models for 2 modalities setup (except HieCoAtt). So, I would like the authors to throw light into this.
3.	Since ternary potential seems to be the main factor in the performance improvement of the proposed model, I would like the authors to compare the proposed model with existing models where answers are also used as inputs such as Revisiting Visual Question Answering Baselines (Jabri et al., ECCV16).
4.	The paper lacks any discussion on failure cases of the proposed model. It would be insightful to look into the failure modes so that future research can be guided accordingly.
5.	Other errors/typos:
a.	L38: mechanism -> mechanisms
b.	L237 mentions that the evaluation is on validation set. However Table 1 reports numbers on the test-dev and test-std sets?
Post-rebuttal comments:
Although the authors' response to the concern of ""Proposed model not outperforming existing models for 2 modalities"" does not sound satisfactory to me due to lack of quantitative evidence, I would like to recommend acceptance because of the generic attention framework for multiple modalities being proposed in the paper and quantitative results of 3-modality attention outperforming SOTA. The quantitative evaluation of the proposed model's attention maps against human attention maps (reported in the rebuttal) also looks good and suggests that the attention maps are more correlation with human maps' than existing models. Although, we don't know what this correlation value is for SOTA models such as MCB, I think it is still significantly better than that for HieCoAtt.
I have a question about one of the responses from the authors --
> Authors' response -- âMCB vs. MCTâ: MCT is a generic extension of MCB for n-modalities. Specifically for VQA the 3-MCT setup yields 68.4% on test-dev where 2-layer MCB yields 69.4%. We tested other combinations of more than 2-modalities MCB and found them to yield inferior results.
Are the numbers swapped here? 69.4% should be for 3-MCT, right? Also, the MCB overall performance in table 1 is 68.6%. So, not sure which number the authors are referring to when they report 68.4%.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1 For the grid search of learning rate, is it done on the validation set? Minor problems:",ICLR_2022_1794,ICLR_2022,"1 Medical imaging are often obtained in 3D volumes, not only limited to 2D images. So experiments should include the 3D volume data as well for the general community, rather than all on 2D images. And the lesion detection is another important task for the medical community, which has not been studied in this work.
2 More analysis and comments are recommended on the performance trending of increasing the number of parameters for ViT (DeiT) in the Figure 3. I disagree with authors' viewpoint that ""Both CNNs and ViTs seem to benefit similarly from increased model capacity"". In the Figure 3, the DeiT-B models does not outperform DeiT-T in APTOS2019, and it does not outperform DeiT-S on APTOS2019, ISIC2019 and CheXpert (0.1% won't be significant). However, CNNs can give more almost consistent model improvements as the capacity goes up except on the ISIC2019.
3 On the segmentation mask involved with cancer on CSAW-S, the segmentation results of DEEPLAB3-DEIT-S cannot be concluded as better than DEEPLAB3-RESNET50. The implication that ViTs outperform CNNs in this segmentation task cannot be validly drawn from an 0.2% difference with larger variance.
Questions: 1 For the grid search of learning rate, is it done on the validation set?
Minor problems: 1 The n number for Camelyon dataset in Table 1 is not consistent with the descriptions in the text in Page 4.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use ""discourse"" for things that are not considered ""discourse"" in other languages in UD?",ACL_2017_433_review,ACL_2017,"- The annotation quality seems to be rather poor. They performed double annotation of 100 sentences and their inter-annotator agreement is just 75.72% in terms of LAS. This makes it hard to assess how reliable the estimate of the LAS of their model is, and the LAS of their model is in fact slightly higher than the inter-annotator agreement. UPDATE: Their rebuttal convincingly argued that the second annotator who just annotated the 100 examples to compute the IAA didn't follow the annotation guidelines for several common constructions. Once the second annotator fixed these issues, the IAA was reasonable, so I no longer consider this a real issue.
- General Discussion: I am a bit concerned about the apparently rather poor annotation quality of the data and how this might influence the results, but overall, I liked the paper a lot and I think this would be a good contribution to the conference.
- Questions for the authors: - Who annotated the sentences? You just mention that 100 sentences were annotated by one of the authors to compute inter=annotator agreement but you don't mention who annotated all the sentences.
- Why was the inter-annotator agreement so low? In which cases was there disagreement? Did you subsequently discuss and fix the sentences for which there was disagreement?
- Table A2: There seem to be a lot of discourse relations (almost as many as dobj relations) in your treebank. Is this just an artifact of the colloquial language or did you use ""discourse"" for things that are not considered ""discourse"" in other languages in UD?
- Table A3: Are all of these discourse particles or discourse + imported vocab? If the latter, perhaps put them in separate tables, and glosses would be helpful.
- Low-level comments: - It would have been interesting if you had compared your approach to the one by Martinez et al. (2017, https://arxiv.org/pdf/1701.03163.pdf). Perhaps you should mention this paper in the reference section.
- You use the word ""grammar"" in a slightly strange way. I think replacing ""grammar"" with syntactic constructions would make it clearer what you try to convey. ( e.g., line 90) - Line 291: I don't think this can be regarded as a variant of it-extraposition. But I agree with the analysis in Figure 2, so perhaps just get rid of this sentence.
- Line 152: I think the model by Dozat and Manning (2016) is no longer state-of-the art, so perhaps just replace it with ""very high performing model"" or something like that.
- It would be helpful if you provided glosses in Figure 2.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- L393: What about racial, economic diversity in the sample? How well might these results generalize to other groups, especially marginalized groups?",FEpAUnS7f7,ICLR_2025,"**Originality**
**[Minor]** The idea to use ML tools to assist users in interpreting privacy policies is not new—in this sense the contribution of this study is marginal. Still, there is certainly value in evaluating this idea using the most recent large language models, and there is certainly value in conducting a study with actual users to see whether the tool really makes interpretation easier. I am not sure whether there are many user studies in prior work on this idea — perhaps the authors could clarify whether this is the first user study of its kind and, if not, whether it tells us anything new. **Quality**
Missing methodological details make it hard to tell whether the empirical findings support the broad claims in the abstract and introduction, and I have lingering questions about some of the results.
- **[Minor]** Section 3: Need a clear description of all the benchmark tasks to understand exactly what’s being evaluated — Section 3 seems to assume the tasks have already been defined. Examples:
- Section 3.1: What does it really mean to identify “User Choice/Control” or “Data Retention”, e.g., as a practice? Does this simply mean the privacy policy describes their user choice allowances or data retention practices, which could range from quite benign to quite egregious? How is this useful to a user?
- Section 3.2: What is the Choice Identification task? Is this the task described in A.1.3? Was this task defined in Wilson et al. (2016) too?
- Section 3.3: What’s “Privacy Question Answering”? (Or is it “Policy Question Answering”? Both terms are used.)
- Section 3.4: What’s in this dataset, as compared to the dataset used in the previous tasks? Who defined the “risky” sentences (what were the human-generated references for the ROUGE score)? Any examples?
- Section 4 provides a bit more detail, and the examples in the Appendix are somewhat helpful. Perhaps this Section could come before Section 3; or alternatively, move parts of Section 3 to the appendix, and just summarize the most important findings (GPT models perform better on X benchmarks) in a paragraph, using that space instead to better explain the tasks at hand.
- **[Major]** Section 4: These results are striking — users seem to comprehend the privacy policies much more easily with LLM assistance! But there are some key methodological details missing that could determine how rigorous the results are:
- Did the Experimental Group also have a copy of the privacy policy that they could read directly during the task (not through QA), or did they rely solely on information from the LLM agent? From the Appendix, I infer they did have access to the raw text — do the gains decrease/increase if the user cannot cross-check the LLM agent responses with the raw legal text?
- Section 6.1: Where/how were users recruited? How many privacy policies did each participant review? How were the privacy policies selected — from one of the previous datasets? Did every participant review the same privacy policy? (How likely is it that these policies appeared in the training data — i.e. leakage?) Where/how was questionnaire administered? This information is key for determining how internally and externally valid these results might be.
- Was the study IRB approved?
- L393: What about racial, economic diversity in the sample? How well might these results generalize to other groups, especially marginalized groups?
- I’m surprised by the finding that the Experimental Group had *higher* trust in info scores than the control group — and I wonder if there’s an issue with construct validity for this question. The relevant question is (L978): “I believe the information I read/received is accurate (1-5).” Given that the control group had direct access to the privacy policies, why would they respond with a 2.6, on average, compared to 4.5 in the experimental group, since the underlying information (the privacy policy) is the same for both groups? My best guess is that the Control Group suspected the company was misrepresenting its privacy practices in its privacy policy, and answered based on their distrust in the company; I suspect the Experimental Group, on the other hand, responded based on their level of trust in the accuracy of the LLM agent’s responses. So the scores may not be directly comparable. The alternative is that using the LLM agent somehow increased people’s confidence in the accuracy of the privacy policy itself, which seems less likely but still possible.
- **[Major]** Generally, it’s not clear how well the benchmarks measure the “correctness” of the agent’s responses — what is the ground truth for each of these tasks? The comprehension questions seem good, but they’re short, and not very granular — whereas the examples in the Appendix show LLM responses with much, much more detailed information about data practices. As the authors point out in the discussion, LLMs often produce incorrect and misleading text, especially when prompted for specific details that are less likely to be represented in training data. Can the authors say anything about the factuality of those more specific responses? How likely are those responses to contain falsehoods about the privacy policy that could mislead users? Can users easily identify false responses by cross-checking with the raw text or the QA feature? **Clarity**
Generally the paper is easy to follow, with the exception of the omitted methodological details listed above. Some **minor** points of clarity that would be worth addressing:
- L132: Have ML techniques actually improved privacy policy accessibility in practice? Or is this just a summary of research, not practice?
- L130: What is the OPP-115 dataset? Readers may not know.
- L131: Broken cite here.
- L136: What’s the difference between an LLM and an LLM agent? Is there a definition the authors can give? What makes this application an LLM agent, rather than just an LLM (the fact that the program scrapes hyperlinks, maybe)?
- Fig. 2: Text is too small to read, and often cropped, so it’s not clear what the different elements are. Simple labels might be better.
- Table 1-2: Suggest combining numbers side-by-side, so it’s easy to compare.
- Table 2, L192: SVM F1-score has a misplaced decimal. **Significance**
- **[Minor]** This is a neat idea, and it seems like it could certainly help users in particular cases. But to frame the significance more precisely, it would be helpful to comment on the scope of a technological solution like this (e.g. in the discussion) — there is a structural issue here with privacy regulations, and with GDPR in particular, that require companies to disclose information about their privacy policies but do not require companies to make that information, and users’ options with respect to their data, truly accessible. In a perfect world, this tool may not be necessary — companies could be required to produce interpretable “privacy labels” similar to Apple’s Privacy Nutrition labels. How does the performance of this LLM-based solution compare to other policy alternatives? (These questions probably cannot be answered in this study, but it is worth mentioning that a technological solution is not necessarily the best solution.)
- **[Major]** Section 3: On a similar note, can the authors report any non-ML baselines here? How does a person do on this task, on their own? It seems less important to know how GPT models compare to BERT or other ML models, and more important to know how this method compares to what users would otherwise be doing in practice. (Unless those traditional models are actually being used by lay users in practice — that would be worth mentioning.)
- L094: “We provide empirical evidence of the superiority of LLMs over traditional models”: I’m assuming these sentence refers specifically to *ML* models (would be worth clarifying). But is this approach superior to the practical alternatives available to users/policymakers? Superior to things like Apple’s “Privacy Nutrition” labels? Superior to writing a simpler privacy policy? Superior to hiring a lawyer? It would help to be more precise with this and similar claims of LLM “superiority”—superior to what?
- Section 3: It seems like the GPT models perform better than traditional ML models, but stepping back, are these scores good enough to be relied on? For example, the recall scores seem really low here — as far as I can tell, the GPT models miss as many as 30% of instances of third party sharing, and as many as 84% of instances of “data retention”? Can this tool be used to balance precision and recall? Is this the right balance for this kind of task? Recall might well be more important to users in this kind of task.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. Output quality is reasonable, but still far from realistic. Recent GAN works have shown amazing quality in synthesized results, and the bar has become much higher than a few years ago. In that aspect, I feel there’s still much room for improvement for the result quality. Overall, given the limited novelty, low resolution output and still high hardware requirement, I’m inclined to reject the paper.",ICLR_2021_1505,ICLR_2021,"1. The novelty is very low. Stage-wise and progressive training have been proposed for such a long time, they have been used everywhere. The way the authors use them don’t really exhibit anything novel to me. 2. The resolution of the outputs (128x128) is lower than prior works (e.g. DVD-GAN has 256x256 outputs). Since the paper claims the computation cost is lower, one would expect the model can generate higher resolution and much longer duration videos, but in fact it’s quite the opposite. To prove the effectiveness, I feel the authors need to show something higher than 256x256, say 512 or 1024 resolution. On the other hand, the hardware requirement is still high (128 GPUs) instead of some normal equipment that everyone can have, so I really don’t see any benefit of the model. If the authors can train DVD-GAN using only a handful of GPUs, that might also be a contribution, but it’s not the case now. 3. Output quality is reasonable, but still far from realistic. Recent GAN works have shown amazing quality in synthesized results, and the bar has become much higher than a few years ago. In that aspect, I feel there’s still much room for improvement for the result quality.
Overall, given the limited novelty, low resolution output and still high hardware requirement, I’m inclined to reject the paper.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1), soft labels is essentially on top of CRM and Cross entropy (for iNaturalist19, it looks like a higher beta value would be directly on top, it's unclear why the authors did not extend the curve further) These results, at first blush, seem fairly impressive. For the leftmost plots, I am concerned that the authors are using subpar hyperparameters, similarly to",ICLR_2021_491,ICLR_2021,"I am very concerned about the experiment sections.
To my understanding, Figure 2/Section 4.1 are factually incorrect. In particular, it appears that the soft-labels technique does essentially the same, or better than, CRM, across all fronts. In detail, a) In Figure 2(a), the leftmost softlabel point is equal to or better than CRM (and cross-entropy)
b) Figure 2(b) really concerns me, as it appears that the hyperparameters have been chosen to make a fairly narrow point - that it is possible to have low average hierarchical distance, and high top-1 error. I agree that that indicates a problem with the metric.
However, the authors make the fair broader claim that CRM is the only method which beats cross-entropy, which I do not think is justified. Looking at Figure 4 in A.1, it is readily apparent that choosing different hyperparameters for existing methods would yield similar error distributions to CRM. Having these plots in the appendix, combined with claims that the authors chose the best hyperparameters, feels a bit misleading.
c) As in 1), soft labels is essentially on top of CRM and Cross entropy (for iNaturalist19, it looks like a higher beta value would be directly on top, it's unclear why the authors did not extend the curve further)
These results, at first blush, seem fairly impressive. For the leftmost plots, I am concerned that the authors are using subpar hyperparameters, similarly to 1)(b) above. Strangely, in this instance the results for other hyperparameters are not included in the appendix. The remaining experiments are fairly convincing, though.
Reccomendation I do not think this paper can be accepted in its current form. While I suspect that CRM is a good method that I would like to use, some of the core arguments (Figure 2/Section 4.1) in the paper appear to be fatally flawed.
Smaller notes:
The paper could use an additional proofread, as there are often odd phrasings. I found the experiment section particularly hard to follow
The acronym HXE is never defined, or linked to a citation","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The simple/traditional experiment for unseen characters is a nice idea, but is presented as an afterthought. I would have liked to see more eval in this direction, i.e. on classifying unseen words - Maybe add translations to Figure 6, for people who do not speak Chinese?",ACL_2017_543_review,ACL_2017,"- Experimental results show only incremental improvement over baseline, and the choice of evaluation makes it hard to verify one of the central arguments: that visual features improve performance when processing rare/unseen words.
- Some details about the baseline are missing, which makes it difficult to interpret the results, and would make it hard to reproduce the work.
- General Discussion: The paper proposes the use of computer vision techniques (CNNs applied to images of text) to improve language processing for Chinese, Japanese, and Korean, languages in which characters themselves might be compositional. The authors evaluate their model on a simple text-classification task (assigning Wikipedia page titles to categories). They show that a simple one-hot representation of the characters outperforms the CNN-based representations, but that the combination of the visual representations with standard one-hot encodings performs better than the visual or the one-hot alone. They also present some evidence that the visual features outperform the one-hot encoding on rare words, and present some intuitive qualitative results suggesting the CNN learns good semantic embeddings of the characters.
I think the idea of processing languages like Chinese and Japanese visually is a great one, and the motivation for this paper makes a lot of sense. However, I am not entirely convinced by the experimental results. The evaluations are quite weak, and it is hard to say whether these results are robust or simply coincidental. I would prefer to see some more rigorous evaluation to make the paper publication-ready. If the results are statistically significant (if the authors can indicate this in the author response), I would support accepting the paper, but ideally, I would prefer to see a different evaluation entirely.
More specific comments below: - In Section 3, paragraph ""lookup model"", you never explicitly say which embeddings you use, or whether they are tuned via backprop the way the visual embeddings are. You should be more clear about how the baseline was implemented. If the baseline was not tuned in a task-specific way, but the visual embeddings were, this is even more concerning since it makes the performances substantially less comparable.
- I don't entirely understand why you chose to evaluate on classifying wikipedia page titles. It seems that the only real argument for using the visual model is its ability to generalize to rare/unseen characters. Why not focus on this task directly? E.g. what about evaluating on machine translation of OOV words? I agree with you that some languages should be conceptualized visually, and sub-character composition is important, but the evaluation you use does not highlight weaknesses of the standard approach, and so it does not make a good case for why we need the visual features. - In Table 5, are these improvements statistically significant?
- It might be my fault, but I found Figure 4 very difficult to understand.
Since this is one of your main results, you probably want to present it more clearly, so that the contribution of your model is very obvious. As I understand it, ""rank"" on the x axis is a measure of how rare the word is (I think log frequency?), with the rarest word furthest to the left? And since the visual model intersects the x axis to the left of the lookup model, this means the visual model was ""better"" at ranking rare words? Why don't both models intersect at the same point on the x axis, aren't they being evaluated on the same set of titles and trained with the same data? In the author response, it would be helpful if you could summarize the information this figure is supposed to show, in a more concise way. - On the fallback fusion, why not show performance for for different thresholds? 0 seems to be an edge-case threshold that might not be representative of the technique more generally.
- The simple/traditional experiment for unseen characters is a nice idea, but is presented as an afterthought. I would have liked to see more eval in this direction, i.e. on classifying unseen words - Maybe add translations to Figure 6, for people who do not speak Chinese?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
3.1. VioT dataset:20 images in each of the 4 catergoreis were provided. However I feel the number of images is small to text the validity of the approach.,t1nZzR7ico,ICLR_2025,"1. The presentation in the experiment section is not upto par with ICLR. The figures and text should be arranged properly.
2. The idea is similar to treating VLM and LLM as two agents helping to jailbreak the T2I diffusion model. How is approach different from [1].
3. 1. VioT dataset: 20 images in each of the 4 catergoreis were provided. However I feel the number of images is small to text the validity of the approach.
4. Lack of scoring function ablation details to understand each of its contribution. Why is there a linear addition? Is there no normalization of the values? Such details are very important to understand the scoring function.
Further details related to weakness are asked using questions below.
1. Dong, Yingkai, et al. ""Jailbreaking Text-to-Image Models with LLM-Based Agents."" arXiv preprint arXiv:2408.00523 (2024).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- I think an ablation study of number of layers vs perf might be interesting. RESPONSE TO AUTHOR REBUTTAL: Thank you very much for a thoughtful response. Given that the authors have agreed to make the content be more specific to NER as opposed to sequence-tagging, I have revised my score upward.",ACL_2017_636_review,ACL_2017,"- Only applied to English NER--this is a big concern since the title of the paper seems to reference sequence-tagging directly. - Section 4.1 could be clearer. For example, I presume there is padding to make sure the output resolution after each block is the same as the input resolution. Might be good to mention this. - I think an ablation study of number of layers vs perf might be interesting.
RESPONSE TO AUTHOR REBUTTAL: Thank you very much for a thoughtful response. Given that the authors have agreed to make the content be more specific to NER as opposed to sequence-tagging, I have revised my score upward.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '3', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,
"• The paper is hard to follow, and more intuitive explanations on the mathematical derivations are needed. Figure captions are lacking, and require additional explanations and legends (e.g., explain the colors in Fig. 2). Fig. 1 and 2 did not contribute much to my understanding, and I had to read the text few times instead.",NIPS_2022_2772,NIPS_2022,"• The paper is hard to follow, and more intuitive explanations on the mathematical derivations are needed. Figure captions are lacking, and require additional explanations and legends (e.g., explain the colors in Fig. 2). Fig. 1 and 2 did not contribute much to my understanding, and I had to read the text few times instead. • At the end of the day the model proposes a method to learn features for detecting boundaries, which is an old computer vision task. Indeed, it uses a new MRF framework, and contrastive loss, but it is not clear why not using DNNs with contrastive loss for doing that, besides that maybe the learned features are more like human vision features. • The results of the models are not compared against unsupervised DNN models. I think it is interesting to see such a comparison, e.g., to unsupervised segmentation models that can be adjusted to the BSDS500 contour detection task.
• There is not review of previous related work, and these is not section for “related work”. Can unsupervised segmentation model be relevant to your work?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
2) how sensitive are the empirical results to hyperparameter choices. This second point is especially crucial since wrong choices can conceivably wipe out whatever improvement is gained from this method. I will be willing to reconsider my rating if this particular issue is resolved.,ICLR_2023_1463,ICLR_2023,"Weakness:
There is quite a bit of redundancy in the writing. e.g. the point that disentangled representations are better than entangled representations is made unnecessarily too many times. e.g. the first 6 lines of first paragraph in section 4.1 are not really needed in my opinion as that has already been said thrice earlier.
I'd have loved to see more discussion of the interpretability/explainability aspect in the main paper, more than just a single case study in experiments section. If redundancy is removed there's plenty of room for discussion about this in the paper.
My biggest concern is the objective function involves a lot of hyperparameters. And while the authors did a good job (from reproducibility perspective) of writing out all hyperparameter choices it is very much unclear how: 1) those choices can actually be made in practice (the current explanation in the appendix is too vague) and 2) how sensitive are the empirical results to hyperparameter choices. This second point is especially crucial since wrong choices can conceivably wipe out whatever improvement is gained from this method. I will be willing to reconsider my rating if this particular issue is resolved.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
2. This work utilizes existing attack methods on a surrogate model. It is similar to use the transferability of adversarial examples directly. The author needs to further claim the novelty and contribution of the proposed method.,NIPS_2021_671,NIPS_2021,"and my questions about this paper: 1. The experiment of this paper is not sufficient. Firstly, there is no comparison with other data poison methods, especially with [1], which is very similar to the proposed one. 2. This work utilizes existing attack methods on a surrogate model. It is similar to use the transferability of adversarial examples directly. The author needs to further claim the novelty and contribution of the proposed method. 3. The proposed method might be invalid when adversarial detections are involved. More precisely, the defender can utilize existing detection methods, such as like LID[2], MD[3], and KB[4], to remove those poisoned examples. Thus, there should be some tests on evaluating the robustness of the proposed method against adversarial detections. 4. The author has pointed out that their method performs unsatisfactorily to the defense of adversarial training techniques. In fact, such an limitation is fatal as the adversarial training is not so expensive as the authors claimed. Some adversarial training method like FastAdv [5] improves the training speed significantly.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
1. Text in table 1 is too small and hard to read 2. Algorithm 1: gradient symbol is missing in line 4 References: [AFKT21] Private Stochastic Convex Optimization: Optimal Rates in ℓ1 Geometry [BGN21] Non-Euclidean Differentially Private Stochastic Convex Optimization [LT18] Private selection from private candidates,NIPS_2022_139,NIPS_2022,"1. The rates for the smooth case depend on the dimension d (not the rank as in the Lipschitz case). While the authors show tight lower bounds up to factors that depend on |w*|, this lower bound is probably obtained by taking rank=d and therefore the upper bound may not have tight dependence on the rank. It is important to clarify this in the paper. 2. One minor weakness is that all the algorithms in the paper are somewhat standard as similar algorithms have been used in the literature. However, the obtained results and some of the techniques are interesting (such as using average stability instead of uniform stability to improve the rates), therefore I don’t consider this to be a real limitation. 3. The rates obtained for the smooth case are very similar to the rates for DP-SCO in ell_1 geometry [AFKT21, BGN21] which also have a phase transition. I’m not sure if there is any fundamental connection here but it may be useful to explore this a little and comment on this. 4. Why didn’t the authors use the algorithms from “private selection from private candidates” [LT18] for adaptivity to |w*|? This will only require to privatize the score functions (as [LT18] assumes a non-private one) but may be simpler than redoing everything from scratch. More minor comments: 1. Text in table 1 is too small and hard to read 2. Algorithm 1: gradient symbol is missing in line 4
References: [AFKT21] Private Stochastic Convex Optimization: Optimal Rates in ℓ1 Geometry [BGN21] Non-Euclidean Differentially Private Stochastic Convex Optimization [LT18] Private selection from private candidates","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2: I think you should be able to render (Wright et al., 1934; Figure 1) more naturally by using the [bracketed arguments] in \citep. ...Not sure how this plays with hyperref. P.",ICLR_2021_309,ICLR_2021,"I don’t have any serious complaints. The contribution is a tad narrow, but it makes progress on some tricky and difficult questions. The experiments also only produce corroborating evidence of CAD’s status as implicating causal variables, and we already know by construction that there is a causal aspect to these perturbations, so it’s not exactly an Earth-shaking result. But the experimental framework gives what seems to be pretty good evidence that other automated methods don’t implicate causal variables, so that’s nice. The main value seems to be in the theoretical observations about the effect of noise on out-of-domain performance and what that tells us about causality. These observations in the paper seem fairly straightforward, but I’m not aware of any literature making the same point. (However, I am not perfectly familiar with the literature and might have missed it.) Regardless, the application of these observations to experiments is interesting and provides useful conceptual scaffolding for future research about causal variables in models (such as deep NNs) without such explicit notions. Recommendation
Accept. The paper seems sound, is well written, and addresses an important problem. The contribution may not be huge but seems to me worth publishing.
More comments/questions
The bolded paragraph on P. 5 might be a bit much. It makes a particular claim about “language meaning” which implicitly views meaning as corresponding to the causal connection between input language and output labels. The notion of language “meaning” is nuanced and it’s not clear whether this claim is true for all tasks, where labels may be annotated based on broader (or indeed narrower) inferences regarding the generative process of the text. Since this isn’t purporting to be a paper about language meaning, I would suggest staying away from this.
Typos, style, etc.
P. 2: I think you should be able to render (Wright et al., 1934; Figure 1) more naturally by using the [bracketed arguments] in \citep. ...Not sure how this plays with hyperref.
P. 3: period before “and unintentional”
P. 6: “use SVM” -> “we use SVM”","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.",NIPS_2017_645,NIPS_2017,"- The main paper is dense. This is despite the commendable efforts by the authors to make their contributions as readable as possible. I believe it is due to NIPS page limit restrictions; the same set of ideas presented at their natural length would make for a more easily digestible paper.
- The authors do not quite discuss computational aspects in detail (other than a short discussion in the appendix), but it is unclear whether their proposed methods can be made practically useful for high dimensions. As stated, their algorithm requires solving several LPs in high dimensions, each involving a parameter that is not easily calculable. This is reflected in the authorsâ experiments which are all performed on very small scale datasets.
- The authors mainly seem to focus on SSC, and do not contrast their method with several other subsequent methods (thresholded subspace clustering (TSC), greedy subspace clustering by Park, etc) which are all computationally efficient as well as come with similar guarantees.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '2', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"9. The equation below Line 502: I think the '+' sign after \nu_j should be a '-' sign. In the definition of B under Line 503, there should be a '-' sign before \sum_{j=1}^m, and the '-' sign after \nu_j should be a '+' sign. In Line 504, we should have \nu_{X_i|Z} = - B/(2A). Minor comments:",NIPS_2019_1397,NIPS_2019,"weakness of the manuscript. Clarity: The manuscript is well-written in general. It does a good job in explaining many results and subtle points (e.g., blessing of dimensionality). On the other hand, I think there is still room for improvement in the structure of the manuscript. The methodology seems fully explainable by Theorem 2.2. Therefore, Theorem 2.1 doesn't seem necessary in the main paper, and can be move to the supplement as a lemma to save space. Furthermore, a few important results could be moved from the supplement back to the main paper (e.g., Algorithm 1 and Table 2). Originality: The main results seem innovative to me in general. Although optimizing information-theoretic objective functions is not new, I find the new objective function adequately novel, especially in the treatment of the Q_i's in relation to TC(Z|X_i). Relevant lines of research are also summarized well in the related work section. Significance: The proposed methodology has many favorable features, including low computational complexity, good performance under (near) modular latent factor models, and blessing of dimensionality. I believe these will make the new method very attractive to the community. Moreover, the formulation of the objective function itself would also be of great theoretical interest. Overall, I think the manuscript would make a fairly significant contribution. Itemized comments: 1. The number of latent factors m is assumed to be constant throughout the paper. I wonder if that's necessary. The blessing of dimensionality still seems to hold if m increases slowly with p, and computational complexity can be still advantageous compared to GLASSO. 2. Line 125: For completeness, please state the final objective function (empirical version of (3)) as a function of X_i and the parameters. 3. Section 4.1: The simulation is conducted under a joint Gaussian model. Therefore, ICA should be identical with PCA, and can be removed from the comparisons. Indeed, the ICA curve is almost identical with the PCA curve in Figure 2. 4. In the covariance estimation experiments, negative log likelihood under Gaussian model is used as the performance metric for both stock market data and OpenML datasets. This seems unreasonable since the real data in the experiment may not be Gaussian. For example, there is extensive evidence that stock returns are not Gaussian. Gaussian likelihood also seems unfair as a performance metric, since it may favor methods derived under Gaussian assumptions, like the proposed method. For comparing the results under these real datasets, it might be better to focus on interpretability, or indirect metrics (e.g., portfolio performance for stock return data). 5. The equation below Line 412: the p(z) factor should be removed in the expression for p(x|z). 6. Line 429: It seems we don't need Gaussian assumption to obtain Cov(Z_j, Z_k | X_i) = 0. 7. Line 480: Why do we need to combine with law of total variance to obtain Cov(X_i, X_{l != i} | Z) = 0? 8. Lines 496 and 501: It seems the Z in the denominator should be p(z). 9. The equation below Line 502: I think the '+' sign after \nu_j should be a '-' sign. In the definition of B under Line 503, there should be a '-' sign before \sum_{j=1}^m, and the '-' sign after \nu_j should be a '+' sign. In Line 504, we should have \nu_{X_i|Z} = - B/(2A). Minor comments: 10. The manuscript could be more reader-friendly if the mathematical definitions for H(X), I(X;Y), TC(X), and TC(X|Z) were state (in the supplementary material if no space in the main article). References to these are necessary when following the proofs/derivations. 11. Line 208: black -> block 12. Line 242: 50 real-world datasets -> 51 real-world datasets (according to Line 260 and Table 2) 13. References [7, 25, 29]: gaussian -> Gaussian Update: Thanks to the authors' for the response. A couple minor comments: - Regarding the empirical version of the objective (3), it might be appropriate to put it in the supplementary materials. - Regarding the Gaussian evaluation metric, I think it would be helpful to include the comments as a note in the paper.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
* Does the ResNet in the experiments in section 7.1 share parameters between the residual blocks? If not a potentially further interesting baseline for the would be to compare to a deeper ResNet with parameter sharing as this would seem to be equivalent to an ODE net with a fixed time-step Euler integrator.,NIPS_2018_606,NIPS_2018,"Although the adjoint sensitivity method is an existing method, exposing this method to machine learning and computational statistics communities where, as far as I am aware it is not widely known about, is a worthwile contribution of this submission its own right. Given the ever increasing importance of AD in both communities, adding to the range of scientific computing primitives for which frameworks such as autograd can efficiently compute derivatives through will hopefully spur more widespread use of gradient based learning and inference methods with ODE models and hopefully spur other frameworks with AD capability in the community such as Stan, TensorFlow and Pytorch to implement adjoint sensitivity methods. The specific suggested applications of the 'ODE solver modelling primitive' in ODE-Nets, CNFs and L-ODEs are all interesting demonstrations of some of the computational and modelling advantages that come from using a continuous-time ODE mode; formulation, with in particular the memory savings possible by avoiding the need to compute all intermediate states by recomputing trajectories backwards through time being a possible major gain given that device memory is often currently a bottleneck. While 'reversing' the integration to recompute the reverse trajectory is an appealing idea, it would have helped to have more discussion of when this would be expected to breakdown - for example it seems likely that highly chaotic dynamical systems would tend to be problematic as even small errors in the initial backwards steps could soon lead to very large divergences in the reversed trajectories compared to the forward ones. It seems like a useful sanity check in an implementation would be to compare the final state of the reversed trajectory to the initial state of the forward trajectory to check how closely they agree. The submission is generally very well written and presented with a clear expository style, with useful illustrative examples given in the experiments to support the claims made and well thought out figures which help to give visual intuitions about the methods and results. There is a lot of interesting commentary and ideas in the submission with there seeming to be a lot of potential in even side notes like the concurrent mixutre of dynamics idea. While this makes for an interesting and thought-provoking read, the content-heavy nature of the paper and slightly rambling exploration of many ideas are perhaps not ideally suited to such a short conference paper format, with the space constraints meaning sacrifices have been made in terms of the depth of discussion of each idea, somewhat terse description of the methods and results in some of the experiments and in some cases quite cramped figure layouts. It might be better to cull some of the content or move it to an appendix to make the main text more focussed and to allow more detailed discussion of the remaining areas. A more significant weakness perhaps is a lack of empirical demonstrations on larger benchmark problems for either the ODE-Nets or CNFs to see how / if the proposed advantages over Res-Nets and NFs respectively carry over to (slightly) more realistic settings, for example using the CNF in a VAE image model on MNIST / CIFAR-10 as in the final experiments in original NF paper. Although I don't think such experiments are vital given the current numerical experiments do provide some validation of the claims already and more pragmatically given that the submission already is quite content heavy already so space is a constaint, some form of larger scale experiments would make a nice addition to an already strong contribution. # Questions * Is the proposal to backward integrate the original ODE to allow access to the (time-reversed) trajectory when solving the adjoint ODE rather than storing the forward trajectory novel or is this the standard approach in implementations of the method? * Does the ResNet in the experiments in section 7.1 share parameters between the residual blocks? If not a potentially further interesting baseline for the would be to compare to a deeper ResNet with parameter sharing as this would seem to be equivalent to an ODE net with a fixed time-step Euler integrator. * What is the definition used for the numerical error on the vertical axis in Figure 4a and how is the 'truth' evaluated? * How much increase in computation time (if any) is there in computing the gradient of a scalar loss based on the output of `odeint` compared to evaluating the scalar loss itself using your Python `autograd` implementation in Appendix C? It seems possible that the multiple sequential calls to the `odeint` function between pairs of successive time points when calculating the gradient may introduce a lot of overhead compared to a single call to `odeint` when calculating the loss itself even if the overall number of inner integration steps is similar? Though even if this is the case this could presumably be overcome with a more efficient implementation it would be interesting to get a ballpark for how quick the gradient calculation is currently. # Minor comments / suggestions / typos: Theorem 1 and proof in appendix B: while the derivation of this result in the appendix is nice to have as an additional intuition for how the expression arises, it seems this might be more succinctly seen as direct implication of the Fokker-Planck equation for a zero noise process or equivalently from Liouville's equation (see e.g. Stochastic Methods 4th ed., Gardiner, pg. 54). Similarly expressing in terms of the time derivative of the density rather than log density would perhaps make the analogy to the standard change of variables formula more explicit. Equation following line 170: missing right parenthesis in integrand of last integral. Algorithm 1: Looks like a notation change might have lead to some inconsistencies - terms subscripted with non-defined $N$: $s_N$, $t_N$, look like they would be more consistent if instead subscripted with 1. Also on first line vector 0 and scalar 0 are swapped in $s_N$ expression References: some incorrect capitalisation in titles and inconsistent used of initials rather than full first names in some references. Appendices L372: 'differentiate by parts' -> 'integrate by parts' L385: Unclear what is meant by 'the function $j(x,\theta,t)$ is unknown - for the standard case of a loss based on a sum of loss terms each depending on state at a finite set of time points, can we not express $j$ as something like $$ j(x,\theta,t) = \sum_{k=1}^K \delta(t_k - t) \ell_k(x, \theta) $$ which we can take partial derivatives of wrt $x$ and then directly subsitute into equation (19)? L394: 'adjoing' -> 'adjoint'","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. Poor motivation (major). The cross-encoder architecture is not ""ignoring cross-entity comparison"". It also ""attends to all candidates at once"" to obtain the final matching scores. Of course, it may be not so fine-grained.",8VK9XXgFHp,EMNLP_2023,"1. Poor figures (minor). Figures in this paper are not clear. I can not obtain the effectiveness of capturing fine-grained cross-entity interaction among candidates in comparison in Figure 1.
2. Poor motivation (major). The cross-encoder architecture is not ""ignoring cross-entity comparison"". It also ""attends to all candidates at once"" to obtain the final matching scores. Of course, it may be not so fine-grained.
3. Poor novelty of methodology (major). The idea of capturing fine-grained cross-entity interaction among candidates has already been proposed in entity linking (ExtEnD: Extractive entity disambiguation).
4. Unfair comparison (major). As far as I know, all previous zero-shot entity linking candidate ranking works use BERT-base parameters rather than RoBERTa-base and top 64 candidates rather than top 56. These may result in unfair comparison in the experimental results.
5. Wrong baseline results (minor). As far as I know, the results in BLINK and E-repeat are Micro Acc. rather than Macro Acc.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).",NIPS_2017_53,NIPS_2017,"Weakness
1. When discussing related work it is crucial to mention related work on modular networks for VQA such as [A], otherwise the introduction right now seems to paint a picture that no one does modular architectures for VQA.
2. Given that the paper uses a billinear layer to combine representations, it should mention in related work the rich line of work in VQA, starting with [B] which uses billinear pooling for learning joint question image representations. Right now the manner in which things are presented a novice reader might think this is the first application of billinear operations for question answering (based on reading till the related work section). Billinear pooling is compared to later.
3. L151: Would be interesting to have some sort of a group norm in the final part of the model (g, Fig. 1) to encourage disentanglement further.
4. It is very interesting that the approach does not use an LSTM to encode the question. This is similar to the work on a simple baseline for VQA [C] which also uses a bag of words representation.
5. (*) Sec. 4.2 it is not clear how the question is being used to learn an attention on the image feature since the description under Sec. 4.2 does not match with the equation in the section. Speficially the equation does not have any term for r^q which is the question representation. Would be good to clarify. Also it is not clear what \sigma means in the equation. Does it mean the sigmoid activation? If so, multiplying two sigmoid activations (with the \alpha_v computation seems to do) might be ill conditioned and numerically unstable.
6. (*) Is the object detection based attention being performed on the image or on some convolutional feature map V \in R^{FxWxH}? Would be good to clarify. Is some sort of rescaling done based on the receptive field to figure out which image regions belong correspond to which spatial locations in the feature map?
7. (*) L254: Trimming the questions after the first 10 seems like an odd design choice, especially since the question model is just a bag of words (so it is not expensive to encode longer sequences).
8. L290: it would be good to clarify how the implemented billinear layer is different from other approaches which do billinear pooling. Is the major difference the dimensionality of embeddings? How is the billinear layer swapped out with the hadarmard product and MCB approaches? Is the compression of the representations using Equation. (3) still done in this case?
Minor Points:
- L122: Assuming that we are multiplying in equation (1) by a dense projection matrix, it is unclear how the resulting matrix is expected to be sparse (arenât we mutliplying by a nicely-conditioned matrix to make sure everything is dense?).
- Likewise, unclear why the attended image should be sparse. I can see this would happen if we did attention after the ReLU but if sparsity is an issue why not do it after the ReLU?
Perliminary Evaluation
The paper is a really nice contribution towards leveraging traditional vision tasks for visual question answering. Major points and clarifications for the rebuttal are marked with a (*).
[A] Andreas, Jacob, Marcus Rohrbach, Trevor Darrell, and Dan Klein. 2015. âNeural Module Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1511.02799.
[B] Fukui, Akira, Dong Huk Park, Daylen Yang, Anna Rohrbach, Trevor Darrell, and Marcus Rohrbach. 2016. âMultimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1606.01847.
[C] Zhou, Bolei, Yuandong Tian, Sainbayar Sukhbaatar, Arthur Szlam, and Rob Fergus. 2015. âSimple Baseline for Visual Question Answering.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1512.02167.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1. This work uses an antiquated GNN model and method, it seriously impacts the performance of this framework. The baseline algorithms/methods are also antiquated.",NIPS_2022_69,NIPS_2022,"1. This work uses an antiquated GNN model and method, it seriously impacts the performance of this framework. The baseline algorithms/methods are also antiquated. 2. The experimental results did not show that this work model obviously outperforms other variant comparison algorithms/models. 3. The innovations of network architecture design and constraint embedding are rather limited.
The authors discussed that the performance is limited by the performance of the oracle expert.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Figure 1: It's unclear how the proposed method produces this type of explanation (which says ""mutagens contain the NO2 group""). This seems like it requires ""additional ad-hoc post-analysis ... to extract the shared motifs to explain a set of instances"" [Line 48]. Perhaps this analysis is easier with the proposed method, but it still seems necessary.",NIPS_2020_1645,NIPS_2020,"It's not clear that this is a global method and calling it this causes some confusion. While each explanation is given by the output of a single learned model rather solved for independently with its own optimization problem (which makes it more ""global"" in a sense), the explanations are still fundamentally local because they apply to a single node/graph. Some of consequences of this are: - Figure 1: It's unclear how the proposed method produces this type of explanation (which says ""mutagens contain the NO2 group""). This seems like it requires ""additional ad-hoc post-analysis ... to extract the shared motifs to explain a set of instances"" [Line 48]. Perhaps this analysis is easier with the proposed method, but it still seems necessary. - Paragraph Starting in Line 185: This reads as if global explanations are strictly better than local explanations when they actually solve different problems. The reviewer believes this is trying to say something along the lines of ""having a single (global) model that produces each explanation has advantages over solving an optimization problem independently for each explanation"". - Line 236-238: It is unclear what metric is going to be used to compare local and global explanations. Generally, this comparison is challenging because local and global explanation solve different problems and (usually) do better at their respective problems and associated metrics. Given the metric described in the Paragraph Starting in Line 294, it seems like a local (per point) metric is used for both GNNExplainer and the proposed method.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- One experiment to estimates the quality of uncertainty estimates measures how often the true feature importance lies within a 95% credible interval. However, the experiments uses pseudo feature importance because no true feature importance is available. The correctness of the pseudo feature importance relies on Prop 3.2 and a large enough perturbation value to be chosen. This makes it difficult to judge to what degree the experiment can be trusted because the difference between the tested method and the pseudo feature importance is only the number of perturbations. The experiment could be strengthened in two ways.",NIPS_2021_422,NIPS_2021,"Experimental results leave some questions open, i.e.: - One experiment to estimates the quality of uncertainty estimates measures how often the true feature importance lies within a 95% credible interval. However, the experiments uses pseudo feature importance because no true feature importance is available. The correctness of the pseudo feature importance relies on Prop 3.2 and a large enough perturbation value to be chosen. This makes it difficult to judge to what degree the experiment can be trusted because the difference between the tested method and the pseudo feature importance is only the number of perturbations. The experiment could be strengthened in two ways. 1. set up a (toy) dataset where true feature importance is clearly defined. 2. What are the results when choosing BayesSHAP N=10k perturbations vs BayesLIME N=10k? These should be nearly identica; otherwise the assumption of the pseudo feature importance being (nearly) equal to true feature importance is compromised. Results could also be reported for BayesSHAP N=100 vs BayesLime N=10k and vice versa. - Correctness of Estimated Number of Perturbations: How can you be sure that G doesn’t simply overestimate? Maybe a value a lot less than G would have been sufficient? - Human Evaluation Experiment: Users were asked to guess a number from an image where the explanation was masked. Figure 8 indicates that each user was given30 such images and line 230 suggests that all 30 images where a masked version of number “4”. This seems like an unbalanced setup which make it difficult to determine the meaningfulness of this experiment. Questions
User Study: Why did you decide to erase the explanations and measure failure to guess the correct image rather than keeping just the explanations and measuring success of guessing the correct image?
B.1: Why is it okay to make these assumptions?
Theorem 3.3: How do we know that S is large enough?
After Authors' Response All my questions and weakness concerns were appropriately addressed, therefore I raised my score.
A ""Broader Impacts and Limitations"" section could be added that discusses potential dangers of trusting explanations. It could for example discuss that the choice of N is important or that explanations can still be wrong.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties.",NIPS_2018_641,NIPS_2018,"weakness. First, the main result, Corollary 10, is not very strong. It is asymptotic, and requires the iterates to lie in a ""good"" set of regular parameters; the condition on the iterates was not checked. Corollary 10 only requires a lower bound on the regularization parameter; however, if the parameter is set too large such that the regularization term is dominating, then the output will be statistically meaningless. Second, there is an obvious gap between the interpretation and what has been proved. Even if Corollary 10 holds under more general and acceptable conditions, it only says that uncertainty sampling iterates along the descent directions of the expected 0-1 loss. I don't think that one may claim that uncertainty sampling is SGD merely based on Corollary 10. Furthermore, existing results for SGD require some regularity conditions on the objective function, and the learning rate should be chosen properly with respect to the conditions; as the conditions were not checked for the expected 0-1 loss and the ""learning rate"" in uncertainty sampling was not specified, it seems not very rigorous to explain empirical observations based on existing results of SGD. The paper is overall well-structured. I appreciate the authors' trying providing some intuitive explanations of the proofs, though there are some over-simplifications in my view. The writing looks very hasty; there are many typos and minor grammar mistakes. I would say that this work is a good starting point for an interesting research direction, but currently not very sufficient for publication. Other comments: 1. ln. 52: Not all convex programs can be efficiently solved. See, e.g. ""Gradient methods for minimizing composite functions"" by Yu. Nesterov. 2. ln. 55: I don't see why the regularized empirical risk minimizer will converge to the risk minimizer without any condition on, for example, the regularization parameter. 3. ln. 180--182: Corollar 10 only shows that uncertainty sampling moves in descent directions of the expected 0-1 loss; this does not necessarily mean that uncertainty sampling is not minimizing the expected convex surrogate. 4. ln. 182--184: Non-convexity may not be an issue for the SGD to converge, if the function Z has some good properties. 5. The proofs in the supplementary material are too terse.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2) the authors did not compare any models other than GPT2. Several sections of the paper read confusing to me. There is a missing citation / reference in Line 99, section 3.1. The notation \hat{D}(c) from Line 165, section 3.4 is unreferenced. The authors made great effort to acknowledge the limitations of their work.",NIPS_2021_725,NIPS_2021,"Comparing the occupational statistics computed by GPT2 vs those by the United States is very interesting and informative. However, the presentation on the methodology and the subsequent discussion is confusing to me. Particularly from section 3.4, I am not sure what “adj.” in equation (1) means and why “adj. Pred” is appropriate as a scaling factor. Would appreciate it if the authors could clarify and make this section clearer.
The analysis of intersection effects is interesting but I fail to see a clear presentation on statistical significance of these results. It may be clearer if the authors could specify p-values on some regressors and offer some discussions. From Table 3, I also do not believe that average pseudo-R2 is necessarily a meaningful measure for the individual factor.
The authors claim the contribution of “benchmarking the extent of bias relative to inherently skewed societal distributions of occupation associations”. However, I have some reservations as 1) the authors did not propose any quantitative measurement to the extent of occupation bias relative to real distributions in society; 2) the authors did not compare any models other than GPT2.
Several sections of the paper read confusing to me. There is a missing citation / reference in Line 99, section 3.1. The notation \hat{D}(c) from Line 165, section 3.4 is unreferenced.
The authors made great effort to acknowledge the limitations of their work.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- What is the meaning of ""$:\sigma_t^2=\alpha_t-\alpha_t^2$"" in Equation (2)?",Jszf4et48m,ICLR_2025,"1. **Presentation of this work requires thorough improvements.**.
- The authors should use `\citep` for most cases in the manuscript, which places the authors' names and the year in parentheses. The current use of `\cite` makes the manuscript cluttered and difficult to read.
- The manuscripts contain many incoherent parts. For instance, in Lines 474-479, the authors state: ""we used x_0 ... and employed LoRA fine-tuning for efficiency. **But** these two variants are crucial to make it work"". The use of the transitional word ""but"" in this context is confusing. Later, the authors assert that ""we **must** fine-tune the entire model instead of using LoRA"". Then why is LoRA fine-tuning described as crucial if it is not the recommended approach?
- Notations are inconsistent in the manuscript. For example, in Equation (2), $x_t=\alpha_t x_0^i+(1-\alpha_t)y^i+\sigma_t^2 \epsilon_t$, whereas in Equation (6)~(16), $x_t=(1-\alpha_t) x_0+\alpha_t y+\sigma_t \epsilon_t$.
- What is the meaning of ""$:\sigma_t^2=\alpha_t-\alpha_t^2$"" in Equation (2)?
2. **Many statements in this work are incorrect or overclaimed**.
- In the abstract: ""surpassing Stable-Diffusion (LDM) performance"". However, all experiments conducted in the main content are compared with the LDM trained on small datasets, which are NOT Stable Diffusion (v1/v2/v3).
- The training objective of LDM or Stable Diffusion is the noise $\epsilon$. However, $\epsilon$ is NOT $x_t-x_0$ in LDM as stated in Line 266 and Table 4.
- In Line 507-509, by the SNR definition in this work, SNR does not tend to 0 as $t \rightarrow T$, as the concatenated condition $y$ also provides some signals to a certain extent similar to the proposed framework even at $t=T$. SNR should NOT be the reason that distinguishes between different conditioning methods (Concatenation v.s. Schrödinger Bridge).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"6: there are two lines in red that should be in green SuppMat, L502: ϵ θ --> z θ SuppMat,L507: (4) --> Table 4 SuppMat, L509: (1) --> Algorithm 1",NIPS_2022_789,NIPS_2022,"(-) It would be nice to show and discuss failure cases, or situations when the proposed approach does not outperform the others.
Minor comments:
table X, figure Y, section Z, etc. --> Table X, Figure Y, Section Z, etc.
Eq. 3: x t --> x ( t )
Fix punctuation at the end of Eqs. 6 and 9 L71: R n --> R m
L76: utilize
L77: rely
Eqs. 13 and 14: for consistency, write them in the discrete setting
L144: uses
L154: recall the definition of p 0
(it was only defined in Section 2)
SuppMat, L465-466: check the sentence
SuppMat, Fig. 6: there are two lines in red that should be in green
SuppMat, L502: ϵ θ --> z θ
SuppMat,L507: (4) --> Table 4
SuppMat, L509: (1) --> Algorithm 1","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
1. Minor - I think a much more comprehensive and data-intensive analysis would improve this paper significantly but since it is a short paper this isn't a strong negative against what has been done by the authors.,NuMemgzPYT,EMNLP_2023,"1. Minor - I think a much more comprehensive and data-intensive analysis would improve this paper significantly but since it is a short paper this isn't a strong negative against what has been done by the authors.
2. I am unsure about the technical novelty of the approach - the paper appears to be simply doing prompt engineering on ChatGPT to meet an end goal. I like the domain and the framework but the experimental results, while positive, show the strength of ChatGPT (LLMs) in performing qualitative analysis but not the author's claim of reducing human burdens of performing TA.
3. Minor - Need to have at least one qualitative analysis of how the machine coder and human coder are selecting similar or different ""codes"" needed for completeness.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
3: The experimental settings are not mentioned properly; result reproducibility is critical using the provided information. The author does not provide the code.,ICLR_2021_2821,ICLR_2021,"weakness: 1: AdpCLR_pre looks intuitive since it uses a pre-trained self-supervised model (simCLR); therefore, we can get a high-quality similarity measure in the pair of image embedding. While in the AdpCLR_full author mentioned that no pre-trained model is used then only we can ensure that P_same is the correct pair and rest of the top-K pair we can not guarantee anything. Because the similarity measure on any other embedding vector will not be of high quality and incorrect pair will degrade the result more compared to the correct one. How will you get the correct top-K positive pair in the AdpCLR_full setup? Maybe I am missing something, or the proper explanation is not provided. In the Algorithm, if pretrain=False the encoder F will be random and it will provide poor embedding, hence we expect poor similarity measure. Please explain the process of embedding in the case of AdpCLR_full if no pre-trained is used.
2: I believe the convergence and generalization should be a function of the number of incorrect positive pair. In the generalization error, we can see that it is a function of K, but at the same time, the error will depend on the expected error in the positive pair. It is counter-intuitive AdpCLR_full convergence does not have any dependency on the correct positive pair or the positive pair. If we have few/may incorrect positive pair the learning will be too difficult, and convergence will slow down, or it will oscillate and will not converge at all. Please provide a detail explanation.
3: The experimental settings are not mentioned properly; result reproducibility is critical using the provided information. The author does not provide the code.
4: can you provide the result for the AdpCLR_full approach for the ResNet-50 (1x,2x,4x) architecture like the result provided for AdpCLR_pre in the table-1.
5: How the pseudo label is defined?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Authors mention in the abstract and other times that ""online learning formulation overlooks key practical considerations. A proper comparison (in evaluation results) against online learning approaches is missing (as well as against RL, mentioned below). In such way, it would be clear why online learning cannot be used. Is it because it doesn't consider retraining cost? How can we compare retraining cost (of the whole model) with the incremental updates applied in online learning? Why is it discarded? What are the challenges of including it in the evaluation?",iGX0lwpUYj,ICLR_2025,"- Very important: It is not mentioned in the title or main parts of the work (Abstract) that the work focuses on classification only. It is important, as there is no discussion or result on how this solution could work for regression tasks. Please make sure you explicitly mention this, including a possible modification of title.
- It is not clear how the solution solves some of the 3 challenges mentioned in the abstract. A reader would think that the challenges highlighted in the abstract would be solved or at least considered in the work. However, it is not clear how, for example, the solution delas with ""1) very limited information"". While some results are provided in the appendix, it would be appreciated if the evaluation of how this challenge is handled by the solution is given more importance and mentioned in the main text. For example, how the performance of the proposed solution and the baselines evolves as the amount of the available information decreases would be really helpful.
- Among the limitations mentioned for the most important baseline (CARA), it is said that CARA assumes that ""the difficulty of the task remains constant"". But it seems that the proposed solution also considers such aspect. The authors fix the complexity of retraining for any time, the number of samples, etc, so the difficulty is not modified throughout the experiment. We can see this in the text between (2) and (3), where the temporal dependance of the retraining cost c, cost loss e, and number of predictions N is dropped.
- Authors mention in the abstract and other times that ""online learning formulation overlooks key practical considerations. A proper comparison (in evaluation results) against online learning approaches is missing (as well as against RL, mentioned below). In such way, it would be clear why online learning cannot be used. Is it because it doesn't consider retraining cost? How can we compare retraining cost (of the whole model) with the incremental updates applied in online learning? Why is it discarded? What are the challenges of including it in the evaluation?
- IMPORTANT: Problems with mathematical notation: Some terms are not defined, some terms are called in different ways, and some mathematical assumptions are not well supported. ... The detailed cases are provided in the following box ""questions"".
- Proposition 3.1 is interesting. However, it is difficult to fully grasp its significance because the result is not shown in the Results Section. Prop. 3.1 provides a bound but we do not know if this bound is tight or so loose that it is meaningless. Some results on the values provided by the bound and the practical values obtained by the algorithms would be extremely appreciated.
- Lack of complexity comparison. The complexity of the algorithm and the corresponding comparison with the baselines is missing. For example, while CARA is evaluated and compared against the proposed algorithm, it is several times mentioned that CARA is computationally intensive (cf. 8.1). Yet, we do not know if the proposed algorithm is similarly demanding or it offers significant improvement in terms of complexity reduction. A quantitative analysis of the complexity of the different solutions would be welcome to strengthen the contribution of the paper.
- While the comparison with Reinforcement Learning (RL) is discussed in the main document and the appendix, highlighting that RL is not sample efficient and is not impractical, and authors show the analytical correspondence of the proposed formulation and the RL formulation, it would have been welcomed if the authors have provided quantitative proofs of such claims. Evaluating a SotA RL algorithm adapted to the problem considered and measuring its performance with the same datasets and use cases would have provided support to the claim of not considering RL for the problem.
- From appendix 8.5, we can see that the selected dataset to show the main results in Section 6 (e.g., fig. 2) is the best case, where the proposed algorithm clearly performs better than the baselines. This behavior is not so evident for the other datasets in 8.5. For example, In Fig. 5, for the yelp dataset, we can see that the proposed algorithm is not among the best performing solutions, and it is overpassed by several cases, some as simple as adwin.
- The results section have some flaws,
- Why Figure 3 only shows the values of alpha between 0 and 0.1, when previous figures show the range between 0 and 1? Please provide an explanation or extend the range evaluated to match that of the other cases.
- In appendix 8.5, the oracle is not provided. Thus, we miss information about how far are algorithms from the best.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*) 6.,NIPS_2017_182,NIPS_2017,"Weakness
1. Paper misses citing a few relevant recent related works [A], [B], which could also benefit from the proposed technique and use region proposals.
2. Another highly relevant work is [C] which does efficient search for object proposals in a similar manner to this approach building on top of the work of Lampert et.al.[22]
3. It is unclear what SPAT means in Table. 2.
4. How was Fig. 6 b) created? Was it by random sub-sampling of concepts?
5. It would be interesting to consider a baseline which just uses the feature maps (used in the work, say shown in Fig. 2) and the phrases and simply regresses to the target coordinates using an MLP. Is it clear that the proposed approach would outperform it? (*)
6. L130: It was unclear to me how the geometry constraints are exactly implemented in the algorithm, i.e. the exposition of how the term k2 is computed was uncler. It would be great to provide details. Clear explanation of this seems especially important since the performance of the system seems highly dependent on this term (as it is trivial to maximize the sum of scores of say detection heat maps by considering the entire image as the set).
Preliminary Evaluation
The paper has a neat idea which is implemented in a very clean manner, and is easy to read. Concerns important for the rebuttal are marked with (*) above.
[A] Hu, Ronghang, Marcus Rohrbach, Jacob Andreas, Trevor Darrell, and Kate Saenko. 2016. âModeling Relationships in Referential Expressions with Compositional Modular Networks.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1611.09978.
[B] Nagaraja, Varun K., Vlad I. Morariu, and Larry S. Davis. 2016. âModeling Context Between Objects for Referring Expression Understanding.â arXiv [cs.CV]. arXiv. http://arxiv.org/abs/1608.00525.
[C] Sun, Qing, and Dhruv Batra. 2015. âSubmodBoxes: Near-Optimal Search for a Set of Diverse Object Proposals.â In Advances in Neural Information Processing Systems 28, edited by C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, 1378â86. Curran Associates, Inc.","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,
"- There is a deeper tie to meta-learning, which has several approaches as well. While these works don't target continual learning directly, they should be cited and the authors should try to distinguish those approaches. The work on RL for architecture search and/or as optimizers for learning (which are already cited) should be more heavily linked to this work, as it seems to directly follow as an application to continual learning.",NIPS_2018_83,NIPS_2018,"- An argument against DEN, a competitor, is hyper-parameter sensitivity. First, this isn't really shown, but second (and more importantly) reinforcement learning is well-known to be extremely unstable and require a great deal of tuning. For example, even random seed changes are known to change the behavior of the same algorithm, and different implementation of the same algorithm can get very different results (this has been heavily discussed in the community; see keynote ICLR talk by Joelle Pineau as an example). This is not to say the proposed method doesn't have an advantage, but the argument that other methods require more tuning is not shown or consistent with known characteristics of RL. * Related to this, I am not sure I understand experiments for Figure 3. The authors say they vary the hyper-parameters but then show results with respect to # of parameters. Is that # of parameters of the final models at each timestep? Isn't that just varying one hyperparameter? I am not sure how this shows that RCL is more stable. - Newer approaches such as FearNet [1] should be compared to, as they demonstrated significant improvement in performance (although they did not compare to all of the methods compared to here). [1] FearNet: Brain-Inspired Model for Incremental Learning, Ronald Kemker, Christopher Kanan, ICLR 2018. - There is a deeper tie to meta-learning, which has several approaches as well. While these works don't target continual learning directly, they should be cited and the authors should try to distinguish those approaches. The work on RL for architecture search and/or as optimizers for learning (which are already cited) should be more heavily linked to this work, as it seems to directly follow as an application to continual learning. - It seems to me that continuously adding capacity while not fine-tuning the underlying features (which training of task 1 will determine) is extremely limiting. If the task is too different and the underlying feature space in the early layers are not appropriate to new tasks, then the method will never be able to overcome the performance gap. Perhaps the authors can comment on this. - Please review the language in the paper and fix typos/grammatical issues; a few examples: * [1] ""have limitation to solve"" => ""are limited in their ability to solve"" * [18] ""In deep learning community"" => ""In THE deep learning community"" * [24] ""incrementally matche"" => ""incrementally MATCH"" * [118] ""we have already known"" => ""we already know"" * and so on Some more specific comments/questions: - This sentence is confusing [93-95] ""After we have trained the model for task t, we memorize each newly added filter by the shape of every layer to prevent the caused semantic drift."" I believe I understood it after re-reading it and the subsequent sentences but it is not immediately obvious what is meant. - [218] Please use more objective terms than remarkable: ""and remarkable accuracy improvement with same size of networks"". Looking at the axes, which are rather squished, the improvement is definitely there but it would be difficult to characterize it as remarkable. - The symbols in the graphs across the conditions/algorithms is sometimes hard to distinguish (e.g. + vs *). Please make the graphs more readable in that regard. Overall, the idea of using reinforcement learning for continual learning is an interesting one, and one that makes sense considering recent advances in architecture search using RL. However, this paper could be strengthened by 1) Strengthening the analysis in terms of the claims made, especially with respect to not requiring as much hyper-parameter tuning, which requires more evidence given that RL often does require significant tuning, and 2) comparison to more recent methods and demonstration of more challenging continual learning setups where tasks can differ more widely. It would be good to have more in-depth analysis of the trade-offs between three approaches (regularization of large-capacity networks, growing networks, and meta-learning). ============================================== Update after rebuttal: Thank you for the rebuttal. However, there wasn't much new information in the rebuttal to change the overall conclusions. In terms of hyper-parameters, there are actually more hyper-parameters for reinforcement learning that you are not mentioning (gamma, learning rate, etc.) which your algorithm might still be sensitive to. You cannot consider only the hyper-parameter related to the continual learning part. Given this and the other limitations mentioned, overall this paper is marginally above acceptance so the score has been kept the same.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- Lack of lexical / syntactic diversity of teacher feedback: assuming the teacher feedback was auto-generated, do you intend to turk the teacher feedback and / or generate a few different kinds of feedback (which is more real-life situation)?",NIPS_2016_93,NIPS_2016,"/ Major concerns: - It is difficult to evaluate whether the MovieQA result should be considered significant given that +10% gap exists between MemN2N on dataset with explicit answers (Task 1) and RBI + FP on dataset with other forms of supervision, especially Task 3. If I understood correctly, the different tasks are coming from the same data, but authors provide different forms of supervision. Also, Task 3 gives full supervision of the answers. Then I wonder why RBI + FP on task 3 (69%) is doing much worse than MemN2N on task 1 (80%). Is it because the supervision is presented in a more implicit way (""No, the answer is kitchen"" instead of ""kitchen"")? - For RBI, they only train on rewarded actions. Then this means rewardless actions that get useful supervision (such as ""No, the answer is Timothy Dalton."" in Task 3) is ignored as well. I think this could be one significant factor that makes FP + RBI better than RBI alone. If not, I think the authors should provide stronger baseline than RBI (that is supervised by such feedback) to prove the usefulness of FP. Questions / Minor concerns: - For bAbI, it seems the model was only tested on single supporting fact dataset (Task 1 of bAbI). How about other tasks? - How is dialog dataset obtained from QA datasets? Are you using a few simple rules? - Lack of lexical / syntactic diversity of teacher feedback: assuming the teacher feedback was auto-generated, do you intend to turk the teacher feedback and / or generate a few different kinds of feedback (which is more real-life situation)? - How does other models than MemN2N do on MovieQA?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
1. The main text should make it more clear that there are additional experiments in the supplement (and preferably summarize their results). Questions:,NIPS_2019_390,NIPS_2019,"1. The distinction between modeling uncertainty about the Q-values and modeling stochasticity of the reward (lines 119-121) makes some sense philosophically but the text should make clearer the practical distinction between this and distributional reinforcement learning. 2. It is not explained (Section 5) why the modifications made in Definition 5.1 aren't important in practice. 3. The Atari game result (Section 7.2) is limited to a single game and a single baseline. It is very hard to interpret this. Less major weaknesses: 1. The main text should make it more clear that there are additional experiments in the supplement (and preferably summarize their results). Questions: 1. You define a modified TD learning algorithm in Definition 5.1, for the purposes of theoretical analysis. Why should we use the original proposal (Algorithm 1) over this modified learning algorithm in practice? 2. Does this idea of propagating uncertainty not naturally combine with that of distributional RL, in that stochasticity of the reward might contribute to uncertainty about the Q-value? Typos, etc.: * Line 124, ""... when experimenting a transition ..."" ---- UPDATE: After reading the rebuttal, I have raised my score. I appreciate that the authors have included additional experiments and have explained further the difference between Definition 5.1 and the algorithm used in practice, as well as the distinction between the current work and distributional RL. I hope that all three of these additions will make their way into the final paper.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3). Important references are missing. The GFF[1] and EfficientFCN[2] both aims to implement the fast semantic segmentation method in the encode-decoder architecture. I encourage the authors to have a comprehensive comparison with these work. [1]. Gated Fully Fusion for Semantic Segmentation, AAAI'20. [2]. EfficientFCN: Holistically-guided Decoding for Semantic Segmentation, ECCV'20. See above. The societal impact is shown one the last page of the manuscript.",NIPS_2021_2247,NIPS_2021,"1). Lack of speed analysis, the experiments have compared GFLOPs of different segmentation networks. However, there is no comparisons of inference speed between the proposed network and prior work. The improvement on inference speed will be more interesting than reducing FLOPs. 2). For the detail of the proposed NRD, it is reasonable that the guidance maps are generated from the low-level feature maps. And the guidance maps can be predicted from the the first-stage feature maps or the second-stage feature maps. It is better to provide one ablation study about the effect for them. 3). Important references are missing. The GFF[1] and EfficientFCN[2] both aims to implement the fast semantic segmentation method in the encode-decoder architecture. I encourage the authors to have a comprehensive comparison with these work.
[1]. Gated Fully Fusion for Semantic Segmentation, AAAI'20. [2]. EfficientFCN: Holistically-guided Decoding for Semantic Segmentation, ECCV'20.
See above. The societal impact is shown one the last page of the manuscript.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
2. The slight improvement （ ChatGPT vs. ChatGPT+DSP in Table 6 and Table 7）can not support the claim “Experimental results across various language pairs and domains proved the effectiveness of our proposed prompts.”,fxdvWG4rJe,EMNLP_2023,"1. The contributions and novelty of the work are limited. After reading the paper, I am trying to figure out the research question, which may be a better translation prompt. The paper is more like a product manual telling me how to use ChatGPT for translation.
2. The slight improvement （ ChatGPT vs. ChatGPT+DSP in Table 6 and Table 7）can not support the claim “Experimental results across various language pairs and domains proved the effectiveness of our proposed prompts.”","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf'], 'labels': ['0', '1']}",,
"2 It is also recommended to compare the performance with ""Multilingual unsupervised neural machine translation with denoising adapters. "" and ""MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer"", which trains adapters on top of a well trained multilingual pretrained model.",u9Fvsy8Brx,EMNLP_2023,"1 The technical contribution is incremental. The model architecture is very similar to that of ""Multilingual unsupervised neural machine translation with denoising adapters. "" However, the authors fail to talk about the relations and differences. However, it is interesting to see the performance of such model on NLU and other NLG tasks.
2 It is also recommended to compare the performance with ""Multilingual unsupervised neural machine translation with denoising adapters. "" and ""MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer"", which trains adapters on top of a well trained multilingual pretrained model.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. The Cycle FC align features at different spatial locations to the same channel, but analysis is slightly insufficient. There could be many different designs of it. For example, the experiments or analysis with different sampling intervals and sample size.",ICLR_2022_50,ICLR_2022,"1. The Cycle FC align features at different spatial locations to the same channel, but analysis is slightly insufficient. There could be many different designs of it. For example, the experiments or analysis with different sampling intervals and sample size. 2. CycleMLP is slightly insufficient in discussion of the design and ablation studies.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- I am concerned about the practical performance of the proposed algorithms. Specifically, Algorithm 1 uses Vandermonde matrix, which is known to be very ill-conditioned and numerically unstable, especially for large $n$ and small $\alpha$. Since $\alpha$ can be as small as $1/k^2L^2$, I am worried that the algorithm may fail in practice even for moderate size $n$, $k$, and $L$. The paper is more convincing if the authors could provide experimental results for the proposed algorithms.",NIPS_2019_945,NIPS_2019,"Weakness: - I am concerned about the practical performance of the proposed algorithms. Specifically, Algorithm 1 uses Vandermonde matrix, which is known to be very ill-conditioned and numerically unstable, especially for large $n$ and small $\alpha$. Since $\alpha$ can be as small as $1/k^2L^2$, I am worried that the algorithm may fail in practice even for moderate size $n$, $k$, and $L$. The paper is more convincing if the authors could provide experimental results for the proposed algorithms. - Section 3.2 (which is claimed to be the main contribution of this paper) skips a lot of the technical details, making it hard for readers to fully understand Algorithm 3. For example, what is the intuition behind the three constants defined in Line 259-261; how do we know when a triplet is good or not (Line 268-270) in practice; the alignment step (Line 274-280) is difficult to follow. I would suggest moving the pseudocode to the appendix, and explain the algorithm in more details in the paper. - No time and space complexity is provided for the proposed algorithm. Minor: In the abstract the author briefly mention the connection between their algorithm and error correcting codes. It would be better to describe that connection in more detail when presenting the algorithm. -----------------After reading the rebuttal------------------- Thanks for the clarification on distinguishing between good or bad triplets. I understand that this work is mainly theoretical and would still appreciate it if small empirical results (or even toy examples) can be added to help evaluate the algorithm in practice. Section 3.2 (which is a major contribution of this paper) skips a lot of the technical details. It would be better if the authors can explain the denoising and alignment steps in more details. For example, move Lemma 18 from the appendix to the main paper so that the readers can understand how Algorithm 3 matches two good query triplets. Overall Section 2 and 3.1 are well-written. The theoretical results are new and interesting. I will increase my score.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- No standard deviations are displayed. Thus, we are not sure if the best method is really the best or if many other RF configurations have performances close to the best one.",dD6b5RREws,ICLR_2025,"- Several related works are missing. Besides, it is not clear from the related work section which contributions are theoretical or practical. See for example:
- For empirical performances, see Section 4 in Random Forests for Big Data https://arxiv.org/abs/1511.08327
- For theoretical results quantifying the uncertainty of random forests, depending on BR, see https://jmlr.org/papers/volume17/14-168/14-168.pdf or https://arxiv.org/pdf/1405.0352
- In https://www.esaim-ps.org/articles/ps/pdf/2018/01/ps170099.pdf, the authors provide the expression of the optimal subsampling rate for median forests
- Regarding Breiman's forests, the analysis of https://arxiv.org/pdf/2006.06998 takes into account bootstrap
- Experiments are done for classification data sets only. The paper would be strengthened if experiments were done also for regression problems.
I have several concerns related to the design of experiments, and more specifically with Table 1:
- No standard deviations are displayed. Thus, we are not sure if the best method is really the best or if many other RF configurations have performances close to the best one.
- Looking at Table 2 in Martinez-Munoz and Suarez (2010), which displays the average test error for different BR ratios, the standard deviations are quite important. For example, the standard deviation of the error (and thus the accuracy) is between 7 and 8 for the audio data set, whereas you display the accuracy with 3 decimals.
- Besides, the test set is used for two different objectives: selecting the best RF configuration and computing its error. Thus, the error of the best RF configuration is rather optimistic. It would be more sound to evaluate the performance on a different data set (or to use the out-of-bag error of the forest). Thus, t-test results do not seem conclusive. Besides, as you consider several t-test, you should probably correct the significance level $\alpha$ accordingly, taking into account multiple testing issues.
- Given the performances displayed in Table 1, I would like to see the same experiment run with a Baseline RF whose number of trees is set to 500. Indeed, this parameter seems to be the most important one, whereas in practice it is fairly easy to tune (the higher the better in terms of accuracy).
- A Table with all values of RF configuration / BR rate with standard deviations would give more precise information.
- l.335 The fact that optimal performances are reached for two very different values of BR (0.2 and 5) by slightly changing the data set is worrying. One reason can be that the two values are good for the two data sets, which is in contradiction with your study, showing that BR >1 is better than the opposite. This is why metrics for all configurations with standard deviations should be displayed.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Your setting is very specific: you need to know the model or/and have access to a generative model (for expanding or generating trajectories), the problem should be episodic and the reward should be given just at the end of a task (i.e., reaching the target goal). Can you extend this approach to more general settings?",NIPS_2018_296,NIPS_2018,"weakness of the proposed approach. Model-based algorithms (LevinTS is model-based) for planning do not have such requirements. On the other hand, if the goal is to refine a policy at the end of some optimization procedure I understand the choice of using a policy-guided heuristic. - Concerning LubyTS it is hard to quantify the meaning of the bound in Thm. 6 (the easy part is to see when it fails, as mentioned before). There are other approaches based on generative models with guarantees, e.g., (Grill, Valko, Munos. Blazing the trails before beating the path: Sample-efficient Monte-Carlo planning, NIPS 2016). How do you perform compared to them? - Your setting is very specific: you need to know the model or/and have access to a generative model (for expanding or generating trajectories), the problem should be episodic and the reward should be given just at the end of a task (i.e., reaching the target goal). Can you extend this approach to more general settings? - In this paper, you perform planning offline since you use a model-based approach (e.g., generative model). Is it possible to remove the assumption of the knowledge of the model? In that case, you would have to interact with the environment trying to minimize the number of times a ""bad"" state is visited. Since the expansion of a node can be seen as an exploratory step, this approach seems to be related to the exploration-exploitation dilemma. Bounding the number of expansions does not correspond to having small regret in general settings. Can you integrate a concept related to the long-term performance in the search strategy? This is what is often done in MCTS. All the proposed approaches have weaknesses that are partially acknowledged by the authors. A few points in the paper can be discussed in more details and clarified but I believe it is a nice contribution overall. -------- after feedback I thank you for the feedback. In particular, I appreciated the way you addressed the zero probability issue. I think that this is a relevant aspect of the proposed approaches and should be addressed in the paper. Another topic should be added to the paper is the comparison with Trailblazer and other MCTS algorithms. Despite that, I personally believe that the main limitation is the assumption of deterministic transitions. While the approach can be extended to stochastic (known) models I would like to know how the theoretical results become.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- Line 201: the dimensionality of each region is 512: using which feature extractor?,NIPS_2018_553,NIPS_2018,"- This paper misses a few details in model design and experiments: A major issue is the ""GTA"" / ""DET"" feature representation in Table 1. As stated in section 4.1, image regions are extracted from ground-truth / detection methods. But what is the feature extractor used on top of those image regions? Comparing resnet / densenet extracted features with vgg / googlenet feature is not fair. - The presentation of this paper can be further improved. E.g. paragraph 2 in intro section is a bit verbose. Also breaking down overly-long sentences into shorter but concise ones will improve fluency. Some additional comments: - Figure 3: class semantic feature should be labeled as ""s"" instead of ""c""? - equation 1: how v_G is fused from V_I? please specify. - equation 5: s is coming from textual representations (attribute / word to vec / PCA'ed TFIDF). It might have positive / negative values? However the first term h(W_{G,S}, v_G) is post ReLU and can only be non-negative? - line 157: the refined region vector is basically u_i = (1 + attention_weight) * v_i. since attention weight is in [0, 1] and sums up to 1 for all image regions. this refined vector would only scales most important regions by a factor of two before global pooling? Would having a scaling variable before attention weight help? - line 170: class semantic information is [not directly] embedded into the network? - Equation 11: v_s and u_G are both outputs from trained-network, and they are not normalized? So minimize L-2 loss could be simply reducing the magnitude of both vectors? - Line 201: the dimensionality of each region is 512: using which feature extractor? - Section 4.2.2: comparing number of attention layers is a good experiment. Another baseline could be not using Loss_G? So attention is only guided by global feature vector. - Table 4: what are the visual / textual representations used in each method? otherwise it is unclear whether the end-to-end performance gain is due to proposed attention model.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
• It would be helpful for readers if computation/algorithm/implementation details are given.,ICLR_2022_2436,ICLR_2022,"• The motivation of using DAG is unclear. DAG is a special case of directed cyclic graphs (DCG). One important advantage of DAG over DCG is its simple factorization, which leads to more efficient computation than DCG. With the NOTEARS framework, this advantage seems largely lost because during the training, at least in the early phase, the graphs may be not be acyclic. Then why one even makes an often unrealistic assumption of acyclicity to begin with? In a typical run of the proposed algorithm, is the DAG factorization ever exploited? If it is exploited in iterations when the graph is acyclic, then typically in how many iterations the graph is acyclic? In summary, acyclicity is, in my opinion, a modeling restriction, not a feature. Is the proposed algorithm faster than the same algorithm without the h(A) penalty (which is both simpler computationally and more general in terms of modeling)?
• The proposed approach seems doing very well in various simulations but doesn’t do well in the real data, which questions the real practical advantage of the proposed method. Perhaps including more real data analyses/comparison can help support it.
• It would be helpful for readers if computation/algorithm/implementation details are given.
• Not clear why Carefl is relevant to this paper. Maybe I missed something, but the only place Carefl is mentioned in the method description is around Equation (6). But it looks to me just a standard definition of a nonlinear SEM? See e.g., Equation (4) in “On Causal Discovery with Cyclic Additive Noise Models”.
• Scalability is claimed but not tested: the largest dimension investigated is 64.
• Although the proposed method can handle missing data, missing at random is a rather strong assumption. Under this assumption, practically any Bayesian methods can easily impute the missing values.
• Is Lemma 1 a well-known result for any DAG? The Jacobian can be transformed to an upper or lower triangular matrix with unit diagonals by permuting the nodes and hence the determinant is trivially 1. I might have missed something here.
• What is the rationale of imposing prior on A but not on theta. This choice loses some uncertainty in addition to that lost by using Variational Bayes.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- Algorithm 1. How did you choose p < 0.4?,ARR_2022_24_review,ARR_2022,"- This paper brings more questions than answers -- many results are counter-intuitive or contradictory without explanation. For example: 1) Setting the vector dimension to 10 can make the entire conditional token distribution close to the Zipfian distribution. Why is that? What if the dimension is larger or smaller than 10?
2) In Figure 2(a), why do uniform and Zipfian token sampling even hurt the perplexity comparing with random weights?
3) In Figure 2(b), why does L1=nesting-parenthesis is significantly worse than L1=flat-parenthesis for Transformer?
4) In Figure 2(c), why does transferring from L1=English non-significantly worse than L1=Japanese while the task language L2=English? The flexibility of the Transformer is not a convincing explanation -- if the closeness between L1 and L2 is not a good indicator of transfer performance, then how do we conclude that a synthetic language L1 is helpful because it is closer to a real language L2?
5) In figure 3(b), why does uniform token sampling is worse than random weights by so much?
- There some technical mistakes.
1) The method of sentence-dependent token sampling can not be called ""random work"". In (Arora et al. 2016), $c_t$ does a slow random walk meaning that $c_{t+1}$ is obtained from $c_t$ by adding a small random displacement vector. BTW, the correct citation should be ""Arora et al. 2016. A Latent Variable Model Approach to PMI-based Word Embeddings. In TACL"".
2) If LSTM/Transformer models are trained with a causal (auto-regressive) LM loss, then they should be decoders, not encoders.
- Algorithm 1. How did you choose p < 0.4?
- L395. "" the combination"" -> ""combine"" - L411 ""train the model with one iteration over the corpus"". Why only one iteration? Is the model converged?
- After fine-tuning a LM pre-trained with conditioned token sampling (L456 ""useful inductive bias""), you could check if embeddings of L2 have interpretable topological relations, such as analogy.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- In Section 5, the authors present Figure 1, Figure 2, and Figure 3, yet fail to provide explanations or analysis for them. The authors need to clarify why there are negative numbers in Figure 1, as well as the implications of Figure 2 and Figure 3.",RWH1WazQqE,EMNLP_2023,"- It appears that this paper merely employs a simplified version of the Self-Refine method (https://arxiv.org/abs/2303.17651) initially devised for GPT, and applies it to open-source LLMs, without presenting any substantial differentiation.
- Recent research indicates a considerable discrepancy between the auto-evaluation methods based on GPT-4 and human evaluations, thus casting doubt on the reliability of the evaluation method used in this paper and the persuasiveness of its experimental results.
- In Section 5, the authors present Figure 1, Figure 2, and Figure 3, yet fail to provide explanations or analysis for them. The authors need to clarify why there are negative numbers in Figure 1, as well as the implications of Figure 2 and Figure 3.
- This paper's methodology has a strong link to Self-Refine, but it lacks results from Self-Refine on the Vicuna benchmark in Section 5.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
* The motivation behind applying CMD in federated learning seems a bit unclear and could benefit from a more explicit demonstration or explanation.,c9xsaASm9L,ICLR_2024,"* The citation format within the text could be improved for consistency and adherence to academic conventions. Utilizing citation commands like \citet or \citep would enhance the readability and professionalism of the references within the text.
* Figure 2 Analysis: The benefits of the CMD method as depicted in Figure 2 are not evidently clear. In the left plot, it would be helpful to see the results over a more extended range of epochs to ascertain the method's effectiveness over a longer training period.
In the middle plot, there appears to be a visual discrepancy or blur on the right side that might need clarification or correction.
* Algorithm Explanation and Comparison: A more detailed explanation and justification of the CMD algorithm's design and implementation are necessary for a thorough understanding. The comparison seems limited, primarily focusing on P-BFGS. It would be enriching to see comparisons with other relevant methods, such as pruning methods, to gauge the CMD algorithm's relative performance and novelty.
* The presentation of some concepts and terminologies, such as the ""modes"" in CMD, might be unclear or lack sufficient explanation, making it challenging for readers unfamiliar with the topic.
* Discussions or demonstrations regarding the scalability of the CMD algorithm when applied to larger or more complex neural network models would be a valuable addition to assess the method's practical applicability.
* The motivation behind applying CMD in federated learning seems a bit unclear and could benefit from a more explicit demonstration or explanation.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- **Lack of Analysis**: There is insufficient analysis of the effectiveness of each data augmentation method. Additionally, comparing their approach to other paraphrasing methods, such as EDA or LLM-based paraphrasing, would clarify the unique advantages of their method. References [1] Allen-Zhu et al., Physics of language models: Part 3.1, knowledge storage and extraction [2] Ovadia et al., Fine-tuning or retrieval? comparing knowledge injection in llms",sNycNM577m,ICLR_2025,"- **Application of SAM**: Is there a particular reason SAM is expected to perform well in knowledge learning? From my reading, it seems this work merely applies SAM in a knowledge learning context without providing new insight into its specific relevance for knowledge learning.
- **Limitations of Observations**: As the authors mention, the effectiveness of rephrasing data has been previously reported (e.g., [1], [2]). Therefore, the observation in Section 3.3 is not novel, although it is valuable to validate their experimental setup.
- **Lack of Analysis**: There is insufficient analysis of the effectiveness of each data augmentation method. Additionally, comparing their approach to other paraphrasing methods, such as EDA or LLM-based paraphrasing, would clarify the unique advantages of their method. References
[1] Allen-Zhu et al., Physics of language models: Part 3.1, knowledge storage and extraction
[2] Ovadia et al., Fine-tuning or retrieval? comparing knowledge injection in llms","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"2) learning with MMD. Without an ablation study, it is hard to see the net effect of each component. For instance, we can try learning the proposed model with typical knowledge distillation loss, or try distilling a Hydra architecture with MMD loss.",NIPS_2022_1622,NIPS_2022,"Only evaluated with accuracy and ECE, so the results do not fully reflect the uncertainty quantification aspect of the models.
Somewhat unfair comparison environment for DM (see below).
No experiments on larger scale datasets such as TinyImageNet or ImageNet.
As far as I see from the paper, DM and Hydra use only 8 heads but still distill from 120 samples; this might seriously harm the performance of the baselines, especially because these methods are designed to distill each teacher member to each subnetwork in a one-to-one fashion. If possible, it would be good to see the comparison in the setting where the teachers come with a small number of ensemble members so that one can directly distill DM and Hydra in a one-to-one fashion.
As mentioned above, no metrics other than ACCs and ECEs.
No experiments on large-scale image classification benchmarks, such as ImageNet. I find the distillation method often suffers when it comes to the scale of ImageNet, so would be good to see the scalability of the proposed methods on such datasets.
The main contribution can be understood as a combination of two ingredients; 1) design of the student networks that introduce Gaussian perturbations for inputs and intermediate layers, and 2) learning with MMD. Without an ablation study, it is hard to see the net effect of each component. For instance, we can try learning the proposed model with typical knowledge distillation loss, or try distilling a Hydra architecture with MMD loss.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
1) What is the performance of a model that simply assigns all negative samples to a distractor class?,NIPS_2021_538,NIPS_2021,"Unfortunately, I am not convinced that POODLE works as described, and cannot judge its significance or impact. Three reasons:
Tables 1 and 2 are interesting, but not convincing. Without confidence intervals over the evaluation trials I cannot tell whether the observed improvements are significant. And more fundamentally, while using the simple CELoss model as the baseline works in demonstrating improvement, it does not indicate improvement that is meaningful. The CE baseline is an inductive model for vanilla FSL, but your evaluation settings in Tables 1 and 2 are semi-supervised SSL and transductive SSL respectively. Thus it is impossible to tell if improvement is coming from your novel loss function, or from the fundamental shift from inductive evaluation to a more friendly evaluation setting. Appropriate baselines would be other semi-supervised learning methods and losses (few-shot or otherwise) such as [1] or [2] for table 1, or other transductive methods for table 2 (as in table 4). As is, the improvement shown is deceptively large.
No analysis or convincing explanation for why this works. All results appear as accuracy scores; it is impossible to tell if POODLE works for the reasons described (suppressing distractor features). There are even some indicators that it doesn’t: the fact that uniform-POODLE is competitive and sometimes superior to base-class-POODLE indicates that the loss function may not be suppressing distractor features at all, but rather acting as a simple regularizer on labels akin to label smoothing.
Lack of simple ablations. Some design decisions for the POODLE loss go unexplored, with important baseline ablations absent. 1) What is the performance of a model that simply assigns all negative samples to a distractor class? 2) The pushing loss incentivizes negative samples to be equidistant from all class vectors. What is the performance of a model that does this explicitly, by maximizing entropy of class predictions for negative samples? And as per above: how would this differ from a similar level of label smoothing? 3) As mentioned in sec.5.4, weighting the pull loss by the class predictions can lead to problematic behavior where large classes dominate behavior. Why not use the ground-truth class assignment instead of the predicted one?
In sum, it is not clear to me why and to what degree POODLE actually works. The second point is the most fundamental issue. Additionally:
Not actually complementary to many FSL methods. Lines 38-39, 84-85 imply that POODLE can be applied to FSL methods broadly. This is not the case: it only applies to methods that fine-tune a classifier layer at test time, which precludes all metric-based approaches to FSL. Thus POODLE is not nearly as versatile as claimed.
I hesitate to call POODLE SOTA for inductive settings. In a fair comparison without bells or whistles, [3] outperforms simple-POODLE in all settings, and going strictly by the highest published numbers, [3] outperforms the best POODLE on tiered-ImageNet and CUB. Also, DeepEMD is cited as related work but not included in Table 3, though in this case that doesn’t change the results.
These last two issues are less important and can probably be fixed with small language changes.
SMALL COMMENTS (feel free to ignore in discussion):
Missing citation: [4] is a more recent and SOTA example of employing a self-supervised auxiliary loss for FSL
Where does the name POODLE come from? I assume it’s “Penalizing Out Of Distribution sampLEs” but you never actually explain it.
In sec4 and later, you use “prototype” to refer to the class representation vector in the learned classification layer. I would consider making explicit the fact that you are referring to this learned vector and not the “class prototype” in the sense of prototypical networks (even if yours are at first initialized to the same thing).
Line 220 is misleading, if not outright factually wrong: your improvement in table 1 is lower than 1pp in many settings (eg. Rot+KD improves by only .09pp on 5-shot CUB).
I suspect you have mislabeled Table 2: L_push should be L_pull and vice versa. As-is, your notation conflicts with eq.4 and text lines 224-227 (and makes results difficult to interpret).
[1] Ren et al. ICLR 2018: Meta-Learning for Semi-Supervised Few-Shot Classification
[2] Saito et al. ICCV 2019: Semi-Supervised Domain Adaptation via Minimax Entropy
[3] Wertheimer, Tang and Hariharan CVPR 2021: Few-Shot Classification with Feature Map Reconstruction Networks
[4] Doersch, Gupta and Zisserman NEURIPS 2020: CrossTransformers: spatially-aware few-shot transfer POST-REBUTTAL:
I'd like to echo the first half of reviewer e7JL's post-rebuttal comment: I really appreciate the clear effort authors put in to address our concerns, there's good value in this work, and in my eyes too novelty is sufficient. Regarding my own review: authors mostly addressed my concerns, with the sole exception that we still have no empirical analysis of POODLE behavior beyond accuracy scores, meaning that we still cannot be entirely sure what POODLE is actually doing in practice. However, after additional clarification the method does make intuitive sense so this is not as troubling to me as it was initially.
No negative impacts are discussed. That said, any risks or dangers inherent to this method are shared with few-shot learning more broadly, so a brief mention of this would likely suffice. Also, if POODLE does in fact work by suppressing distractor features, the ability to target certain distractors (i.e. age, race, gender) could be a very large impact, though of course that is outside the scope of this particular paper to examine or demonstrate.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
3. The advantage of RLCD over RLAIF shrinks going from 7B to 30B (Tab. 2). It remains to be seen whether RLCD (or RLCD-Rescore) can scale to yet larger language models that are arguably better at differentiating responses near the decision boundary.,v3XXtxWKi6,ICLR_2024,"1. The analysis on the preference model shows that the preference model produced by RLCD is, while better than the baseline, still not very good, especially on the harmlessness attribute (Tab. 5). It is not clear how this slight advantage over chance (2.4%~5.9%) translates into a much better downstream performance after RLHF.
2. As shown in Appendix C, RLAIF-Few-30B produces both a better preference model and a better-aligned language model than RLCD-30B on the harmlessness benchmark, which is attributed to few-shot prompting by the authors. It seems that this technique can also be integrated into RLCD to enable a fairer comparison.
3. The advantage of RLCD over RLAIF shrinks going from 7B to 30B (Tab. 2). It remains to be seen whether RLCD (or RLCD-Rescore) can scale to yet larger language models that are arguably better at differentiating responses near the decision boundary.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"* The proposed NC measure takes the whole training and test datasets as input. I can hardly imagine how this method can be learned and applied to large scale datasets (e.g. ImageNet). Is there any solution to address the scalability issue? Otherwise, the practical contribution of this paper will be significantly reduced.",NIPS_2020_813,NIPS_2020,"* The proposed NC measure takes the whole training and test datasets as input. I can hardly imagine how this method can be learned and applied to large scale datasets (e.g. ImageNet). Is there any solution to address the scalability issue? Otherwise, the practical contribution of this paper will be significantly reduced. * There are many missing details regarding the experiments, which make the proposed method hard to reproduce. See the Clarity section for more comments.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. Lack of Quantitative Analysis on Computational Gains: While the paper claims computational benefits from replacing the MAE model with a CNN-based data augmentation strategy, it lacks specific measurements or comparisons to substantiate these gains. A quantitative analysis—such as GPU hours, memory usage, or training time—would provide stronger evidence of the efficiency improvements in DQ V2.",xajif1l65R,ICLR_2025,"1. Lack of Quantitative Analysis on Computational Gains: While the paper claims computational benefits from replacing the MAE model with a CNN-based data augmentation strategy, it lacks specific measurements or comparisons to substantiate these gains. A quantitative analysis—such as GPU hours, memory usage, or training time—would provide stronger evidence of the efficiency improvements in DQ V2.
2. Missing Baselines: I noticed that some recent coreset selection baselines for deep learning are missing: D2 Pruning[1], CCS[2], Moderate[3]. Those baselines seem to have a stronger performance than the proposed methods.
3. Missing evaluation on ImageNet-1k: the paper argues that DQ-V2 is more efficient than DQ, but the method is only evaluated on the ImageNet subset. Previous methods including DQ all conducted evaluation on ImageNet-1k. It will be good to include an ImageNet-1k evaluation to demonstrate the scalability of the proposed methods.
4. The data augmentation part is confusing: the goal of data quantization and coreset selection is to reduce the size of the training dataset, but the data augmentation method proposed in the paper expands the datasets -- the final expanded training dataset can be even larger, which is contradicted to the goal of coreset selection.
5. Ablation study on data augmentation: The paper would benefit from a more detailed ablation study to assess the effectiveness of the data augmentation method used in DQ V2. Testing different data augmentation configurations (e.g., no augmentation, alternate augmentation techniques) would clarify its impact and help refine the methodology.
[1] Maharana, Adyasha, Prateek Yadav, and Mohit Bansal. ""D2 pruning: Message passing for balancing diversity and difficulty in data pruning."" ICLR 2024
[2] Zheng, Haizhong, Rui Liu, Fan Lai, and Atul Prakash. ""Coverage-centric coreset selection for high pruning rates."" ICLR 2023
[3] Xia, Xiaobo, Jiale Liu, Jun Yu, Xu Shen, Bo Han, and Tongliang Liu. ""Moderate coreset: A universal method of data selection for real-world data-efficient deep learning."" ICLR 2023","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1) the time for COLMAP and scene-by-scene fine-tuning should be taken into account when comparing, rendering the method less efficient for these scenes;",ICLR_2023_75,ICLR_2023,"According to Appendix B.2, the authors applied different MVS networks (at least different weights) when evaluating on different datasets. Especially, when evaluating on LLFF data, they finetune the MVS model scene-by-scene. This may cause concern/confusion because 1) the time for COLMAP and scene-by-scene fine-tuning should be taken into account when comparing, rendering the method less efficient for these scenes; 2) it is unclear why not using COLMAP point clouds directly. The authors should clarify these and ideally use a same generalized MVS model for a fair comparison.
Pulsar is a very related direct baseline of this method. However, a comparison with Pulsar seems to be missing in this paper.
The intuition behind employing a U-Net for rendering is unclear. Is it possible to use per-pixel MLP to render the features (MLP: feat -> RGB)? Is the U-Net pre-trained and weight-shared across datasets?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?",NIPS_2017_370,NIPS_2017,"- There is almost no discussion or analysis on the 'filter manifold network' (FMN) which forms the main part of the technique. Did authors experiment with any other architectures for FMN? How does the adaptive convolutions scale with the number of filter parameters? It seems that in all the experiments, the number of input and output channels is small (around 32). Can FMN scale reasonably well when the number of filter parameters is huge (say, 128 to 512 input and output channels which is common to many CNN architectures)?
- From the experimental results, it seems that replacing normal convolutions with adaptive convolutions in not always a good. In Table-3, ACNN-v3 (all adaptive convolutions) performed worse that ACNN-v2 (adaptive convolutions only in the last layer). So, it seems that the placement of adaptive convolutions is important, but there is no analysis or comments on this aspect of the technique.
- The improvements on image deconvolution is minimal with CNN-X working better than ACNN when all the dataset is considered. This shows that the adaptive convolutions are not universally applicable when the side information is available. Also, there are no comparisons with state-of-the-art network architectures for digit recognition and image deconvolution. Suggestions:
- It would be good to move some visual results from supplementary to the main paper. In the main paper, there is almost no visual results on crowd density estimation which forms the main experiment of the paper. At present, there are 3 different figures for illustrating the proposed network architecture. Probably, authors can condense it to two and make use of that space for some visual results.
- It would be great if authors can address some of the above weaknesses in the revision to make this a good paper.
Review Summary:
- Despite some drawbacks in terms of experimental analysis and the general applicability of the proposed technique, the paper has several experiments and insights that would be interesting to the community. ------------------
After the Rebuttal: ------------------
My concern with this paper is insufficient analysis of 'filter manifold network' architecture and the placement of adaptive convolutions in a given CNN. Authors partially addressed these points in their rebuttal while promising to add the discussion into a revised version and deferring some other parts to future work.
With the expectation that authors would revise the paper and also since other reviewers are fairly positive about this work, I recommend this paper for acceptance.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The proposed CoNO model uses a complex UNet part after the fractional transform. It is impossible to guess what brings the claimed performance boost - the fractional transform or the UNet operation in the fractional Fourier domain, which is comparable to pointwise multiplication as done in FNOs? At least comparisons to UNets are therefore inevitable. Especially, since on regular gridded domains UNets / convolutional operators have shown strong performances, see e.g. Raonic et al or Gupta et al.",5vJe8XKFv0,ICLR_2024,"- There is no information about baseline models and training/inference times. There is a link to the code repo added, but it is impossible to figure out the settings and hyperparamters of the models. Given that the proposed model performs marginally better than FNO models this makes it impossible to judge. Side remark: Pytorch FFTs on complex inputs are much slower than on real inputs (torch.fft vs torch.rfft), thus runtime comparisons would be needed.
- The main formula (Eq 3) is hardly explained. For example in the literature the Fractional Fourier Transform is often defined as $$\mathcal{F}_{\alpha}\[f\](u) = \sqrt{1 - i \cot(\alpha)} e^{i\pi\cot(\alpha)u^2} \int e^{-2\pi i \left( \csc(\alpha) u t - \frac{\cot(\alpha)}{2}x^2\right)} f(t) dt \ .$$ What is the relation of Eq. 3 to the presented formula, and more importantly how can this be implemented? In the code the CoNO model looks very similar to the FNO model, but I assume that the Fourier transform needs to be changed since there is another term which depends on $t$?
- The proposed CoNO model uses a complex UNet part after the fractional transform. It is impossible to guess what brings the claimed performance boost - the fractional transform or the UNet operation in the fractional Fourier domain, which is comparable to pointwise multiplication as done in FNOs? At least comparisons to UNets are therefore inevitable. Especially, since on regular gridded domains UNets / convolutional operators have shown strong performances, see e.g. Raonic et al or Gupta et al.
- Ablation studies are not revealing a lot, they are basically showing the same results as the main table.
- There has been work for example on Clifford Fourier Neural Operators (Brandstetter et al) which includes complex numbers and more complicated algebras. Possibly missing a few others here. Discussions of related work and comparisons against those are missing.
Raonić, B., Molinaro, R., Rohner, T., Mishra, S., & de Bezenac, E. (2023). Convolutional Neural Operators. arXiv preprint arXiv:2302.01178.
Gupta, Jayesh K., and Johannes Brandstetter. ""Towards multi-spatiotemporal-scale generalized pde modeling."" arXiv preprint arXiv:2209.15616 (2022).
Brandstetter, J., Berg, R. V. D., Welling, M., & Gupta, J. K. (2022). Clifford neural layers for PDE modeling. arXiv preprint arXiv:2209.04934.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"4. The proposed PSA method requires more computation than baselines. In algorithm 1, when feeding forward, the PSA requires the calculation of all the flipped previous layer output into the current layer. The comparison of computation complexity is expected in the experiment part.",NIPS_2020_1080,NIPS_2020,"1. The experiments setups are not persuasive. For the gradient estimation accuracy, the author conduct experiment only on 2 classes 2D simulation data. The author does not mention how the 100 training data generated, which is in quite a small amount even in the simulation study. The network is in special design as 5-3-3 Bernoulli cases, which is insufficient to conclude the proposed method is better in gradient estimation. The reviewer expects to see more simulation results by varying the unit number in each layer. 2. The performance on the real-world dataset is not satisfying enough. The PSA method seems not to achieve the best accuracy or the fastest convergence. The ST method is previously proposed, which I think cannot be recognized as the author's contribution. Besides, only the validation results are reported. What is the performance on the testing set? 3. Important baselines are not compared. The ARM gradient is a competitive baseline, which the author only compared under a special simulation setup. What is the reason that the author does not compare with ARM on the CIFAR classification task? 4. The proposed PSA method requires more computation than baselines. In algorithm 1, when feeding forward, the PSA requires the calculation of all the flipped previous layer output into the current layer. The comparison of computation complexity is expected in the experiment part. 5. The notations of the method are difficult to follow. It is much better if the authors can begin their analysis with 1-hidden-layer SBN first, which will simplify the notation a lot.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The proposed model benefits from two factors : noise and keeping an exponential moving average. It would be good to see how much each factor contributes on its own. The \Pi model captures just the noise part, so it would be useful to know how much gain can be obtained by just using a noise-free exponential moving average.",NIPS_2017_114,NIPS_2017,"Weakness-
- Comparison to other semi-supervised approaches : Other approaches such as variants of Ladder networks would be relevant models to compare to. Questions/Comments-
- In Table 3, what is the difference between \Pi and \Pi (ours) ?
- In Table 3, is EMA-weighting used for other baseline models (""Supervised"", \Pi, etc) ? To ensure a fair comparison, it would be good to know that all the models being compared to make use of the EMA benefits.
- The proposed model benefits from two factors : noise and keeping an exponential moving average. It would be good to see how much each factor contributes on its own. The \Pi model captures just the noise part, so it would be useful to know how much gain can be obtained by just using a noise-free exponential moving average.
- If averaging in parameter space is being used, it seems that it should be possible to apply the consistency cost in the intermediate layers of the model as well. That could potentially provide a richer consistency gradient. Was this tried ?
Minor comments and typos-
- In the abstract : ""The recently proposed Temporal Ensembling has ... "": Please cite.
- ""when learning large datasets."" -> ""when learning on large datasets.""
- ""zero-dimensional data points of the input space"": It may not be accurate to say that the data points are zero-dimensional.
- ""barely applying"", "" barely replicating"" : ""barely"" -> ""merely""
- ""softmax output of a model does not provide a good Bayesian approximation outside training data"". Bayesian approximation to what ? Please explain. Any model will have some more generalization error outside training data. Is there another source of error being referred to here ? Overall-
The paper proposes a simple and effective way of using unlabelled data and
improving generalization with labelled data. The most attractive property is
probably the low overhead of using this in practice, so it is quite likely that
this approach could be impactful and widely used.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. Excellent drawing on figures. However, fonts could be larger fig 1. The words in grey box may be larger. V_mem, Th_i, U_i^t too small. “CTRL” long form explanation. Also, font in figure 2 is too small. (Conv5 +BN) 4. Lack of details comparison, such as epochs and number of params, with other state-of-the-art Transformer design. A “table” manner may better emphases the data for readers to justify the improved accuracy is not because of brute-force parameters increment.",wPK65O4pqS,ICLR_2024,"1. What is the baseline model on the ablation experiments? Is the baseline model for your own architecture or other study’s baseline? The study has shown that without the STCore and SGA, the trained model already has excellent performance (80.9% on DVS-CIFAR10) while general accuracy from other studies as shown in your table are below 80%. Would this also mean that the work in accuracy boost may not be very effective (1-2% boost) while spending extra computing resources?
2. Minor mistake on 3.1 preliminaries. Equation 3 referencing.
3. Excellent drawing on figures. However, fonts could be larger fig 1. The words in grey box may be larger. V_mem, Th_i, U_i^t too small. “CTRL” long form explanation. Also, font in figure 2 is too small. (Conv5 +BN)
4. Lack of details comparison, such as epochs and number of params, with other state-of-the-art Transformer design. A “table” manner may better emphases the data for readers to justify the improved accuracy is not because of brute-force parameters increment.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The results are only reported after a bunch of training has occurred, but in RL we are often also interested in how the agent behaves *while* learning. I presume that early in training the model parameters are essentially garbage and the planning component of the network might actually *hurt* more than it helps. This is pure speculation, but I wonder if the CNN is able to perform reasonably well with less data.",NIPS_2016_241,NIPS_2016,"/challenges of this approach. For instance... - The paper does not discuss runtime, but I assume that the VIN module adds a *lot* of computational expense. - Though f_R and f_P can be adapted over time, the experiments performed here did incorporate a great deal of domain knowledge into their structure. A less informed f_R/f_P might require an impractical amount of data to learn. - The results are only reported after a bunch of training has occurred, but in RL we are often also interested in how the agent behaves *while* learning. I presume that early in training the model parameters are essentially garbage and the planning component of the network might actually *hurt* more than it helps. This is pure speculation, but I wonder if the CNN is able to perform reasonably well with less data. - I wonder whether more could be said about when this approach is likely to be most effective. The navigation domains all have a similar property where the *dynamics* follow relatively simple, locally comprehensible rules, and the state is only complicated due to the combinatorial number of arrangements of those local dynamics. WebNav is less clear, but then the benefit of this approach is also more modest. In what kinds of problems would this approach be inappropriate to apply? ---Clarity--- I found the paper to be clear and highly readable. I thought it did a good job of motivating the approach and also clearly explaining the work at both a high level and a technical level. I thought the results presented in the main text were sufficient to make the paper's case, and the additional details and results presented in the supplementary materials were a good compliment. This is a small point, but as a reader I personally don't like the supplementary appendix to be an entire long version of the paper; it makes it harder to simply flip to the information I want to look up. I would suggest simply taking the appendices from that document and putting them up on their own. ---Summary of Review--- I think this paper presents a clever, thought-provoking idea that has the potential for practical impact. I think it would be of significant interest to a substantial portion of the NIPS audience and I recommend that it be accepted.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it. P. 5, p. 3, l.",ICLR_2023_3811,ICLR_2023,"Most of the paper is poorly written and difficult to understand.
The idea of scheduled sampling is not new, so I would categorize this paper a purely empirical contribution. However the amount of inconsistencies, and overall lack of rigor in reporting and interpreting the results, paired with the lack of clarity in the exposition significantly subtract from its empirical value.
Some claims are unsupported.
Suggestions and questions for the authors.
The whole second page is devoted to setting up the stage of the paper's contributions, however any reader not familiar with the term ""coherence"" will have a hard time grasping the need of the sampling strategies that you are suggesting. On the next page you mention that several previous works model coherence as Natural Language Inference (NLI). It is only by looking at equation 2 and the meaning of f c
that we understand that coherence is also modelled as NLI in this paper. I strongly suggest better defining ""coherence"" in the introduction to better contextualize the contributions that the paper is proposing.
Usually in dialogue literature, the words ""turn"" and ""utterance"" are ambiguous. I suggest defining both terms precisely. For example: can a turn contain more than one utterance? Does one utterance correspond to one turn? Can one utterance span several turns? Can there be adjacent turns/utterances for a single role (i.e. the same role sending several messages one after the other)? I cannot deduce any of this from reading sections 3 and 4.
Somewhat related to the previous question: do you train your models to generate both system and user responses? Or do have your models assume only one of those roles during training?
When describing the online Evaluation you formally define the coherence between response r ^ i
and context $\bf{\hat{U}^{i-1}1} a s
c_k = \sum{i=1}^{D}{\frac{\mathbb{1}(f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i) = 1)}{D}} w h e r e
f_c(\bf{\hat{U}^{i-1}_1}, \bf{\hat{r}}_i)$ is an entailment classifier. However NLI classification usually has 3 possible classes: ""entailment"", ""contradiction"" and ""neutral"". I suggest specifying that the 1 label in the numerator corresponds to the ""entailment"" class, and whether you consider both ""contradiction"" and ""neutral"" as a single ""non-entailment"" class, or you treat them separately.
Also in equation 2, I don't understand what D
is supposed to represent. Shouldn't it be k instead? 𝟙 c k = ∑ i = 1 k 1 ( f c ( U ^ 1 i − 1 , r ^ i ) = 1 ) k
. If not, then what are the ""instances for evaluation"" you mention after the equation? Further when i = 1
there's a U ^ 1 0
term that shows up in the numerator. How is it defined?
In the ""Utterance Level Sampling"" paragraph in section 4.1 you justify the use of a Geometrical distribution by saying it ""tends to sample previous utterance to be replaced"" but I still not understand what this means, or why it is desired. I suggest clarifying this.
In the ""Coherence Rate"" paragraph in section 5.1 you say you use a v g n
as the average coherence rate, but equation (2) already defines c k
as an average. Was this intended or is it a typo?
In Table 1 you report results ""w/ Noise"" described on page 6, ""w/ Utterance"" and ""w/ Semi-Utterance"" described on page 4, but you also mention ""w/ Hierarchical"". Up to this point I had understood both ""Utterance Level Sampling"" and ""Semi-utterance Level Sampling"" as two instances of Hierarchical Sampling, so I was baffled to see an additional row for Hierarchical Sampling on this Table. I suggest being more explicit about what the ""w/ Hierarchical"" row means. On page 8 you mention that Hierarchical sampling is the combination of both Utterance and Semi-Utterance level Sampling, but I suggest explaining this earlier, in section 4.1.
On page 6 you also mention that you measured Pearson correlation between human-annotated and automatic coherence rates. Why did you do this only for coherence and not for non-repetition?
Why did you not report the turn-level coherences in Table 2?
In Table 4 did you average the non-repetition count for unigrams, bigrams and trigrams for calculating ""Rep""? I suggest clarifying this.
On page 7 in the ""Sampling vs w/o Sampling"" paragraph, how did you obtain the p-value? What were the null and alternative hypotheses? In the same paragraph you state that Blender improved 2.8% when using the hierarchical sampling strategy, but table 2 actually shows a 4% difference. Why this discrepancy in numbers?
Same question for the p-value reported on page 8 in the ""Human Evaluation"" paragraph. In the same paragraph you state that human-evaluated coherence increases from 0.96 to 1.53, while actually these numbers refer to the human-evaluated non-repetition metric. Further you conclude from a 0.78 Pearson correlation score that model-based evaluation methods are effective, but there are many relationships between human and automated metrics that can give rise to such a score (see https://janhove.github.io/teaching/2016/11/21/what-correlations-look-like for an example). I suggest at least plotting the human vs. automated metric values + their correlations before making such a strong claim.
In the ""Explicit Coherence Optimization"" paragraph on page 8 you conclude from figure 4 that training the model with RL outperforms training the model with MLE in terms of coherence rate. However figure 4 shows that this statement holds only for the first 5 turns, then coherence dips below the BART baseline with beam-search based reranking, so the conclusion you reach does not follow from the evidence. Also why do you think this dip in coherence happened?
In the same paragraph you describe the reranking setup. I suggest putting this description before, where you define the other experiments.
In that same paragraph you conclude that your ""hierarchical-sampling based methods consistently perform better than multi-turn BART by introducing coherence reranking"". Again, this cannot be concluded from Figure 4. It does perform better in terms of coherence rate, for the first 5 turns, but you did not report on the other performance metrics under the reranking scheme. To support this claim, it would be necessary to show how the fluency and non-repetition rate change when reranking based on coherence only. My intuition tells me that these two metrics would be negatively impacted, but I would like my intuition to be proved wrong and see that actually optimizing for coherence impacts fluency and non-repetition positively.
The claim made at the end of the introduction that you ""demonstrate these methods make chatbots more robust in real-world testing"" is not supported, as you did not test your chatbots in the real world. They were tested in a lab setting with humans that were told to follow some experimental instructions. Moving from this setting to the real world would require a considerable amount of additional effort.
Typos and minor corrections
Page 2, paragraph 1, line 3: the term ""coherence"" is mentioned here for the first time. However you define it in Fig. 2's caption. I suggest defining it either as a footnote or in the main text to not disrupt the reading flow. Also, the definition ""Coherence rate (Equation 2) measures the percentage of responses is coherence [sic] with the corresponding contexts."" is self referential. What does it mean for a response to be coherent with the corresponding contexts? Finally, ""is coherence"" should be ""that is coherent"".
P. 3, p. 4, l. 1: You write ""a conventional practice [REFERENCES] for evaluating [...].""; I suggest writing ""a conventional practice for evaluating [...] [REFERENCES]."" to improve the reading flow.
P. 3, p. 4, l. 7: What does the sub-index ""1"" mean in U ^ 1 i − 1
? Does it mean ""starting from index 1""? If this is the case and you never use anything other than ""1"" as the starting index, I suggest removing it, and simply defining U ^ i − 1
as the context up to the i − 1
-th utterance.
P. 3, p. 5, l. 7: relative -> relatively
P. 3, p. 5, l. 9: to ""conduct"" a classifier does not make much sense. You can either ""conduct"" classification or ""train"", ""use"", ""create"", etc. a classifier.
P. 4, p. 4, l. 4: here you say ""we first ask the model to predict the response r ^ i
based on the previous context U ^ 1 ′ i − 1
but if I understand the explanation correctly, then it should be U ^ 1 i − 1
i.e. the original context.
P. 4, p. 4, l. 7: ""Given a training pair U ^ 1 t − 1
"" should be ""Given a training pair U ^ 1 ′ t − 1
"", i.e. the training pair contains an utterance replaced through the ""Utterance Level Sampling"" method.
P. 4, eq. 3: U ^ 1 ′ l − 1
should be U ^ 1 ′ t − 1
i.e. the super-index of U ′
should be t − 1 not l − 1
P. 4, p. 5, l. 5: I can't understand the meaning of the sentence ""While a smaller j to simulate more accumulate errors along with the inference steps."", please rewrite it.
P. 5, p. 3, l. 3: ""two annotators are employed"" -> ""two annotators were employed""
P. 5, p. 5, l. 3: The sentence starting with ""As model-based methods"" is ungrammatical. I suggest reformulating it.
P. 5, p. 7, l. 1: ""Following previous work (Ritter et al., 2011)"" -> ""Following the work by Ritter et al. (2011),""
P. 5, p. 8, l. 2: ""to online evaluate these two methods"" -> ""to evaluate these two methods online""
P. 6, p. 1, l. 2: ""non-repetitive"" -> ""non-repetitiveness""
P. 6, p. 6, l. 1: ""After sample an utterance"" -> ""After sampling an utterance""
P. 6, p. 8, l. 2: ""generate coherence response"" -> ""generate coherent responses""
P. 6, p. 8, l. 4: ""with the number of turns increases"" -> ""as the number of turns increases""
P. 7, Figure 4, a - b: The y-axis should be labeled ""coherence (%)"" instead of ""coherent (%)"". Same for figure 5 (b) on the next page.
P. 7, p. 3, l. 7: the sentence ""since sampled noises are difficult to accurately simulate errors of the inference scene during training"" makes no sense. Please rewrite it.
P. 8, Figure 5: Both y-axes have a typo: (a): ""Contradition"" -> ""Contradiction""; (b): ""Coherent"" -> ""Coherence"". Is the x-axis in (a) different to the x-axis in (b) and to those in figure 4? if not, I suggest being consistent with the x-labels.
P. 8, p. 1, l. 7: ""hierarchy way"" -> ""hierarchical way""
P. 9, p. 2, l. 1: ""incoherence response"" -> ""incoherent response""
Overall it feels like the paper was rushed at the end. Its earlier 25% is well written and has almost no typos, while the conclusion is barely legible. I suggest proofreading the later half of the paper on top of the corrections I make above.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- For DocRED, did you consider the documents as an entire sentence? How do you deal with concepts (multiple entity mentions referring to the same entity)? This information is currently missing from the manuscript.",ARR_2022_237_review,ARR_2022,"of the paper include: - The introduction of relation embeddings for relation extraction is not new, for example look at all Knowledge graph completion approaches that explicitly model relation embeddings or works on distantly supervised relation extraction. However, an interesting experiment would be to show the impact that such embeddings can have by comparing with a simple baseline that does not take advantage of those.
- Improvements are incremental across datasets, with the exception of WebNLG. Why mean and standard deviation are not shown for the test set of DocRED?
- It is not clear if the benefit of the method is just performance-wise. Could this particular alignment of entity and relation embeddings (that gives the most in performance) offer some interpretability? ( perhaps this could be shown with a t-SNE plot, i.e. check that their embeddings are close in space).
Comments/Suggestions: - Lines 26-27: Multiple entities typically exist in both sentences and documents and this is the case even for relation classification, not only document-level RE or joint entity and relation extraction.
- Lines 39-42: Point to figure 1 for this particular example.
- Lines 97-98: Rephrase the sentence ""one that searches for ... objects"" as it is currently confusing - Line 181, Equations 4: $H^s$, $E^s$, $E^o$, etc are never explained.
- Could you show ablations on EPO and SEO? You mention in the Appendix that the proposed method is able to solve all those cases but you don't show if your method is better than others.
- It would be interesting to also show how the method performs when different number of triples reside in the input sequence. Would the method help more sequences with more triples?
Questions: - Improvement still be observed with a better encoder, e.g. RoBERTa-base, instead of BERT?
- How many seeds did you use to report mean and stdev on the development set?
- For DocRED, did you consider the documents as an entire sentence? How do you deal with concepts (multiple entity mentions referring to the same entity)? This information is currently missing from the manuscript.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
1. The contribution looks marginal to me since all the methods used in different stage are well designed and demonstrated. Adding another stream for low-resolution might not be a major contribution for a top-tier venue like ICLR.,gwDuW7Ok5f,ICLR_2024,"1. The contribution looks marginal to me since all the methods used in different stage are well designed and demonstrated. Adding another stream for low-resolution might not be a major contribution for a top-tier venue like ICLR.
2. I got some questions for the experimental results which can be seen in the questions part.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1: The main contribution is somehow a little bit unclear. From the ablation study, we can see the performance gain is mostly from PBSD. However this paper is mostly motivated by supervised contrastive learning, that is, the DSCL part. Other than improving the discriminative of the learned representation on tail classes, any other motivations for PBSD?",ICLR_2023_4605,ICLR_2023,"1: The main contribution is somehow a little bit unclear. From the ablation study, we can see the performance gain is mostly from PBSD. However this paper is mostly motivated by supervised contrastive learning, that is, the DSCL part. Other than improving the discriminative of the learned representation on tail classes, any other motivations for PBSD?
2: No experiments regarding smaller-size datasets (CIFAR). Both ImageNet and iNaturaList contain relatively large images. It is necessary to validate whether the proposed method, especially the PBSD part can scale to small-size images (let's say, 32*32 resolution). It would be very interesting to add an study regarding image resolution (some candidates: 32, 64, 128, 224, 384).
Minor issues:
1: Fig 1 can be modified. The size of the clouds is not identical.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2 gives a tester for the spread parameter, but then it's not clear if it immediately yields an ( ϵ , δ ) -identity tester as well? For e.g., how is it dealing with ( π , ϕ ) pairs where ϕ = ϕ 0 , but d K ( π 0 , π ) is large?",NIPS_2021_1917,NIPS_2021,"The guarantee of the efficient algorithm i.e, Theorem 5.4 seems arguably weaker than that of Theorem 5.2.
Comments/Questions for the Authors:
Line 33: that is -> that are
Line 62: from computational -> from a computational
Line 79: is devised -> was devised
Line 129: access the -> access to the
Remark 1: where is N k
used in the definition? Reference for Mahonian number?
Section 5.3: I am a bit confused about the main result of this section, as far as I understand, Alg. 2 gives a tester for the spread parameter, but then it's not clear if it immediately yields an ( ϵ , δ )
-identity tester as well? For e.g., how is it dealing with ( π , ϕ )
pairs where ϕ = ϕ 0
, but d K ( π 0 , π )
is large?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
* The comprehensive Appendix is appreciated to provide additional detail about parts of the paper. I did not carefully read additional experiments described in the Appendix (e.g. the Brusselator) out of time consideration.,NIPS_2019_663,NIPS_2019,"of their work?""] The submission is overall reasonably sound, although I have some comments and questions: * Regarding the model itself, I am confused by the GRU-Bayes component. I must be missing something, but why is it not possible to ingest observed data using the GRU itself, as in equation 2? This confusion would perhaps be clarified by an explanation in line 89 of why continuous observations are required. As it is written, I am not sure why it you couldn't just forecast (by solving the ODE defined by equation 3) the hidden state until the next measurement arrives, at which point g(t) and z(t) can be updated to define a new evolution equation for the hidden state. I am guessing the issue here is that this update only changes the derivative of the hidden state and not its value itself, but since the absolute value of the hidden state is not necessarily meaningful, the problem with this approach isn't very clear to me. I imagine the authors have considered such a model, so I would like to understand why it wouldn't be feasible here. * In lines 143-156, it is mentioned that the KL term of the loss can be computed empirically for binomial and Gaussian distributions. I understand that in the case of an Ornstein-Uhlenbeck SDE, the distribution of the observations are known to be (conditionally) Gaussian, but in the case of arbitrary data (e.g. health data), as far as I'm aware, few assumptions can be made of the underlying process. In this case, how is the KL term managed? Is a Gaussian distribution assumption made? Line 291 indicates this is the case, but it should be made clear that this is an assumption imposed on the data. For example, in the case of lab test results as in MIMIC, these values are rarely Gaussian-distributed and may not have Gaussian-distributed observation noise. On a similar note, it's mentioned in line 154 that many real-world cases have very little observation noise relative to the predicted distribution - I assume this is because the predicted distribution has high variance, but this statement could be better qualified (e.g. which real-world cases?). * It is mentioned several times (lines 203, 215) that the GRU (and by extension GRU-ODE-Bayes) excels at long-term forecasting problems, however in both experiments (sections 5.2 and 5.3) only near-term forecasting is explored - in both cases only the next 3 observations are predicted. To support this claim, longer prediction horizons should be considered. * I find it interesting that the experiments on MIMIC do not use any regularly-measured vital signs. I assume this was done to increase the ""sporadicity"" of the data, but it makes the application setting very unrealistic. It would be very unusual for values such as heart rate, respiratory rate, blood pressure and temperature not to be available in a forecasting problem in the ICU. I also think it's a missed opportunity to potentially highlight the ability of the proposed model to use the relationship between the time series to refine the hidden state. I would like to know why these variables were left out, and ideally how the model would perform in their presence. * I think the experiment in Section 5.5 is quite interesting, but I think a more direct test of the ""continuity prior"" would be to explicitly test how the model performs (in the low v. high data cases) on data which is explicitly continuous and *not* continuous (or at least, not 2-Lipschitz). The hypothesis that this continuity prior is useful *because* it encodes prior information about the data would be more directly tested by such a setup. At present, we can see that the model outperforms the discretised version in the low data regime, but I fear this discretisation process may introduce other factors which could explain this difference. It is slightly hard to evaluate because I'm not entirely sure what the discretised version consists of , however - this should be explained (perhaps in the appendix). Furthermore, at present there is no particular reason to believe that the data in MIMIC *is* Lipschitz-2 - indeed, in the case of inputs and outputs (Table 4, Appendix), many of these values can be quite non-smooth (e.g. a patient receiving aspirin). * It is mentioned (lines 240-242, section H.1.3) that this approach can handle ""non-aligned"" time series well. As mentioned, this is quite a challenging problem in the healthcare setting, so I read this with some interest. Do these statements imply that this ability is unique to GRU-ODE-Bayes, and is there a way to experimentally test this claim? My intuition is that any latent-variable model could in theory capture the unobserved ""stage"" of a patient's disease process, but if GRU-ODE-Bayes has some unique advantage in this setting it would be a valuable contribution. At present it is not clearly demonstrated - the superior performance shown in Table 1 could arise from any number of differences between this model and the baselines. 2.c Clarity: [""Is the submission clearly written? Is it well organized? (If not, please make constructive suggestions for improving its clarity.) Does it adequately inform the reader? (Note: a superbly written paper provides enough information for an expert reader to reproduce its results.)""] While I quite like the layout of the paper (specifically placing related work after a description of the methodology, which is somewhat unusual but makes sense here) and think it is overall well written, I have some minor comments: * Section 4 is placed quite far away from the Figure it refers to (Figure 1). I realise this is because Figure 1 is mentioned in the introduction of the paper, but it makes section 4 somewhat hard to follow. A possible solution would be to place section 4 before the related research, since the only related work it draws on is the NeuralODE-VAE, which is already mentioned in the Introduction. * I appreciate the clear description of baseline methods in Section 5.1. * The comprehensive Appendix is appreciated to provide additional detail about parts of the paper. I did not carefully read additional experiments described in the Appendix (e.g. the Brusselator) out of time consideration. * How are negative log-likelihoods computed for non-probabilistic models in this paper? * Typo on line 426 (""me"" instead of ""we""). * It would help if the form of p was described somewhere near line 135. As per my above comment, I assume it is a Gaussian distribution, but it's not explicitly stated. 2.d Significance: [""Are the results important? Are others (researchers or practitioners) likely to use the ideas or build on them? Does the submission address a difficult task in a better way than previous work? Does it advance the state of the art in a demonstrable way? Does it provide unique data, unique conclusions about existing data, or a unique theoretical or experimental approach?""] This paper describes quite an interesting approach to the modelling of sporadically-measured time series. I think this will be of interest to the community, and appears to advance state of the art even if it is not explicitly clear where these gains come from.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The authors claimed that they used active learning in step 2. Is the “active learning pipeline” method the same as traditional active learning that select informative samples to label? If not, the description can mislead the readers.",F0XXA9OG13,ICLR_2024,"- The framework is quite straightforward, and there is not much technical contribution. It is mostly a combination of multiple existing models. And the idea of transferring tabular data into text is not novel at all. There are a bunch of existing works [1][2][3], including one of their baselines TabLLM[4]. The further incorporation of text information from samples from other datasets is just one trivial step forward. Furthermore, [4] actually proved that a template for transferring the tabular data works better than an LLM. Yet, in this paper, there is no comparison for such serialization methods.
- The author didn’t specify what exact features are included in these experimental datasets. Also, it is unclear how many columns are overlapped between different datasets. Yet, if there is a large portion of feature overlapping, maybe simple concatenation and removing or recoding of the missing columns will work just as well. There is no discussion regarding this whatsoever.
- The step 2 in section 2.2 is confusing:
- The authors claimed that they used active learning in step 2. Is the “active learning pipeline” method the same as traditional active learning that select informative samples to label? If not, the description can mislead the readers.
- The authors claimed that they cleaned supplementary dataset T_{1, sup} with a data audit module based on data Shapley scores. More experiments are expected to demonstrate the effectiveness of the audit module. Moreover, it would be better if the authors conducted more ablation studies to show whether the supplementary dataset improve the prediction performance.
- The datasets in Table 1 contain less than 3000 patients. It is very easy for the LLMs (e.g., BioBERT) to overfit the training set. It is unclear how the authors prevent overfitting during the fine-tuning phase.
- In Table 3, the proposed MediTab exhibits the capability to access multiple datasets during its training, in contrast to the other baseline models, which are constrained to employing a single dataset. This discrepancy in data utilization introduces an element of unfairness in the comparison. It would be more appropriate to conduct a comparison against models that have undergone training on multiple datasets. For instance, TabLLM, being a large language model, can readily undertake multi-dataset training with minor adjustments to its data preprocessing procedures. Therefore, a more equitable comparison would involve evaluating MediTab and TabLLM under identical conditions, both in the context of training on a single dataset and across multiple datasets.
- Most medical data, like MIMIC-IV, includes timestamp information of the patients’ multiple visits or collections. This framework completely ignores this part of the medical data, which limits their application to real-world clinical environments. Reference:
1. Bertsimas, Dimitris & Carballo, Kimberly & Ma, Yu & Na, Liangyuan & Boussioux, Léonard & Zeng, Cynthia & Soenksen, Luis & Fuentes, Ignacio. (2022). TabText: a Systematic Approach to Aggregate Knowledge Across Tabular Data Structures. 10.48550/arXiv.2206.10381.
2. Yin, Pengcheng & Neubig, Graham & Yih, Wen-tau & Riedel, Sebastian. TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data. ACL 2020.
3. Li, Y., Li, J., Suhara, Y., Doan, A., and Tan, W.-C. (2020). Deep entity matching with pre-trained language models. Proc. VLDB Endow., 14(1):50–60.
4. Stefan Hegselmann, Alejandro Buendia, Hunter Lang, Monica Agrawal, Xiaoyi Jiang, and David Sontag. Tabllm: Few-shot classification of tabular data with large language models. arXiv preprint arXiv:2210.10723, 2022.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
1. The detailed distribution of the proposed dataset is unclear;,1WJoJPXwiG,EMNLP_2023,"1. The detailed distribution of the proposed dataset is unclear;
2. Only three entities (company, organization, asset class) are annotated;
3. The experiments are a bit simple.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- The proposed method necessitates annotated labels for learning semantic tokens, limiting its application to supervised training. A self-supervised pretraining approach without annotations could be more appealing.",EraNITdn34,ICLR_2024,"- The contrastive loss with the label has limited novelty.
- The section on related works should be integrated into the main article, as it is difficult to discern the specific improvements in comparison to previous methods.
- While the authors experimented with diverse domains of datasets, both the pretraining and finetuning datasets for each experiment originate from the same dataset. It remains uncertain whether the proposed method can be generalized across domains.
- The proposed method necessitates annotated labels for learning semantic tokens, limiting its application to supervised training. A self-supervised pretraining approach without annotations could be more appealing.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Most continuous control experiments are performed on simple and low-dimensional tasks, such as cartpole or mountain car. To fully demonstrate the scalability of LFF, it’s important to show LFF can also help to solve more challenging DRL tasks with higher input dimensionality, such as locomotion of ants or humanoids.",ICLR_2022_3111,ICLR_2022,"of the paper: - Since standard FF is used in most experiments in this paper, it seems that the exponential growth of basis in standard FF is not a severe issue for tasks considered in the paper. As a result, the motivation and necessity of proposing LFF needs more elaboration. - Although this paper shows Fourier features bring neural net training benefits, the reason why these benefits convert to better sample efficiency remain missing. Considering how these benefits improve Q value function estimation accuracy is a possible way to bridge such gaps. - Most continuous control experiments are performed on simple and low-dimensional tasks, such as cartpole or mountain car. To fully demonstrate the scalability of LFF, it’s important to show LFF can also help to solve more challenging DRL tasks with higher input dimensionality, such as locomotion of ants or humanoids. - The PPO algorithm considered in this paper is a policy optimization algorithm, not originally designed to work together with value-based DQN methods. My suggestion is to add similar discussions for some actor-critic like algorithms, such as soft actor critic (SAC), A3C, etc. For such algorithms, improvement of DQN training efficiency should bring more performance gains. - It's better to add a related work section to systematically review previous feature-based methods in DRL or classic RL scenarios.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- in the abstract, I think it should say something like ""...attain greater expressivity, as measured by the change in linear regions in output space after [citation]. "" instead of just ""attain greater expressivity"" - it would be nice to see learning curves for all experiments, at least in an appendix.",NIPS_2018_743,NIPS_2018,"- quality: It seems to me that the chosen ""algorithm"" for choosing dendrite synapses is very much like dropout with a fixed mask. Introducing this sparsity is a form of regularization, and a more fair comparison would be to do a similar regularization for the feed-forward nets (e.g. dropout, instead of bn/ln; for small networks like this as far as I know bn/ln are more helpful for optimization than regularization). It also seems to me that the proposed structure is very much like alternating layers of maxout and regular units, with this random-fixed dropout; I think this would be worth comparing to. I think there are some references missing, in the area of similar/relevant neuroscience models and in the area of learned piecewise activation functions. It would be reassuring to mention the computation time required and whether this differs from standard ff nets. Also, most notably, there are no accuracy results presented, no val/test results, and no mention is made of generalization performance for the MNIST/CIFAR experiments. - clarity: Some of the sentences are oddly constructed, long, or contain minor spelling and grammar errors .The manuscript should be further proofread for these issues. For readers not familiar with the biological language, it would be helpful to have a diagram of a neuron/dendritic arbour; in the appendix if necessary. It was not 100% clear to me from the explanations whether or not the networks compared have the same numbers of parameters; this seems like an important point to confirm. - significance: I find it hard to assess the significance without generalizaion/accuracy results for the MNIST/CIFAR experiments. REPRODUCABILITY: For the most part the experiments and hyperparameters are well-explained (some specific comments below), and I would hope the authors would make their code available. SPECIFIC COMMENTS: - in the abstract, I think it should say something like ""...attain greater expressivity, as measured by the change in linear regions in output space after [citation]. "" instead of just ""attain greater expressivity"" - it would be nice to see learning curves for all experiments, at least in an appendix. - in Figure 1, it would be very helpful to show a FNN and D-Net with the same number of parameters in each (unless I misunderstood, the FNN has 20 and the DNN has 16). - There are some ""For the rest part"" -> for the rest of (or rephrase) - missing references: instead of having a section just about Maxout networks, I think the related work should have a section called something like ""learned piecewise-linear activation functions"" which includes maxout and other works in this category, e.g. Noisy Activation Functions (Gulcehre 2016). Also, it's not really my field but I believe there is some work on two-compartment models in neuroscience and modeling these as deep nets which would be quite relevant for this work. - It always bothers me somewhat when people refer to the brain as 'sparse' and use this as a justification for sparse neural networks. Yes, overall/considering all neurons the brain as one network it would be sparse, but building a 1000 unit network to do classification is much more analogous to a functional subunit of the brain (e.g. a subsection of the visual pathway), and connections in these networks are frequently quite dense. The authors are not the first to make this argument and I am certainly not blaming them for its origin, but I take this opportunity to point it out as (I believe) flawed. :) - the definition of ""neuron transition"" is not clear to me - the sentence before Definition 2 suggests that it is a change in _classification_ (output space), which leads to a switch in the linear region of a piecewise linear function, but the Definition and the subsequent sentence seem to imply it is only the latter part (a change from one linear region of the activation function to another; nothing to do with the output space). If the latter, it is not clear to me how/whether or not these ""transitions"" say anything useful about learning. If it is the former (more similar to Raghu et al), I find the definition given unclear. - I like the expressiveness experiments, but It would be nice to see some actual numbers instead of just descriptions. - unless I missed it somehow, the ""SNN"" is never defined. and it is very unclear to me whether it refers to a self-organizing neural network cited in [12]. or a ""sparse"" neural network, and in any case what exactly this architecture is. - also possible I missed it despite looking, but I could not find what non-linearity is used on D-Nets for the non-dendrite units OVERALL ASSESSMENT: My biggest issue with the paper is the lack of mention/results about generalization on MNIST/CIFAR, and the ambiguity about fair comparison. If these issues are resolved I would be very willing to change my rating. CONFIDENCE IN MY SCORE: This is the first time I've given a confidence of 5. With due credit to the authors, I believe I've understood most things about the paper, and I am familiar with the relevant work. Of course I'm not infallible and it's possible I've missed or misunderstood something, especially relating to the things I noted finding unclear.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- It's unclear what this paper's motivation is as I do not see a clear application from the proposed method. The paper showed results mapping one RGB image to another RGB image (with a different style). When do we need this domain adaptation, and how would this be useful? For example, it would have been better to demonstrate the methodology's use on some actual tasks involving domain adaptation, such as adapting a model trained on a synthetic dataset to a real dataset.",ICLR_2021_2929,ICLR_2021,"Weakness - It's unclear what this paper's motivation is as I do not see a clear application from the proposed method. The paper showed results mapping one RGB image to another RGB image (with a different style). When do we need this domain adaptation, and how would this be useful? For example, it would have been better to demonstrate the methodology's use on some actual tasks involving domain adaptation, such as adapting a model trained on a synthetic dataset to a real dataset. - Apart from the motivation, there are no comparisons against any other potential baseline approaches in the evaluation. For example, from the results shown in the paper and the supplementary material, I believe that a simple photographic style transfer method would achieve similar, if not better, effects. - When reporting the semantic segmentation IoU as a metric, it would be essential to show the baseline performance. What is the IoU when applying the segmentation model to the original visual domain? How much improvement can we get by mapping the images in the input domain to the output domain? Showing results only after the image-to-image translation is not informative. - The technical novelty of the paper is somewhat limited as well. Existing work (Pix2PixHD) has shown that one can improve the visual quality of the synthesized images using an instance edge map. This paper extends that to use edges (which include both object contour and internal structures). However, from the image synthesis perspective, this EPS input needs to be obtained from some input RGB images in the first place. Since we already start with an RGB image, why should we resort to this somewhat complicated image synthesis pipeline (as opposed to simple color/style transfer)?
In sum, while the paper showcased improved visual quality on the translated images, I have concerns about this paper in its unclear motivation and limited evaluation. I would appreciate it if the authors could clarify and, if possible, provide comparisons with the baselines (e.g., style transfer).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis, ACM MM 2020 2. M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation, CVPR workshop 2022 3. MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations, ICASSP 2022 * The paper regards MULT as the only deep learning based baseline that considers cross-sensory interaction but MULT was proposed in 2019 and thus sort of out of fashion.",NrI1OkZkiy,ICLR_2024,"* The motivation of this paper should be further enhanced. What issues do this paper address that previous works have not solved?
* I do not agree some statements in the introduction, e.g.’ the majority of these methods fail to consider the interaction between different senses’. There are tons of works that focus on multimodal/multisensory interactions. To name a few:
1. MISA: Modality-Invariant and -Specific Representations for Multimodal Sentiment Analysis, ACM MM 2020
2. M2FNet: Multi-modal Fusion Network for Emotion Recognition in Conversation, CVPR workshop 2022
3. MM-DFN: Multimodal Dynamic Fusion Network for Emotion Recognition in Conversations, ICASSP 2022
* The paper regards MULT as the only deep learning based baseline that considers cross-sensory interaction but MULT was proposed in 2019 and thus sort of out of fashion.
* Authors state their concern about MULT on computational efficiency, but I don’t see any discussion and comparison on efficiency so it is not clear how this method addresses this issue.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- (major) Comparison against other models in the experiments are unclear. The value of the used ranks for all the models are omitted which make not possible a fair comparison. To show the superiority of TW over TT and TR, the authors must compare the tensor completion results for all the models but having the same number of model parameters. The number of model parameters can be computed by adding the number of entries of all core tensors for each model (see my question about experiment settings below).",NIPS_2022_2592,NIPS_2022,"- (major) I don’t agree with the limitation (ii) of current TN models: “At least one Nth-order factor is required to physically inherit the complex interactions from an Nth-order tensor”. TT and TR can model complex modes interactions if the ranks are large enough. The fact that there is a lack of direct connections from any pair of nodes is not a limitation because any nodes are fully connected through a TR or TT. However, the price to pay with TT or TR to model complex modes interactions is having bigger core tensor (larger number of parameters). The new proposed topology has also a large price to pay in terms of model size because the core tensor C grows exponentially with the number of dimensions, which makes it intractable in practice. The paper lacks from a comparison of TR/TT and TW for a fixed size of both models (see my criticism to experiments below). - The new proposed model can be used only with a small number of dimensions because of the curse of dimensionality imposed by the core tensor C. - (major) I think the proposed TW model is equivalent to TR by noting that, if the core tensor C is represented by a TR (this can be done always), then by fusing this TR with the cores G_n we can reach to TR representation equivalent to the former TW model. I would have liked to see this analysis in the paper and a discussion justifying TW over TR. - (major) Comparison against other models in the experiments are unclear. The value of the used ranks for all the models are omitted which make not possible a fair comparison. To show the superiority of TW over TT and TR, the authors must compare the tensor completion results for all the models but having the same number of model parameters. The number of model parameters can be computed by adding the number of entries of all core tensors for each model (see my question about experiment settings below). - (minor) The title should include the term “tensor completion” because that is the only application of the new model that is presented in the paper. - (minor) The absolute value operation in the definition of the Frobenius norm in line 77 is not needed because tensor entries are real numbers. - (minor) I don’t agree with the statement in line 163: “Apparently, the O(NIR^3+R^N) scales exponentially”. The exponential grow is not apparent, it is a fact.
I updated my scores after rebuttal. See my comments below
Yes, the authors have stated that the main limitation of their proposed model is its exponentionally grow of model parameters with the number of dimensions.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"4. In Table 2, under the leave one out setting the proposed method only be compared to “+LFP”. As ATA is a bit better than FP according to the results in Table 1, it would be more convincing to also including it in the comparison.",ICLR_2022_2725,ICLR_2022,"1. The differences of the proposed instance normalization from other normalization methods, such as batch/group/layer normalization, should be explained in detail. What’s more, its advantages should be elaborated. 2. The memorized restitution sounds to be the most important contribution of the proposed method. For memorized restitution, why not consider to combine general feature map G with a memory bank? On the other hand, as D is updated with a memory vector, G can be computed as R - the updated D. Please clarify the logic behind using current G. 3. It would be better if more information about memory vector M is provided. To be specific, what is the relation between M and D in multiple iterations? 4. In Table 2, under the leave one out setting the proposed method only be compared to “+LFP”. As ATA is a bit better than FP according to the results in Table 1, it would be more convincing to also including it in the comparison. 5. The ablation study is helpful to check the significance of each module. It would be better to also show the results of w/o memory bank and w/o restitution.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2: the normalization module seems different in the two versions, but reading the text it seems the very same. Figures are great value for giving an intuition of how a system works, but a standardization of the pictograms is needed. Fig. 4 is a bit confusing in the 0/50 latency range, 2.5/4.0 MAE: the chosen symbols overlap. minor problems about the text: -- pag. 4, after eq.",ICLR_2022_2033,ICLR_2022,"The choice to call with the same name (EfficientPhys) two different approaches is a bit confusing. Models are indeed three: EfficientPhys-C (convolution), EfficientPhys-T1 (transformer-regular), EfficientPhys-T2 (transformer-shrinked). They share the same normalization block and arguably a similar output block, but with a different core. They also perform in a very different way: only '-C' outperforms other approaches in terms of MAE, RMSE, ro (except DualGAN), and is the Pareto optimum for MAE/latency; '-T1' is good in terms of MAE, RMSE, ro, but not in latency; '-T2' in contrast show interesting latency but not so good accuracy. It is difficult to consider these models as unique, and results can not be claimed for EfficientPhys but for EfficientPhys-C.
the lack of a complete comparison with the best other approach, DualGAN, is disappointing. As explained by the authors, this is due to the lack of results of DualGAN on the public datasets PURE, MMSE, and cannot be totally addressed to them. Nonetheless, this diminishes the claimed results since DualGAN outperforms EfficientPhys models in UBFC dataset.
Self-Attention-Shifted Network is described by eq. 3 which is too verbose and somehow obscure, and needs to be better explained as it describes a core component of the model. It fails to give an intution of how the module works.
It is not explained at all which is the final task (and consequently modules on top of architecture) of both models: probably regression?
Fig. 2: the normalization module seems different in the two versions, but reading the text it seems the very same. Figures are great value for giving an intuition of how a system works, but a standardization of the pictograms is needed.
Fig. 4 is a bit confusing in the 0/50 latency range, 2.5/4.0 MAE: the chosen symbols overlap.
minor problems about the text: -- pag. 4, after eq. 1: 'To address this, we add a batch-normalization layer followed by the difference layer' should be: 'To address this, we add a batch-normalization layer following the difference layer' -- pag. 5, second paragraph of section 3.2: the first sentence, 'Since the 2D Swin transfromer...', is not correct and needs to be rephrased -- pag. 9, last two sentences of section 6: 'However, we are aware...': they are quite involuted and the meaning is not clear.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1.The author claim that ‘The observation consistently shows that only parts of subdivision splines are useful for decision boundary; and the goal of pruning is to remove those (redundant) subdivision splines and find winning tickets.’, however, in theoretical part, the author didn’t provide how the proposed algorithm in detail to remove the subdivision splines. Will the algorithm need extra computation cost for such space partition building?",ICLR_2022_1905,ICLR_2022,"Weakness: 1.The author claim that ‘The observation consistently shows that only parts of subdivision splines are useful for decision boundary; and the goal of pruning is to remove those (redundant) subdivision splines and find winning tickets.’, however, in theoretical part, the author didn’t provide how the proposed algorithm in detail to remove the subdivision splines. Will the algorithm need extra computation cost for such space partition building? 2. When the author introduces the proposed algorithm, the author didn’t analysis if such method has the same convergence guarantee as Lottery Ticket Hypothesis. If so, what is the bound of the error probability? 3.In the experiment, the author didn’t consider Vision Transformer, which is an important SOTA model in image classification. And it is unsure if such technique is still working for larger image dataset such as ImageNet. Will the pruning strategy will be different in self attention layers?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2: W1 and W2 are not defined. I guess they denote the Encoder and the Decoder network. p.3, A4, eq.3: W and V not defined, same as above Eq.",ICLR_2022_1912,ICLR_2022,"(OR SUGGESTIONS) ==
There is a certain amount of minor typos (see some below) but also a lack of some term definitions (because of late copy-paste in Appendix to observe the page limitation rule probably). Table 3 is actually very useful for the understanding, I would not put it in Appendix.
Some things which could be clarified:
Reference to deep learning methods is done for state-of-the-approaches. In the paper, it seems it is more about shallow networks (max 2 layers) that are discussed.
p.3, A3, Eq. 2: W1 and W2 are not defined. I guess they denote the Encoder and the Decoder network.
p.3, A4, eq.3: W and V not defined, same as above
Eq. 4: N is not defined
p.4, A6: M not defined.
Eq. 13, p.7: operation dMat not defined (too late to put it in p.2 of supplementary material), same in eq. for W* (which should be indexed)
Also to help the reader, it would be nice to start Sections 2 and 3 with a summary of what the section plans to achieve/demonstrate. In particular, this is quite difficult in Section 3 to follow the objective. For instance, a sentence such as ""Prop. 2 also leads to the following Corollary."" does not help to understand the implication of such result.
Unless I missed it, differences of implication between LR-EDLAE-1 and LR-EDLAE-2 (the proposed methods) are not enough carefully detailed.
In Table 1, it would be clearer for the reader to identify the best methods for instance, 1) by putting in bold the best obtained results and 2) by underlying the 2nd best result. Table 1 refers to Table 3 for more details: I think this is a mistake because it does not let the paper ""self-contained"".
Am I missing something or Mult-VAE/DAE are used as baselines during experiments but are not discussed before (contrarily to the other approaches)? Why?
Some typos :
in abstract : « (surprisnig) »
abstract: missing ""-"" for low-rank and closed form
p.1 in introduction: ""the linear autoencoders [...] which encompasses""
missing upper case : p.2, 2nd paragraph « . we generalize the »
p.2 missing ""-"", ""These models produce closed form full-rank estimators""; ""However, no closed form solutions""; ""ADMM based solutions""; ""the full rank W""
end of p.2 : « The weighted unclear norm »
beginning of p. 3 : missing lower case « Therefore, Nuclear-norm regularizers »
inconsistency in convention naming for equations : eq. Or Eq. / eq (1) or eq. 1 ; 5 times in p. 3, same for Table or table, Proposition or Prop.
p.3, A2 : « This approach useS »
p.3, A4 : « probabolistic »
p.4, first row: choose between ""hyperparameter"" (p.3 after Lemma 1) and ""hyper-parameter"" (p. 4)
p.4, (ii): ""choices of p [...] produces"" ; ""all entries in X is""
p.4, (ii): inconsistency in naming convention: nuclear-norm-based and nuclear-norm based in the same sentence, sometimes it is nuclear norm (check everywhere in the paper)
Before eq.7: ""Its a closed-form solution is.""
choose between ""Frobenius-norm regularizer"" p.4,5 and ""Frobenius norm regularizers"" p.5
before Sec.3, p.5: ""two low-rank Frobenius-norm-based modelS""
p.8 in Table 1: ""Frobinius""
eq.8, missing "" "" in 3rd norm, extra space to remoce: ""equivalently ,""
missing ""-"" for closed form solutions after Proposition 2 p.5, in Section 4 p.7, in Section 5 p.7, in Q1 p.8, Q.2 in p.9 twice, Q.3, in conclusion ; check for low-rank everywhere also
p.5 choose between ""rearrangement"" and ""re-arrangement""
Corollary 1, p.6: Missing Eq. before (9)
Section 4, p.7: missing ""-"" for state-of-the-art
Section 4, p.7: after ADMM: \cite instead of \citep, same after closed-form and EDLAE
Table 1: refer TO
p.8 in Q1: (eq. (11))
p8, Q1: ""one of the most popular implicit matrix factorization algorithmS""
Q2: ""most of them reaches""
Q2, missing ""-"" in EDLAE based approaches
Q2, p.9: ADMM method performS slightly better ; none-the-less
p.9: ""all the models add either a nuclear-norm-based [...] or a Frobenius-norm based regularizer."" ; check also the abstract
p.9 ""The Frobenius-norm models are more express""
Supplementary: section A autoencoder vs auto-encoder -Supp, section A: \cite instead of \citep for 2nd paragraph","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- The comparison with some baselines is somehow unfair since they lack the prior knowledge of users or any language embedding computation. A better comparison should be considered.,i3e92uSZCp,ICLR_2025,"- The experimental scenarios are simple, in which the exampled prompts and semantically controlled spaces are easy to follow yet fail to demonstrate the generalizablity and scalability --- after all, the method relies much on the description of states. LGSD’s dependence on LLMs for real-time distance evaluation might limit scalability to complex, real-time environments.
- As I understand, users have to provide specified ""skill constraints"" (via prompts, such as ""move north"" etc.), then how can it still be called ""skill discovery"" since users are specifying skills?
- The comparison with some baselines is somehow unfair since they lack the prior knowledge of users or any language embedding computation. A better comparison should be considered.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3) why the output-side layers do not benefit from it? Furthermore, Figure 4 is not clearly illustrated. The details of Pixel-shuffle are not clearly presented. Is it the pixel-shuffle operation used in the super-resolution field? Then, why the dimensionality remains the same after upsampling in Figure 2. (b)? The authors did not provide the limitations and potential negative societal impact of their work.",NIPS_2022_2523,NIPS_2022,"Novelty is incremental. The major change over the baseline ResTv1 is the pixel-shuffle only, and the rest of the modifications are not new and cannot be one of the contributions.
Any intuitions or insights of why the architecture should be designed like this are missing: why the upsampling module should be involved? What can we learn from the architectural modifications from ResTv1 to RestV2 such as the block number at the first stage is halved.
Experimental justifications in Sections 3.4 and 4.3 do not seem to be enough backups for the explanation of the proposed architectural design. For example, Figure 3 tells us the upsampling module seemingly reduces the difference in the log amplitude between particular frequencies and the center frequency. However, this does not indicate the upsampling module is necessarily used. Furthermore, one may naturally ask the questions: 1) why do some specific frequencies only benefit from information recovery?; 2) If the upsampling module really helps information flow, shouldn't the entire frequency have the same effect?; 3) why the output-side layers do not benefit from it? Furthermore, Figure 4 is not clearly illustrated.
The details of Pixel-shuffle are not clearly presented. Is it the pixel-shuffle operation used in the super-resolution field? Then, why the dimensionality remains the same after upsampling in Figure 2. (b)?
The authors did not provide the limitations and potential negative societal impact of their work.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance?,NIPS_2018_857,NIPS_2018,"Weakness: - Long range contexts may be helpful for object detection as shown in [a, b]. For example, the sofa in Figure 1 may help detect the monitor. But in the SNIPER, images are cropped into chips, which makes the detector cannot benefit from long range contexts. Is there any idea to address this? - The writing should be improved. Some points in the paper is unclear to me. 1. In line 121, authors said partially overlapped ground-truth instances are cropped. But is there any threshold for the partial overlap? In the lower left figure of the Figure 1 right side, there is a sofa whose bounding-box is partially overlapped with the chip, but not shown in a red rectangle. 2. In line 165, authors claimed that a large object which may generate a valid small proposal after being cropped. This is a follow-up question of the previous one. In the upper left figure of the Figure 1 right side, I would imagine the corner of the sofa would make some very small proposals to be valid and labelled as sofa. Does that distract the training process since there may be too little information to classify the little proposal to sofa? 3. Are the negative chips fixed after being generated from the lightweight RPN? Or they will be updated while the RPN is trained in the later stage? Would this (alternating between generating negative chips and train the network) help the performance? 4. What are the r^i_{min}'s, r^i_{max}'s and n in line 112? 5. In the last line of table3, the AP50 is claimed to be 48.5. Is it a typo? [a] Wang et al. Non-local neural networks. In CVPR 2018. [b] Hu et al. Relation Networks for Object Detection. In CVPR 2018. ----- Authors' response addressed most of my questions. After reading the response, I'd like to remain my overall score. I think the proposed method is useful in object detection by enabling BN and improving the speed, and I vote for acceptance. The writing issues should be fixed in the later versions.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
2. What if the patients are the first time visitors without historical reports. The authors need to evaluate the proposed approach on new patients and old patients respectively.,tSfZo6nSN1,EMNLP_2023,"1. The proposed approach fails to outperform existing works. For example, in Table 1, the B-4 of proposed approach is lower than the basic baseline ViT-transformer on MIMIC-ABN. Why the ViT-transformer is not evaluated on MIMIC-CXR data set.
2. What if the patients are the first time visitors without historical reports. The authors need to evaluate the proposed approach on new patients and old patients respectively.
3. The experiment setting is not fair. For the proposed approach, the historical reports of patients are used to generate reports for current input. While these data are unseen to baseline works. These historical reports should be added into the training data set of baselines.
4. One existing work which also includes historical reports in modeling should be referenced and discussed.
DeltaNet: Conditional medical report generation for COVID-19 diagnosis, Coling 2022.
5. Since the proposed approach targets to mine the progress of diseases to generate better results. Such intermediate results (the disease progress) should be evaluated in experiments.
6. The IU data set should be included in experiments.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
• Insightful discussions.• Method and motivation are easy to follow.,NIPS_2022_1567,NIPS_2022,", I want to discuss the novelty aspect of this paper. On one hand, the novelty is not remarkable: the authors are adopting the Swin backbone, while concatenation fusion is also used in Stark. However, the novelty is not limited to these aspects. On the contrary, I believe that this paper brings substantial value to the tracking field by consolidating existing techniques, while investigating important details in order to achieve a simple, yet highly powerful tracking framework. Importantly the authors provide valuable insights when motivating their approach and comparing to other techniques (modifications of fusion, transformer architecture, losses, etc.). Lastly, I find the motion token a very interesting novelty that could reopen a long-forgotten direction in tracking, namely exploiting motion prediction and other dynamic information. Although seemingly incremental at the first glance, I consider the novelty to be significant based on how much this paper advances the useful knowledge in the field.
Other strong points of the paper are:
• Very strong results, clearly SOTA.
• Simple and elegant architecture.
• Insightful discussions.
• Method and motivation are easy to follow.
• Interesting ablative experiments on multiple datasets.
• Relatively fast frame-rates. Weaknesses:
Details regarding pre-training are missing. I assume that the authors use ImageNet-22k pre-training, while most trackers use ImageNet-1k. This has shown to give about 2-3% on LaSOT in previous papers. The authors should therefore analyze this in a separate experiment.
By looking into the more detailed results in the supplementary material, it seems that the improvements mostly stem from increased accuracy. That is, better bounding box regression. The robustness (low overlap scores in the success plot) seems to be on par with recent trackers. While accuracy is also important, the major challenge in tracking is to improve the robustness. It is important to discuss these aspects in the paper in order to understand where and how SwinTrack performs better compared to other trackers. Moreover, please add more high-performing trackers to the success plots in the supplementary material.
Unfortunately, the design of the motion token is motivated. For instance, why are the past box encodings concatenated in the channel dimension, and not processed in some other manner? Why add it as a single token in the transformer, instead of one per box?
Results on UAV and VOT should be added, even if the tracker does not beat SOTA.
There are quite a few language mistakes.
I could not find discussion of negative social impact or limitations. It would be good to add these.","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', 'X']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0']}",,,"{'annotators': ['HXjcIUXf'], 'labels': ['1']}",,
"1. Unclear experimental methodology. The paper states that 300W-LP is used to train the model, but later it is claimed same procedure is used as was used for baselines. Most baselines do not use 300W-LP dataset in their training. Is 300W-LP used in all experiments or just some? If it is used in all this would provide an unfair advantage to the proposed method.",NIPS_2019_220,NIPS_2019,"1. Unclear experimental methodology. The paper states that 300W-LP is used to train the model, but later it is claimed same procedure is used as was used for baselines. Most baselines do not use 300W-LP dataset in their training. Is 300W-LP used in all experiments or just some? If it is used in all this would provide an unfair advantage to the proposed method. 2. Missing link to similar work on Continuous Conditional Random Fields [Ristovski 2013] and Continuous Conditional Neural Fields [Baltrusaitis 2014] that has a similar structure of the CRF and ability to perform exact inference. 3. What is Gaussian NLL? This seems to come out of nowhere and is not mentioned anywhere in the paper, besides the ablation study? Trivia: Consider replacing ""difference mean"" with ""expected difference"" between two landmarks (I believe it would be clearer)","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"2. Some technique behind the algorithm may not be that novel, such as computation offloading and gradient augmentation.",ICLR_2023_2721,ICLR_2023,"Weakness: 1.In 2-WAY GRADIENT TRANSFER, the client's gradient information will be passed to the server. However, issues related to data privacy during gradient tranmission do not appear to be explored in the paper. 2. Some technique behind the algorithm may not be that novel, such as computation offloading and gradient augmentation.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1) I have understood that the integral in Equation (1) corresponds to bag observation model in [Law et al., NeurIPS'18] or spatial aggregation process in [4]. The formulation introduced by the authors assume that the observations are obtained by averaging over the corresponding support $v$. However, the data might be aggregated by another procedure, e.g., simple summation or population weighted average; actually the disease incident data are often available in count, or rate per the number of residents.",NIPS_2019_1350,NIPS_2019,". Some important related works are not discussed. Multi-task (i.e., multivariate) GPs have been widely studied in machine learning community. Although most of them assume that data values are associated with points, it would be better to mention several related multi-task GPs (e.g., [1],[2],[3]). Especially, [1] designed the dependent GP by a linear mixing of latent GPs, which is similar to this submission. Also, there is an important related work missing here: [4]. I think this paper essentially addressed a related task: Predicting the fine-grained data by using auxiliary data sets with various granularities. I would like the authors to clarify the differences and advantages of this submission. [Quality] Strengths. This paper is technically sound except for some concerns. The authors evaluate the proposed model in the simple experimental setting using synthetic and real data sets. Weaknesses. My concerns about the proposed model are as follows: 1) I have understood that the integral in Equation (1) corresponds to bag observation model in [Law et al., NeurIPS'18] or spatial aggregation process in [4]. The formulation introduced by the authors assume that the observations are obtained by averaging over the corresponding support $v$. However, the data might be aggregated by another procedure, e.g., simple summation or population weighted average; actually the disease incident data are often available in count, or rate per the number of residents. 2) In order to handle various data types (e.g., count and rate), shouldn't the corresponding aggregation processes be performed at the likelihood level? 3) I think it would be more efficient to estimate ${a_{d,q}}$ instead of $B_q$ since $b^q_{d,d'} = a_{d,q}a_{d',q}$. The major weakness of this submission is in the experiments. First, the proposed model should be compared with any typical baseline, such as regression-based model with aggregation process (e.g., Law et al., NeurIPS'18, [4]) and multi-task GP with point-referenced data (e.g., [1]). I believe the previous multi-task GP can be applied via the simplification; that is, each data value at the support $v$ is assumed to be associated with the representative point (e.g., centroid) of its support (as in the previous work [4]). Second, the extensive experiments are helpful to verify the effectiveness of the proposed model. In all the experiments, the authors consider two tasks. I would like to see the experimental results considering more tasks; then it is a good idea to discuss how to determine the number of latent GPs $Q$. Short question: I was wondering if you could give me the detail of *resolution 5 \times 5* in the experimental setting of fertility rates. [Clarity] This paper is easy to understand. Some typos: 1) In line 235, *low-cost* should be *low-accuracy*? 2) In line 239, *GP process* should be *GP*. [Significance] Aggregated data with different supports are commonplace in a wide variety of applications, so I think this is an important problem to tackle. However, the major weakness of the submission in my view is that the evaluation of the proposed model is not enough, so the effectiveness/usefulness of the model is unclear from the experimental results. I think it would be great to compare the proposed model with baseline methods. [1] Y. W. Teh et al., Semiparametric Latent Factor Models, AISTATS, 333-340, 2005. [2] P. Boyle et al., Dependent Gaussian Processes, NeurIPS, 217-224, 2005. [3] E. Bonilla et al., Multi-task Gaussian Process Prediction, NeurIPS, 153-160, 2008. [4] Y. Tanaka et al., Refining Coarse-grained Spatial Data Using Auxiliary Spatial Data Sets with Various Granularities, AAAI, 2019. https://arxiv.org/abs/1809.07952 ------------------------------ After author feedback: I appreciate the responses to my questions. The new experimental results in the rebuttal is a welcome addition. In light of this, I upgraded my score. The proposal is a combination of the coregionalization and the concept of aggregation process used in block-kriging; this is a simple but effective way. I also agree that a sensor experiment is one of the applications with the proposed model. But I'm still of the opinion that there is not enough experiments and/or discussions to support the authors' claims. The authors state that the model is a general framework and has many applications related to geostatistics (lines 14-23); the support $v$ corresponds to the 2-dimensional region, e.g., borough (line 92). As described in Related work (lines 222-229), the proposed model strongly relates to spatial downscaling and disaggregation in geostatistics. If anything, I think this application that contains spatial aggregation is a more critical one for the proposed model. In the spatial data setting, a wide variety of data sets is available at various spatial granularities (for instance, New York City publish open data in [https://opendata.cityofnewyork.us]). Naturally, one would like to handle these data sets simultaneously (as in Law et al., NeurIPS'18, [4]); namely the setting with a large number of tasks. In that case, I believe the authors should discuss several issues; for example, the sensitivity of the number of latent GPs $Q$, the approximate accuracy of integral over regions, etc. I think it would be better to clarify the scope of this study and discuss the above issues.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- No in-depth analysis. The authors found inverse scaling happens over compute, but why? It would make the paper much more solid if the authors can provide some analysis explaining such training dynamics.",5BWvVIa5Uz,EMNLP_2023,"- The contribution is too limited. The paper only took a pre-trained model family and evaluated them on 4 existing datasets.
- No in-depth analysis. The authors found inverse scaling happens over compute, but why? It would make the paper much more solid if the authors can provide some analysis explaining such training dynamics.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
". Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend acceptance. The number of updates needed to learn realistic brain-like representations is a fair criticism of current models, and this paper demonstrates that this number can be greatly reduced, with moderate reduction in Brain-Score. I was surprised that it worked so well. Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment.",ICLR_2021_973,ICLR_2021,".
Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend acceptance. The number of updates needed to learn realistic brain-like representations is a fair criticism of current models, and this paper demonstrates that this number can be greatly reduced, with moderate reduction in Brain-Score. I was surprised that it worked so well.
Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. - Is the third method (updating only down-sampling layers) meant to be biologically relevant? If so, can anything more specific be said about this, other than that different cortical layers learn at different rates? - Given that the brain does everything in parallel, why is the number of weight updates a better metric than the number of network updates?
Provide additional feedback with the aim to improve the paper. - Bottom of pg. 4: I think 37 bits / synapse (Zador, 2019) relates to specification of the target neuron rather than specification of the connection weight. So I’m not sure its obvious how this relates to the weight compression scheme. The target neurons are already fully specified in CORnet-S. - Pg. 5: “The training time reduction is less drastic than the parameter reduction because most gradients are still computed for early down-sampling layers (Discussion).” This seems not to have been revisited in the Discussion (which is fine, just delete “Discussion”). - Fig. 3: Did you experiment with just training the middle Conv layers (as opposed to upsample or downsample layers)? - Fig. 3: Why go to 0 trained parameters for downstream training, but minimum ~1M trained parameters for CT? - Fig. 4: On the color bar, presumably one of the labels should say “worse”. - Section B.1: How many Gaussian components were used, or how many parameters total? Or if different for each layer, what was the maximum across all layers? - Section B.3: I wasn’t clear on the numbers of parameters used in each approach. - D.1: How were CORnet-S clusters mapped to ResNet blocks? I thought different clusters were used in each layer. If not, maybe this could be highlighted in Section 4.","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1']}",,
"- While the general architecture of the model is described well and is illustrated by figures, architectural details lack mathematical definition, for example multi-head attention. Why is there a split arrow in Figure 2 right, bottom right? I assume these are the inputs for the attention layer, namely query, keys, and values. Are the same vectors used for keys and values here or different sections of them? A formal definition of this would greatly help readers understand this.",NIPS_2017_575,NIPS_2017,"- While the general architecture of the model is described well and is illustrated by figures, architectural details lack mathematical definition, for example multi-head attention. Why is there a split arrow in Figure 2 right, bottom right? I assume these are the inputs for the attention layer, namely query, keys, and values. Are the same vectors used for keys and values here or different sections of them? A formal definition of this would greatly help readers understand this.
- The proposed model contains lots of hyperparameters, and the most important ones are evaluated in ablation studies in the experimental section. It would have been nice to see significance tests for the various configurations in Table 3.
- The complexity argument claims that self-attention models have a maximum path length of 1 which should help maintaining information flow between distant symbols (i.e. long-range dependencies). It would be good to see this empirically validated by evaluating performance on long sentences specifically.
Minor comments:
- Are you using dropout on the source/target embeddings?
- Line 146: There seems to be dangling ""2""","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The result for the proposed algorithm seems to require an additional assumption that each individual’s data is iid drawn from the same distribution. Otherwise I don’t see how that \sqrt{m} argument in the beginning of Section 5.1 works out and how Theorem 6 can be applied to prove Theorem 7. I find such assumption unjustifiable because in practice, each users’ preferred set of “emojis” are very different.",NIPS_2020_1759,NIPS_2020,"- The result for the proposed algorithm seems to require an additional assumption that each individual’s data is iid drawn from the same distribution. Otherwise I don’t see how that \sqrt{m} argument in the beginning of Section 5.1 works out and how Theorem 6 can be applied to prove Theorem 7. I find such assumption unjustifiable because in practice, each users’ preferred set of “emojis” are very different. - It is unreasonable to assume each user contributes m data points. Usually users are very heterogeneous in terms of the number of data points they produce. This affects the user-complexity calculations and makes the problem more interesting. In the worst case, m can be as large as n, still with appropriate truncation, one can obtain meaningful frequency estimates with data-dependent privacy-mechanisms and utility bounds. Unfortunately this is not considered in the paper. - Han et al that the authors cited for the lower bounds of non-private estimation of L1-distance establishes an adaptive (upper and lower) bound that depends on the entropy of the distribution. While this, in the worst case, is proportional to k, it is often much smaller. The results in this paper do not seem to concern dependence on k. Replacing k with the entropy in the term that is introduced by DP will make the result much more interesting. - The research in estimating discrete distributions have evolved quite a bit nowadays. Other loss functions are of interest too, e.g., KL-divergence, \chi-square distance. Those metrics sometimes allow for more interesting rates and more interesting worst-case dependence on k (often log k). There are competitive notions of optimality that I encourage the authors to look into, see, e.g.: Orlitsky, A., & Suresh, A. T. (2015). Competitive distribution estimation: Why is good-turing good. In Advances in Neural Information Processing Systems (pp. 2143-2151).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- With a fixed policy, this setting is a subset of reinforcement learning. Can tasks get more complicated (like what explained in the last paragraph of the paper) so that the policy is not fixed. Then, the authors can compare with a reinforcement learning algorithm baseline.",NIPS_2016_93,NIPS_2016,"- The claims made in the introduction are far from what has been achieved by the tasks and the models. The authors call this task language learning, but evaluate on question answering. I recommend the authors tone-down the intro and not call this language learning. It is rather a feedback driven QA in the form of a dialog. - With a fixed policy, this setting is a subset of reinforcement learning. Can tasks get more complicated (like what explained in the last paragraph of the paper) so that the policy is not fixed. Then, the authors can compare with a reinforcement learning algorithm baseline. - The details of the forward-prediction model is not well explained. In particular, Figure 2(b) does not really show the schematic representation of the forward prediction model; the figure should be redrawn. It was hard to connect the pieces of the text with the figure as well as the equations. - Overall, the writing quality of the paper should be improved; e.g., the authors spend the same space on explaining basic memory networks and then the forward model. The related work has missing pieces on more reinforcement learning tasks in the literature. - The 10 sub-tasks are rather simplistic for bAbi. They could solve all the sub-tasks with their final model. More discussions are required here. - The error analysis on the movie dataset is missing. In order for other researchers to continue on this task, they need to know what are the cases that such model fails.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. A majority of the experiments focus on the presentation of results. The analyses of the method itself and the experimental outcomes are not comprehensive enough. Given that the authors' method underperforms the baseline in some instances, one might question to what extent the performance improvement brought by this pretraining method can be attributed to the authors' claim of ""moving code-switched pretraining from the word level to the sense level, by leveraging word sense-specific information from Knowledge Bases"".",Bou2YHsRvG,EMNLP_2023,"1. For some experimental results, there is a lack of reasonable and sufficient explanation. For instance, in figure 3, the authors' method underperforms the baseline in the en-fr and fr-en settings. The reason and analysis for this are missing.
2. A majority of the experiments focus on the presentation of results. The analyses of the method itself and the experimental outcomes are not comprehensive enough. Given that the authors' method underperforms the baseline in some instances, one might question to what extent the performance improvement brought by this pretraining method can be attributed to the authors' claim of ""moving code-switched pretraining from the word level to the sense level, by leveraging word sense-specific information from Knowledge Bases"".","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1. This paper mainly focuses on explaining multi-task models, which somehow limits the applicability.",ICLR_2022_1255,ICLR_2022,"Weakness 1. This paper mainly focuses on explaining multi-task models, which somehow limits the applicability. 2. Why does the author use $\boldsymbol{p}\otimes \mathcal{E}(\mathcal{T}\theta(\boldsymbol{p}, G)) b u t p\otimes\mathcal{E}(\mathcal{T}\theta(\boldsymbol{p},G)) . I f t h e r e i s a p e r f o r m a n c e g a p b e t w e e n t h i s t w o f o r m u l a t i o n s , I w o n d e r h o w e a c h o f t h e m a f f e c t t h e q u a l i t y o f g e n e r a t e d e x p l a n a t i o n s . 3. D u r i n g t h e s e l f − t r a i n i n g o f t h e e m b e d d i n g m o d e l ,
p$ is sampled from a multivariate Laplace distribution, while later, the input is the conditional embeddings generated by the gradient. The distributions of the two groups of inputs could be different numerically and thus may affect the specific performance of the embedding model. Can the authors comment on this a bit? 4. Some typos: last row of page 5 “as input” should be “an input”; In Section 4.1, “include four graph classiﬁcation tasks” should be “include three graph classiﬁcation tasks”.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1) in the next section. Literature review ignores several papers that are seemed to be relevant [1], [2]. It seems VR-MARINA for online problems from [1] and DASHA-MVR from [2] both satisfy Assumption 2 and have a better rate than QSGD in the stochastic regime. See Question",NIPS_2022_948,NIPS_2022,"The main theoretical flaw is that the analysis of NEOLITHIC relies on a restrictive Assumption 5 (bounded dissimilarity): 1 n ∑ i = 1 n ‖ ∇ f i ( x ) − ∇ f ( x ) ‖ 2 ≤ b 2 , ∀ x ∈ R d
One can easily come up with an example for which this assumption does not generally hold. For example, let us consider f i ( x ) = x ⊤ A i x
, where A i ∈ R d × d
. Since ∇ f i ( x ) = B i x
, where B i = A i + A i ⊤
. The bounded dissimilarity assumption (Assumption 5), which can be written in the form 1 n ∑ i = 1 n | ( B i − 1 n ∑ j = 1 n B j ) x | 2 ≤ b 2
, also does not hold, unless B i = B j
for all i , j
, which reduces to the identical data regime, which is of limited interest.
Some details on the experimental setting are missing. See Question 1) in the next section.
Literature review ignores several papers that are seemed to be relevant [1], [2]. It seems VR-MARINA for online problems from [1] and DASHA-MVR from [2] both satisfy Assumption 2 and have a better rate than QSGD in the stochastic regime. See Question 2) in the next section.
Table 1 contains possible typos:
It mentions a paper on MEM-SGD that does not have a non-convex rate. Their rate is applicable to strongly convex functions. I would recommend here to mention another, more relevant work [3]. See Question 3) in the next section.
Similar problems with CSER, Double Squeeze, and QSGD. See corresponding Questions 4), 5) and 6) in the next section. References:
[1] Gorbunov, Eduard, Konstantin Burlachenko, Zhize Li, and Peter Richtárik. 2021. “MARINA: Faster Non-Convex Distributed Learning with Compression.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2102.07845.
[2] Tyurin, Alexander, and Peter Richtárik. 2022. “DASHA: Distributed Nonconvex Optimization with Communication Compression, Optimal Oracle Complexity, and No Client Synchronization.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2202.01268.
[3] Koloskova, A., Lin, T., Stich, S. U., & Jaggi, M.. Decentralized deep learning with arbitrary communication compression. arXiv preprint arXiv:1907.09356, 2019
I would recommend making the y-axis in the right subfigure of Figure 1 logarithmic scale. Otherwise, it is hard to distinguish plots corresponding to different methods.
One more possibly relevant and missing citation is [7]. The Algorithm 3PCv3 (Appendix C.6, page 26) already employs a similar nested structure as proposed in Paper8019 by FCC.
I would recommend running least squares and logistic regression experiments for a longer period. It looks like the methods on the left subfigure of Figure 1 and Figure 2 were stopped quite early and did not reach the SGD-specific oscillation region.
Some minor notes:
(line 685): instead of Cauchy-Schwarz, one needs to refer to Young's inequality for product;
(line 693): instead of Cauchy-Schwarz, one needs to refer to Jensen's inequality
FINAL REMARKS:
I would be happy to rate this paper an 8 for its solid theoretical contributions and reliable experiments.
However, at this moment, I can not do so since the paper still contains several crucial issues that are needed to be clarified or fixed.
I am ready to reconsider my current rate during rebuttals once you respond to me on Weaknesses 2-4 and Questions 1 - 6.
UPDATE: After the Authors-Reviewers discussion, I decided to increase the score since my concerns were resolved.
References: [7] Richtárik, Peter, Igor Sokolov, Ilyas Fatkhullin, Elnur Gasanov, Zhize Li, and Eduard Gorbunov. 2022. “3PC: Three Point Compressors for Communication-Efficient Distributed Training and a Better Theory for Lazy Aggregation.” arXiv [cs.LG]. arXiv. http://arxiv.org/abs/2202.00998.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
2. The presentation of this paper is hard to follow for the reviewer.,NIPS_2022_670,NIPS_2022,1. Lack of numerical results. The reviewer is curious about how to apply it to some popular algorithms and their performance compared with existing DP algorithms. 2. The presentation of this paper is hard to follow for the reviewer.,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '0']}",,
"1. Do you have any additional insights into modest performance gains on Clothing1M 2. How does the algorithm perform on other real-world datasets like WebVision, evaluated by DivideMix?",ICLR_2021_2926,ICLR_2021,"and suggestions: 1. It is not clear to me if the warm-up phase makes a difference in performance on larger, more realistic datasets like Clothing1M. More careful analysis of how the warm-up phase affects the sample separation in SSL versus a fully supervised setting would have been useful, including experiments on CIFAR-10. 2. Additional experiments on realistic noisy datasets like WebVision would have provided more support for C2D. 3. The paper is not clearly written. Important components like MixMatch are not explained. For instance, the Method section contains discussion on various design decisions, rather than a step-by-step description of the method itself. An algorithm figure detailing C2D method would be useful for exposition. In sum, the paper definitely has a good idea and interesting results, but it is not well-structured, which makes it harder to parse the method and results.
Questions and suggestions: 1. Do you have any additional insights into modest performance gains on Clothing1M 2. How does the algorithm perform on other real-world datasets like WebVision, evaluated by DivideMix?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- While not familiar with the compared models DMM and DVBF in details, the reviewer understood from the paper their differences with KVAE. However, the reviewer would appreciate a little bit more detailed presentation of the compared models. Specifically, the KVAE is simpler as the state space transition are linear, but it requires the computation of the time-dependant LGSSM parameters \gamma. Can the authors comment on the computation requirements of the 3 methods compared in Table 1 ?",NIPS_2017_345,NIPS_2017,"of the paper are mainly on the experiments:
- While not familiar with the compared models DMM and DVBF in details, the reviewer understood from the paper their differences with KVAE. However, the reviewer would appreciate a little bit more detailed presentation of the compared models. Specifically, the KVAE is simpler as the state space transition are linear, but it requires the computation of the time-dependant LGSSM parameters \gamma. Can the authors comment on the computation requirements of the 3 methods compared in Table 1 ?
- Why the authors did not test DMM and DVBF on the task of imputing missing data ?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,
"1. The authors conduct experiments on T5, PaLM and GPT series LLMs and show the influence of parameter size on benchmark score. However, I think more experiments on different famous LLMs like LLaMA, Falcon, etc are needed as benchmark baselines.",q38SZkUmUh,ICLR_2024,"1.	The authors conduct experiments on T5, PaLM and GPT series LLMs and show the influence of parameter size on benchmark score. However, I think more experiments on different famous LLMs like LLaMA, Falcon, etc are needed as benchmark baselines.
2.	For better visualization, the best results in Table 1 need to be displayed in bold.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- The paper does not describe what hyperparameters are used by each defense nor how those hyperparameters are derived. A maximally charitable evaluation of defenses would optimize hyperparameters against the attack and show how much clean data is required to remove the attack.,OE67D1Oatr,ICLR_2025,"- The novelty of the proposed extension to Sleeper Agent is limited.
- Limited datasets used in experiments. All experiments are done on CIFAR-10, except for one ASR experiment on GTSRB.
- The attack is very computationally intensive, requiring retraining a surrogate that approximates the attacked model (number of optimization cycles) * (number of cycle rounds) * (number of triggers) times. This implicitly assumes the attacker has access to very significant computational resources or is only able to attack small models.
- Standard ResNet-18 training uses random crop as a data augmentation [1], which is not use here. I suspect random crop would make this attack less effective.
- The paper does not describe what hyperparameters are used by each defense nor how those hyperparameters are derived. A maximally charitable evaluation of defenses would optimize hyperparameters against the attack and show how much clean data is required to remove the attack.
- The experiments in section 4.4 do not add very much beyond proving that a ResNet-18 has the capacity to learn 2-3 backdoors when training on CIFAR-10.
- It is unclear whether the increased ASR comes from the partitioning mechanism or from having multiple optimization cycles $S$ where the dataset containing the best randomized perturbations are returned.
- The paper does not provide an analysis on how different settings of cycle rounds $R$ and optimization cycles $S$ effects the success of the backdoor. The experiments section only examines one setting of these parameters without justifying how this setting is derived. This is a missed opportunity to demonstrate how valuable the proposed modification is to achieving a successful attack. Minor
- Line 22 of algorithm 1 contains a typo.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- The theoretical results don't have immediate practical implications, although this is certainly understandable given the novelty of the work. As someone who is more of an applied researcher who occasionally dabbles in theory, it would be ideal to see more take-away points for practitioners. The main take-away point that I observed is to query a cluster proportionally to the square root of its size, but it's unclear if this is a novel finding in this paper.",NIPS_2016_394,NIPS_2016,"- The theoretical results don't have immediate practical implications, although this is certainly understandable given the novelty of the work. As someone who is more of an applied researcher who occasionally dabbles in theory, it would be ideal to see more take-away points for practitioners. The main take-away point that I observed is to query a cluster proportionally to the square root of its size, but it's unclear if this is a novel finding in this paper. - The proposed model produces only 1 node changing cluster per time step on average because the reassignment probability is 1/n. This allows for only very slow dynamics. Furthermore, the proposed evolution model is very simplistic in that no other edges are changed aside from edges with the (on average) 1 node changing cluster. - Motivation by the rate limits of social media APIs is a bit weak. The motivation would suggest that it examines the error given constraints on the number of queries. The paper actually examines the number of probes/queries necessary to achieve a near-optimal error, which is a related problem but not necessarily applicable to the social media API motivation. The resource-constrained sampling motivation is more general and a better fit to the problem actually considered in this paper, in my opinion. Suggestions: Please comment on optimality in the general case. From the discussion in the last paragraph in Section 4.3, it appears that the proposed queue algorithm would is a multiplicative factor of 1/beta from optimality. Is this indeed the case? Why not also show experiment results for just using the algorithm of Theorem 4 in addition to the random baselines? This would allow the reader to see how much practical benefit the queue algorithm provides. Line 308: You state that you show the average and standard deviation, but standard deviation is not visible in Figure 1. Are error bars present but just too small to be visible? If so, state that it is the case. Line 93: ""asymptoticall"" -> ""asymptotically"" Line 109: ""the some relevant features"" -> Remove ""the"" or ""some"" Line 182: ""queries per steps"" -> ""queries per step"" Line 196-197: ""every neighbor of neighbor of v"" -> ""neighbor of"" repeated Line 263: Reference to Appendix in supplementary material shows ?? Line 269: In the equation for \epsilon, perhaps it would help to put parentheses around log n, i.e. (log n)/n rather than log n/n. Line 276: ""issues query"" -> I believe this should be ""issues 1 query"" Line 278: ""loosing"" -> ""losing"" I have read the author rebuttal and other reviews and have decided not to change my scores.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?",ACL_2017_108_review,ACL_2017,"Clarification is needed in several places.
1. In section 3, in addition to the description of the previous model, MH, you need point out the issues of MH which motivate you to propose a new model.
2. In section 4, I don't see the reason why separators are introduced. what additional info they convene beyond T/I/O?
3. section 5.1 does not seem to provide useful info regarding why the new model is superior.
4. the discussion in section 5.2 is so abstract that I don't get the insights why the new model is better than MH. can you provide examples of spurious structures? - General Discussion: The paper presents a new model for detecting overlapping entities in text. The new model improves the previous state-of-the-art, MH, in the experiments on a few benchmark datasets. But it is not clear why and how the new model works better.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- L235: There could be different approaches to pooling the tokens. For instance, why is it that mean pooling works? What about other pooling strategies?",XWPp9FJ0uJ,ICLR_2025,"- L187-188: $x=\{(k_i, v_i)|i=1\ldots,n\}$ where $n$ denotes the number of features, $k$ and $v$ denote feature-name and value-pairs should be defined more clearly. I understand that you are trying to refer to a specific feature having a single possible value i.e. age = 50. However, an alternative way of interpreting this is where you can have $n$ features for $k_i$ i.e. age, education etc. but there can be more than $n$ different value-pairs. For example, for age where age = {1,2,3,…} where len(age) > $n$.
- L199-208: No citations of relevant papers i.e. justification for “LLMs often attend more strongly to tokens at the end of a sequence”.
- L235: There could be different approaches to pooling the tokens. For instance, why is it that mean pooling works? What about other pooling strategies?
- L300: Although the datasets are extensive, I would like to inquire regarding results and ablations on the most popular datasets such as the UCI [Adult](https://archive.ics.uci.edu/dataset/2/adult), [Bank](https://archive.ics.uci.edu/dataset/222/bank+marketing), and [Default](https://archive.ics.uci.edu/dataset/350/default+of+credit+card+clients) datasets.
- L368: Baselines from recent SOTA methods are missing. This includes the mentioned [TabLLM](https://arxiv.org/abs/2210.10723), and other methods such as [TabTransformer](https://arxiv.org/abs/2012.06678), [InterpreTabNet](https://arxiv.org/abs/2406.00426), [SAINT](https://arxiv.org/abs/2106.01342), [TabPFN](https://arxiv.org/abs/2207.01848) etc.
- L460: It is unclear how the ablation is conducted. What is the baseline model that the serialization is applied to? If it is ZET-LLM, would you please elaborate on the whole ablation process?
- Although “feature-wise serialization consistently outperforms sample-wise serialization”, can I clarify that this can only be applied to your framework where you are required to first encode text into embeddings?
- L467: For w/o mask results, does that mean that you drop the samples instead of ignoring the masked features? On the other hand, in cases where missing values convey implicit information (e.g., non-response bias), could masking be suboptimal compared to other techniques, such as imputation or attention-based weighting?
- Given the adaptation of a transformer-based model for tabular data, is there an assessment of computational efficiency compared to traditional tabular models?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. In Table 3, besides the number of queries, it would be better to compare the real search cost (e.g. in terms of GPU days).",ICLR_2022_2660,ICLR_2022,"1. There is an assumption “graphs are topological close should have also comparable performance”. Nevertheless, it may not hold for architectures. For example, by only modifying one node/edge (add or remove skip connection), the architecture may incur significant performance drop. Thus, it is questionable to use spectral distance to evaluate the similarity. 2. In Table 3, besides the number of queries, it would be better to compare the real search cost (e.g. in terms of GPU days). 3. This paper only considers small search spaces, e.g. NASBench. Can the proposed method be used in DARTS and MobileNet search spaces? It would be better to report the results on these spaces. 4. In Section 4.2, the authors claimed that “our model selection approach is very stable”. However, there seem no empirical results to support it. 5. Since this paper focuses on learning predictors, several recent related work [a,b] should be discussed and/or compared.
Reference: [a] ReNAS: Relativistic evaluation of neural architecture search. CVPR 2021. [b] Contrastive Neural Architecture Search with Neural Architecture Comparators. CVPR 2021.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. Missing training details? Specifically, I am wondering if the VQGAN is pre-trained? Or only trained on the 88,635 images from the Computer Vision Figures dataset.",NIPS_2022_1913,NIPS_2022,"1. I believe the paper would benefit from a comparison with a one-shot baseline. While the proposed approach does not require fine-tuning, it would be interesting to see how it competes.
2. The prompt engineering details are unclear. I was only able to get a vague idea on how to construct the visual prompt. Is it possible to document them more precisely? Additionally, how are different image sizes being handled? Are the images being resized?
3. Missing training details? Specifically, I am wondering if the VQGAN is pre-trained? Or only trained on the 88,635 images from the Computer Vision Figures dataset.
4. The authors have stated that they repeated the experiment with four random seed (line. 194). It would be great to report the standard deviation on the quantitative results. Also, I wonder how sensitive the approach is to the prompted image. For example, would the approach still work if the cat, in Fig.3, is prompted by an image of a white cat that's outdoor? Misc.
Line 173: ""224x224"" --> ""224 \times 224""
The paper contains a limitation section and adequately discussed the shortcomings of the approach. Specifically, the inherit ambiguity when prompting from a single image for a task.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- As this work has the perspective of task-oriented recommendation, it seems that works such as [] Li, Xiujun, et al. ""End-to-end task-completion neural dialogue systems."" arXiv preprint arXiv:1703.01008 (2017). are important to include, and compare to, at least conceptually. Also, discussion in general on how their work differs from other chatbox research works e.g. [] He, Ji, et al. ""Deep reinforcement learning with a natural language action space."" arXiv preprint arXiv:1511.04636(2015). would be very useful.",NIPS_2018_894,NIPS_2018,"- As this work has the perspective of task-oriented recommendation, it seems that works such as [] Li, Xiujun, et al. ""End-to-end task-completion neural dialogue systems."" arXiv preprint arXiv:1703.01008 (2017). are important to include, and compare to, at least conceptually. Also, discussion in general on how their work differs from other chatbox research works e.g. [] He, Ji, et al. ""Deep reinforcement learning with a natural language action space."" arXiv preprint arXiv:1511.04636(2015). would be very useful. - It is important that the authors highlight the strengths as well as the weaknesses of their released dataset: e.g. what are scenarios under which such a dataset would not work well? are 10,000 conversations enough for proper training? Similarly, a discussion on their approaches, in terms of things to further improve would be useful for the research community to extend -- e.g. a discussion on how the domain of movie recommendation can differ from other tasks, or a discussion on the exploration-exploitation trade-off. Particularly, it seems that this paper envisions conversational recommendations as a goal oriented chat dialogue. However, conversational recommendations could be more ambiguous.. - Although it is great that the authors have included these different modules capturing recommendations, sentiment analysis and natural language, more clear motivation on why each component is needed would help the reader. For example, the cold-start setting, and the sampling aspect of it, is not really explained. The specific choices for the models for each module are not explained in detail (why were they chosen? Why is a sentiment analysis model even needed -- can't we translate the like/dislike as ratings for the recommender?) - Evaluation -- since one of the contributions argued in the paper is ""deep conversational recommenders"", evaluation-wise, a quantitative analysis is needed, apart from user study results provided (currently the other quantitative results evaluate the different sub-modules independently). Also, the authors should make clearer the setup of how exactly the dataset is used to train/evaluate on the Amazon Turk conversations -- is beam-search used as in other neural language models? Overall, although I think that this paper is a nice contribution in the domain of movie conversational recommendation, I believe that the authors should better position their paper, highlighting also the weaknesses/ things to improve in their work, relating it to work on neural dialogue systems, and expanding on the motivation and details of their sub-modules and overall architecture. Some discussion also on how quantitative evaluation of the overall dialogue quality should happen would be very useful. == I've read the authors' rebuttal. It would be great if the authors add some of their comments from the rebuttal in the revised paper regarding the size of the dataset, comparison with goal-oriented chatbots and potential for quantitative evaluation.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"* The proposed methods (DualIS and DualDIS) are not generic on some cross-model retrieval tasks, i.e., the performance in MSVD (Table 3) shows minor improvements.",Md1YdfqAed,EMNLP_2023,"* The proposed methods (DualIS and DualDIS) are not generic on some cross-model retrieval tasks, i.e., the performance in MSVD (Table 3) shows minor improvements.
* I think the proposed gallery bank is supplementary and less effective compared to the query bank to address hubness issues in cross-model retrieval tasks. This conclusion is also verified by the authors' experiments.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"1. I am not completely convinced by the experimental strengths of this approach. To run the proposed algorithm, the authors need to run a descent procedure for 40 different networks from the training phase. In contrast, you could simply run vanilla Adam on the final network with 40 random initial points, and one of these restarts would reach the global minimum. It is not important that EACH initialization reach the global minimum, as long as AT LEAST one initialization reaches the global minimum.",NIPS_2019_1346,NIPS_2019,"below. 2. Theorem 3.1 is interesting in itself, because it applies to vectors which are not in the range of the generative model. Clarity: The paper is well written and ideas clearly expressed. I believe that others can reproduce the algorithm described. I only have a problem with the way the set $S(x,theta, tau)$ is defined in line 177, since the authors do not require the signs to strictly differ on this set. Significance: I think other researchers can build on Theorem 3.1. The conditions proposed for Theorem 3.3 are novel and could be used for future results. Weaknesses: 1. I am not completely convinced by the experimental strengths of this approach. To run the proposed algorithm, the authors need to run a descent procedure for 40 different networks from the training phase. In contrast, you could simply run vanilla Adam on the final network with 40 random initial points, and one of these restarts would reach the global minimum. It is not important that EACH initialization reach the global minimum, as long as AT LEAST one initialization reaches the global minimum. 2. While Theorem 3.3 is interesting, it does not directly influence the experiments because the authors never perform the search operation in line 3 of algorithm 2. Because of this, it provides a proof of correctness for an algorithm that is quite different from the algorithm used in practice. Although Algorithm 2 and the empirical algorithm are similar in spirit, lines 1 and 3 in algorithm 2 are crucial for proof of correctness. Clarifications: 1. For the case where $y= G(z) + noise$, where noise has sufficiently low energy, you would expect a local minimum close to $z$. Would this not contradict the result of Theorem 3.1? ---Edit after author response--- Thank you for your response. After reading your rebuttal and other reviews, I have updated my score to a 8. I think Table in the rebuttal and Theorem 3.1 are solid contributions. Regarding my criticism of the definition of S(x,tau,theta)- I only meant that defining the complement of this set may make things clearer, since you only seem to work with its complement later on (this did not influence my score).","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"4. Incomplete study, the relationship between the top selected patches and the disease is not yet established",FTSUDBM6lu,ICLR_2024,"1. The novelty is limited, the proposed model mostly involves straight application existing feature selection methods.
2. The evaluation is limited (only on one dataset)
3. Presentations could also be improved.
4. Incomplete study, the relationship between the top selected patches and the disease is not yet established","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"6: In the phrase ""for 'in-between' uncertainty"", the first quotation mark on 'in-between' needs to be the forward mark rather than the backward mark (i.e., ‘ i n − b e t w e e n ′ ). p.",ICLR_2021_872,ICLR_2021,"The authors push on the idea of scalable approximate inference, yet the largest experiment shown is on CIFAR-10. Given this focus on scalability, and the experiments in recent literature in this space, I think experiments on ImageNet would greatly strengthen the paper (though I sympathize with the idea that this can a high bar from a resources standpoint).
As I noted down below, the experiments currently lack results for the standard variational BNN with mean-field Gaussians. More generally, I think it would be great to include the remaining models from Ovadia et al. (2019). More recent results from ICML could also useful to include (as referenced in the related works sections). Recommendation
Overall, I believe this is a good paper, but the current lack of experiments on a dataset larger than CIFAR-10, while also focusing on scalability, make it somewhat difficult to fully recommend acceptance. Therefore, I am currently recommending marginal acceptance for this paper.
Additional comments
p. 5-7: Including tables of results for each experiment (containing NLL, ECE, accuracy, etc.) in the main text would be helpful to more easily assess
p. 7: For the MNIST experiments, in Ovadia et al. (2019) they found that variational BNNs (SVI) outperformed all other methods (including deep ensembles) on all shifted and OOD experiments. How does your proposed method compare? I think this would be an interesting experiment to include, especially since the consensus in Ovadia et al. (2019) (and other related literature) is that full variational BNNs are quite promising but generally methodologically difficult to scale to large problems, with relative performance degrading even on CIFAR-10. Minor
p. 6: In the phrase ""for 'in-between' uncertainty"", the first quotation mark on 'in-between' needs to be the forward mark rather than the backward mark (i.e., ‘ i n − b e t w e e n ′ ).
p. 7: s/out of sitribution/out of distribution/
p. 8: s/expensive approaches 2) allows/expensive approaches, 2) allows/
p. 8: s/estimates 3) is/estimates, and 3) is/
In the references:
Various words in many of the references need capitalization, such as ""ai"" in Amodei et al. (2016), ""bayesian"" in many of the papers, and ""Advances in neural information processing systems"" in several of the papers.
Dusenberry et al. (2020) was published in ICML 2020
Osawa et al. (2019) was published in NeurIPS 2019
Swiatkowski et al. (2020) was published in ICML 2020
p. 13, supplement, Fig. 5: error bar regions should be upper and lowered bounded by [0, 1] for accuracy.
p. 13, Table 2: Splitting this into two tables, one for MNIST and one for CIFAR-10, would be easier to read.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. If the theme is mainly about FedSP, the performance of FedSP is not the best in Table 1 and Table 2 on some datasets.",57yfvVESPE,EMNLP_2023,"1. The writing is hard to follow. There is no contribution list at the end of Introduction part. I read it several times but I am sorry that I cannot catch your theme. Is this paper mainly about model privacy in FL (FedSP) or soft prompt usage?
2. If the theme is mainly about FedSP, the performance of FedSP is not the best in Table 1 and Table 2 on some datasets.
3. If the paper is about FedSP, the author should do more experiments about FL settings, like the number of clients, communication rounds, etc.
4. The settings of the global and client model are not clear, like the model structure, etc.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"* L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee?",NIPS_2016_386,NIPS_2016,", however. For of all, there is a lot of sloppy writing, typos and undefined notation. See the long list of minor comments below. A larger concern is that some parts of the proof I could not understand, despite trying quite hard. The authors should focus their response to this review on these technical concerns, which I mark with ** in the minor comments below. Hopefully I am missing something silly. One also has to wonder about the practicality of such algorithms. The main algorithm relies on an estimate of the payoff for the optimal policy, which can be learnt with sufficient precision in a ""short"" initialisation period. Some synthetic experiments might shed some light on how long the horizon needs to be before any real learning occurs. A final note. The paper is over length. Up to the two pages of references it is 10 pages, but only 9 are allowed. The appendix should have been submitted as supplementary material and the reference list cut down. Despite the weaknesses I am quite positive about this paper, although it could certainly use quite a lot of polishing. I will raise my score once the ** points are addressed in the rebuttal. Minor comments: * L75. Maybe say that pi is a function from R^m \to \Delta^{K+1} * In (2) you have X pi(X), but the dimensions do not match because you dropped the no-op action. Why not just assume the 1st column of X_t is always 0? * L177: ""(OCO )"" -> ""(OCO)"" and similar things elsewhere * L176: You might want to mention that the learner observes the whole concave function (full information setting) * L223: I would prefer to see a constant here. What does the O(.) really mean here? * L240 and L428: ""is sufficient"" for what? I guess you want to write that the sum of the ""optimistic"" hoped for rewards is close to the expected actual rewards. * L384: Could mention that you mean |Y_t - Y_{t-1}| \leq c_t almost surely. ** L431: \mu_t should be \tilde \mu_t, yes? * The algorithm only stops /after/ it has exhausted its budget. Don't you need to stop just before? (the regret is only trivially affected, so this isn't too important). * L213: \tilde \mu is undefined. I guess you mean \tilde \mu_t, but that is also not defined except in Corollary 1, where it just given as some point in the confidence ellipsoid in round t. The result holds for all points in the ellipsoid uniformly with time, so maybe just write that, or at least clarify somehow. ** L435: I do not see how this follows from Corollary 2 (I guess you meant part 1, please say so). So first of all mu_t(a_t) is not defined. Did you mean tilde mu_t(a_t)? But still I don't understand. pi^*(X_t) is (possibly random) optimal static strategy while \tilde \mu_t(a_t) is the optimistic mu for action a_t, which may not be optimistic for pi^*(X_t)? I have similar concerns about the claim on the use of budget as well. * L434: The \hat v^*_t seems like strange notation. Elsewhere the \hat is used for empirical estimates (as is standard), but here it refers to something else. * L178: Why not say what Omega is here. Also, OMD is a whole family of algorithms. It might be nice to be more explicit. What link function? Which theorem in [32] are you referring to for this regret guarantee? * L200: ""for every arm a"" implies there is a single optimistic parameter, but of course it depends on a ** L303: Why not choose T_0 = m Sqrt(T)? Then the condition becomes B > Sqrt(m) T^(3/4), which improves slightly on what you give. * It would be nice to have more interpretation of theta (I hope I got it right), since this is the most novel component of the proof/algorithm.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline) A few comments on the text:,ICLR_2022_113,ICLR_2022,"- The part of the contrastive loss is not totally clear. The authors should provide a better intuition of why the contrastive loss improves the feature representation. For example, how are image-latent pairs defined as positive? - The method focuses on learning cluster granularity for the object only, and not for the background. - It's unclear why the transformation matrix is used (other than the fact that it's part of PerturbGAN's pipeline)
A few comments on the text: - The phrase ""coarse-grained images"" is inaccurate, the ""coarse-grained"" adjective should refer to the clustering and not the images (in the intro). - The authors should share more details about the auxiliary distribution mentioned in the abstract and the intro. - Overall proofreading is required. It would be great to add some of the model's notations to figure 2 (e.g. D_base, psi_r, psi_h)","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,
1) the models are learned directly from pixels without a Markovian state,ICLR_2023_226,ICLR_2023,"The world modelling task is definitely interesting but it is hard to see how it is directly relevant outside of this environment. We would likely never have access to a Markovian state in such a controlled setting. The section appears to be motivated by works such as World Models and Dreamer, but in those cases 1) the models are learned directly from pixels without a Markovian state 2) there is an agent taking actions in the world. So this is a totally different paradigm. The fact that the model generalizes better with more data here is expected, as the authors note this has been the case in a variety of other settings already.
How can we be sure the hand designed tasks are unbiased? For all we know they could be somewhat arbitrary.
While the motivation in the intro is that this world is more general than others such as MiniGrid/Crafter/MiniHack, the only RL task presented is just sand pushing. How is this more diverse and useful than for example the tasks in Crafter/MiniHack which vary from navigation to tool use?
It looks like the experiments were all just one seed. When we know RL training is volatile, it seems like an oversight to have done this given the environment is meant to be fast.
One of the motivations in the intro is the potential use for UED, but there is no demonstration of this. It would be interesting to see if this environment offers something unique here vs. the alternatives. It may be beyond the scope to run this for a rebuttal but it would likely see an increased score.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- The use of the sequence example at different step of the paper is really useful, however I'm a bit surprised that you mention in Example 2 a 'common' practice in the context of CRF corresponding to using as a scoring loss the Hamming distance over entire parts of the sequence. I've never seen this type of approach and am only aware of works reporting the hamming loss defined node wise. It would be great if you could point out some references there.",NIPS_2019_656,NIPS_2019,"Despite the shown results and the details added in the appendix K, I think that the experimental part remains the weak part of this paper. The results displayed are convincing but I am disappointed that the authors did not tried their approach on more popular problems mentioned in the supplementary such as hierarchical classification. Even if this could be improved (in order to be at the level of the theoretical treatment), the proposed content is already solid and does not change my decision concerning the quality of this work. Remark: - The use of the sequence example at different step of the paper is really useful, however I'm a bit surprised that you mention in Example 2 a 'common' practice in the context of CRF corresponding to using as a scoring loss the Hamming distance over entire parts of the sequence. I've never seen this type of approach and am only aware of works reporting the hamming loss defined node wise. It would be great if you could point out some references there. - After reading the paper a few times, I still think that the notation $\Delta(z,y|x)$ is a bit strange and I would have preferred something of the form $\Delta(z,y)$ since in practice the losses you mention never takes into account the input data and $z$ is already a function of $x$. Maybe this is only personal taste and will be contradicted by the other reviewers. Minor remarks : missing brackets [ ] in theorem 4.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice.",ARR_2022_329_review,ARR_2022,"Although the paper is mostly easy to follow due to its simple and clear organization, it is not very clearly written. Some sentences are not clear and the text contains many typos. Although the included tasks can definitely be helpful, the proposed benchmark does not include many important tasks that require higher-level language understanding such as natural language inference, question answering, coreference resolution, etc. Also, the authors do not mention making the benchmark publicly available by providing a link which is very important and I hope will be the case.
General comments and suggestions: 1) The name of the ""Evaluation"" element can be changed to ""Metrics"" since 'evaluation' can have a more general meaning. Even better, the corresponding sections can be removed and the metrics can be briefly mentioned along with the datasets or in the captions of the tables since most, if not all, of the metrics are well-known and used as standard practice. 2) I feel like sentence segmentation and spellchecking & correction tasks do not fit well in the same benchmark alongside the other tasks that focus more on semantics. Ideally, one should be able to use the same model/architecture for all of the tasks in the benchmark by training it on their corresponding training sets. However, these two tasks feel lower level than the other tasks probably better handled somewhat separately than others. 3) Are there any reasons behind the selected baselines? For instance, why the Haded attention- RNN and Adaptive transformer instead of more traditional LSTMs and normal encoder-decoder transformers (or decoder only transformers like GPT models)?
Specific questions, comments, and typos: - Lines 89-90: -> In Mukayese we focus on - Lines 99-100: rewrite more clearly using full sentences or more clear format (i.e., We define two requirements ...: (i) accessible with ... (ii) sharable format) - Line 100: what is 'sharable format'?
- Lines 104-105: I believe the argument about inefficiency and cost of large supervised datasets is not correct. Finetuning a model on labeled training data for the target task is usually computationally not so costly and having more diverse supervised data almost always greatly helps. - Line 119: -> one or more metrics - Line 119-120: please rewrite (a) - Lines 127-129: please rewrite in a more clear way.
- Line 135: -> for each of the following ... - Lines 138-145: The paragraph starts with talking about general language modeling and formulates it, and then claims this is only a special type of language modeling (i.e., autoregressive language modeling).
- Line 151: -> AR language modeling Line 156: What exactly is a language modeling dataset? Is it simply a text corpus? - Table 2 caption:""mean average"" ?
- Lines 173-174: What is a character language modeling dataset? What is the difference from the normal language modeling dataset? If there is no difference, is there a need for a separate dataset?
- Line 188 -> generalize to - Lines 201-204: please rewrite in a more clear way.
- Lines 214-215: Are the same models trained for both normal language modeling and character-level language modeling without any architectural changes? Is this possible? More details are needed.
- Table 4 caption: these are not the results - Line 247: ""Where"" cannot be in a new sentence - Table 7 caption: metric (F1) should be mentioned in the caption as well - Line 338: ""converting them to"" converting what?
- Line 344: with -> on - Line 363: no space before ""In"" - Lines 396-400: Do you remove the remaining 9% from the dataset? If annotators could not come up with the correct words after 10 predictions probably it means the target word is not recoverable, right?
- Line 465: -> perform - Line 471: -> such as - Table 13 is not referenced in the text - Lines 497-498: one with (pre-training) and two without pre-training - Line 509: -> in","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- The new proposed dataset, DRRI, could have been explored more in the paper.",ARR_2022_295_review,ARR_2022,"- The paper would be easy to follow with an English-proofreading even though the overall idea is still understandable.
- The new proposed dataset, DRRI, could have been explored more in the paper.
- It is not clear how named entities were extracted from the datasets.
An English-proofreading would significantly improve the readability of the paper.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- [218] Please use more objective terms than remarkable: ""and remarkable accuracy improvement with same size of networks"". Looking at the axes, which are rather squished, the improvement is definitely there but it would be difficult to characterize it as remarkable.",NIPS_2018_83,NIPS_2018,"- An argument against DEN, a competitor, is hyper-parameter sensitivity. First, this isn't really shown, but second (and more importantly) reinforcement learning is well-known to be extremely unstable and require a great deal of tuning. For example, even random seed changes are known to change the behavior of the same algorithm, and different implementation of the same algorithm can get very different results (this has been heavily discussed in the community; see keynote ICLR talk by Joelle Pineau as an example). This is not to say the proposed method doesn't have an advantage, but the argument that other methods require more tuning is not shown or consistent with known characteristics of RL. * Related to this, I am not sure I understand experiments for Figure 3. The authors say they vary the hyper-parameters but then show results with respect to # of parameters. Is that # of parameters of the final models at each timestep? Isn't that just varying one hyperparameter? I am not sure how this shows that RCL is more stable. - Newer approaches such as FearNet [1] should be compared to, as they demonstrated significant improvement in performance (although they did not compare to all of the methods compared to here). [1] FearNet: Brain-Inspired Model for Incremental Learning, Ronald Kemker, Christopher Kanan, ICLR 2018. - There is a deeper tie to meta-learning, which has several approaches as well. While these works don't target continual learning directly, they should be cited and the authors should try to distinguish those approaches. The work on RL for architecture search and/or as optimizers for learning (which are already cited) should be more heavily linked to this work, as it seems to directly follow as an application to continual learning. - It seems to me that continuously adding capacity while not fine-tuning the underlying features (which training of task 1 will determine) is extremely limiting. If the task is too different and the underlying feature space in the early layers are not appropriate to new tasks, then the method will never be able to overcome the performance gap. Perhaps the authors can comment on this. - Please review the language in the paper and fix typos/grammatical issues; a few examples: * [1] ""have limitation to solve"" => ""are limited in their ability to solve"" * [18] ""In deep learning community"" => ""In THE deep learning community"" * [24] ""incrementally matche"" => ""incrementally MATCH"" * [118] ""we have already known"" => ""we already know"" * and so on Some more specific comments/questions: - This sentence is confusing [93-95] ""After we have trained the model for task t, we memorize each newly added filter by the shape of every layer to prevent the caused semantic drift."" I believe I understood it after re-reading it and the subsequent sentences but it is not immediately obvious what is meant. - [218] Please use more objective terms than remarkable: ""and remarkable accuracy improvement with same size of networks"". Looking at the axes, which are rather squished, the improvement is definitely there but it would be difficult to characterize it as remarkable. - The symbols in the graphs across the conditions/algorithms is sometimes hard to distinguish (e.g. + vs *). Please make the graphs more readable in that regard. Overall, the idea of using reinforcement learning for continual learning is an interesting one, and one that makes sense considering recent advances in architecture search using RL. However, this paper could be strengthened by 1) Strengthening the analysis in terms of the claims made, especially with respect to not requiring as much hyper-parameter tuning, which requires more evidence given that RL often does require significant tuning, and 2) comparison to more recent methods and demonstration of more challenging continual learning setups where tasks can differ more widely. It would be good to have more in-depth analysis of the trade-offs between three approaches (regularization of large-capacity networks, growing networks, and meta-learning). ============================================== Update after rebuttal: Thank you for the rebuttal. However, there wasn't much new information in the rebuttal to change the overall conclusions. In terms of hyper-parameters, there are actually more hyper-parameters for reinforcement learning that you are not mentioning (gamma, learning rate, etc.) which your algorithm might still be sensitive to. You cannot consider only the hyper-parameter related to the continual learning part. Given this and the other limitations mentioned, overall this paper is marginally above acceptance so the score has been kept the same.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps). In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.",ICLR_2021_140,ICLR_2021,"Weakness
When discussing the difference over [Tulyakov et al. 2018], the paper states “…applies h_t as the motion code for the frame to be generated, while the content code is fixed for all frames. However, such a design requires a recurrent network to estimate the motion while preserving consistent content from the latent vector, … difficult to learn in practice”. I do not fully understand why this is the case. It would be clearer if the paper can explain why such a design causes difficulty in learning and why the proposed design could alleviate such problems.
For motion diversity, why maximizing the mutual information between the hidden vector and the noise vector can prevent mode collapse?
It seems to me that the proposed method can only handle 1) “subtle” motion, such as facial expressions and 2) short video sequences (e.g., 16 frames). One can see the problem in the synthesized results for UCF-101: inconsistent motion, changing color, or object disappearing over time. It would be interesting to videos with a longer duration (by running the LSTM over many time steps).
In sum, this is a paper with an interesting idea and extensive experiments. While the results are still not perfect and seem to handle subtle motion, the quantitative and qualitative evaluation show clearly improved results over the previous state-of-the-art.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- As with most work on pruning, it is not yet possible to realize efficiency gains on GPU.",NIPS_2020_1710,NIPS_2020,"- While the baselines are strong, the way they are reported may be a bit misleading. In particular, models are compared based on the sparsity percentage, which puts models with fewer parameters (e.g., MiniBERT) at a disadvantage. - As with most work on pruning, it is not yet possible to realize efficiency gains on GPU.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- The numerical evaluation is not fully convincing as the method is only evaluated on synthetic data. The comparison with [5] is not completely fair as [5] is designed for a more complex problem, i.e., no knowledge of the camera pose parameters.",NIPS_2017_35,NIPS_2017,"- The applicability of the methods to real world problems is rather limited as strong assumptions are made about the availability of camera parameters (extrinsics and intrinsics are known) and object segmentation.
- The numerical evaluation is not fully convincing as the method is only evaluated on synthetic data. The comparison with [5] is not completely fair as [5] is designed for a more complex problem, i.e., no knowledge of the camera pose parameters.
- Some explanations are a little vague. For example, the last paragraph of Section 3 (lines 207-210) on the single image case. Questions/comments:
- In the Recurrent Grid Fusion, have you tried ordering the views sequentially with respect to the camera viewing sphere?
- The main weakness to me is the numerical evaluation. I understand that the hypothesis of clean segmentation of the object and known camera pose limit the evaluation to purely synthetic settings. However, it would be interesting to see how the architecture performs when the camera pose is not perfect and/or when the segmentation is noisy. Per category results could also be useful.
- Many typos (e.g., lines 14, 102, 161, 239 ), please run a spell-check.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '3', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"2. Did the authors study numbers of bits in logits helps against a larger epsilon in the PGD attack? Because intuition suggests that having a 32 bit logit should improve robustness against a more powerful adversary. This experiment isn't absolutely necessary, but does strengthen the paper.",NIPS_2019_776,NIPS_2019,"1. When the authors say `white box attacks`, I assume this means that the adversary can see the full network with the final layers, every network in the ensemble, every rotation used by networks in the ensemble. I would like them to confirm this is correct. 2. Did the authors study numbers of bits in logits helps against a larger epsilon in the PGD attack? Because intuition suggests that having a 32 bit logit should improve robustness against a more powerful adversary. This experiment isn't absolutely necessary, but does strengthen the paper. 3. Did the authors study the same approach on Cifar? It seems like this approach should be readily applicable there as well. ---Edit after rebuttal--- I am updating my score to 8. The improved experiments on Cifar10 make a convincing argument for your method.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
* How much do men and women pay for insurance after this method is applied?,Q7uE3M5aMD,ICLR_2025,"The experiments and evaluation section in this paper claims to show that the method ""achieves fair pricing effectively"", but it answers none of the questions that would allow us to determine if such pricing is fair, effective, or desirable.
* How much do men and women pay for insurance after this method is applied?
* How does this compare to the benefit they receive from insurance payouts?
* Which other subgroups benefit or are made worse off by this method?
* If insurance is under/overpriced it could lead to adverse selection, where, for example, high-risk male drivers buy more insurance because it’s cheap, increasing premiums for everyone else. It could even lead to people driving more dangerously at the margin because they know that an incident won’t increase their premiums much. Is there risk of adverse selection or other negative equilibrium effects from using this pricing?
This paper paper is clearly trying to develop a method for pricing that can be used by real insurers. Unfortunately, though, it treats insurance pricing as an exercise in privacy math, and not as an input to a crucially important product for people's physical and financial health.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.",NIPS_2017_401,NIPS_2017,"Weakness:
1. There are no collaborative games in experiments. It would be interesting to see how the evaluated methods behave in both collaborative and competitive settings.
2. The meta solvers seem to be centralized controllers. The authors should clarify the difference between the meta solvers and the centralized RL where agents share the weights. For instance, Foester et al., Learning to communicate with deep multi-agent reinforcement learning, NIPS 2016.
3. There is not much novelty in the methodology. The proposed meta algorithm is basically a direct extension of existing methods.
4. The proposed metric only works in the case of two players. The authors have not discussed if it can be applied to more players.
Initial Evaluation:
This paper offers an analysis of the effectiveness of the policy learning by existing approaches with little extension in two player competitive games. However, the authors should clarify the novelty of the proposed approach and other issues raised above. Reproducibility:
Appears to be reproducible.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. The paper split the papers according to their publication years on the ACL anthology. However, many papers are posted on arxiv much earlier than ACL anthology. For example, the BERT paper is available on arxiv from Oct.",qhwYFIrSm7,EMNLP_2023,"1. The paper split the papers according to their publication years on the ACL anthology. However, many papers are posted on arxiv much earlier than ACL anthology. For example, the BERT paper is available on arxiv from Oct. 2018 (when its influence started), but is on ACL Anthology from 2019. This may more or less influence the soundness of the causal analysis.
2. The paper claims that the framework aids the literature survey, which is true. However, the listed overviews are mostly ""Global Research Trends"" that are well-known by the community. For example, it is known that BLEU and Transformers fuel machine translation. The paper didn't illustrate how to use the framework to identify ""Local Research Trends"" which are more useful for daily research: for example, what is the impact of instruction-following LLMs and in-context learning on NLP tasks? How do multi-task learning and reinforcement learning transfer to instruction learning?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '4', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"2. Page 3, it seems to me that M_T is defined over the probabilities of atomic events. The notation as it is used not making it difficult to make sense of this concept. Please consider providing examples to explain M_T.",ICLR_2022_1267,ICLR_2022,"Weakness
1. The proposed model's parameterization depends on the number of events and predicates making it difficult to generalize to unseen events or required retraining.
2. The writing needs to be improved to clearly discuss the proposed approach.
3. The experiments baselines are of the authors' own design; it lacks a comparison to the literature baselines using the same dataset. If there is no such baseline, please discuss the criteria in choosing such baselines. Details:
1. Page 1, ""causal mechanisms"", causality is different from temporal relationship. Please use the terms carefully.
2. Page 3, it seems to me that M_T is defined over the probabilities of atomic events. The notation as it is used not making it difficult to make sense of this concept. Please consider providing examples to explain M_T.
3. Page 4, equation (2), it is not usual to feed probabilities to convolution.
a. Please discuss in section 3 how your framework can handle raw inputs, such as video or audio? Do you need an atomic event predictor or human label to use your proposed system? If so, is it possible to extend your framework to directly have video as input instead of event probability distributions? Can you do end2end training from raw inputs, such as video or audio? (although you mentioned Faster R-CNN in the experiment section, it is better to discuss the whole pipeline in the methodology).
b. Have you tried discrete event embeddings to represent the atomic and composite events so as the framework can learn distributional embedding representation of events so as to learn the temporal rules?
4. Page 4, please explain what you want to achieve with M_A = M_C \otimes M_D. It is unusual to multiple length by conv1D output. Also please define \otimes here. I am guessing it is elementwise multiplication from the context.
5. Page 4, ""M_{D:,:,l}=l. This can be thought as a positional encoding. It is not clear to me why this can be taken as positional encoding?
6. Page 6, please detail how do you sample top c predicates. Please define what is s in a = softmax(s). It seems to me the dimension of s with \sum_i (c i) can be quite large making it softmax(s) very costly.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1 - The authors propose a relaxation of rejection sampling which is using an arbitrary parameter β instead of the true upper bound of the ratio p q when the latter cannot be computed. The reviewer fails to understand why the authors did not directly use Importance sampling in the first place. 2- In algorithm 1, the reviewer fails to see a difference between QRS and RS, and will change their opinion if the authors can point out a value of u for which QRS and RS will behave differently.",ICLR_2022_1678,ICLR_2022,"1 - The authors propose a relaxation of rejection sampling which is using an arbitrary parameter β
instead of the true upper bound of the ratio p q
when the latter cannot be computed. The reviewer fails to understand why the authors did not directly use Importance sampling in the first place.
2- In algorithm 1, the reviewer fails to see a difference between QRS and RS, and will change their opinion if the authors can point out a value of u for which QRS and RS will behave differently.
3 - Uninteresting Section 2.2: - Equation 1 needs a parenthesis to avoid confusion - Equation 3 is pretty much obvious from the definition of TVD (Lemma 2.19 Aldous and Fill https://www.stat.berkeley.edu/users/aldous/RWG/book.html) - Equation 4 is obvious since using the apropriate upper bound gives you rejection sampling which is a perfect sampling algorithm.
4 - In the abstract the authors require the proposal distribution to upper bound the target everywhere which is not true as the authors themselves clarify in the text.
5 - While Equation 9 and 10 are great in that they can be used to compute TVD and KL between the true and QRS distributions, there are multiple issues which are neither stated as assumptions nor addressed appropriately, namely: - They're not unbiased estimators since Z
is not known and needs to be estimated, this point is not explicitly stated. - It is assumed that the normalizing constant of q
is known, which is not always the case. - They rely on importance sampling, which begs question 1.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '0']}",,
"- The observed performance enhancements are somewhat modest, suggesting room for further refinement in the future.",ZJua8VeHCh,EMNLP_2023,"- Conceptually the proposed idea is similar to existing techniques and it could be viewed as incremental.
- The observed performance enhancements are somewhat modest, suggesting room for further refinement in the future.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '2', '2']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '2', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: ""In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence""; ""Some previous works apply this framework to summarization generation tasks."" - Figure 2: What is ""MLP""? It seems not to be described in the paper.",ACL_2017_333_review,ACL_2017,"There are some few details on the implementation and on the systems to which the authors compared their work that need to be better explained. - General Discussion: - Major review: - I wonder if the summaries obtained using the proposed methods are indeed abstractive. I understand that the target vocabulary is build out of the words which appear in the summaries in the training data. But given the example shown in Figure 4, I have the impression that the summaries are rather extractive.
The authors should choose a better example for Figure 4 and give some statistics on the number of words in the output sentences which were not present in the input sentences for all test sets.
- page 2, lines 266-272: I understand the mathematical difference between the vector hi and s, but I still have the feeling that there is a great overlap between them. Both ""represent the meaning"". Are both indeed necessary? Did you trying using only one of them.
- Which neural network library did the authors use for implementing the system?
There is no details on the implementation.
- page 5, section 44: Which training data was used for each of the systems that the authors compare to? Diy you train any of them yourselves?
- Minor review: - page 1, line 44: Although the difference between abstractive and extractive summarization is described in section 2, this could be moved to the introduction section. At this point, some users might no be familiar with this concept.
- page 1, lines 93-96: please provide a reference for this passage: ""This approach achieves huge success in tasks like neural machine translation, where alignment between all parts of the input and output are required.""
- page 2, section 1, last paragraph: The contribution of the work is clear but I think the authors should emphasize that such a selective encoding model has never been proposed before (is this true?). Further, the related work section should be moved to before the methods section.
- Figure 1 vs. Table 1: the authors show two examples for abstractive summarization but I think that just one of them is enough. Further, one is called a figure while the other a table.
- Section 3.2, lines 230-234 and 234-235: please provide references for the following two passages: ""In the sequence-to-sequence machine translation (MT) model, the encoder and decoder are responsible for encoding input sentence information and decoding the sentence representation to generate an output sentence""; ""Some previous works apply this framework to summarization generation tasks.""
- Figure 2: What is ""MLP""? It seems not to be described in the paper.
- page 3, lines 289-290: the sigmoid function and the element-wise multiplication are not defined for the formulas in section 3.1.
- page 4, first column: many elements of the formulas are not defined: b (equation 11), W (equation 12, 15, 17) and U (equation 12, 15), V (equation 15).
- page 4, line 326: the readout state rt is not depicted in Figure 2 (workflow).
- Table 2: what does ""#(ref)"" mean?
- Section 4.3, model parameters and training. Explain how you achieved the values to the many parameters: word embedding size, GRU hidden states, alpha, beta 1 and 2, epsilon, beam size.
- Page 5, line 450: remove ""the"" word in this line? "" SGD as our optimizing algorithms"" instead of ""SGD as our the optimizing algorithms.""
- Page 5, beam search: please include a reference for beam search.
- Figure 4: Is there a typo in the true sentence? "" council of europe again slams french prison conditions"" (again or against?)
- typo ""supper script"" -> ""superscript"" (4 times)","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"4. As for the experiments: a) The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in",ICLR_2021_2455,ICLR_2021,"In spite of the strengths mentioned above, there are a few questions that are confusing. 1. As for the simulated experiment: What is the purpose of the third figure in Figure 1? It shows that the perfect causal model performs bad under unobserved, while the other three methods performs almost the same. Further, the performance of the proposed DIRM and DRO is quite similar in this setting, which does not account for the effectiveness of the method. Besides, the result of IRM for this experiment is missed. 2. As for the theoretical analysis: a) For Theorem 1, the right hand equation uses L_2 norm of a function of beta. I read the prove and I think this norm is defined as an integral which has nothing do with beta any more. Therefore, I wonder what does the regularizer proposed in equation(6) means since beta has already been integrated. b) For Theorem 1, the core assumption is ‘the expected loss function as a function of beta belongs to a Sobolev space’, which is confusing. Could you provide some explanations of this assumption or give some examples of it? c) Theorem 1 provides an upper bound for one specific kind of DRO problem whose uncertainty set is formulated as an affine combination of training distributions. However, in this article, the authors do not state what is the definition of the invariance here and why solve such DRO problem could achieve the invariance. 3. As for the proposed objective function: a) As mentioned above, the L_2 norm is taken over a function of beta, which I think is not the Euclidean norm of the vector. Beta has already been integrated and this regularizer has nothing do with beta. I wonder how to compute this when optimizing? b) I wonder how this objective function can be optimized efficiently? The first concern is mentioned above as the computation of L_2 norm. The second concern is how to optimize the variance which is non-convex and hard to optimize. Namkoong et al. [1] convert the optimization of a variance-regularized problem to a f-divergence DRO for better optimization, while in this paper the authors take the opposite way. I wonder is there any theoretical guarantee of the optimization of the objective function(6). 4. As for the experiments: a) The experimental results on the last two datasets are not convincing enough to validate the effectiveness of the proposed method, since the performance is similar to IRM, which I wonder if it is caused by the problems mentioned above(in 3).
[1] Duchi, J. , & Namkoong, H. . (2016). Variance-based regularization with convex objectives.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '3', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?",ACL_2017_108_review,ACL_2017,"The problem itself is not really well motivated. Why is it important to detect China as an entity within the entity Bank of China, to stay with the example in the introduction? I do see a point for crossing entities but what is the use case for nested entities? This could be much more motivated to make the reader interested. As for the approach itself, some important details are missing in my opinion: What is the decision criterion to include an edge or not? In lines 229--233 several different options for the I^k_t nodes are mentioned but it is never clarified which edges should be present!
As for the empirical evaluation, the achieved results are better than some previous approaches but not really by a large margin. I would not really call the slight improvements as ""outperformed"" as is done in the paper. What is the effect size? Does it really matter to some user that there is some improvement of two percentage points in F_1? What is the actual effect one can observe? How many ""important"" entities are discovered, that have not been discovered by previous methods? Furthermore, what performance would some simplistic dictionary-based method achieve that could also be used to find overlapping things? And in a similar direction: what would some commercial system like Google's NLP cloud that should also be able to detect and link entities would have achieved on the datasets. Just to put the results also into contrast of existing ""commercial"" systems.
As for the result discussion, I would have liked to see some more emphasis on actual crossing entities. How is the performance there? This in my opinion is the more interesting subset of overlapping entities than the nested ones. How many more crossing entities are detected than were possible before? Which ones were missed and maybe why? Is the performance improvement due to better nested detection only or also detecting crossing entities? Some general error discussion comparing errors made by the suggested system and previous ones would also strengthen that part.
General Discussion: I like the problems related to named entity recognition and see a point for recognizing crossing entities. However, why is one interested in nested entities? The paper at hand does not really motivate the scenario and also sheds no light on that point in the evaluation. Discussing errors and maybe advantages with some example cases and an emphasis on the results on crossing entities compared to other approaches would possibly have convinced me more.
So, I am only lukewarm about the paper with maybe a slight tendency to rejection. It just seems yet another try without really emphasizing the in my opinion important question of crossing entities.
Minor remarks: - first mention of multigraph: some readers may benefit if the notion of a multigraph would get a short description - previously noted by ... many previous: sounds a little odd - Solving this task: which one?
- e.g.: why in italics?
- time linear in n: when n is sentence length, does it really matter whether it is linear or cubic?
- spurious structures: in the introduction it is not clear, what is meant - regarded as _a_ chunk - NP chunking: noun phrase chunking?
- Since they set: who?
- pervious -> previous - of Lu and Roth~(2015) - the following five types: in sentences with no large numbers, spell out the small ones, please - types of states: what is a state in a (hyper-)graph? later state seems to be used analogous to node?!
- I would place commas after the enumeration items at the end of page 2 and a period after the last one - what are child nodes in a hypergraph?
- in Figure 2 it was not obvious at first glance why this is a hypergraph.
colors are not visible in b/w printing. why are some nodes/edges in gray. it is also not obvious how the highlighted edges were selected and why the others are in gray ... - why should both entities be detected in the example of Figure 2? what is the difference to ""just"" knowing the long one?
- denoting ...: sometimes in brackets, sometimes not ... why?
- please place footnotes not directly in front of a punctuation mark but afterwards - footnote 2: due to the missing edge: how determined that this one should be missing?
- on whether the separator defines ...: how determined?
- in _the_ mention hypergraph - last paragraph before 4.1: to represent the entity separator CS: how is the CS-edge chosen algorithmically here?
- comma after Equation 1?
- to find out: sounds a little odd here - we extract entities_._\footnote - we make two: sounds odd; we conduct or something like that?
- nested vs. crossing remark in footnote 3: why is this good? why not favor crossing? examples to clarify?
- the combination of states alone do_es_ not?
- the simple first order assumption: that is what?
- In _the_ previous section - we see that our model: demonstrated? have shown?
- used in this experiments: these - each of these distinct interpretation_s_ - published _on_ their website - The statistics of each dataset _are_ shown - allows us to use to make use: omit ""to use"" - tried to follow as close ... : tried to use the features suggested in previous works as close as possible?
- Following (Lu and Roth, 2015): please do not use references as nouns: Following Lu and Roth (2015) - using _the_ BILOU scheme - highlighted in bold: what about the effect size?
- significantly better: in what sense? effect size?
- In GENIA dataset: On the GENIA dataset - outperforms by about 0.4 point_s_: I would not call that ""outperform"" - that _the_ GENIA dataset - this low recall: which one?
- due to _an_ insufficient - Table 5: all F_1 scores seems rather similar to me ... again, ""outperform"" seems a bit of a stretch here ... - is more confident: why does this increase recall?
- converge _than_ the mention hypergraph - References: some paper titles are lowercased, others not, why?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
- No empirical validation. I would have like to see some experiments where the bounds are validated.,NIPS_2018_43,NIPS_2018,"- Theoretical analyses are not particularly difficult, even if they do provide some insights. That is, the analyses are what I would expect any competent grad student to be able to come up with within the context of a homework assignment. I would consider the contributions there to be worthy of a posted note / arXiv article. - Section 4 is interesting, but does not provide any actionable advice to the practitioner, unlike Theorem 4. The conclusion I took was that the learned function f needs to achieve a compression rate of \zeta / m with a false positive rate F_p and false negative rate F_n. To know if my deep neural network (for example) can do that, I would have to actually train a fixed size network and then empirically measure its errors. But if I have to do that, the current theory on standard Bloom filters would provide me with an estimate of the equivalent Bloom filter that achieves the same error false positive as the learned Bloom filter. - To reiterate the above point, the analysis of Section 4 doesn't change how I would build, evaluate, and decide on whether to use learned Bloom filters. - The analytical approach of Section 4 gets confusing by starting with a fixed f with known \zeta, F_p, F_n, and then drawing the conclusion for an a priori fixed F_p, F_n (lines 231-233) before fixing the learned function f (lines 235-237). In practice, one typically fixes the function class (e.g. parameterized neural networks with the same architecture) *first* and measures F_p, F_n after. For such settings where \zeta and b are fixed a priori, one would be advised to minimize the learned Bloom filter's overall false positive (F_p + (1-F_p)\alpha^{b/F_n}) in the function class. An interesting analysis would then be to say whether this is feasible, and how it compares to the log loss function. Experiments can then conducted to back this up. This could constitute actionable advice to practitioners. Similarly for the sandwiched learned Bloom filter. - Claim (first para of Section 3.2) that ""this methodology requires significant additional assumptions"" seems too extreme to me. The only additional assumption is that the test set be drawn from the same distribution as the query set, which is natural for many machine learning settings where the train, validation, test sets are typically assumed to be from the same iid distribution. (If this assumption is in fact too hard to satisfy, then Theorem 4 isn't very useful too.) - Inequality on line 310 has wrong sign; compare inequality line 227 --- base \alpha < 1. - No empirical validation. I would have like to see some experiments where the bounds are validated.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"5) Line 285: a bit confused about the phrase ""non-sequential information such as chunks"". Is chunk still sequential information???",ACL_2017_49_review,ACL_2017,"There are some minor points, listed as follows: 1) Figure 1: I am a bit surprised that the function words dominate the content ones in a Japanese sentence. Sorry I may not understand Japanese. 2) In all equations, sequences/vectors (like matrices) should be represented as bold texts to distinguish from scalars, e.g., hi, xi, c, s, ... 3) Equation 12: s_j-1 instead of s_j.
4) Line 244: all encoder states should be referred to bidirectional RNN states.
5) Line 285: a bit confused about the phrase ""non-sequential information such as chunks"". Is chunk still sequential information???
6) Equation 21: a bit confused, e.g, perhaps insert k into s1(w) like s1(w)(k) to indicate the word in a chunk. 7) Some questions for the experiments: Table 1: source language statistics? For the baselines, why not running a baseline (without using any chunk information) instead of using (Li et al., 2016) baseline (|V_src| is different)? It would be easy to see the effect of chunk-based models. Did (Li et al., 2016) and other baselines use the same pre-processing and post-processing steps? Other baselines are not very comparable. After authors's response, I still think that (Li et al., 2016) baseline can be a reference but the baseline from the existing model should be shown. Figure 5: baseline result will be useful for comparison? chunks in the translated examples are generated *automatically* by the model or manually by the authors? Is it possible to compare the no. of chunks generated by the model and by the bunsetsu-chunking toolkit? In that case, the chunk information for Dev and Test in Table 1 will be required. BTW, the authors's response did not address my point here. 8) I am bit surprised about the beam size 20 used in the decoding process. I suppose large beam size is likely to make the model prefer shorter generated sentences. 9) Past tenses should be used in the experiments, e.g., Line 558: We *use* (used) ... Line 579-584: we *perform* (performed) ... *use* (used) ... ... - General Discussion: Overall, this is a solid work - the first one tackling the chunk-based NMT; and it well deserves a slot at ACL.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', 'X']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '0']}",,
"- Clarity: The paper is well-written, makes the connections with the relevant earlier work, and includes important details that can facilitate reproducibility (e.g. the learning rate, number of layers, etc.).",NIPS_2019_634,NIPS_2019,"see section 5 (""improvements"") below. Originality: while the methods are not particularly novel (autoregressive and masked language modelling pretraining have both been used before for ELMo and BERT; this work extends these objectives to the multi-lingual case), the performance gains on all four tasks are still very impressive. - Quality: This paper's contributions are mostly empirical. The empirical results are strong, and the methodology is sound and explained in sufficient technical details. - Clarity: The paper is well-written, makes the connections with the relevant earlier work, and includes important details that can facilitate reproducibility (e.g. the learning rate, number of layers, etc.). - Significance: The empirical results constitute a new state of the art and are important to drive progress in the field. ---------- Update after authors' response: the response clearly addressed most of my concerns. I look forward to the addition of supervised MT experiments on other languages (beyond the relatively small Romanian-English dataset) on subsequent versions of the paper. I maintain my initial assessment that this is a strong submission with impressive empirical results, which would be useful for the community. I maintain my final recommendation of ""8"".","{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,8,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1']}",,,"{'annotators': ['HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '0']}",,
"- Discrepancy between eq. 9 and Figure 1. From eq. 9, it seems like the output patches are not cropped parts of the input image but just masked versions of the input image where most pixels are black. Is this correct? In this case, Figure 1 is misleading. And if so, wouldn't zooming on the region of interest using bilinear sampling provide better results?",NIPS_2019_1338,NIPS_2019,", this paper is a solid submission. The idea is interesting and effective. It outperforms the state of the art. Strength: + The paper is well written and the explanations are clear. + The quantitative results (especially Table 2) clearly demonstrate the effectiveness of the proposed method. + Figure 1 is well designed and useful to understand the model. + Qualitative results in Figure 2 is convincing and demonstrates the consistency of the attention module across different classes. Weakness: - Motivation behind 3.2 Section 3.2 describes the cropping network that uses a 2d continuous boxcar function. Motivation for this design choice is weak, as previous attempts in local attention have used Gaussian masks [a], simple bilinear sampling using spatial transformers [b], or even pooling methods [c]. If this makes a difference, it would be great to demonstate it in an experiment. At minimum, bilinear sampling should be compared against. [a] Gregor, Karol, et al. ""Draw: A recurrent neural network for image generation."" ICML, 2015. [b] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" NeurIPS, 2015. [c] He, Kaiming, et al. ""Mask r-cnn."" Proceedings of the IEEE international conference on computer vision. 2017. - Discrepancy between eq. 9 and Figure 1. From eq. 9, it seems like the output patches are not cropped parts of the input image but just masked versions of the input image where most pixels are black. Is this correct? In this case, Figure 1 is misleading. And if so, wouldn't zooming on the region of interest using bilinear sampling provide better results? - Class-Center Triplet Loss The formulation of class-center triplet loss (L_CCT) is not entirely convincing. While the authors claim L2 normalization is introduced to ease the setting of a proper margin, this also has a different effect. This would in fact, divert the formulation to be different from the traditional definition of a margin. For example, these two points in the semantic feature space could be close, but far away after the normalization that projects them on a unit hypersphere. And the other way around is also true. Especially given the fact that the unnormalized version of phi is used also in L_CLS, the effect of this formulation is not obvious. In fact, the formulation resembles the cosine distance in an inner product, and the margin would be set -- roughly speaking -- on the cosine angle. The authors should discuss this in their paper. I find the current explanation misleading. - Backbone CNN Although I assume so, in Section 3.3 / Figure 1, it is not clear which backbone CNNs share their weights, and which don't (if some don't). Is the input image going through the same CNN as the local patches? Are the local patches going through the same CNN? I suggest some coloring to make it clear if not all are shared. - Minor issues L15: ""must be limited to one paragraph"". L193: L_CAT --> L_CCT Equation 11: it would be clearer with indices under the max function. L215: ""unit sphere"" -> ""unit hypersphere"". Unless the dimension of the semantic feature space is 3, which in this case should be mentioned. Potential Enhancements: * This paper is targeting zero-shot classification but since the multi-attention module is a major contribution by itself, it could have been validated on other tasks. An obvious one is fine-grained classification, on CUB-200 for instance. It is maybe possible for the authors to report this result since they already use CUB-200, but I would understand if it is not done in the rebuttal. ==== POST REBUTTAL ==== The additional results have made the submission even stronger than before. I am therefore more confident in the rating.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"4. Although Theorem 1 seems correct to me, I have a question here. Assume we have a separate node with 0 neighbors, that means the upper bound here is 0. It is obviously not true. So, how to explain this exception?",ICLR_2022_234,ICLR_2022,"Weakness and Questions: 1. The analysis is only limited to GCNs, while the paper title is too general (GNNs). Not very 2. One concern is that the performance of GCN on Chameleon and Squirrel (Table1) differs a lot from the one reported in other papers (e.g. Geom-GCN (Pei et al. 2020), H2GNN (Zhu et al. 2020)). It seems the settings are the same as Pei et al. 2020, why is the result on these two datasets so different (2 to 3 times different)? Generally, I do not think the hyperprameter tunning should impact so much. Could the authors explain more details about it? In fact I also run some experiments on those datasets before and I cannot get those high numbers either. 3. The cross-class neighborhood similarity metric is intuitive and a good idea. However, it lacks of a direct theoretic connection to GCNs’ performance. Same as the heterophily metric, I do not think it can completely decide the GCNs’ performance, because the node feature distribution is also important here. In fact, the assumptions in Theorem 1 are quite strong. If the nodes with the same label are sampled from the same feature distribution, it means generally MLP can also have a good performance. When this assumption does not meet, the analysis will become very complex. That is why I think Figure5 may be not enough to explain everything (but it is still interesting to see this empirical result). 4. Although Theorem 1 seems correct to me, I have a question here. Assume we have a separate node with 0 neighbors, that means the upper bound here is 0. It is obviously not true. So, how to explain this exception?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1. Limited technical novelty. Compare with the two mentioned papers (Xing and Tsang, 2022a, b), although the previous papers focus on graph-based approaches, the idea, co-attention mechanism, and architecture of this paper are quite similar to the previous.",CsCRTvEZg1,EMNLP_2023,"1. Limited technical novelty. Compare with the two mentioned papers (Xing and Tsang, 2022a, b), although the previous papers focus on graph-based approaches, the idea, co-attention mechanism, and architecture of this paper are quite similar to the previous.
2. Calculating and presenting the averaged overall accuracies over two datasets (the Avg columns) in table 1 and table 2 seems kind of unfounded.
3. In the part of Task-shared encoder, a character-level word embedding has been concatenated to the vector. It might be better to briefly explain the purpose of the concatenation.
4. In the PLM part, It might be better to add an experiment with MISCA + BERT, since several strong baselines have not been applied to RoBERTa. Only one experiment with PLM seems to be quite few. .","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"- Pg.5: “The training time reduction is less drastic than the parameter reduction because most gradients are still computed for early down-sampling layers (Discussion).” This seems not to have been revisited in the Discussion (which is fine, just delete “Discussion”).",ICLR_2021_973,ICLR_2021,".
Clearly state your recommendation (accept or reject) with one or two key reasons for this choice. I recommend acceptance. The number of updates needed to learn realistic brain-like representations is a fair criticism of current models, and this paper demonstrates that this number can be greatly reduced, with moderate reduction in Brain-Score. I was surprised that it worked so well.
Ask questions you would like answered by the authors to help you clarify your understanding of the paper and provide the additional evidence you need to be confident in your assessment. - Is the third method (updating only down-sampling layers) meant to be biologically relevant? If so, can anything more specific be said about this, other than that different cortical layers learn at different rates? - Given that the brain does everything in parallel, why is the number of weight updates a better metric than the number of network updates?
Provide additional feedback with the aim to improve the paper. - Bottom of pg. 4: I think 37 bits / synapse (Zador, 2019) relates to specification of the target neuron rather than specification of the connection weight. So I’m not sure its obvious how this relates to the weight compression scheme. The target neurons are already fully specified in CORnet-S. - Pg. 5: “The training time reduction is less drastic than the parameter reduction because most gradients are still computed for early down-sampling layers (Discussion).” This seems not to have been revisited in the Discussion (which is fine, just delete “Discussion”). - Fig. 3: Did you experiment with just training the middle Conv layers (as opposed to upsample or downsample layers)? - Fig. 3: Why go to 0 trained parameters for downstream training, but minimum ~1M trained parameters for CT? - Fig. 4: On the color bar, presumably one of the labels should say “worse”. - Section B.1: How many Gaussian components were used, or how many parameters total? Or if different for each layer, what was the maximum across all layers? - Section B.3: I wasn’t clear on the numbers of parameters used in each approach. - D.1: How were CORnet-S clusters mapped to ResNet blocks? I thought different clusters were used in each layer. If not, maybe this could be highlighted in Section 4.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"1) that problem applies to other downstream tasks or is just specific to binding affinity prediction — and if so, why?",ICLR_2023_879,ICLR_2023,"The ablations for the different pre-training tasks in section 4.5 / Figure 6 are a bit puzzling. It does seem that the CRD task has destructive value on that particular binding affinity prediction task since: a) the performance of CRD + MLM or CRD + PPI leads to both lower performance Vs MLM or PPI alone respectively b) the performance of CRD + MLM + PPI is also lower vs just using MLM + PPI. This seems particularly important from a practical standpoint, and additional experiments are needed to confirm whether: 1) that problem applies to other downstream tasks or is just specific to binding affinity prediction — and if so, why? 2) there is something fundamentally wrong with the CRD pre-training as currently implemented? 3) there is a way to anticipate ex ante (or post fine tuning) which tokens should be used to ensure optimal task performance ?
The ablation in section in Table 3 is a bit puzzling as well: it appears that the performance of PromptProtein without layer skip is lower than the performance from the conventional MTL. Could you please explain why that might be the case? (I would have assumed intermediate performance between conventional MTL and full PromptProtein as I presume the attention masks are still used in that ablation?)
Several points (in section 4 primarily) were not fully clear (see clarity paragraph below).
The following claim in conclusion does not seem fully substantiated: “PromptProtein beats state-of-the-art baselines by significant margins”. Authors do report the relevant baselines listed in the FLIP paper [1]. But since that paper was released, several methods have shown markedly superior performance for protein modeling & achieving high spearman with deep mutational scanning assays — see for example, [2] and [3]. I would suggest adding these two baselines to the analysis or tone done the SOTA claims.
[1] Dallago, C., Mou, J., Johnston, K.E., Wittmann, B.J., Bhattacharya, N., Goldman, S., Madani, A., & Yang, K.K. (2022). FLIP: Benchmark tasks in fitness landscape inference for proteins. bioRxiv.
[2] Hsu, C., Verkuil, R., Liu, J., Lin, Z., Hie, B.L., Sercu, T., Lerer, A., & Rives, A. (2022). Learning inverse folding from millions of predicted structures. bioRxiv.
[3] Notin, P., Dias, M., Frazer, J., Marchena-Hurtado, J., Gomez, A.N., Marks, D.S., & Gal, Y. (2022). Tranception: Protein Fitness Prediction with Autoregressive Transformers and Inference-time Retrieval. ICML.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '4', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3. The handling of rumors generated by GPT: The paper points out the challenges of detecting rumors generated by GPT, but further analysis or solutions can be proposed. There is no analysis of why GPT-generated Rumor is closer to Natural Rumor, or in other words, why is GPT-generated Rumor about as difficult to detect as Natural Rumor? After all, Artificial Rumor is also written by humans, so it should be about the same difficulty as Natural Rumor, but the experimental result is that Natural Rumor is the easiest to detect.",D6zn6ozJs7,ICLR_2025,"1. Limited exploration of external knowledge sources: The paper acknowledges that reliance on sources such as Wikipedia is a limitation but can further explore its impact on detection performance and possible solutions.
2. Evaluation depth: Although the paper evaluated multiple models, it mainly focused on the zero-shot setting. If fine-tuning models or more ablation experiments are added, the experimental results may be more convincing.
3. The handling of rumors generated by GPT: The paper points out the challenges of detecting rumors generated by GPT, but further analysis or solutions can be proposed. There is no analysis of why GPT-generated Rumor is closer to Natural Rumor, or in other words, why is GPT-generated Rumor about as difficult to detect as Natural Rumor? After all, Artificial Rumor is also written by humans, so it should be about the same difficulty as Natural Rumor, but the experimental result is that Natural Rumor is the easiest to detect.
4. It is suggested to discuss and compare more related works such as [1] in this paper.
[1] Detecting and Grounding Multi-Modal Media Manipulation and Beyond. TPAMI 2024.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
"2 The technical contribution is limited. For example, the contents of Section 4 are not about a formal and principled solution, but most about heuristics.",ICLR_2023_2640,ICLR_2023,"Weakness: 1 For key issues in federated recommendation, the authors do not contribute/discuss much, e.g., communication cost, privacy protection, time complexity.
2 The technical contribution is limited. For example, the contents of Section 4 are not about a formal and principled solution, but most about heuristics.
3 For the studied problem, there are many recent works, which are not studied in the experiments.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '1', '3']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '1', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
- Figure 6: the font size is a little bit small.,ARR_2022_113_review,ARR_2022,"- Although BFS is briefly introduced in Section 3, it's still uneasy to understand for people who have not studied the problem. More explanation is preferable.
- Algorithm 1, line 11: the function s(·) should accept a single argument according to line 198.
- Figure 6: the font size is a little bit small.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- It's regrettable that the probability mass function is practically unexploited. In MixBoost it is set to a quasi-uniform distribution, which depends on only one single parameter. Intuitively, each learner class should be considered individually, even in the case of BDT of different depths. I think that considering various probability mass function would've added further depth to the experimental setting (unless I'm missing an obvious reason why the quasi-uniform distribution is well suited...).",NIPS_2020_936,NIPS_2020,"I have a few comments on this paper, even though it would be unfair to call them weaknesses. They are listed below in no particular order. - It's regrettable that the probability mass function is practically unexploited. In MixBoost it is set to a quasi-uniform distribution, which depends on only one single parameter. Intuitively, each learner class should be considered individually, even in the case of BDT of different depths. I think that considering various probability mass function would've added further depth to the experimental setting (unless I'm missing an obvious reason why the quasi-uniform distribution is well suited...). - Continuing from the previous point, it would have been interesting to have a discussion on how the choice of the probability mass function influences the theoretical guarantees of in section 2.4. - The main strength of HNBM resides in using arbitrary mass functions, yet MixBoost only relies in BDT and LR. I strongly think that combining other types of classifiers should provide further insight on MixBoost.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '1', '1']}",,
2. ChatGPT shows a great percentage of abstention than other models. Is that fair to compare their accuracies?,AGVANImv7S,EMNLP_2023,"**Weaknesses：**
1. The proposed evaluation pipeline is similar to prior works.
2. ChatGPT shows a great percentage of abstention than other models. Is that fair to compare their accuracies?
3. Reproducibility. The author doesn't mention if the code will be available to the public.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '2', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['2', '2', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- Missing references: the references below are relevant to your topic, especially [a]. Please discuss connections with [a], which uses supervised learning in QBF solving, where QBF generalizes SMT, in my understanding. [a] Samulowitz, Horst, and Roland Memisevic. ""Learning to solve QBF."" AAAI. Vol.",NIPS_2018_947,NIPS_2018,"weakness of the paper, in its current version, is the experimental results. This is not to say that the proposed method is not promising - it definitely is. However, I have some questions that I hope the authors can address. - Time limit of 10 seconds: I am quite intrigued as to the particular choice of time limit, which seems really small. In comparison, when I look at the SMT Competition of 2017, specifically the QF_NIA division (http://smtcomp.sourceforge.net/2017/results-QF_NIA.shtml?v=1500632282), I find that all 5 solvers listed require 300-700 seconds. The same can be said about QF_BF and QF_NRA (links to results here http://smtcomp.sourceforge.net/2017/results-toc.shtml). While the learned model definitely improves over Z3 under the time limit of 10 seconds, the discrepancy with the competition results on similar formula types is intriguing. Can you please clarify? I should note that while researching this point, I found that the SMT Competition of 2018 will have a ""10 Second wonder"" category (http://smtcomp.sourceforge.net/2018/rules18.pdf). - Pruning via equivalence classes: I could not understand what is the partial ""current cost"" you mention here. Thanks for clarifying. - Figure 3: please annotate the axes!! - Bilinear model: is the label y_i in {-1,+1}? - Dataset statistics: please provide statistics for each of the datasets: number of formulas, sizes of the formulas, etc. - Search models comparison 5.1: what does 100 steps here mean? Is it 100 sampled strategies? - Missing references: the references below are relevant to your topic, especially [a]. Please discuss connections with [a], which uses supervised learning in QBF solving, where QBF generalizes SMT, in my understanding. [a] Samulowitz, Horst, and Roland Memisevic. ""Learning to solve QBF."" AAAI. Vol. 7. 2007. [b] Khalil, Elias Boutros, et al. ""Learning to Branch in Mixed Integer Programming."" AAAI. 2016. Minor typos: - Line 283: looses -> loses","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"2.When training, a mono tonic relationship is imposed between the degree of a single-task predictor participation and the weight of the corresponding task loss. As a result, the ensemble engenders a subspace that explicitly encodes tradeoffs and results in a continuous parameterization of the Pareto Front. Whether the mono tonic relationship can be replaced by other relationships? Explaining this point may be better. [1]Navon A, Shamsian A, Fetaya E, et al. Learning the Pareto Front with Hypernetworks[C]//International Conference on Learning Representations. 2020.",ICLR_2023_2237,ICLR_2023,"1.Similar methods have already been proposed for multi-task learning and has not been disccussed in this paper [1].
1.When sampling on the convex hull parameterization, authors choose to adopt the Dirichlet distribution since its support is the T-dimensional simplex. Does this distribution have other properties. Why using this distribution? If p≫1，how the ensemble will change.
2.When training, a mono tonic relationship is imposed between the degree of a single-task predictor participation and the weight of the corresponding task loss. As a result, the ensemble engenders a subspace that explicitly encodes tradeoffs and results in a continuous parameterization of the Pareto Front. Whether the mono tonic relationship can be replaced by other relationships? Explaining this point may be better.
[1]Navon A, Shamsian A, Fetaya E, et al. Learning the Pareto Front with Hypernetworks[C]//International Conference on Learning Representations. 2020.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '4']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '4']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
6. How about the comparison in terms of computation cost / running time?,NIPS_2020_576,NIPS_2020,"1. Although the problem studied in this paper is interesting, this pape is not easy to follow. 2. More baselines (sampling approaches designed for GNNs) are needed. In Table 2, S-GCN [5] is a simple sampler. ClusterGCN and GraphSAINT are designed for sampling (sub)graphs. The same for Table 3. 3. To be honest, I am kind of confused about Table 3. It would be better if the authors provide more analysis for Table 3. And more analysis when Tables 2,3 are considered together. 4. How did the authors determine the hyper-parameter settings? 5. I am interested in seeing more experimental comparison on the datasets with a large number of nodes. 6. How about the comparison in terms of computation cost / running time?","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '5', '3']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '4', '2']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"- I did not get a clear picture from the goal of the paper in the introduction. My guess is that the examples chosen did not convince me that there are problems which require a lot of inter-process communication. This holds particularly for the second paragraph where sampling-based Bayesian methods are particularly mentioned as an example where the paper's results are irrelevant as they are already embarrassingly parallel. Instead, I suggest the authors try to focus on problems where the loss function does not decompose as the sum of sample losses and other ERM-based distributed algorithms such as Hogwild.",NIPS_2018_330,NIPS_2018,"weakness of the paper is its insufficient motivation of the problem, and thus, I fail to see the relevance. Specifically, the authors do not show that the process communication is indeed a problem in practice. This is particularly surprising as typical empirical risk minimization (ERM) problems naturally decompose as sum of losses and the gradient is the sum of the local loss gradients. Thus, the required inter-process communication limits to one map-reduce and one broadcast per iteration. The paper lacks experiments reporting runtime vs. number of processors, in particular with respect to different communication strategies. In effect, I cannot follow _why_ the authors would like to proof convergence results for this setting, as I am not convinced _why_ this setting is preferable to other communication and optimization strategies. I am uncertain whether this is only a problem of presentation and not of significance, but it is unfortunately insufficient in either case. Here are some suggetions which I hope the authors might find useful for future presentations of the subject: - I did not get a clear picture from the goal of the paper in the introduction. My guess is that the examples chosen did not convince me that there are problems which require a lot of inter-process communication. This holds particularly for the second paragraph where sampling-based Bayesian methods are particularly mentioned as an example where the paper's results are irrelevant as they are already embarrassingly parallel. Instead, I suggest the authors try to focus on problems where the loss function does not decompose as the sum of sample losses and other ERM-based distributed algorithms such as Hogwild. - From the discussion in lines 60-74 (beginning of Sect. 2), I had the impression that the authors want to focus on a situation where the gradient of the sum is not the sum of the individual gradients, but this is not communicated in the text. In particular, the paragraph lines 70-74 is a setting that is shared in all ERM approaches and could be discussed in less space. A situation where the the gradient of the sum of the losses is not the sum of the individual loss gradients is rare and could require some space. - Line 114, the authors should introduce the notation for audiences not familiar with manifolds ('D' in ""D R_theta ..."") - From the presentations in the supplement, I cannot see how the consideration of the events immediately imply the Theorem. I assume this presentation is incomplete? Generally, the authors could point which parts explicitely of the proof need to be adapted, why, and how it's done. The authors also seem to refer to statements in Lemmata defined in other texts (Lemma 3, Lemma 4). These should be restated or more clearly referenced. - In algorithm 1, the subscript 's' denotes the master machine, but also iteration times. I propose to use subscript '1' to denote to the master machine (problematic lines are line 6 ""Transmit the local ..."" and lines 8-9 ""Form the surrogate function ..."") - the text should be checked for typos (""addictive constant"" and others) --- Post-rebuttal update: I appreciate the author's feedback. I am afraid my missing point was on a deeper level than the authors anticipated: I fail to see in what situations the access to the global higher-derivatives is required which seems to be the crux and the motivation of the analysis. In particular, to me this is still in stark contrast with the rebuttal to comment 2. If I'm using gradient descent and if the gradient of the distributed sum is the sum of the distributed gradients, how is the implementation more costly than a map-reduce followed-up by a broadcast? For gradient descent to converge, no global higher-order derivative information is required, no? So this could be me not having a good overview of more elaborate optimization algorithms or some other misunderstanding of the paper. But I'm seeing that my colleagues seem to grasp the significance so I'm looking forward to be convinced in the future.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '1']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"* Why is this approach more privacy preserving than other federated learning approaches? Is privacy preservation an issue for traffic signal control, i.e. one traffic signal not to know what is the color of the next one? One would think that this is a very bad example of an application of federated learning.",tUiYbVqcuQ,ICLR_2024,"* The claims of the paper are unclear. What does it mean that the ""optimal actions are personalized?"". How do we measure personalization?
* What kind of communication overhead we are talking about, during training or during inference? How big is the communication cost for a stoplight? Although this is one of the three claims, the paper does not seem to measure communication cost anywhere in the rest of the paper.
* Why is this approach more privacy preserving than other federated learning approaches? Is privacy preservation an issue for traffic signal control, i.e. one traffic signal not to know what is the color of the next one? One would think that this is a very bad example of an application of federated learning.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['3', '3', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['X', 'X', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['4', '4', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,
"3) Lack of fair comparison: In Figure 3, the authors compared CPEF with PMEF to demonstrate the advantages of the pre-trained question representation model under data scarcity conditions (line 529-lin534). However, emphasizing the advantages of CPEF through this comparison is unjust since PMEF lacks a pre-training module. To ensure fairness, it is recommended to compare CPEF with another pre-trained model, such as ExpertBert, to showcase the advantage of the innovative pre-training module design of CPEF.",0DkaimvWs0,EMNLP_2023,"1) Inadequate method details: The ""expert ID embedding"" mentioned in Section 4.3 is somewhat confusing as it lacks specific clarification. It remains unclear whether this ID refers to the registered name of the expert or some other form of identification. If it simply represents the expert's registered name, its ability to capture the personalized characteristics of the expert is questionable.
2) Insufficient experiments: The ""Further Analysis"" section of the paper and the experiments of ""ExpertPLM: Pre-training Expert Representation for Expert Finding (https://aclanthology.org/2022.findings-emnlp.74.pdf)"" share significant similarities. However, there is a lack of experimental analysis specific to certain parameters within the paper itself, such as the length of the question body and title and the number of negative samples (K value).
3) Lack of fair comparison: In Figure 3, the authors compared CPEF with PMEF to demonstrate the advantages of the pre-trained question representation model under data scarcity conditions (line 529-lin534). However, emphasizing the advantages of CPEF through this comparison is unjust since PMEF lacks a pre-training module. To ensure fairness, it is recommended to compare CPEF with another pre-trained model, such as ExpertBert, to showcase the advantage of the innovative pre-training module design of CPEF.","{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,8,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['5', '5', '5']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['0', '0', '0']}",,,"{'annotators': ['boda', 'HXjcIUXf', 'T5yf801Z'], 'labels': ['1', '1', '1']}",,

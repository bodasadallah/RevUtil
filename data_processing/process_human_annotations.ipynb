{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "import os\n",
    "import sys\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_review_points = pd.read_csv('all_review_points.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>venue</th>\n",
       "      <th>focused_review</th>\n",
       "      <th>point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ACL_2017_614_review</td>\n",
       "      <td>ACL_2017</td>\n",
       "      <td>- I don't understand effectiveness of the mult...</td>\n",
       "      <td>- I don't understand effectiveness of the mult...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ACL_2017_614_review</td>\n",
       "      <td>ACL_2017</td>\n",
       "      <td>- I don't understand effectiveness of the mult...</td>\n",
       "      <td>- The paper is not fully clear on a first read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ACL_2017_614_review</td>\n",
       "      <td>ACL_2017</td>\n",
       "      <td>- I don't understand effectiveness of the mult...</td>\n",
       "      <td>- The relatively poor performance on nouns mak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ACL_2017_614_review</td>\n",
       "      <td>ACL_2017</td>\n",
       "      <td>- I don't understand effectiveness of the mult...</td>\n",
       "      <td>-General Discussion: The paper is mostly strai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACL_2017_614_review</td>\n",
       "      <td>ACL_2017</td>\n",
       "      <td>- I don't understand effectiveness of the mult...</td>\n",
       "      <td>- Lines 367-368 : Why does X^{P} need to be sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207182</th>\n",
       "      <td>01wSNY5T60</td>\n",
       "      <td>EMNLP_2023</td>\n",
       "      <td>* No novel techniques or salutations are propo...</td>\n",
       "      <td>* The paper uses only two datasets. There are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207183</th>\n",
       "      <td>01wSNY5T60</td>\n",
       "      <td>EMNLP_2023</td>\n",
       "      <td>* No novel techniques or salutations are propo...</td>\n",
       "      <td>* The subgroup partitioning is not clearly def...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207184</th>\n",
       "      <td>01wSNY5T60</td>\n",
       "      <td>EMNLP_2023</td>\n",
       "      <td>* No novel techniques or salutations are propo...</td>\n",
       "      <td>* Lacks deeper analysis on why TinyBERT is an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207185</th>\n",
       "      <td>01wSNY5T60</td>\n",
       "      <td>EMNLP_2023</td>\n",
       "      <td>1. I would have appreciated more datasets, esp...</td>\n",
       "      <td>1. I would have appreciated more datasets, esp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207186</th>\n",
       "      <td>01wSNY5T60</td>\n",
       "      <td>EMNLP_2023</td>\n",
       "      <td>1. I would have appreciated more datasets, esp...</td>\n",
       "      <td>2. I find the overall conclusion a bit lacklus...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207187 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   paper_id       venue  \\\n",
       "0       ACL_2017_614_review    ACL_2017   \n",
       "1       ACL_2017_614_review    ACL_2017   \n",
       "2       ACL_2017_614_review    ACL_2017   \n",
       "3       ACL_2017_614_review    ACL_2017   \n",
       "4       ACL_2017_614_review    ACL_2017   \n",
       "...                     ...         ...   \n",
       "207182           01wSNY5T60  EMNLP_2023   \n",
       "207183           01wSNY5T60  EMNLP_2023   \n",
       "207184           01wSNY5T60  EMNLP_2023   \n",
       "207185           01wSNY5T60  EMNLP_2023   \n",
       "207186           01wSNY5T60  EMNLP_2023   \n",
       "\n",
       "                                           focused_review  \\\n",
       "0       - I don't understand effectiveness of the mult...   \n",
       "1       - I don't understand effectiveness of the mult...   \n",
       "2       - I don't understand effectiveness of the mult...   \n",
       "3       - I don't understand effectiveness of the mult...   \n",
       "4       - I don't understand effectiveness of the mult...   \n",
       "...                                                   ...   \n",
       "207182  * No novel techniques or salutations are propo...   \n",
       "207183  * No novel techniques or salutations are propo...   \n",
       "207184  * No novel techniques or salutations are propo...   \n",
       "207185  1. I would have appreciated more datasets, esp...   \n",
       "207186  1. I would have appreciated more datasets, esp...   \n",
       "\n",
       "                                                    point  \n",
       "0       - I don't understand effectiveness of the mult...  \n",
       "1       - The paper is not fully clear on a first read...  \n",
       "2       - The relatively poor performance on nouns mak...  \n",
       "3       -General Discussion: The paper is mostly strai...  \n",
       "4       - Lines 367-368 : Why does X^{P} need to be sy...  \n",
       "...                                                   ...  \n",
       "207182  * The paper uses only two datasets. There are ...  \n",
       "207183  * The subgroup partitioning is not clearly def...  \n",
       "207184  * Lacks deeper analysis on why TinyBERT is an ...  \n",
       "207185  1. I would have appreciated more datasets, esp...  \n",
       "207186  2. I find the overall conclusion a bit lacklus...  \n",
       "\n",
       "[207187 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_review_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dict = {\n",
    "    'paper_id': '',\n",
    "    'venue': '',\n",
    "    'focused_review': '',\n",
    "    'review_point': '',\n",
    "    'actionability': '',\n",
    "    'actionability_label': '',\n",
    "    'actionability_label_type': '',\n",
    "    'grounding_specificity': '',\n",
    "    'grounding_specificity_label': '',\n",
    "    'grounding_specificity_label_type': '',\n",
    "    'verifiability': '',\n",
    "    'verifiability_label': '',\n",
    "    'verifiability_label_type': '',\n",
    "    'helpfulness': '',\n",
    "    'helpfulness_label': '',\n",
    "    'helpfulness_label_type': '',\n",
    "    \"professional_tone\": '',\n",
    "    \"professional_tone_label\": '',\n",
    "    \"professional_tone_label_type\": '',\n",
    "    'valid_point': '',\n",
    "    'valid_point_label': '',\n",
    "    'valid_point_label_type': '',\n",
    "    'addressed_to_author': '',\n",
    "    'addressed_to_author_label': '',\n",
    "    'addressed_to_author_label_type': '',\n",
    "    'batch' : ''   \n",
    "}\n",
    "\n",
    "aspects = [ 'actionability', 'grounding_specificity', 'verifiability', 'helpfulness', \"professional_tone\", 'valid_point', 'addressed_to_author']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepted_annotators = {\n",
    "\n",
    "    'batch_1': ['fZav2G06', 'DVRTnFRi', 'boda'],\n",
    "    'batch_2': [\n",
    "        \"TxZsPCly\",\n",
    "        \"LbMNie2g\",\n",
    "        \"ZGOQesrg\",\n",
    "        \"2B7nNvBS\",\n",
    "        \"3O1LNSAk\",\n",
    "        \"9BW3mEvI\",\n",
    "        \"pGkwDdmV\",\n",
    "        \"boda\"\n",
    "    ],\n",
    "    'batch_3': [\n",
    "        \"tpuEIMI7\",\n",
    "        \"nZ3mMZhw\",\n",
    "        \"aAISOUiD\",\n",
    "        \"boda\"\n",
    "    ],\n",
    "    'batch_4': ['x8G3vam1', 'sBdHWtl1','boda'],\n",
    "    'batch_5': ['qDjzAPQZ', 'JMHUapmO','boda'],\n",
    "    'batch_6': ['hgdl9t28', 's46d5sbV','boda'],\n",
    "    'batch_7': ['DFou9r7M', 'BXzyXMPh','boda'],\n",
    "    'batch_8': ['boda', 'HXjcIUXf', 'T5yf801Z'],\n",
    "    'batch_9': ['boda', 'xT364YSG', 'KfBeSZkz'],\n",
    "    'batch_10': ['boda', 'cYVX6CPX', 'qhBkNx6n'],\n",
    "}\n",
    "\n",
    "\n",
    "all_annotations_1 = pd.read_excel(f'human_annotation_1.xlsx', sheet_name=None)\n",
    "\n",
    "## for each annotator map the values of actionability\n",
    "mapping = {\n",
    "    'Unactionable' :1,\n",
    "    'Borderline Actionable': 2,\n",
    "    'Somewhat Actionable': 3,\n",
    "    'Mostly Actionable': 4,\n",
    "    'Highly Actionable': 5}\n",
    "for key in all_annotations_1.keys():\n",
    "    all_annotations_1[key]['actionability'] = all_annotations_1[key]['actionability'].map(mapping)\n",
    "\n",
    "\n",
    "all_annotations_2 = pd.read_excel(f'human_annotation_2/human_annotation_2.xlsx', sheet_name=None)\n",
    "all_annotations_2_1 = pd.read_excel(f'human_annotation_2.1/human_annotation_2.1.xlsx', sheet_name=None) \n",
    "## concat the two into all_annotations\n",
    "for key in all_annotations_2_1.keys():\n",
    "    all_annotations_2[key] = all_annotations_2_1[key]\n",
    "all_annotations_3 = pd.read_excel(f'human_annotation_3/human_annotation_3.xlsx', sheet_name=None)\n",
    "all_annotations_4 = pd.read_excel(f'human_annotation_4/human_annotation_4.xlsx', sheet_name=None)\n",
    "all_annotations_5 = pd.read_excel(f'human_annotation_5/human_annotation_5.xlsx', sheet_name=None)\n",
    "all_annotations_6 = pd.read_excel(f'human_annotation_6/human_annotation_6.xlsx', sheet_name=None)\n",
    "all_annotations_7 = pd.read_excel(f'human_annotation_7/human_annotation_7.xlsx', sheet_name=None)\n",
    "all_annotations_8 = pd.read_excel(f'human_annotation_8/human_annotation_8.xlsx', sheet_name=None)\n",
    "all_annotations_9 = pd.read_excel(f'human_annotation_9/human_annotation_9.xlsx', sheet_name=None)\n",
    "all_annotations_10 = pd.read_excel(f'human_annotation_10/human_annotation_10.xlsx', sheet_name=None)\n",
    "\n",
    "all_annotations = [all_annotations_1, \n",
    "                   all_annotations_2, \n",
    "                   all_annotations_3, \n",
    "                   all_annotations_4,\n",
    "                   all_annotations_5, \n",
    "                   all_annotations_6, \n",
    "                   all_annotations_7, \n",
    "                   all_annotations_8,\n",
    "                   all_annotations_9,\n",
    "                   all_annotations_10\n",
    "                   ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_annotations = []\n",
    "main_3_annotators = []\n",
    "\n",
    "for BATCH in range(1, 11):\n",
    "    ## iterate over the rows\n",
    "    batch_annotations = []\n",
    "    current_annotations = all_annotations[BATCH -1]\n",
    "    main_annotator = accepted_annotators[f'batch_{BATCH}'][-1]\n",
    "    curr_annotators = accepted_annotators[f'batch_{BATCH}']\n",
    "    for i, row in current_annotations[main_annotator].iterrows():\n",
    "        current_dict = {}\n",
    "        current_dict['review_point'] = row['review_point']\n",
    "        ## search for the entry in the all_review_points that have the save review point and take the paper_id and venue and focused_review\n",
    "        paper_id = all_review_points[all_review_points['point'] == row['review_point']]['paper_id'].values[0]\n",
    "        venue = all_review_points[all_review_points['point'] == row['review_point']]['venue'].values[0]\n",
    "        focused_review = all_review_points[all_review_points['point'] == row['review_point']]['focused_review'].values[0]\n",
    "        current_dict['paper_id'] = paper_id\n",
    "        current_dict['venue'] = venue\n",
    "        current_dict['focused_review'] = focused_review\n",
    "\n",
    "        # ##\n",
    "        # if BATCH >= 5:\n",
    "        #     aspects = aspects + ['addressed_to_author']\n",
    "\n",
    "        for aspect in aspects:\n",
    "            labels = []\n",
    "            annotators = []\n",
    "            for annotator in curr_annotators:\n",
    "                \n",
    "                unified_annotator_id = utils.annotators_unique_id_batch_id_map_inv[annotator]\n",
    "                if BATCH < 4 and unified_annotator_id not in main_3_annotators:\n",
    "                    continue \n",
    "                \n",
    "\n",
    "                label = str(current_annotations[annotator].iloc[i][aspect])\n",
    "                if label != 'nan':\n",
    "                    ## if label is not X, then convert it to int then to str\n",
    "                    if label != 'X':\n",
    "                        label = str(int(float(label)))\n",
    "                    labels.append(label)\n",
    "                    annotators.append(annotator)\n",
    "            current_dict[aspect] =  {\n",
    "                'annotators': annotators,\n",
    "                'labels': labels}\n",
    "            current_dict[aspect + '_label'] = ''\n",
    "            current_dict[aspect + '_label_type'] = ''\n",
    "            current_dict['batch'] = BATCH\n",
    "        batch_annotations.append(current_dict)\n",
    "    batch_df = pd.DataFrame(batch_annotations)\n",
    "    final_annotations.append(batch_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## concat the df together and then save it to a csv file\n",
    "final_df = pd.concat(final_annotations)\n",
    "\n",
    "final_df.to_csv('human_annotation_gathered/all_human_annotations.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

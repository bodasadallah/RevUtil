{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/iclr2024_papers.json', 'r') as f:\n",
    "    iclr_24 = json.load(f)\n",
    "with open('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/iclr2025_papers.json', 'r') as f:\n",
    "    iclr_25 = json.load(f)\n",
    "with open('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/emnlp2023_papers.json', 'r') as f:\n",
    "    emnlp_23 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class Paper:\n",
    "    arr_id: str = None\n",
    "    title: str = None\n",
    "    venue: str = None\n",
    "    year: int = None\n",
    "    url: str = None\n",
    "    focused_review: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'forum', 'content', 'invitations', 'cdate', 'odate', 'mdate', 'signatures', 'writers', 'readers', 'reviews'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr_24[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': {'value': 'Learning SO(3)-Invariant Correspondence via Point-wise Local Shape Transform'},\n",
       " 'authors': {'value': ['Chunghyun Park',\n",
       "   'Seungwook Kim',\n",
       "   'Jaesik Park',\n",
       "   'Minsu Cho']},\n",
       " 'authorids': {'value': ['~Chunghyun_Park1',\n",
       "   '~Seungwook_Kim2',\n",
       "   '~Jaesik_Park3',\n",
       "   '~Minsu_Cho1']},\n",
       " 'keywords': {'value': ['Point cloud understanding',\n",
       "   '3D dense correspondence',\n",
       "   'SO(3)-invariance',\n",
       "   'Part label transfer']},\n",
       " 'TLDR': {'value': 'We present a novel method to establish SO(3)-invariant dense correspondences between two different 3D instances of the same category, even under arbitrary rotation transformations.'},\n",
       " 'abstract': {'value': 'Establishing accurate dense 3D correspondences between diverse shapes stands as a pivotal challenge with profound implications for computer vision and robotics. However, existing self-supervised methods for this problem assume perfect input shape alignment, restricting their real-world applicability. In this work, we introduce a novel self-supervised SO(3)-invariant 3D correspondence learner, dubbed LSTNet, that learns to establish dense correspondences between shapes even under challenging intra-class variations. Specifically, LSTNet learns to dynamically formulate an SO(3)-invariant local shape transform for each point, which maps the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor. These local shape descriptors are provided as inputs to our decoder to facilitate point cloud self- and cross-reconstruction. Our proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish the dense point-wise correspondences. LSTNet demonstrates state-of-the-art performances on 3D semantic keypoint transfer and part segmentation label transfer given arbitrarily rotated point cloud pairs, outperforming existing methods by significant margins.'},\n",
       " 'primary_area': {'value': 'unsupervised, self-supervised, semi-supervised, and supervised representation learning'},\n",
       " 'code_of_ethics': {'value': 'I acknowledge that I and all co-authors of this work have read and commit to adhering to the ICLR Code of Ethics.'},\n",
       " 'submission_guidelines': {'value': 'I certify that this submission complies with the submission instructions as described on https://iclr.cc/Conferences/2024/AuthorGuide.'},\n",
       " 'anonymous_url': {'value': \"I certify that there is no URL (e.g., github page) that could be used to find authors' identity.\"},\n",
       " 'no_acknowledgement_section': {'value': 'I certify that there is no acknowledgement section in this submission for double blind review.'},\n",
       " 'venue': {'value': 'ICLR 2024 Conference Withdrawn Submission'},\n",
       " 'venueid': {'value': 'ICLR.cc/2024/Conference/Withdrawn_Submission'},\n",
       " 'pdf': {'value': '/pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf'},\n",
       " '_bibtex': {'value': '@misc{\\npark2024learning,\\ntitle={Learning {SO}(3)-Invariant Correspondence via Point-wise Local Shape Transform},\\nauthor={Chunghyun Park and Seungwook Kim and Jaesik Park and Minsu Cho},\\nyear={2024},\\nurl={https://openreview.net/forum?id=zzv4Bf50RW}\\n}'},\n",
       " 'paperhash': {'value': 'park|learning_so3invariant_correspondence_via_pointwise_local_shape_transform'}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr_24[0]['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['arr_id', 'title', 'venue', 'url','focused_review']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': {'summary': {'value': 'This work proposes LSTNet, a self-supervised method to establish reliable 3D dense correspondences irrespective of the input point clouds’ rotational orientation.\\n\\nSpecifically, LSTNet learns to formulate SO(3)-invariant local shape transform for each point in a dynamic, input-dependent manner. Each point-wise local shape transform can map the SO(3)-equivariant global shape descriptor of the input shape to a local shape descriptor, which is passed to the decoder to reconstruct the shape and pose of the input point cloud. \\n\\nThe proposed self-supervised training pipeline encourages semantically corresponding points from different shape instances to be mapped to similar local shape descriptors, enabling LSTNet to establish dense point-wise correspondences via nearest point pairs between cross-reconstructed point clouds.'},\n",
       "   'soundness': {'value': '3 good'},\n",
       "   'presentation': {'value': '2 fair'},\n",
       "   'contribution': {'value': '3 good'},\n",
       "   'strengths': {'value': 'The self- and cross-reconstruction training strategy is simple yet effective. \\n\\nLSTNet demonstrates state-of-the-art performance on 3D semantic matching when evaluated on the KeypointNet dataset and part segmentation label transfer when evaluated on the ShapeNet dataset.'},\n",
       "   'weaknesses': {'value': 'The performance of aligned shape pairs under the setting of I/I shows that other methods, such as CPAE, are much better than LSTNet.'},\n",
       "   'questions': {'value': 'The reason why other methods are much better than LSTNet under the setting of I/I should be clarified.\\n\\nLack of limitations.'},\n",
       "   'flag_for_ethics_review': {'value': ['No ethics review needed.']},\n",
       "   'rating': {'value': '6: marginally above the acceptance threshold'},\n",
       "   'confidence': {'value': '2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.'},\n",
       "   'code_of_conduct': {'value': 'Yes'}},\n",
       "  'id': 'aU4SaZOeh4',\n",
       "  'forum': 'zzv4Bf50RW',\n",
       "  'replyto': 'zzv4Bf50RW',\n",
       "  'signatures': ['ICLR.cc/2024/Conference/Submission1647/Reviewer_eS3u'],\n",
       "  'nonreaders': [],\n",
       "  'readers': ['everyone'],\n",
       "  'writers': ['ICLR.cc/2024/Conference',\n",
       "   'ICLR.cc/2024/Conference/Submission1647/Reviewer_eS3u'],\n",
       "  'number': 1,\n",
       "  'invitations': ['ICLR.cc/2024/Conference/Submission1647/-/Official_Review',\n",
       "   'ICLR.cc/2024/Conference/-/Edit'],\n",
       "  'domain': 'ICLR.cc/2024/Conference',\n",
       "  'tcdate': 1698243150596,\n",
       "  'cdate': 1698243150596,\n",
       "  'tmdate': 1699636093263,\n",
       "  'mdate': 1699636093263,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'version': 2},\n",
       " {'content': {'summary': {'value': '1) This paper proposes a self-supervised method to find semantically corresponding points for a point cloud pair;\\n\\n2）The main idea is to decouple a point cloud feature learning process into a SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant point-wise local shape transforms;\\n\\n3) Experiments on the KeypointNet dataset show the effectiveness of the proposed method.'},\n",
       "   'soundness': {'value': '3 good'},\n",
       "   'presentation': {'value': '3 good'},\n",
       "   'contribution': {'value': '2 fair'},\n",
       "   'strengths': {'value': '1) This paper is generally well-written;\\n\\n2) The idea of factorizing point cloud descriptors into SO(3)-equivariant global shape descriptor and dynamic SO(3)-invariant\\npoint-wise local shape transforms seems to be novel;\\n\\n3) Experimental results are good.'},\n",
       "   'weaknesses': {'value': \"1) The main weakness of this paper could be all experiments are performed on synthetic datasets, with simple point cloud. It's good for authors' to show some examples/experiments on real-world datasets. For example, the 3Dmatch dataset. \\n\\n2) Since the proposed method can estimate dense correspondences, I wonder whether the proposed method can be used to estimate the relative rotation/translation for a point cloud pair. For example, the estimated dense correspondences can be fed to an ICP method to estimate the relative rotation/translation. \\n\\n3) The running time and GPU memory cost is blurry for me;\\n\\n4) Please compare the proposed method with more recent papers, e.g., [SC3K: Self-supervised and Coherent 3D Keypoints Estimation\\nfrom Rotated, Noisy, and Decimated Point Cloud Data].\"},\n",
       "   'questions': {'value': 'Please refer to the weaknesses.'},\n",
       "   'flag_for_ethics_review': {'value': ['No ethics review needed.']},\n",
       "   'rating': {'value': '5: marginally below the acceptance threshold'},\n",
       "   'confidence': {'value': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'},\n",
       "   'code_of_conduct': {'value': 'Yes'}},\n",
       "  'id': 'wKVoAVtxek',\n",
       "  'forum': 'zzv4Bf50RW',\n",
       "  'replyto': 'zzv4Bf50RW',\n",
       "  'signatures': ['ICLR.cc/2024/Conference/Submission1647/Reviewer_jP4i'],\n",
       "  'nonreaders': [],\n",
       "  'readers': ['everyone'],\n",
       "  'writers': ['ICLR.cc/2024/Conference',\n",
       "   'ICLR.cc/2024/Conference/Submission1647/Reviewer_jP4i'],\n",
       "  'number': 2,\n",
       "  'invitations': ['ICLR.cc/2024/Conference/Submission1647/-/Official_Review',\n",
       "   'ICLR.cc/2024/Conference/-/Edit'],\n",
       "  'domain': 'ICLR.cc/2024/Conference',\n",
       "  'tcdate': 1698652503617,\n",
       "  'cdate': 1698652503617,\n",
       "  'tmdate': 1699636093190,\n",
       "  'mdate': 1699636093190,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'version': 2},\n",
       " {'content': {'summary': {'value': 'This paper introduces LSTNet, which leverages an SO(3)-equivariant encoder-decoder architecture(Vector Neuron Networks, VNNs) and proposes a novel function called local shape transform to further transform the learned features. The proposed method is validated on both the 3D keypoint transfer and part segmentation label transformer tasks.'},\n",
       "   'soundness': {'value': '2 fair'},\n",
       "   'presentation': {'value': '2 fair'},\n",
       "   'contribution': {'value': '2 fair'},\n",
       "   'strengths': {'value': '1. The idea of cross-reconstruction for generating inter-object correspondences in a self-supervised way is interesting;\\n\\n2. The overall writing is good and the methodology part is well-organized and easy to follow.'},\n",
       "   'weaknesses': {'value': '1. The novelty of this work seems insufficient for ICLR. The whole pipeline heavily relies on VNNs and the main contribution I personally consider is the local shape transform and the self-supervised mechanism for correspondences.\\n\\n2. Regarding the local shape transform:\\n   2.1. From 3.1.1, the SO(3)-invariant output is $\\\\mathbf{V}\\\\mathbf{U}^T \\\\in \\\\mathbb{R}^{C \\\\times C}$, while in 3.1.2, the obtained SO(3)-invariant features $\\\\mathbf{V} \\\\in \\\\mathbb{R}^{C^\\\\prime \\\\times 3 \\\\times N}$ have a different shape;\\n\\n   2.2 The authors claimed that the local shape transform transforms the global features to local ones. Regarding this, I have two questions. \\n\\n      2.2.1 First, why are the features obtained by the Encoder global? They are generated by a DGCNN-based VNN, but DGCNN is not guaranteed to capture the global context, as it is graph-based and really depends on the number of layers together with the number of rings of each layer. \\n\\n      2.2.2 Second, the so-called local shape transform is predicted by a multi-layer perception from some SO(3)-invariant features that obtained from the input. Why after transforming the \"global\" features by such a mechanism, the features turn to \"local\"? I cannot see any specific design that enables it. It should be further explained. (I personally do not think so)\\n\\n3. Regarding the experiments:\\n    3.1 The experiments are only conducted on synthetic data, which cannot support the proposed method can work for real applications. I think it would be better to have additional real-data experiments;\\n\\n     3.2 As this paper also targets on correspondence estimation, whose typical downstream task is pose estimation. Therefore, I consider it worthwhile to also conduct experiments on tasks of 6D pose estimation or point cloud registration (there you always use real data), to further validate the estimated correspondences.\\n\\n    3.3 In Tab.1, only CPAE proposed in 2021 is used as the baseline. Some recent methods, e.g., [1], should also be included. Otherwise the results are not convincing at all (only compared to a single baseline which was proposed years ago). And it seems CPAE is the only baseline method for all the experiments. More baselines are required on both tasks.\\n\\n   3.4 The method is claimed to generate SO(3)-invariant correspondences. However, in Tab. 1, even on the synthetic data, the I/SO(3) and SO(3)/SO(3) experiments perform unsimilarly (I would expect to have similar results per category, as it is on synthetic and clean data). Could this be explained?\\n\\n4. For the SO(3)-equivariant and -invariant methods, some works for point cloud registration [2, 3, 4, 5] should also be discussed.\\n---------------------------------------------\\n[1]. Zohaib et al. SC3K: Self-supervised and Coherent 3D Keypoints Estimation from Rotated, Noisy, and Decimated Point Cloud Data, ICCV 2023;\\n\\n[2]. Dent et al. PPF-FoldNet: Unsupervised Learning of Rotation Invariant 3D Local Descriptors, ECCV 2018\\n\\n[3]. Ao et al. SpinNet: Learning a General Surface Descriptor for 3D Point Cloud Registration, CVPR 2021\\n\\n[4]. Wang et al. You Only Hypothesize Once: Point Cloud Registration with Rotation-equivariant Descriptors, ACM MM 2022\\n\\n[5]. Yu et al. Rotation-Invariant Transformer for Point Cloud Matching, CVPR 2023'},\n",
       "   'questions': {'value': 'See weaknesses.'},\n",
       "   'flag_for_ethics_review': {'value': ['No ethics review needed.']},\n",
       "   'rating': {'value': '3: reject, not good enough'},\n",
       "   'confidence': {'value': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'},\n",
       "   'code_of_conduct': {'value': 'Yes'}},\n",
       "  'id': 'NLfuL11uwO',\n",
       "  'forum': 'zzv4Bf50RW',\n",
       "  'replyto': 'zzv4Bf50RW',\n",
       "  'signatures': ['ICLR.cc/2024/Conference/Submission1647/Reviewer_wiS9'],\n",
       "  'nonreaders': [],\n",
       "  'readers': ['everyone'],\n",
       "  'writers': ['ICLR.cc/2024/Conference',\n",
       "   'ICLR.cc/2024/Conference/Submission1647/Reviewer_wiS9'],\n",
       "  'number': 3,\n",
       "  'invitations': ['ICLR.cc/2024/Conference/Submission1647/-/Official_Review',\n",
       "   'ICLR.cc/2024/Conference/-/Edit'],\n",
       "  'domain': 'ICLR.cc/2024/Conference',\n",
       "  'tcdate': 1698706547448,\n",
       "  'cdate': 1698706547448,\n",
       "  'tmdate': 1699636093122,\n",
       "  'mdate': 1699636093122,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'version': 2},\n",
       " {'content': {'summary': {'value': 'This paper attempts to register point cloud properties to their templates without precise correspondences and exact shape matching. To achieve this, the authors trained a local shape transform (LST) network that produces SO(3) invariant correspondences. The training is self-supervised. The experimental results on ShapeNet look nice.'},\n",
       "   'soundness': {'value': '3 good'},\n",
       "   'presentation': {'value': '3 good'},\n",
       "   'contribution': {'value': '3 good'},\n",
       "   'strengths': {'value': '- Valid motivation. Unlike the abused topic, vanilla point cloud registration, the motivation stands and could potentially benefit practical usages.\\n- The SO(3)-invariant network design intrinsically ensures robustness against rotations.\\n- The joint usage of a global descriptor and a local descriptor makes sense and may help with classification and recognition directly.\\n- The self-supervision scheme looks plausible by self and cross-reconstruction.'},\n",
       "   'weaknesses': {'value': 'My major concern is with the experimental setup. While the experiments on ShapeNet is common in the community and shows good result, I am in general doubtful whether such an approach could be really applied to the real world. \\nIn motivation, the authors talk about usage in vision, graphics, and robotics. In vision and robotics, we are interested in fitting real-world scans to templates (e.g. [Scan2CAD, CVPR 2019]), where in most cases, only noisy, partial, and sparse point clouds are provided. The authors do not have experiments or discussions in such cases. \\n\\nThe authors also take groundtruth keypoints and semantic segmentations from datasets for the experiments. In the real-world, however, obtaining such accurate high-level semantic information already requires a deep understanding of the point cloud, and its segmentation backbone may already be SO(3) invariant. This impairs the strength that the authors proposed.'},\n",
       "   'questions': {'value': 'Following my points in the \"weaknesses\" section, I am curious about several relevant problems in the practical setup (i.e., scan to model). \\n1. Would SO(3) invariance be sufficient? Do we need SE(3) or even Sim(3) invariance, if we cannot easily normalize the input due to the noise and sparsity? \\n2. Will the network still be functional if the density distributions are different across input and output? \\n3. Will it work out of the 16-category domain? Do we need more training data, or would it work out-of-box?\\n4. Would non-gt and/or biased key points and semantic parts be transferred properly?\\n\\nIt would be nice if the authors could conduct a minimal set of experiments in the real-world setup (e.g., extract a reconstruction from a ScanNet model and attempt to apply keypoint/semantic part transfer). Otherwise, it would be good to see a justification that this paper itself is an inevitable intermediate step toward real-world usage, and what can be done to further extend it.'},\n",
       "   'flag_for_ethics_review': {'value': ['No ethics review needed.']},\n",
       "   'rating': {'value': '5: marginally below the acceptance threshold'},\n",
       "   'confidence': {'value': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'},\n",
       "   'code_of_conduct': {'value': 'Yes'}},\n",
       "  'id': 'PUauntqQsB',\n",
       "  'forum': 'zzv4Bf50RW',\n",
       "  'replyto': 'zzv4Bf50RW',\n",
       "  'signatures': ['ICLR.cc/2024/Conference/Submission1647/Reviewer_a6Ps'],\n",
       "  'nonreaders': [],\n",
       "  'readers': ['everyone'],\n",
       "  'writers': ['ICLR.cc/2024/Conference',\n",
       "   'ICLR.cc/2024/Conference/Submission1647/Reviewer_a6Ps'],\n",
       "  'number': 4,\n",
       "  'invitations': ['ICLR.cc/2024/Conference/Submission1647/-/Official_Review',\n",
       "   'ICLR.cc/2024/Conference/-/Edit'],\n",
       "  'domain': 'ICLR.cc/2024/Conference',\n",
       "  'tcdate': 1698768293694,\n",
       "  'cdate': 1698768293694,\n",
       "  'tmdate': 1699636092942,\n",
       "  'mdate': 1699636092942,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'version': 2},\n",
       " {'content': {'summary': {'value': 'This paper presents a method of learning dense 3D correspondence between shapes in a self-supervised manner. Specifically, it is built on an existing SO(3)-equivariant representation. The input point clouds are independently encoded to SO(3)-equivariant global shape descriptor Z and dynamic SO(3)-invariant point-wise local shape transforms. Then the network is trained via penalizing errors in self- and cross- reconstructions via the decoder. The experiment validates the effectiveness of the proposed method.'},\n",
       "   'soundness': {'value': '3 good'},\n",
       "   'presentation': {'value': '3 good'},\n",
       "   'contribution': {'value': '2 fair'},\n",
       "   'strengths': {'value': '1. The paper is in general well organized and easy to follow. \\n2. The proposed method is straightforward and shown to be effective on the test data.'},\n",
       "   'weaknesses': {'value': \"1. The main issue of the proposed method lies in the experimental evaluation. Only one learned-based method is adopted for comparison in the main paper on a rather simple dataset. More methods including some traditional methods should be also evaluated for better comparison. The experiment on the real dataset should be also provided to show the robustness of the proposed method. \\n2. From Fig. 6 in the supplementary, we can see that the performance of the proposed method on the I/I scenario is much worse than the SOTA method. More analysis of the drop of performance should be given. Moreover, the performance of different methods with different rotation angles should be provided for better comparison.\\n3. How about the performance of other methods with a rough alignment of the initial shape? If a rough alignment is enough for the existing methods, why should we learn SO(3)-invariant correspondence in an end-to-end manner?\\n4. The whole method is mainly built upon the existing SO(3)-equivariant representation. The main contribution lies in introducing this representation to the specific task. I didn't get too much novel insight in terms of network design.\"},\n",
       "   'questions': {'value': 'Please refer to the Weaknees part.'},\n",
       "   'flag_for_ethics_review': {'value': ['No ethics review needed.']},\n",
       "   'rating': {'value': '5: marginally below the acceptance threshold'},\n",
       "   'confidence': {'value': '4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.'},\n",
       "   'code_of_conduct': {'value': 'Yes'}},\n",
       "  'id': 'Upm4GL7iRr',\n",
       "  'forum': 'zzv4Bf50RW',\n",
       "  'replyto': 'zzv4Bf50RW',\n",
       "  'signatures': ['ICLR.cc/2024/Conference/Submission1647/Reviewer_Frem'],\n",
       "  'nonreaders': [],\n",
       "  'readers': ['everyone'],\n",
       "  'writers': ['ICLR.cc/2024/Conference',\n",
       "   'ICLR.cc/2024/Conference/Submission1647/Reviewer_Frem'],\n",
       "  'number': 5,\n",
       "  'invitations': ['ICLR.cc/2024/Conference/Submission1647/-/Official_Review',\n",
       "   'ICLR.cc/2024/Conference/-/Edit'],\n",
       "  'domain': 'ICLR.cc/2024/Conference',\n",
       "  'tcdate': 1699350072271,\n",
       "  'cdate': 1699350072271,\n",
       "  'tmdate': 1699636092872,\n",
       "  'mdate': 1699636092872,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'version': 2},\n",
       " {'content': {'title': {'value': 'Response to all reviewers'},\n",
       "   'comment': {'value': 'We appreciate all the constructive comments from the reviewers (eS3u, jP4i, wiS9, a6Ps, Frem). We will do our best to reflect each of them, and improve the organization and the presentation of our paper. We will revise our paper by reflecting on the comments and resubmit it to another venue.'}},\n",
       "  'id': 'ELKoXB9JIa',\n",
       "  'forum': 'zzv4Bf50RW',\n",
       "  'replyto': 'zzv4Bf50RW',\n",
       "  'signatures': ['ICLR.cc/2024/Conference/Submission1647/Authors'],\n",
       "  'readers': ['everyone'],\n",
       "  'writers': ['ICLR.cc/2024/Conference',\n",
       "   'ICLR.cc/2024/Conference/Submission1647/Authors'],\n",
       "  'number': 3,\n",
       "  'invitations': ['ICLR.cc/2024/Conference/Submission1647/-/Official_Comment',\n",
       "   'ICLR.cc/2024/Conference/-/Edit'],\n",
       "  'domain': 'ICLR.cc/2024/Conference',\n",
       "  'tcdate': 1699959316805,\n",
       "  'cdate': 1699959316805,\n",
       "  'tmdate': 1710476724836,\n",
       "  'mdate': 1710476724836,\n",
       "  'license': 'CC BY 4.0',\n",
       "  'version': 2}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iclr_24[0]['reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'value': 'EMNLP 2023 Main'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnlp_23[0]['content']['venue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_papers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "for v in [iclr_24, iclr_25]:\n",
    "    for paper in v:\n",
    "        ### create a paper object and fill it from the current paper\n",
    "        ### get the focused review\n",
    "        for review in paper['reviews']:\n",
    "            weakneses = review['content']['weaknesses']['value'] if 'weaknesses' in review['content'].keys() else ''\n",
    "            if weakneses and len(weakneses.split(' ')) > 5:\n",
    "                p = Paper()\n",
    "                p.arr_id = paper['id']\n",
    "                p.title = paper['content']['title']['value']\n",
    "                paper_venue = paper['content']['venue']['value']\n",
    "                if 'Submitted to' in paper_venue:\n",
    "                    paper_venue = paper_venue.replace('Submitted to', '').strip()\n",
    "                p.venue = ' '.join(paper_venue.split(' ')[:2])\n",
    "                if p.venue == 'Submitted to':\n",
    "                    print(paper['content']['venue']['value'])\n",
    "                    break\n",
    "                p.url = paper['content']['pdf']['value'] if 'pdf' in paper['content'].keys() else ''\n",
    "                p.focused_review = weakneses\n",
    "                all_papers.append(p)\n",
    "                \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'zwqDROxClj',\n",
       " 'forum': 'zwqDROxClj',\n",
       " 'content': {'title': {'value': 'IAG: Induction-Augmented Generation Framework for Answering Reasoning Questions'},\n",
       "  'authors': {'value': ['Zhebin Zhang',\n",
       "    'Xinyu Zhang',\n",
       "    'Yuanhang Ren',\n",
       "    'Saijiang Shi',\n",
       "    'Meng Han',\n",
       "    'Yongkang Wu',\n",
       "    'Ruofei Lai',\n",
       "    'Zhao Cao']},\n",
       "  'authorids': {'value': ['~Zhebin_Zhang3',\n",
       "    '~Xinyu_Zhang6',\n",
       "    '~Yuanhang_Ren1',\n",
       "    '~Saijiang_Shi1',\n",
       "    '~Meng_Han5',\n",
       "    '~Yongkang_Wu1',\n",
       "    '~Ruofei_Lai1',\n",
       "    '~Zhao_Cao1']},\n",
       "  'keywords': {'value': ['Open-domain question answering',\n",
       "    'Inductive reasoning',\n",
       "    'Prompting']},\n",
       "  'abstract': {'value': 'Retrieval-Augmented Generation (RAG), by incorporating external knowledge with parametric memory of language models, has become the state-of-the-art architecture for open-domain QA tasks. However, common knowledge bases are inherently constrained by limited coverage and noisy information, making retrieval-based approaches inadequate to answer implicit reasoning questions. In this paper, we propose an Induction-Augmented Generation (IAG) framework that utilizes inductive knowledge along with the retrieved documents for implicit reasoning. We leverage large language models (LLMs) for deriving such knowledge via a novel prompting method based on inductive reasoning patterns. On top of this, we implement two versions of IAG named IAG-GPT and IAG-Student, respectively. IAG-GPT directly utilizes the knowledge generated by GPT-3 for answer prediction, while IAG-Student gets rid of dependencies on GPT service at inference time by incorporating a student inductor model. The inductor is firstly trained via knowledge distillation and further optimized by back-propagating the generator feedback via differentiable beam scores. Experimental results show that IAG outperforms RAG baselines as well as ChatGPT on two Open-Domain QA tasks. Notably, our best models have won the first place in the official leaderboards of CSQA2.0 (since Nov 1, 2022) and StrategyQA (since Jan 8, 2023).'},\n",
       "  'Submission_Type': {'value': 'Regular Long Paper'},\n",
       "  'Submission_Track': {'value': 'Commonsense Reasoning'},\n",
       "  'Submission_Track_2': {'value': 'Question Answering'},\n",
       "  'venue': {'value': 'EMNLP 2023 Main'},\n",
       "  'venueid': {'value': 'EMNLP/2023/Conference'},\n",
       "  'pdf': {'value': '/attachment/f76c54710bd329de16049867e2f420442b0f2888.pdf'},\n",
       "  '_bibtex': {'value': '@inproceedings{\\nzhang2023iag,\\ntitle={{IAG}: Induction-Augmented Generation Framework for Answering Reasoning Questions},\\nauthor={Zhebin Zhang and Xinyu Zhang and Yuanhang Ren and Saijiang Shi and Meng Han and Yongkang Wu and Ruofei Lai and Zhao Cao},\\nbooktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},\\nyear={2023},\\nurl={https://openreview.net/forum?id=zwqDROxClj}\\n}'},\n",
       "  'paperhash': {'value': 'zhang|iag_inductionaugmented_generation_framework_for_answering_reasoning_questions'}},\n",
       " 'invitations': ['EMNLP/2023/Conference/-/Submission',\n",
       "  'EMNLP/2023/Conference/-/Post_Submission',\n",
       "  'EMNLP/2023/Conference/Submission1595/-/Revision',\n",
       "  'EMNLP/2023/Conference/-/Edit',\n",
       "  'EMNLP/2023/Conference/Submission1595/-/Camera_Ready_Revision'],\n",
       " 'cdate': 1686829048749,\n",
       " 'pdate': 1696709252048,\n",
       " 'odate': 1701459729362,\n",
       " 'mdate': 1701459729376,\n",
       " 'signatures': ['EMNLP/2023/Conference/Submission1595/Authors'],\n",
       " 'writers': ['EMNLP/2023/Conference',\n",
       "  'EMNLP/2023/Conference/Submission1595/Authors'],\n",
       " 'readers': ['everyone'],\n",
       " 'reviews': [{'content': {'Paper_Topic_and_Main_Contributions': {'value': 'The authors propose an induction-augmented framework that utilizes inductive knowledge derived from LLMs and the retrieved documents for better implicit reasoning. Specifically, they enhance the conventional RAG with an inductor that generates inductive knowledge.\\n\\nThe authors propose an IAG-GPT model which directly utilizes GPT-3 and IAG-Student which is first trained via knowledge distillation with GPT-3 pseudo labels, and then optimized through a differentiable beam search algorithm.\\n\\nThe experiments show that IAG-GPT has significant advantages over ChatGPT and performs extremely well on CSQA2.0 and StrategyQA. IAG-Student outperforms RAG baselines.\\n'},\n",
       "    'Reasons_to_accept': {'value': 'This paper addresses a non-trivial problem for implicit reasoning by introducing an inductor with the assistance of LLMs.  \\n\\nThe authors conduct extensive experiments and the experiment results are reasonable.\\n\\n'},\n",
       "    'Reasons_to_reject': {'value': '1. The generalization ability of the model is a major concern. \\n- The model assigns inductive information to every question, even when some questions do not require it. A potential solution could be implementing a question classifier to identify the type of question and determine whether inductive information is necessary for a particular query.\\n- The strict structure/formulation of the prompt, especially the Knowledge part, is another issue (lines 245-247). \\n\\n2. Another minor issue is that there is a huge gap between the performance of IAG-GPT and IAG-Student, which makes the distilled model and its corresponding algorithm less convincing. More experiments on larger models are expected. \\n\\n'},\n",
       "    'Questions_for_the_Authors': {'value': '1. In the IAG-student algorithm, the generator is first trained followed by the inductor, will finetuning the generator together with the inductor help?\\n\\n2. In line 228, How to calculate the mu and sigma for the distribution?\\n\\n'},\n",
       "    'Soundness': {'value': '4: Strong: This study provides sufficient support for all of its claims/arguments. '},\n",
       "    'Excitement': {'value': '4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.'},\n",
       "    'Reproducibility': {'value': '4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.'},\n",
       "    'Ethical_Concerns': {'value': 'No'},\n",
       "    'Reviewer_Confidence': {'value': \"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.\"}},\n",
       "   'id': 'LKMFvBsLIp',\n",
       "   'number': 1,\n",
       "   'cdate': 1691035545695,\n",
       "   'tcdate': 1691035545695,\n",
       "   'mdate': 1701461113393,\n",
       "   'tmdate': 1701461113393,\n",
       "   'signatures': ['EMNLP/2023/Conference/Submission1595/Reviewer_HrRJ'],\n",
       "   'nonreaders': [],\n",
       "   'readers': ['everyone',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_HrRJ'],\n",
       "   'writers': ['EMNLP/2023/Conference',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_HrRJ'],\n",
       "   'forum': 'zwqDROxClj',\n",
       "   'replyto': 'zwqDROxClj',\n",
       "   'invitations': ['EMNLP/2023/Conference/Submission1595/-/Official_Review',\n",
       "    'EMNLP/2023/Conference/-/Edit'],\n",
       "   'domain': 'EMNLP/2023/Conference',\n",
       "   'license': 'CC BY 4.0',\n",
       "   'version': 2},\n",
       "  {'content': {'Paper_Topic_and_Main_Contributions': {'value': 'This paper introduces the Induction-Augmented Generation framework, which integrates inductive knowledge statements for Open-Domain QA tasks to enhance implicit reasoning. The framework proposes two models: IAG-GPT and IAG-student. IAG-GPT generates inductive knowledge statements using GPT-3 and utilizes both generated knowledge prompts and retrieved documents for QA tasks. Since IAG-GPT is dependent on the GPT-3 API, IAG-student is trained to distill GPT-3.'},\n",
       "    'Reasons_to_accept': {'value': 'The proposed approach achieved state-of-the-art performance by integrating inductive knowledge into the prompt on the CSQA and Strategy QA datasets. The presentation of quality examples (such as Table 6 and 7) further supports the validity of the work.'},\n",
       "    'Reasons_to_reject': {'value': '* The consistency in reporting the results needs to be done. Figure 3-4 and Table 2-4 uses StrategyQA dev which makes it hard to compare with Table 1 StrategyQA test baselines.  \\n* Table 2 shows that knowledge statements generated with inductive prompting support QA performance. However, to fully verify the effectiveness of inductive knowledge statements, additional comparisons need to be made on CSQA dev with 15 retrieved documents versus CSQA dev with 10 retrieved documents and 5 inductive knowledge statements. On StrategyQA, a comparison between 10 retrieved documents and 5 retrieved documents with 5 inductive knowledge statements needs to be conducted.'},\n",
       "    'Questions_for_the_Authors': {'value': '* Is the concatenation style of Retrieval Only in IAG the same as FiD [1]? If it is FiD-style, then there will be N number of passages to encode with the query. If not, as shown in concept figure 2, there will be a single long input which is a concatenation of the query and all N passages for IAG-GPT and IAG-student. It needs more clarification because in the knowledge fusion experiment, it seems like all of the question, knowledge statements, and retrieved documents are concatenated into a single long input.\\n* On the CSQA dev dataset, it is stated that top-5 snippets are used (Line 423). However, in Line 465, on CSQA, it uses 10 retrieved documents. Is the top-5 snippets transformed into 10 retrieved documents? Or does it use top-10 snippets?\\n* Inductive prompting generates implicit knowledge through in-context learning of 5 demonstrations. How are trivial and CoT prompting done? The examples are shown in Table 7, but it is not clear how the prompts are formed.\\n* Are there cases where inductive knowledge statements are hallucinated?\\n\\n[1] Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering'},\n",
       "    'Typos_Grammar_Style_and_Presentation_Improvements': {'value': '* In Line 467, naming of the dataset should be consistent\\n\\t* sudden acronym for StrategyQA -> SQA\\n* Including the model size could strengthen your Table 1.'},\n",
       "    'Soundness': {'value': '3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.'},\n",
       "    'Excitement': {'value': '4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.'},\n",
       "    'Reproducibility': {'value': '3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.'},\n",
       "    'Ethical_Concerns': {'value': 'No'},\n",
       "    'Reviewer_Confidence': {'value': \"3: Pretty sure, but there's a chance I missed something. Although I have a good feel for this area in general, I did not carefully check the paper's details, e.g., the math, experimental design, or novelty.\"}},\n",
       "   'id': 'ttwwsOhAIT',\n",
       "   'number': 2,\n",
       "   'cdate': 1691458202749,\n",
       "   'tcdate': 1691458202749,\n",
       "   'mdate': 1701461113300,\n",
       "   'tmdate': 1701461113300,\n",
       "   'signatures': ['EMNLP/2023/Conference/Submission1595/Reviewer_i5uE'],\n",
       "   'nonreaders': [],\n",
       "   'readers': ['everyone',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_i5uE'],\n",
       "   'writers': ['EMNLP/2023/Conference',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_i5uE'],\n",
       "   'forum': 'zwqDROxClj',\n",
       "   'replyto': 'zwqDROxClj',\n",
       "   'invitations': ['EMNLP/2023/Conference/Submission1595/-/Official_Review',\n",
       "    'EMNLP/2023/Conference/-/Edit'],\n",
       "   'domain': 'EMNLP/2023/Conference',\n",
       "   'license': 'CC BY 4.0',\n",
       "   'version': 2},\n",
       "  {'content': {'Paper_Topic_and_Main_Contributions': {'value': \"The paper's key contributions are as follows:\\n\\n1. The paper proposes an inductive prompting method inspired by cognitive functions of inductive reasoning. This method guides language models (LLMs), specifically GPT-3, to generate knowledge statements that establish reasoning paths. \\n\\n2.  The paper introduces the Induction-Augmented Generation (IAG) framework, which enhances the traditional Retrieval-Augmented Generation (RAG) architecture. \\n\\n3. The paper presents two variants of the IAG framework. IAG-GPT leverages the inductive knowledge statements sampled from GPT-3 as evidence for the generator.\"},\n",
       "    'Reasons_to_accept': {'value': 'This paper offers several strengths and benefits:\\n\\n1.The paper introduces a novel approach, Induction-Augmented Generation (IAG), which effectively combines inductive reasoning with language generation for answering implicit reasoning questions. \\n\\n2.  Implicit reasoning questions pose a significant challenge for open-domain question answering systems. By focusing on this challenge, the paper contributes to solving an important problem in the field of NLP, advancing the state of the art in understanding and generating reasoned answers.\\n\\n3. The paper presents a well-defined framework (IAG) and a detailed methodology for integrating inductive knowledge into the answer generation process.'},\n",
       "    'Reasons_to_reject': {'value': 'there are also some potential weaknesses:\\n\\n1.  The proposed Induction-Augmented Generation (IAG) framework involves multiple components, including retrieval, induction, and generation, which might make it challenging for researchers to reproduce and implement the approach. \\n\\n2.  The paper heavily relies on external language models, such as GPT-3, for generating inductive knowledge and improving performance. This reliance raises concerns about the availability, cost, and access to these models, which could limit the adoption of the proposed approach by researchers with limited resources or access.\\n\\n\\n3.  While the paper highlights successful cases where inductive knowledge enhances answer prediction, it does not thoroughly analyze or discuss cases where the approach might fail or provide incorrect answers. Understanding the limitations and potential pitfalls of the IAG framework is crucial for its safe and reliable application.'},\n",
       "    'Soundness': {'value': '4: Strong: This study provides sufficient support for all of its claims/arguments. '},\n",
       "    'Excitement': {'value': '4: Strong: This paper deepens the understanding of some phenomenon or lowers the barriers to an existing research direction.'},\n",
       "    'Reproducibility': {'value': '3: Could reproduce the results with some difficulty. The settings of parameters are underspecified or subjectively determined; the training/evaluation data are not widely available.'},\n",
       "    'Ethical_Concerns': {'value': 'No'},\n",
       "    'Reviewer_Confidence': {'value': '5: Positive that my evaluation is correct. I read the paper very carefully and I am very familiar with related work.'}},\n",
       "   'id': 'LIuV4fZqAO',\n",
       "   'number': 3,\n",
       "   'cdate': 1691695718152,\n",
       "   'tcdate': 1691695718152,\n",
       "   'mdate': 1701461113219,\n",
       "   'tmdate': 1701461113219,\n",
       "   'signatures': ['EMNLP/2023/Conference/Submission1595/Reviewer_bY1q'],\n",
       "   'nonreaders': [],\n",
       "   'readers': ['everyone',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_bY1q'],\n",
       "   'writers': ['EMNLP/2023/Conference',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_bY1q'],\n",
       "   'forum': 'zwqDROxClj',\n",
       "   'replyto': 'zwqDROxClj',\n",
       "   'invitations': ['EMNLP/2023/Conference/Submission1595/-/Official_Review',\n",
       "    'EMNLP/2023/Conference/-/Edit'],\n",
       "   'domain': 'EMNLP/2023/Conference',\n",
       "   'license': 'CC BY 4.0',\n",
       "   'version': 2},\n",
       "  {'content': {'Paper_Topic_and_Main_Contributions': {'value': 'This paper is based on Induction Augmented Generation Framework that uses LLMs for the implicit reasoning approach. The framework outperforms baselines for Retrieval Augmented Generation and ChatGPT on two Open domain tasks.\\n\\nOverall Contributions : \\n1) Novel inductive prompting method which improves the factuality of knowledge elicited from LLMs.\\n2)  A GPT implementation of the framework that improves over strong baseline models and ChatGPT.\\n3) A TAILBACK optimization algorithm that trains the inductor which allows IAG-Student to outperform Retrieval Augmented Generation baselines.'},\n",
       "    'Reasons_to_accept': {'value': 'The description of the methodology and building up the framework was well explained.\\nEvaluation conducted on two large Open-domain QA benchmark datasets.\\n'},\n",
       "    'Reasons_to_reject': {'value': '1) Although the Student Inductor model is shown to surpass the benchmark, the explanation and the underlying working principle was a bit hard to follow.\\n2) For tailback differential beam searching is used, but it is hard to follow what are the steps pertaining to it.'},\n",
       "    'Questions_for_the_Authors': {'value': \"1) Can you explain the tailback workflow using a pseudo code with detailed steps? Currently, it's a bit difficult to follow.\\n2) Are there any plan to switch to later GPT versions from GPT-3? What would be the implications in terms of evaluation outcomes ?\"},\n",
       "    'Soundness': {'value': '3: Good: This study provides sufficient support for its major claims/arguments, some minor points may need extra support or details.'},\n",
       "    'Excitement': {'value': \"3: Ambivalent: It has merits (e.g., it reports state-of-the-art results, the idea is nice), but there are key weaknesses (e.g., it describes incremental work), and it can significantly benefit from another round of revision. However, I won't object to accepting it if my co-reviewers champion it.\"},\n",
       "    'Reproducibility': {'value': '4: Could mostly reproduce the results, but there may be some variation because of sample variance or minor variations in their interpretation of the protocol or method.'},\n",
       "    'Ethical_Concerns': {'value': 'No'},\n",
       "    'Reviewer_Confidence': {'value': \"4: Quite sure. I tried to check the important points carefully. It's unlikely, though conceivable, that I missed something that should affect my ratings.\"}},\n",
       "   'id': 'gJTMXmoGmD',\n",
       "   'number': 4,\n",
       "   'cdate': 1691732126505,\n",
       "   'tcdate': 1691732126505,\n",
       "   'mdate': 1701461113139,\n",
       "   'tmdate': 1701461113139,\n",
       "   'signatures': ['EMNLP/2023/Conference/Submission1595/Reviewer_1Ny9'],\n",
       "   'nonreaders': [],\n",
       "   'readers': ['everyone',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_1Ny9'],\n",
       "   'writers': ['EMNLP/2023/Conference',\n",
       "    'EMNLP/2023/Conference/Submission1595/Reviewer_1Ny9'],\n",
       "   'forum': 'zwqDROxClj',\n",
       "   'replyto': 'zwqDROxClj',\n",
       "   'invitations': ['EMNLP/2023/Conference/Submission1595/-/Official_Review',\n",
       "    'EMNLP/2023/Conference/-/Edit'],\n",
       "   'domain': 'EMNLP/2023/Conference',\n",
       "   'license': 'CC BY 4.0',\n",
       "   'version': 2},\n",
       "  {'content': {'metareview': {'value': 'This paper proposes the Induction-Augmented Generation (IAG) framework, designed to enhance implicit reasoning in Open-Domain Question-Answering (QA) tasks. The framework includes two models: IAG-GPT and IAG-Student. IAG-GPT generates inductive knowledge statements using GPT-3 and combines these statements with retrieved documents for QA tasks. Removing the dependency on GPT-3 at inference time, the IAG-Student model is trained through knowledge distillation with GPT-3 pseudo labels and optimized using a differentiable beam search algorithm.\\n\\nThis paper presents a well-explained methodology and framework for solving the challenging problem of implicit reasoning in open-domain question answering. This work proposes a novel optimization scheme to train the inductor model through distillation and back-propagation of the generator feedback via differentiable beam scores. The authors conduct a convincing evaluation on two open-domain QA benchmark datasets (CSQA2 and Strategy QA) and achieve state-of-the-art performance on both (at least as of January 2023 for Strategy QA).\\n\\nAs highlighted by the reviewers, the authors could further strengthen their contribution by conducting, for example, a more thorough analysis of the failure modes of the approach, of the generalization of the method, and of the effectiveness of the inductive knowledge statements.'},\n",
       "    'recommendation': {'value': '4: Sound and Moderately Exciting: Accept to Main Conference or Findings'}},\n",
       "   'id': 'U3MEiqgK7j',\n",
       "   'number': 1,\n",
       "   'cdate': 1695166230335,\n",
       "   'tcdate': 1695166230335,\n",
       "   'mdate': 1701462111766,\n",
       "   'tmdate': 1701462111766,\n",
       "   'signatures': ['EMNLP/2023/Conference/Submission1595/Area_Chair_nLPG'],\n",
       "   'nonreaders': [],\n",
       "   'readers': ['everyone'],\n",
       "   'writers': ['EMNLP/2023/Conference',\n",
       "    'EMNLP/2023/Conference/Submission1595/Senior_Area_Chairs',\n",
       "    'EMNLP/2023/Conference/Submission1595/Area_Chair_nLPG'],\n",
       "   'forum': 'zwqDROxClj',\n",
       "   'replyto': 'zwqDROxClj',\n",
       "   'invitations': ['EMNLP/2023/Conference/Submission1595/-/Meta_Review',\n",
       "    'EMNLP/2023/Conference/-/Edit'],\n",
       "   'domain': 'EMNLP/2023/Conference',\n",
       "   'license': 'CC BY 4.0',\n",
       "   'version': 2},\n",
       "  {'content': {'title': {'value': 'Paper Decision'},\n",
       "    'decision': {'value': 'Accept-Main'},\n",
       "    'comment': {'value': 'This paper proposes the Induction-Augmented Generation (IAG) framework, designed to enhance implicit reasoning in Open-Domain Question-Answering (QA) tasks. The framework includes two models: IAG-GPT and IAG-Student. IAG-GPT generates inductive knowledge statements using GPT-3 and combines these statements with retrieved documents for QA tasks. Removing the dependency on GPT-3 at inference time, the IAG-Student model is trained through knowledge distillation with GPT-3 pseudo labels and optimized using a differentiable beam search algorithm.\\n\\nThis paper presents a well-explained methodology and framework for solving the challenging problem of implicit reasoning in open-domain question answering. This work proposes a novel optimization scheme to train the inductor model through distillation and back-propagation of the generator feedback via differentiable beam scores. The authors conduct a convincing evaluation on two open-domain QA benchmark datasets (CSQA2 and Strategy QA) and achieve state-of-the-art performance on both (at least as of January 2023 for Strategy QA).\\n\\nAs highlighted by the reviewers, the authors could further strengthen their contribution by conducting, for example, a more thorough analysis of the failure modes of the approach, of the generalization of the method, and of the effectiveness of the inductive knowledge statements.'}},\n",
       "   'id': 'JEprHc2swO',\n",
       "   'signatures': ['EMNLP/2023/Conference/Program_Chairs'],\n",
       "   'nonreaders': [],\n",
       "   'readers': ['everyone'],\n",
       "   'writers': ['EMNLP/2023/Conference',\n",
       "    'EMNLP/2023/Conference/Program_Chairs'],\n",
       "   'forum': 'zwqDROxClj',\n",
       "   'replyto': 'zwqDROxClj',\n",
       "   'number': 1,\n",
       "   'invitations': ['EMNLP/2023/Conference/Submission1595/-/Decision',\n",
       "    'EMNLP/2023/Conference/-/Edit'],\n",
       "   'domain': 'EMNLP/2023/Conference',\n",
       "   'tcdate': 1696707518886,\n",
       "   'cdate': 1696707518886,\n",
       "   'tmdate': 1701465435630,\n",
       "   'mdate': 1701465435630,\n",
       "   'license': 'CC BY 4.0',\n",
       "   'version': 2}]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emnlp_23[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for paper in emnlp_23:\n",
    "    ### get the focused review\n",
    "    for review in paper['reviews']:\n",
    "        weakneses = review['content']['Reasons_to_reject']['value'] if 'Reasons_to_reject' in review['content'].keys() else ''\n",
    "        if weakneses and len(weakneses.split(' ')) > 5:\n",
    "            p = Paper()\n",
    "            p.arr_id = paper['id']\n",
    "            p.title = paper['content']['title']['value']\n",
    "            paper_venue = paper['content']['venue']['value']\n",
    "            if 'Submitted to' in paper_venue:\n",
    "                paper_venue = paper_venue.replace('Submitted to', '').strip()\n",
    "            p.venue = ' '.join(paper_venue.split(' ')[:2])\n",
    "            if p.venue == 'Submitted to':\n",
    "                print(paper['content']['venue']['value'])\n",
    "                break\n",
    "            p.url = paper['content']['pdf']['value'] if 'pdf' in paper['content'].keys() else ''\n",
    "            p.focused_review = weakneses\n",
    "            all_papers.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arr_id</th>\n",
       "      <th>title</th>\n",
       "      <th>venue</th>\n",
       "      <th>year</th>\n",
       "      <th>url</th>\n",
       "      <th>focused_review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>ICLR 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>/pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf</td>\n",
       "      <td>The performance of aligned shape pairs under t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>ICLR 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>/pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf</td>\n",
       "      <td>1) The main weakness of this paper could be al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>ICLR 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>/pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf</td>\n",
       "      <td>1. The novelty of this work seems insufficient...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>ICLR 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>/pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf</td>\n",
       "      <td>My major concern is with the experimental setu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zzv4Bf50RW</td>\n",
       "      <td>Learning SO(3)-Invariant Correspondence via Po...</td>\n",
       "      <td>ICLR 2024</td>\n",
       "      <td>None</td>\n",
       "      <td>/pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf</td>\n",
       "      <td>1. The main issue of the proposed method lies ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       arr_id                                              title      venue  \\\n",
       "0  zzv4Bf50RW  Learning SO(3)-Invariant Correspondence via Po...  ICLR 2024   \n",
       "1  zzv4Bf50RW  Learning SO(3)-Invariant Correspondence via Po...  ICLR 2024   \n",
       "2  zzv4Bf50RW  Learning SO(3)-Invariant Correspondence via Po...  ICLR 2024   \n",
       "3  zzv4Bf50RW  Learning SO(3)-Invariant Correspondence via Po...  ICLR 2024   \n",
       "4  zzv4Bf50RW  Learning SO(3)-Invariant Correspondence via Po...  ICLR 2024   \n",
       "\n",
       "   year                                                url  \\\n",
       "0  None  /pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf   \n",
       "1  None  /pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf   \n",
       "2  None  /pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf   \n",
       "3  None  /pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf   \n",
       "4  None  /pdf/a251fcea8ea8b2cae3c0c9a7c69287aa2619480c.pdf   \n",
       "\n",
       "                                      focused_review  \n",
       "0  The performance of aligned shape pairs under t...  \n",
       "1  1) The main weakness of this paper could be al...  \n",
       "2  1. The novelty of this work seems insufficient...  \n",
       "3  My major concern is with the experimental setu...  \n",
       "4  1. The main issue of the proposed method lies ...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the list of dataclasses into a dataframe\n",
    "df_papers = pd.DataFrame([paper.__dict__ for paper in all_papers])\n",
    "\n",
    "# Display the dataframe\n",
    "df_papers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "venue\n",
       "ICLR 2025     46289\n",
       "ICLR 2024     27753\n",
       "EMNLP 2023     6250\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_papers['venue'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_papers.to_csv('/home/abdelrahman.sadallah/mbzuai/review_rewrite/data/processed/iclr24,25_emnlp23.csv', index=False,escapechar='\\\\')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

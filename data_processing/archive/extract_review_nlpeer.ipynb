{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpeer import DATASETS, PAPERFORMATS, PaperReviewDataset\n",
    "\n",
    "dataset_type = DATASETS.ARR22\n",
    "paper_format = PAPERFORMATS.ITG\n",
    "version = 1\n",
    "\n",
    "# load data paperwise\n",
    "data = PaperReviewDataset(\"../data\", dataset_type, version, paper_format)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over papers with associated reviews\n",
    "paperwise = [(paper_id, meta, paper, reviews) for paper_id, meta, paper, reviews in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "476"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paperwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGNMN: Video-grounded Neural Module Networks for Video-Grounded Dialogue Systems\n",
      "Abstract\n",
      "Neural module networks (NMN) have achieved success in image-grounded tasks such as Visual Question Answering (VQA) on synthetic images. However, very limited work on NMN has been studied in the video-grounded dialogue tasks. These tasks extend the complexity of traditional visual tasks with the additional visual temporal variance and language cross-turn dependencies. Motivated by recent NMN approaches on image-grounded tasks, we introduce Videogrounded Neural Module Network (VGNMN) to model the information retrieval process in video-grounded language tasks as a pipeline of neural modules. VGNMN first decomposes all language components in dialogues to explicitly resolve any entity references and detect corresponding action-based inputs from the question. The detected entities and actions are used as parameters to instantiate neural module networks and extract visual cues from the video. Our experiments show that VGNMN can achieve promising performance on a challenging video-grounded dialogue benchmark as well as a video QA benchmark.\n",
      "Introduction\n",
      "Vision-language tasks have been studied to build intelligent systems that can perceive information from multiple modalities, such as images, videos, and text. Extended from image-grounded tasks, e.g. (Antol et al., 2015), recently Jang et al. (2017); Lei et al. (2018) propose to use video as the grounding features. This modification poses a significant challenge to previous image-based models with the additional temporal variance through video frames.\n",
      "Recently  further develop videogrounded language research into the dialogue domain. In the proposed task, video-grounded dialogues, the dialogue agent is required to answer questions about a video over multiple dialogue turns. Using Figure 1 as an example, to answer\n",
      "Figure 1: A sample video-grounded dialogue with a demonstration of a reasoning process questions correctly, a dialogue agent has to resolve references in dialogue context, e.g. \"he\" and \"it\", and identify the original entity, e.g. \"a boy\" and \"a backpack\". Besides, the agent also needs to identify the actions of these entities, e.g. \"carrying a backpack\" to retrieve information from the video.\n"
     ]
    }
   ],
   "source": [
    "paper = paperwise[0][2]\n",
    "\n",
    "for n in paper.unroll_graph()[:7]:\n",
    "    print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "reviews_pd = []\n",
    "for paper_id, meta, paper, reviews in paperwise:\n",
    "    for review in reviews:\n",
    "        review_entry  ={}\n",
    "        review_entry[\"paper_id\"] = paper_id\n",
    "        review_entry[\"title\"] = meta[\"title\"]\n",
    "        review_entry[\"review_id\"] = review[\"rid\"]\n",
    "        review_entry[\"review\"] = review[\"report\"]['paper_summary'] + '\\n' + review[\"report\"]['summary_of_weaknesses'] \n",
    "        review_entry[\"human_questions\"] = ''\n",
    "        review_entry[\"LLM_questions\"] = ''\n",
    "\n",
    "        reviews_pd.append(review_entry)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_pd = pd.DataFrame(reviews_pd)\n",
    "reviews_pd.head()\n",
    "reviews_pd.to_csv(\"ARR22_reviews.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
